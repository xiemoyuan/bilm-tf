nohup: ignoring input
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/Upgrade_ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-28 07:16:53.123214: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-28 07:16:53.325118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:07:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-10-28 07:16:53.325204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-28 07:16:53.768380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-28 07:16:53.768468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-28 07:16:53.768485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-28 07:16:53.768653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21568 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:07:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 51 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Found 51 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/add_39:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(1, 512) dtype=float32>)
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/add_39:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(1, 512) dtype=float32>)
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 384324440 batches
Batch 0, train_perplexity=148677.88, train_loss=11.909537

Batch 10, train_perplexity=23326.102, train_loss=10.057328

Batch 20, train_perplexity=180.38786, train_loss=5.1951094

Batch 30, train_perplexity=23.578135, train_loss=3.1603198

Batch 40, train_perplexity=24.052025, train_loss=3.1802192

Batch 50, train_perplexity=23.430904, train_loss=3.1540558

Batch 60, train_perplexity=22.390236, train_loss=3.108625

Batch 70, train_perplexity=21.76816, train_loss=3.0804484

Batch 80, train_perplexity=21.209883, train_loss=3.0544672

Batch 90, train_perplexity=20.771389, train_loss=3.0335765

Batch 100, train_perplexity=20.418705, train_loss=3.0164514

Batch 110, train_perplexity=20.147503, train_loss=3.0030804

Batch 120, train_perplexity=19.917032, train_loss=2.9915752

Batch 130, train_perplexity=19.721615, train_loss=2.9817152

Batch 140, train_perplexity=19.558191, train_loss=2.9733942

Batch 150, train_perplexity=19.42247, train_loss=2.9664307

Batch 160, train_perplexity=19.309082, train_loss=2.9605756

Batch 170, train_perplexity=19.213966, train_loss=2.9556375

Batch 180, train_perplexity=19.133894, train_loss=2.9514613

Batch 190, train_perplexity=19.066256, train_loss=2.94792

Batch 200, train_perplexity=19.008825, train_loss=2.9449034

Batch 210, train_perplexity=18.959965, train_loss=2.9423296

Batch 220, train_perplexity=18.918562, train_loss=2.9401436

Batch 230, train_perplexity=18.882332, train_loss=2.9382267

Batch 240, train_perplexity=18.852549, train_loss=2.9366481

Batch 250, train_perplexity=18.82817, train_loss=2.9353542

Batch 260, train_perplexity=18.806305, train_loss=2.9341922

Batch 270, train_perplexity=18.787985, train_loss=2.9332175

Batch 280, train_perplexity=18.772501, train_loss=2.932393

Batch 290, train_perplexity=18.759373, train_loss=2.9316936

Batch 300, train_perplexity=18.748285, train_loss=2.9311023

Batch 310, train_perplexity=18.739025, train_loss=2.9306083

Batch 320, train_perplexity=18.731585, train_loss=2.930211

Batch 330, train_perplexity=18.726204, train_loss=2.9299238

Batch 340, train_perplexity=18.723671, train_loss=2.9297886

Batch 350, train_perplexity=18.725376, train_loss=2.9298797

Batch 360, train_perplexity=18.732637, train_loss=2.9302673

Batch 370, train_perplexity=18.744387, train_loss=2.9308944

Batch 380, train_perplexity=18.756516, train_loss=2.9315412

Batch 390, train_perplexity=18.764961, train_loss=2.9319913

Batch 400, train_perplexity=18.767963, train_loss=2.9321513

Batch 410, train_perplexity=18.767069, train_loss=2.9321036

Batch 420, train_perplexity=18.766039, train_loss=2.9320488

Batch 430, train_perplexity=18.768044, train_loss=2.9321556

Batch 440, train_perplexity=18.770952, train_loss=2.9323106

Batch 450, train_perplexity=18.769163, train_loss=2.9322152

Batch 460, train_perplexity=18.762419, train_loss=2.931856

Batch 470, train_perplexity=18.753588, train_loss=2.931385

Batch 480, train_perplexity=18.744486, train_loss=2.9308996

Batch 490, train_perplexity=18.735899, train_loss=2.9304414

Batch 500, train_perplexity=18.728079, train_loss=2.930024

Batch 510, train_perplexity=18.721083, train_loss=2.9296503

Batch 520, train_perplexity=18.714884, train_loss=2.9293191

Batch 530, train_perplexity=18.709427, train_loss=2.9290276

Batch 540, train_perplexity=18.704634, train_loss=2.9287713

Batch 550, train_perplexity=18.700455, train_loss=2.9285479

Batch 560, train_perplexity=18.69679, train_loss=2.9283519

Batch 570, train_perplexity=18.6936, train_loss=2.9281812

Batch 580, train_perplexity=18.690813, train_loss=2.9280322

Batch 590, train_perplexity=18.688381, train_loss=2.927902

Batch 600, train_perplexity=18.68626, train_loss=2.9277885

Batch 610, train_perplexity=18.68442, train_loss=2.92769

Batch 620, train_perplexity=18.682798, train_loss=2.9276032

Batch 630, train_perplexity=18.6814, train_loss=2.9275284

Batch 640, train_perplexity=18.680162, train_loss=2.927462

Batch 650, train_perplexity=18.679089, train_loss=2.9274046

Batch 660, train_perplexity=18.678148, train_loss=2.9273543

Batch 670, train_perplexity=18.67733, train_loss=2.9273105

Batch 680, train_perplexity=18.676617, train_loss=2.9272723

Batch 690, train_perplexity=18.675993, train_loss=2.927239

Batch 700, train_perplexity=18.675459, train_loss=2.9272103

Batch 710, train_perplexity=18.674988, train_loss=2.927185

Batch 720, train_perplexity=18.6746, train_loss=2.9271643

Batch 730, train_perplexity=18.674267, train_loss=2.9271464

Batch 740, train_perplexity=18.674, train_loss=2.9271321

Batch 750, train_perplexity=18.673786, train_loss=2.9271207

Batch 760, train_perplexity=18.673626, train_loss=2.927112

Batch 770, train_perplexity=18.673527, train_loss=2.9271069

Batch 780, train_perplexity=18.67347, train_loss=2.9271038

Batch 790, train_perplexity=18.67345, train_loss=2.9271028

Batch 800, train_perplexity=18.673464, train_loss=2.9271035

Batch 810, train_perplexity=18.673527, train_loss=2.9271069

Batch 820, train_perplexity=18.6736, train_loss=2.9271107

Batch 830, train_perplexity=18.673687, train_loss=2.9271154

Batch 840, train_perplexity=18.673777, train_loss=2.9271202

Batch 850, train_perplexity=18.67387, train_loss=2.9271252

Batch 860, train_perplexity=18.673954, train_loss=2.9271297
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 870, train_perplexity=18.674017, train_loss=2.927133

Batch 880, train_perplexity=18.67406, train_loss=2.9271355

Batch 890, train_perplexity=18.674074, train_loss=2.9271362

Batch 900, train_perplexity=18.67406, train_loss=2.9271355

Batch 910, train_perplexity=18.674013, train_loss=2.9271328

Batch 920, train_perplexity=18.673933, train_loss=2.9271286

Batch 930, train_perplexity=18.67382, train_loss=2.9271226

Batch 940, train_perplexity=18.673687, train_loss=2.9271154

Batch 950, train_perplexity=18.673527, train_loss=2.9271069

Batch 960, train_perplexity=18.67334, train_loss=2.9270968

Batch 970, train_perplexity=18.673143, train_loss=2.9270864

Batch 980, train_perplexity=18.67294, train_loss=2.9270754

Batch 990, train_perplexity=18.67273, train_loss=2.9270642

Batch 1000, train_perplexity=18.672512, train_loss=2.9270525

Batch 1010, train_perplexity=18.672289, train_loss=2.9270406

Batch 1020, train_perplexity=18.672075, train_loss=2.9270291

Batch 1030, train_perplexity=18.671862, train_loss=2.9270177

Batch 1040, train_perplexity=18.671658, train_loss=2.9270067

Batch 1050, train_perplexity=18.671461, train_loss=2.9269962

Batch 1060, train_perplexity=18.671274, train_loss=2.9269862

Batch 1070, train_perplexity=18.671097, train_loss=2.9269767

Batch 1080, train_perplexity=18.670927, train_loss=2.9269676

Batch 1090, train_perplexity=18.670767, train_loss=2.926959

Batch 1100, train_perplexity=18.670616, train_loss=2.926951

Batch 1110, train_perplexity=18.670477, train_loss=2.9269435

Batch 1120, train_perplexity=18.67035, train_loss=2.9269366

Batch 1130, train_perplexity=18.670223, train_loss=2.92693

Batch 1140, train_perplexity=18.670109, train_loss=2.9269238

Batch 1150, train_perplexity=18.669992, train_loss=2.9269176

Batch 1160, train_perplexity=18.669895, train_loss=2.9269123

Batch 1170, train_perplexity=18.669796, train_loss=2.926907

Batch 1180, train_perplexity=18.669703, train_loss=2.926902

Batch 1190, train_perplexity=18.66961, train_loss=2.926897

Batch 1200, train_perplexity=18.669529, train_loss=2.9268928

Batch 1210, train_perplexity=18.669445, train_loss=2.9268882

Batch 1220, train_perplexity=18.669378, train_loss=2.9268847

Batch 1230, train_perplexity=18.669308, train_loss=2.9268808

Batch 1240, train_perplexity=18.669231, train_loss=2.9268768

Batch 1250, train_perplexity=18.669155, train_loss=2.9268727

Batch 1260, train_perplexity=18.669088, train_loss=2.9268692

Batch 1270, train_perplexity=18.669022, train_loss=2.9268656

Batch 1280, train_perplexity=18.66896, train_loss=2.9268622

Batch 1290, train_perplexity=18.668888, train_loss=2.9268584

Batch 1300, train_perplexity=18.668827, train_loss=2.926855

Batch 1310, train_perplexity=18.668764, train_loss=2.9268517

Batch 1320, train_perplexity=18.668701, train_loss=2.9268484

Batch 1330, train_perplexity=18.66864, train_loss=2.926845

Batch 1340, train_perplexity=18.668568, train_loss=2.9268413

Batch 1350, train_perplexity=18.66851, train_loss=2.9268382

Batch 1360, train_perplexity=18.668444, train_loss=2.9268346

Batch 1370, train_perplexity=18.66838, train_loss=2.9268312

Batch 1380, train_perplexity=18.66832, train_loss=2.926828

Batch 1390, train_perplexity=18.668253, train_loss=2.9268243

Batch 1400, train_perplexity=18.66818, train_loss=2.9268205

Batch 1410, train_perplexity=18.668123, train_loss=2.9268174

Batch 1420, train_perplexity=18.66806, train_loss=2.926814

Batch 1430, train_perplexity=18.667994, train_loss=2.9268105

Batch 1440, train_perplexity=18.667923, train_loss=2.9268067

Batch 1450, train_perplexity=18.66786, train_loss=2.9268034

Batch 1460, train_perplexity=18.667799, train_loss=2.9268

Batch 1470, train_perplexity=18.667723, train_loss=2.926796

Batch 1480, train_perplexity=18.66767, train_loss=2.926793

Batch 1490, train_perplexity=18.667599, train_loss=2.9267893

Batch 1500, train_perplexity=18.667536, train_loss=2.926786

Batch 1510, train_perplexity=18.667473, train_loss=2.9267826

Batch 1520, train_perplexity=18.667412, train_loss=2.9267793

Batch 1530, train_perplexity=18.667353, train_loss=2.9267762

Batch 1540, train_perplexity=18.667286, train_loss=2.9267726

Batch 1550, train_perplexity=18.667229, train_loss=2.9267695

Batch 1560, train_perplexity=18.667162, train_loss=2.926766

Batch 1570, train_perplexity=18.667109, train_loss=2.926763

Batch 1580, train_perplexity=18.667046, train_loss=2.9267597

Batch 1590, train_perplexity=18.666992, train_loss=2.9267569

Batch 1600, train_perplexity=18.666922, train_loss=2.926753

Batch 1610, train_perplexity=18.666878, train_loss=2.9267507

Batch 1620, train_perplexity=18.666824, train_loss=2.9267478

Batch 1630, train_perplexity=18.666761, train_loss=2.9267445

Batch 1640, train_perplexity=18.666708, train_loss=2.9267416

Batch 1650, train_perplexity=18.666655, train_loss=2.9267387

Batch 1660, train_perplexity=18.666605, train_loss=2.926736

Batch 1670, train_perplexity=18.666552, train_loss=2.9267333

Batch 1680, train_perplexity=18.666512, train_loss=2.926731

Batch 1690, train_perplexity=18.666458, train_loss=2.9267282

Batch 1700, train_perplexity=18.66641, train_loss=2.9267256

Batch 1710, train_perplexity=18.66636, train_loss=2.926723

Batch 1720, train_perplexity=18.666317, train_loss=2.9267206

Batch 1730, train_perplexity=18.666271, train_loss=2.9267182

Batch 1740, train_perplexity=18.666227, train_loss=2.9267159

Batch 1750, train_perplexity=18.666183, train_loss=2.9267135

Batch 1760, train_perplexity=18.666134, train_loss=2.9267108

Batch 1770, train_perplexity=18.666103, train_loss=2.9267092

Batch 1780, train_perplexity=18.666058, train_loss=2.9267068

Batch 1790, train_perplexity=18.666014, train_loss=2.9267044

Batch 1800, train_perplexity=18.665977, train_loss=2.9267025

Batch 1810, train_perplexity=18.665934, train_loss=2.9267

Batch 1820, train_perplexity=18.685747, train_loss=2.927761

Batch 1830, train_perplexity=18.677044, train_loss=2.9272952

Batch 1840, train_perplexity=18.675245, train_loss=2.927199

Batch 1850, train_perplexity=18.674742, train_loss=2.927172

Batch 1860, train_perplexity=18.674667, train_loss=2.927168

Batch 1870, train_perplexity=18.674711, train_loss=2.9271703

Batch 1880, train_perplexity=18.674778, train_loss=2.9271739

Batch 1890, train_perplexity=18.674845, train_loss=2.9271774

Batch 1900, train_perplexity=18.674915, train_loss=2.9271812

Batch 1910, train_perplexity=18.675028, train_loss=2.9271872

Batch 1920, train_perplexity=18.675201, train_loss=2.9271965

Batch 1930, train_perplexity=18.675495, train_loss=2.9272122

Batch 1940, train_perplexity=18.675959, train_loss=2.927237

Batch 1950, train_perplexity=18.6766, train_loss=2.9272714

Batch 1960, train_perplexity=18.677387, train_loss=2.9273136

Batch 1970, train_perplexity=18.678225, train_loss=2.9273584

Batch 1980, train_perplexity=18.678959, train_loss=2.9273977

Batch 1990, train_perplexity=18.679432, train_loss=2.927423

Batch 2000, train_perplexity=18.679565, train_loss=2.9274302

Batch 2010, train_perplexity=18.679325, train_loss=2.9274173

Batch 2020, train_perplexity=18.682388, train_loss=2.9275813

Batch 2030, train_perplexity=18.682425, train_loss=2.9275832

Batch 2040, train_perplexity=18.682482, train_loss=2.9275863

Batch 2050, train_perplexity=18.682425, train_loss=2.9275832

Batch 2060, train_perplexity=18.682255, train_loss=2.9275742

Batch 2070, train_perplexity=18.681952, train_loss=2.927558

Batch 2080, train_perplexity=18.681524, train_loss=2.927535

Batch 2090, train_perplexity=18.681007, train_loss=2.9275074

Batch 2100, train_perplexity=18.68041, train_loss=2.9274755

Batch 2110, train_perplexity=18.679762, train_loss=2.9274406

Batch 2120, train_perplexity=18.679058, train_loss=2.927403

Batch 2130, train_perplexity=18.678328, train_loss=2.9273639

Batch 2140, train_perplexity=18.67757, train_loss=2.9273233

Batch 2150, train_perplexity=18.676813, train_loss=2.9272828

Batch 2160, train_perplexity=18.67606, train_loss=2.9272425

Batch 2170, train_perplexity=18.675318, train_loss=2.9272027

Batch 2180, train_perplexity=18.674591, train_loss=2.9271638

Batch 2190, train_perplexity=18.673883, train_loss=2.927126

Batch 2200, train_perplexity=18.673216, train_loss=2.9270902
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 2210, train_perplexity=18.672575, train_loss=2.9270558

Batch 2220, train_perplexity=18.671951, train_loss=2.9270225

Batch 2230, train_perplexity=18.671364, train_loss=2.926991

Batch 2240, train_perplexity=18.67082, train_loss=2.926962

Batch 2250, train_perplexity=18.670303, train_loss=2.9269342

Batch 2260, train_perplexity=18.669819, train_loss=2.9269083

Batch 2270, train_perplexity=18.669369, train_loss=2.9268842

Batch 2280, train_perplexity=18.668951, train_loss=2.9268618

Batch 2290, train_perplexity=18.668568, train_loss=2.9268413

Batch 2300, train_perplexity=18.668207, train_loss=2.926822

Batch 2310, train_perplexity=18.66788, train_loss=2.9268043

Batch 2320, train_perplexity=18.66758, train_loss=2.9267883

Batch 2330, train_perplexity=18.667295, train_loss=2.926773

Batch 2340, train_perplexity=18.667051, train_loss=2.92676

Batch 2350, train_perplexity=18.666805, train_loss=2.9267468

Batch 2360, train_perplexity=18.666592, train_loss=2.9267354

Batch 2370, train_perplexity=18.666397, train_loss=2.926725

Batch 2380, train_perplexity=18.666218, train_loss=2.9267154

Batch 2390, train_perplexity=18.66605, train_loss=2.9267063

Batch 2400, train_perplexity=18.665903, train_loss=2.9266984

Batch 2410, train_perplexity=18.665764, train_loss=2.926691

Batch 2420, train_perplexity=18.66564, train_loss=2.9266844

Batch 2430, train_perplexity=18.665524, train_loss=2.9266782

Batch 2440, train_perplexity=18.665417, train_loss=2.9266725

Batch 2450, train_perplexity=18.66532, train_loss=2.9266672

Batch 2460, train_perplexity=18.66523, train_loss=2.9266624

Batch 2470, train_perplexity=18.66544, train_loss=2.9266737

Batch 2480, train_perplexity=18.66523, train_loss=2.9266624

Batch 2490, train_perplexity=18.665062, train_loss=2.9266534

Batch 2500, train_perplexity=18.664923, train_loss=2.926646

Batch 2510, train_perplexity=18.664812, train_loss=2.92664

Batch 2520, train_perplexity=18.664715, train_loss=2.9266348

Batch 2530, train_perplexity=18.664625, train_loss=2.92663

Batch 2540, train_perplexity=18.664555, train_loss=2.9266262

Batch 2550, train_perplexity=18.664501, train_loss=2.9266233

Batch 2560, train_perplexity=18.664438, train_loss=2.92662

Batch 2570, train_perplexity=18.664389, train_loss=2.9266174

Batch 2580, train_perplexity=18.664349, train_loss=2.9266152

Batch 2590, train_perplexity=18.664318, train_loss=2.9266136

Batch 2600, train_perplexity=18.664278, train_loss=2.9266114

Batch 2610, train_perplexity=18.664248, train_loss=2.9266098

Batch 2620, train_perplexity=18.664225, train_loss=2.9266086

Batch 2630, train_perplexity=18.664198, train_loss=2.9266071

Batch 2640, train_perplexity=18.664171, train_loss=2.9266057

Batch 2650, train_perplexity=18.664154, train_loss=2.9266047

Batch 2660, train_perplexity=18.664135, train_loss=2.9266038

Batch 2670, train_perplexity=18.664114, train_loss=2.9266026

Batch 2680, train_perplexity=18.6641, train_loss=2.926602

Batch 2690, train_perplexity=18.664082, train_loss=2.926601

Batch 2700, train_perplexity=18.664064, train_loss=2.9266

Batch 2710, train_perplexity=18.664047, train_loss=2.926599

Batch 2720, train_perplexity=18.664034, train_loss=2.9265983

Batch 2730, train_perplexity=18.664015, train_loss=2.9265974

Batch 2740, train_perplexity=18.664001, train_loss=2.9265966

Batch 2750, train_perplexity=18.663994, train_loss=2.9265962

Batch 2760, train_perplexity=18.663975, train_loss=2.9265952

Batch 2770, train_perplexity=18.712738, train_loss=2.9292045

Batch 2780, train_perplexity=18.687351, train_loss=2.927847

Batch 2790, train_perplexity=18.679293, train_loss=2.9274156

Batch 2800, train_perplexity=18.674654, train_loss=2.9271672

Batch 2810, train_perplexity=18.67168, train_loss=2.927008

Batch 2820, train_perplexity=18.669659, train_loss=2.9268997

Batch 2830, train_perplexity=18.66824, train_loss=2.9268236

Batch 2840, train_perplexity=18.667206, train_loss=2.9267683

Batch 2850, train_perplexity=18.66645, train_loss=2.9267278

Batch 2860, train_perplexity=18.665876, train_loss=2.926697

Batch 2870, train_perplexity=18.665443, train_loss=2.926674

Batch 2880, train_perplexity=18.66512, train_loss=2.9266565

Batch 2890, train_perplexity=18.664865, train_loss=2.926643

Batch 2900, train_perplexity=18.664665, train_loss=2.9266322

Batch 2910, train_perplexity=18.664509, train_loss=2.9266238

Batch 2920, train_perplexity=18.664385, train_loss=2.9266171

Batch 2930, train_perplexity=18.664288, train_loss=2.926612

Batch 2940, train_perplexity=18.664207, train_loss=2.9266076

Batch 2950, train_perplexity=18.664145, train_loss=2.9266043

Batch 2960, train_perplexity=18.664087, train_loss=2.9266012

Batch 2970, train_perplexity=18.664042, train_loss=2.9265988

Batch 2980, train_perplexity=18.664007, train_loss=2.9265969

Batch 2990, train_perplexity=18.663975, train_loss=2.9265952

Batch 3000, train_perplexity=18.663944, train_loss=2.9265935

Batch 3010, train_perplexity=18.663921, train_loss=2.9265924

Batch 3020, train_perplexity=18.663904, train_loss=2.9265914

Batch 3030, train_perplexity=18.663877, train_loss=2.92659

Batch 3040, train_perplexity=18.663868, train_loss=2.9265895

Batch 3050, train_perplexity=18.663841, train_loss=2.926588

Batch 3060, train_perplexity=18.663834, train_loss=2.9265876

Batch 3070, train_perplexity=18.663815, train_loss=2.9265866

Batch 3080, train_perplexity=18.663807, train_loss=2.9265862

Batch 3090, train_perplexity=18.663788, train_loss=2.9265852

Batch 3100, train_perplexity=18.66378, train_loss=2.9265847

Batch 3110, train_perplexity=18.66377, train_loss=2.9265842

Batch 3120, train_perplexity=18.663754, train_loss=2.9265833

Batch 3130, train_perplexity=18.663744, train_loss=2.9265828

Batch 3140, train_perplexity=18.663734, train_loss=2.9265823

Batch 3150, train_perplexity=18.663727, train_loss=2.9265819

Batch 3160, train_perplexity=18.663717, train_loss=2.9265814

Batch 3170, train_perplexity=18.663704, train_loss=2.9265807

Batch 3180, train_perplexity=18.663694, train_loss=2.9265802

Batch 3190, train_perplexity=18.663681, train_loss=2.9265795

Batch 3200, train_perplexity=18.663673, train_loss=2.926579

Batch 3210, train_perplexity=18.663664, train_loss=2.9265785

Batch 3220, train_perplexity=18.663654, train_loss=2.926578

Batch 3230, train_perplexity=18.663647, train_loss=2.9265776

Batch 3240, train_perplexity=18.663633, train_loss=2.9265769

Batch 3250, train_perplexity=18.663624, train_loss=2.9265764

Batch 3260, train_perplexity=18.66362, train_loss=2.9265761

Batch 3270, train_perplexity=18.663607, train_loss=2.9265754

Batch 3280, train_perplexity=18.663597, train_loss=2.926575

Batch 3290, train_perplexity=18.663584, train_loss=2.9265742

Batch 3300, train_perplexity=18.663584, train_loss=2.9265742

Batch 3310, train_perplexity=18.663574, train_loss=2.9265738

Batch 3320, train_perplexity=18.663557, train_loss=2.9265728

Batch 3330, train_perplexity=18.663557, train_loss=2.9265728

Batch 3340, train_perplexity=18.663548, train_loss=2.9265723

Batch 3350, train_perplexity=18.66354, train_loss=2.9265718

Batch 3360, train_perplexity=18.663527, train_loss=2.9265711

Batch 3370, train_perplexity=18.663517, train_loss=2.9265707

Batch 3380, train_perplexity=18.663504, train_loss=2.92657

Batch 3390, train_perplexity=18.663504, train_loss=2.92657

Batch 3400, train_perplexity=18.663494, train_loss=2.9265695

Batch 3410, train_perplexity=18.663486, train_loss=2.926569

Batch 3420, train_perplexity=18.663477, train_loss=2.9265685

Batch 3430, train_perplexity=18.663464, train_loss=2.9265678

Batch 3440, train_perplexity=18.66346, train_loss=2.9265676

Batch 3450, train_perplexity=18.66345, train_loss=2.926567

Batch 3460, train_perplexity=18.66344, train_loss=2.9265666

Batch 3470, train_perplexity=18.663433, train_loss=2.9265661

Batch 3480, train_perplexity=18.663424, train_loss=2.9265656

Batch 3490, train_perplexity=18.663414, train_loss=2.9265652

Batch 3500, train_perplexity=18.663406, train_loss=2.9265647

Batch 3510, train_perplexity=18.663397, train_loss=2.9265642

Batch 3520, train_perplexity=18.663397, train_loss=2.9265642

Batch 3530, train_perplexity=18.663387, train_loss=2.9265637

Batch 3540, train_perplexity=18.66338, train_loss=2.9265633
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3550, train_perplexity=18.66337, train_loss=2.9265628

Batch 3560, train_perplexity=18.66336, train_loss=2.9265623

Batch 3570, train_perplexity=18.663353, train_loss=2.9265618

Batch 3580, train_perplexity=18.663347, train_loss=2.9265616

Batch 3590, train_perplexity=18.663334, train_loss=2.9265609

Batch 3600, train_perplexity=18.66333, train_loss=2.9265606

Batch 3610, train_perplexity=18.66333, train_loss=2.9265606

Batch 3620, train_perplexity=18.663317, train_loss=2.92656

Batch 3630, train_perplexity=18.663313, train_loss=2.9265597

Batch 3640, train_perplexity=18.663307, train_loss=2.9265594

Batch 3650, train_perplexity=18.6633, train_loss=2.926559

Batch 3660, train_perplexity=18.66329, train_loss=2.9265585

Batch 3670, train_perplexity=18.66328, train_loss=2.926558

Batch 3680, train_perplexity=18.663273, train_loss=2.9265575

Batch 3690, train_perplexity=18.663267, train_loss=2.9265573

Batch 3700, train_perplexity=18.66326, train_loss=2.9265568

Batch 3710, train_perplexity=18.663254, train_loss=2.9265566

Batch 3720, train_perplexity=18.66325, train_loss=2.9265563

Batch 3730, train_perplexity=18.663237, train_loss=2.9265556

Batch 3740, train_perplexity=18.663237, train_loss=2.9265556

Batch 3750, train_perplexity=18.66322, train_loss=2.9265547

Batch 3760, train_perplexity=18.663214, train_loss=2.9265544

Batch 3770, train_perplexity=18.66321, train_loss=2.9265542

Batch 3780, train_perplexity=18.663206, train_loss=2.926554

Batch 3790, train_perplexity=18.663197, train_loss=2.9265535

Batch 3800, train_perplexity=18.663183, train_loss=2.9265528

Batch 3810, train_perplexity=18.663183, train_loss=2.9265528

Batch 3820, train_perplexity=18.663174, train_loss=2.9265523

Batch 3830, train_perplexity=18.66317, train_loss=2.926552

Batch 3840, train_perplexity=18.663166, train_loss=2.9265518

Batch 3850, train_perplexity=18.66316, train_loss=2.9265516

Batch 3860, train_perplexity=18.663147, train_loss=2.9265509

Batch 3870, train_perplexity=18.66314, train_loss=2.9265504

Batch 3880, train_perplexity=18.66314, train_loss=2.9265504

Batch 3890, train_perplexity=18.66313, train_loss=2.92655

Batch 3900, train_perplexity=18.66312, train_loss=2.9265494

Batch 3910, train_perplexity=18.66312, train_loss=2.9265494

Batch 3920, train_perplexity=18.663113, train_loss=2.926549

Batch 3930, train_perplexity=18.663107, train_loss=2.9265487

Batch 3940, train_perplexity=18.6631, train_loss=2.9265482

Batch 3950, train_perplexity=18.663094, train_loss=2.926548

Batch 3960, train_perplexity=18.663086, train_loss=2.9265475

Batch 3970, train_perplexity=18.663086, train_loss=2.9265475

Batch 3980, train_perplexity=18.663076, train_loss=2.926547

Batch 3990, train_perplexity=18.663063, train_loss=2.9265463

Batch 4000, train_perplexity=18.66306, train_loss=2.926546

Batch 4010, train_perplexity=18.66306, train_loss=2.926546

Batch 4020, train_perplexity=18.66305, train_loss=2.9265456

Batch 4030, train_perplexity=18.66304, train_loss=2.9265451

Batch 4040, train_perplexity=18.663036, train_loss=2.926545

Batch 4050, train_perplexity=18.663033, train_loss=2.9265447

Batch 4060, train_perplexity=18.663023, train_loss=2.9265442

Batch 4070, train_perplexity=18.663027, train_loss=2.9265444

Batch 4080, train_perplexity=18.663013, train_loss=2.9265437

Batch 4090, train_perplexity=18.700584, train_loss=2.9285548

Batch 4100, train_perplexity=18.70659, train_loss=2.928876

Batch 4110, train_perplexity=18.717772, train_loss=2.9294734

Batch 4120, train_perplexity=18.728502, train_loss=2.9300466

Batch 4130, train_perplexity=18.739115, train_loss=2.930613

Batch 4140, train_perplexity=18.749706, train_loss=2.931178

Batch 4150, train_perplexity=18.759981, train_loss=2.931726

Batch 4160, train_perplexity=18.769135, train_loss=2.9322138

Batch 4170, train_perplexity=18.774658, train_loss=2.932508

Batch 4180, train_perplexity=18.77519, train_loss=2.9325364

Batch 4190, train_perplexity=18.770863, train_loss=2.9323058

Batch 4200, train_perplexity=18.763023, train_loss=2.931888

Batch 4210, train_perplexity=18.753698, train_loss=2.931391

Batch 4220, train_perplexity=18.744183, train_loss=2.9308834

Batch 4230, train_perplexity=18.73504, train_loss=2.9303956

Batch 4240, train_perplexity=18.726507, train_loss=2.92994

Batch 4250, train_perplexity=18.718672, train_loss=2.9295216

Batch 4260, train_perplexity=18.711569, train_loss=2.929142

Batch 4270, train_perplexity=18.7052, train_loss=2.9288015

Batch 4280, train_perplexity=18.699532, train_loss=2.9284985

Batch 4290, train_perplexity=18.694534, train_loss=2.9282312

Batch 4300, train_perplexity=18.690163, train_loss=2.9279974

Batch 4310, train_perplexity=18.686357, train_loss=2.9277937

Batch 4320, train_perplexity=18.683048, train_loss=2.9276166

Batch 4330, train_perplexity=18.68019, train_loss=2.9274635

Batch 4340, train_perplexity=18.677721, train_loss=2.9273314

Batch 4350, train_perplexity=18.675598, train_loss=2.9272177

Batch 4360, train_perplexity=18.673767, train_loss=2.9271197

Batch 4370, train_perplexity=18.672201, train_loss=2.9270358

Batch 4380, train_perplexity=18.67085, train_loss=2.9269636

Batch 4390, train_perplexity=18.669703, train_loss=2.926902

Batch 4400, train_perplexity=18.66871, train_loss=2.926849

Batch 4410, train_perplexity=18.667866, train_loss=2.9268036

Batch 4420, train_perplexity=18.66714, train_loss=2.9267647

Batch 4430, train_perplexity=18.666521, train_loss=2.9267316

Batch 4440, train_perplexity=18.665997, train_loss=2.9267035

Batch 4450, train_perplexity=18.665543, train_loss=2.9266791

Batch 4460, train_perplexity=18.665155, train_loss=2.9266584

Batch 4470, train_perplexity=18.664822, train_loss=2.9266405

Batch 4480, train_perplexity=18.664541, train_loss=2.9266255

Batch 4490, train_perplexity=18.664295, train_loss=2.9266124

Batch 4500, train_perplexity=18.664091, train_loss=2.9266014

Batch 4510, train_perplexity=18.663914, train_loss=2.9265919

Batch 4520, train_perplexity=18.663754, train_loss=2.9265833

Batch 4530, train_perplexity=18.66362, train_loss=2.9265761

Batch 4540, train_perplexity=18.663504, train_loss=2.92657

Batch 4550, train_perplexity=18.663406, train_loss=2.9265647

Batch 4560, train_perplexity=18.663326, train_loss=2.9265604

Batch 4570, train_perplexity=18.663254, train_loss=2.9265566

Batch 4580, train_perplexity=18.663193, train_loss=2.9265532

Batch 4590, train_perplexity=18.66314, train_loss=2.9265504

Batch 4600, train_perplexity=18.66309, train_loss=2.9265478

Batch 4610, train_perplexity=18.66305, train_loss=2.9265456

Batch 4620, train_perplexity=18.663013, train_loss=2.9265437

Batch 4630, train_perplexity=18.662987, train_loss=2.9265423

Batch 4640, train_perplexity=18.662956, train_loss=2.9265406

Batch 4650, train_perplexity=18.662933, train_loss=2.9265394

Batch 4660, train_perplexity=18.662907, train_loss=2.926538

Batch 4670, train_perplexity=18.66289, train_loss=2.926537

Batch 4680, train_perplexity=18.662876, train_loss=2.9265363

Batch 4690, train_perplexity=18.662859, train_loss=2.9265354

Batch 4700, train_perplexity=18.66284, train_loss=2.9265344

Batch 4710, train_perplexity=18.662836, train_loss=2.9265342

Batch 4720, train_perplexity=18.662819, train_loss=2.9265332

Batch 4730, train_perplexity=18.66281, train_loss=2.9265327

Batch 4740, train_perplexity=18.662806, train_loss=2.9265325

Batch 4750, train_perplexity=18.662792, train_loss=2.9265318

Batch 4760, train_perplexity=18.662786, train_loss=2.9265316

Batch 4770, train_perplexity=18.662773, train_loss=2.9265308

Batch 4780, train_perplexity=18.662773, train_loss=2.9265308

Batch 4790, train_perplexity=18.662766, train_loss=2.9265304

Batch 4800, train_perplexity=18.66276, train_loss=2.9265301

Batch 4810, train_perplexity=18.662756, train_loss=2.92653

Batch 4820, train_perplexity=18.662746, train_loss=2.9265294

Batch 4830, train_perplexity=18.662739, train_loss=2.926529

Batch 4840, train_perplexity=18.662733, train_loss=2.9265287

Batch 4850, train_perplexity=18.66273, train_loss=2.9265285

Batch 4860, train_perplexity=18.66273, train_loss=2.9265285

Batch 4870, train_perplexity=18.66272, train_loss=2.926528

Batch 4880, train_perplexity=18.662712, train_loss=2.9265275
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4890, train_perplexity=18.662712, train_loss=2.9265275

Batch 4900, train_perplexity=18.662706, train_loss=2.9265273

Batch 4910, train_perplexity=18.662703, train_loss=2.926527

Batch 4920, train_perplexity=18.662693, train_loss=2.9265265

Batch 4930, train_perplexity=18.662693, train_loss=2.9265265

Batch 4940, train_perplexity=18.66269, train_loss=2.9265263

Batch 4950, train_perplexity=18.662676, train_loss=2.9265256

Batch 4960, train_perplexity=18.662676, train_loss=2.9265256

Batch 4970, train_perplexity=18.662676, train_loss=2.9265256

Batch 4980, train_perplexity=18.662666, train_loss=2.926525

Batch 4990, train_perplexity=18.662659, train_loss=2.9265246

Batch 5000, train_perplexity=18.662663, train_loss=2.9265249

Batch 5010, train_perplexity=18.662659, train_loss=2.9265246

Batch 5020, train_perplexity=18.662653, train_loss=2.9265244

Batch 5030, train_perplexity=18.66265, train_loss=2.9265242

Batch 5040, train_perplexity=18.66264, train_loss=2.9265237

Batch 5050, train_perplexity=18.66264, train_loss=2.9265237

Batch 5060, train_perplexity=18.662636, train_loss=2.9265234

Batch 5070, train_perplexity=18.662632, train_loss=2.9265232

Batch 5080, train_perplexity=18.662626, train_loss=2.926523

Batch 5090, train_perplexity=18.662622, train_loss=2.9265227

Batch 5100, train_perplexity=18.662619, train_loss=2.9265225

Batch 5110, train_perplexity=18.662613, train_loss=2.9265223

Batch 5120, train_perplexity=18.662613, train_loss=2.9265223

Batch 5130, train_perplexity=18.662605, train_loss=2.9265218

Batch 5140, train_perplexity=18.662605, train_loss=2.9265218

Batch 5150, train_perplexity=18.6626, train_loss=2.9265215

Batch 5160, train_perplexity=18.662596, train_loss=2.9265213

Batch 5170, train_perplexity=18.662586, train_loss=2.9265208

Batch 5180, train_perplexity=18.662586, train_loss=2.9265208

Batch 5190, train_perplexity=18.662586, train_loss=2.9265208

Batch 5200, train_perplexity=18.662579, train_loss=2.9265203

Batch 5210, train_perplexity=18.662579, train_loss=2.9265203

Batch 5220, train_perplexity=18.662569, train_loss=2.9265199

Batch 5230, train_perplexity=18.662569, train_loss=2.9265199

Batch 5240, train_perplexity=18.662565, train_loss=2.9265196

Batch 5250, train_perplexity=18.66256, train_loss=2.9265194

Batch 5260, train_perplexity=18.662552, train_loss=2.926519

Batch 5270, train_perplexity=18.662552, train_loss=2.926519

Batch 5280, train_perplexity=18.662552, train_loss=2.926519

Batch 5290, train_perplexity=18.662542, train_loss=2.9265184

Batch 5300, train_perplexity=18.662542, train_loss=2.9265184

Batch 5310, train_perplexity=18.662533, train_loss=2.926518

Batch 5320, train_perplexity=18.662529, train_loss=2.9265177

Batch 5330, train_perplexity=18.662529, train_loss=2.9265177

Batch 5340, train_perplexity=18.662529, train_loss=2.9265177

Batch 5350, train_perplexity=18.662525, train_loss=2.9265175

Batch 5360, train_perplexity=18.662516, train_loss=2.926517

Batch 5370, train_perplexity=18.662516, train_loss=2.926517

Batch 5380, train_perplexity=18.662516, train_loss=2.926517

Batch 5390, train_perplexity=18.662506, train_loss=2.9265165

Batch 5400, train_perplexity=18.662506, train_loss=2.9265165

Batch 5410, train_perplexity=18.662506, train_loss=2.9265165

Batch 5420, train_perplexity=18.662498, train_loss=2.926516

Batch 5430, train_perplexity=18.662493, train_loss=2.9265158

Batch 5440, train_perplexity=18.662493, train_loss=2.9265158

Batch 5450, train_perplexity=18.662485, train_loss=2.9265153

Batch 5460, train_perplexity=18.66248, train_loss=2.926515

Batch 5470, train_perplexity=18.66248, train_loss=2.926515

Batch 5480, train_perplexity=18.66248, train_loss=2.926515

Batch 5490, train_perplexity=18.662472, train_loss=2.9265146

Batch 5500, train_perplexity=18.662466, train_loss=2.9265144

Batch 5510, train_perplexity=18.662472, train_loss=2.9265146

Batch 5520, train_perplexity=18.662462, train_loss=2.9265141

Batch 5530, train_perplexity=18.662458, train_loss=2.926514

Batch 5540, train_perplexity=18.662453, train_loss=2.9265137

Batch 5550, train_perplexity=18.662453, train_loss=2.9265137

Batch 5560, train_perplexity=18.662449, train_loss=2.9265134

Batch 5570, train_perplexity=18.662445, train_loss=2.9265132

Batch 5580, train_perplexity=18.662445, train_loss=2.9265132

Batch 5590, train_perplexity=18.66244, train_loss=2.926513

Batch 5600, train_perplexity=18.662436, train_loss=2.9265127

Batch 5610, train_perplexity=18.662432, train_loss=2.9265125

Batch 5620, train_perplexity=18.662426, train_loss=2.9265122

Batch 5630, train_perplexity=18.662426, train_loss=2.9265122

Batch 5640, train_perplexity=18.662426, train_loss=2.9265122

Batch 5650, train_perplexity=18.662418, train_loss=2.9265118

Batch 5660, train_perplexity=18.662418, train_loss=2.9265118

Batch 5670, train_perplexity=18.662409, train_loss=2.9265113

Batch 5680, train_perplexity=18.662409, train_loss=2.9265113

Batch 5690, train_perplexity=18.662405, train_loss=2.926511

Batch 5700, train_perplexity=18.662405, train_loss=2.926511

Batch 5710, train_perplexity=18.6624, train_loss=2.9265108

Batch 5720, train_perplexity=18.662395, train_loss=2.9265106

Batch 5730, train_perplexity=18.662392, train_loss=2.9265103

Batch 5740, train_perplexity=18.662392, train_loss=2.9265103

Batch 5750, train_perplexity=18.662386, train_loss=2.92651

Batch 5760, train_perplexity=18.662386, train_loss=2.92651

Batch 5770, train_perplexity=18.662382, train_loss=2.9265099

Batch 5780, train_perplexity=18.662382, train_loss=2.9265099

Batch 5790, train_perplexity=18.662373, train_loss=2.9265094

Batch 5800, train_perplexity=18.662373, train_loss=2.9265094

Batch 5810, train_perplexity=18.662369, train_loss=2.9265091

Batch 5820, train_perplexity=18.662369, train_loss=2.9265091

Batch 5830, train_perplexity=18.662365, train_loss=2.926509

Batch 5840, train_perplexity=18.662365, train_loss=2.926509

Batch 5850, train_perplexity=18.662355, train_loss=2.9265084

Batch 5860, train_perplexity=18.662355, train_loss=2.9265084

Batch 5870, train_perplexity=18.662352, train_loss=2.9265082

Batch 5880, train_perplexity=18.662346, train_loss=2.926508

Batch 5890, train_perplexity=18.662346, train_loss=2.926508

Batch 5900, train_perplexity=18.70966, train_loss=2.92904

Batch 5910, train_perplexity=18.67009, train_loss=2.9269228

Batch 5920, train_perplexity=18.666338, train_loss=2.9267218

Batch 5930, train_perplexity=18.664839, train_loss=2.9266415

Batch 5940, train_perplexity=18.664082, train_loss=2.926601

Batch 5950, train_perplexity=18.663637, train_loss=2.926577

Batch 5960, train_perplexity=18.663343, train_loss=2.9265614

Batch 5970, train_perplexity=18.66314, train_loss=2.9265504

Batch 5980, train_perplexity=18.662983, train_loss=2.926542

Batch 5990, train_perplexity=18.662863, train_loss=2.9265356

Batch 6000, train_perplexity=18.662773, train_loss=2.9265308

Batch 6010, train_perplexity=18.662693, train_loss=2.9265265

Batch 6020, train_perplexity=18.662632, train_loss=2.9265232

Batch 6030, train_perplexity=18.662579, train_loss=2.9265203

Batch 6040, train_perplexity=18.662533, train_loss=2.926518

Batch 6050, train_perplexity=18.662498, train_loss=2.926516

Batch 6060, train_perplexity=18.662462, train_loss=2.9265141

Batch 6070, train_perplexity=18.662436, train_loss=2.9265127

Batch 6080, train_perplexity=18.662418, train_loss=2.9265118

Batch 6090, train_perplexity=18.662392, train_loss=2.9265103

Batch 6100, train_perplexity=18.662373, train_loss=2.9265094

Batch 6110, train_perplexity=18.662355, train_loss=2.9265084

Batch 6120, train_perplexity=18.662342, train_loss=2.9265077

Batch 6130, train_perplexity=18.662338, train_loss=2.9265075

Batch 6140, train_perplexity=18.66232, train_loss=2.9265065

Batch 6150, train_perplexity=18.662312, train_loss=2.926506

Batch 6160, train_perplexity=18.662312, train_loss=2.926506

Batch 6170, train_perplexity=18.662292, train_loss=2.926505

Batch 6180, train_perplexity=18.662289, train_loss=2.9265049

Batch 6190, train_perplexity=18.662285, train_loss=2.9265046

Batch 6200, train_perplexity=18.662275, train_loss=2.9265041

Batch 6210, train_perplexity=18.662275, train_loss=2.9265041
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 6220, train_perplexity=18.662266, train_loss=2.9265037

Batch 6230, train_perplexity=18.662262, train_loss=2.9265034

Batch 6240, train_perplexity=18.662258, train_loss=2.9265032

Batch 6250, train_perplexity=18.662252, train_loss=2.926503

Batch 6260, train_perplexity=18.662249, train_loss=2.9265027

Batch 6270, train_perplexity=18.662249, train_loss=2.9265027

Batch 6280, train_perplexity=18.662245, train_loss=2.9265025

Batch 6290, train_perplexity=18.66224, train_loss=2.9265022

Batch 6300, train_perplexity=18.662235, train_loss=2.926502

Batch 6310, train_perplexity=18.662235, train_loss=2.926502

Batch 6320, train_perplexity=18.662231, train_loss=2.9265018

Batch 6330, train_perplexity=18.662231, train_loss=2.9265018

Batch 6340, train_perplexity=18.662226, train_loss=2.9265015

Batch 6350, train_perplexity=18.662222, train_loss=2.9265013

Batch 6360, train_perplexity=18.662222, train_loss=2.9265013

Batch 6370, train_perplexity=18.662218, train_loss=2.926501

Batch 6380, train_perplexity=18.662212, train_loss=2.9265008

Batch 6390, train_perplexity=18.662209, train_loss=2.9265006

Batch 6400, train_perplexity=18.662209, train_loss=2.9265006

Batch 6410, train_perplexity=18.662205, train_loss=2.9265003

Batch 6420, train_perplexity=18.662205, train_loss=2.9265003

Batch 6430, train_perplexity=18.662199, train_loss=2.9265

Batch 6440, train_perplexity=18.662195, train_loss=2.9264998

Batch 6450, train_perplexity=18.662195, train_loss=2.9264998

Batch 6460, train_perplexity=18.662191, train_loss=2.9264996

Batch 6470, train_perplexity=18.662186, train_loss=2.9264994

Batch 6480, train_perplexity=18.662186, train_loss=2.9264994

Batch 6490, train_perplexity=18.662186, train_loss=2.9264994

Batch 6500, train_perplexity=18.662178, train_loss=2.926499

Batch 6510, train_perplexity=18.662178, train_loss=2.926499

Batch 6520, train_perplexity=18.662178, train_loss=2.926499

Batch 6530, train_perplexity=18.662172, train_loss=2.9264987

Batch 6540, train_perplexity=18.662169, train_loss=2.9264984

Batch 6550, train_perplexity=18.662169, train_loss=2.9264984

Batch 6560, train_perplexity=18.662169, train_loss=2.9264984

Batch 6570, train_perplexity=18.662165, train_loss=2.9264982

Batch 6580, train_perplexity=18.662165, train_loss=2.9264982

Batch 6590, train_perplexity=18.662159, train_loss=2.926498

Batch 6600, train_perplexity=18.662159, train_loss=2.926498

Batch 6610, train_perplexity=18.662151, train_loss=2.9264975

Batch 6620, train_perplexity=18.662155, train_loss=2.9264977

Batch 6630, train_perplexity=18.662151, train_loss=2.9264975

Batch 6640, train_perplexity=18.662151, train_loss=2.9264975

Batch 6650, train_perplexity=18.662151, train_loss=2.9264975

Batch 6660, train_perplexity=18.662142, train_loss=2.926497

Batch 6670, train_perplexity=18.662142, train_loss=2.926497

Batch 6680, train_perplexity=18.662138, train_loss=2.9264967

Batch 6690, train_perplexity=18.662132, train_loss=2.9264965

Batch 6700, train_perplexity=18.662132, train_loss=2.9264965

Batch 6710, train_perplexity=18.662132, train_loss=2.9264965

Batch 6720, train_perplexity=18.662125, train_loss=2.926496

Batch 6730, train_perplexity=18.662125, train_loss=2.926496

Batch 6740, train_perplexity=18.662119, train_loss=2.9264958

Batch 6750, train_perplexity=18.662125, train_loss=2.926496

Batch 6760, train_perplexity=18.662115, train_loss=2.9264956

Batch 6770, train_perplexity=18.662115, train_loss=2.9264956

Batch 6780, train_perplexity=18.662115, train_loss=2.9264956

Batch 6790, train_perplexity=18.662115, train_loss=2.9264956

Batch 6800, train_perplexity=18.662106, train_loss=2.926495

Batch 6810, train_perplexity=18.662106, train_loss=2.926495

Batch 6820, train_perplexity=18.662106, train_loss=2.926495

Batch 6830, train_perplexity=18.662106, train_loss=2.926495

Batch 6840, train_perplexity=18.662102, train_loss=2.9264948

Batch 6850, train_perplexity=18.662102, train_loss=2.9264948

Batch 6860, train_perplexity=18.662102, train_loss=2.9264948

Batch 6870, train_perplexity=18.662092, train_loss=2.9264944

Batch 6880, train_perplexity=18.662102, train_loss=2.9264948

Batch 6890, train_perplexity=18.662092, train_loss=2.9264944

Batch 6900, train_perplexity=18.662092, train_loss=2.9264944

Batch 6910, train_perplexity=18.662092, train_loss=2.9264944

Batch 6920, train_perplexity=18.662088, train_loss=2.9264941

Batch 6930, train_perplexity=18.662079, train_loss=2.9264936

Batch 6940, train_perplexity=18.662079, train_loss=2.9264936

Batch 6950, train_perplexity=18.662079, train_loss=2.9264936

Batch 6960, train_perplexity=18.662079, train_loss=2.9264936

Batch 6970, train_perplexity=18.662079, train_loss=2.9264936

Batch 6980, train_perplexity=18.662071, train_loss=2.9264932

Batch 6990, train_perplexity=18.662071, train_loss=2.9264932

Batch 7000, train_perplexity=18.662066, train_loss=2.926493

Batch 7010, train_perplexity=18.662062, train_loss=2.9264927

Batch 7020, train_perplexity=18.662062, train_loss=2.9264927

Batch 7030, train_perplexity=18.662062, train_loss=2.9264927

Batch 7040, train_perplexity=18.662058, train_loss=2.9264925

Batch 7050, train_perplexity=18.662058, train_loss=2.9264925

Batch 7060, train_perplexity=18.662052, train_loss=2.9264922

Batch 7070, train_perplexity=18.662052, train_loss=2.9264922

Batch 7080, train_perplexity=18.662048, train_loss=2.926492

Batch 7090, train_perplexity=18.662052, train_loss=2.9264922

Batch 7100, train_perplexity=18.662045, train_loss=2.9264917

Batch 7110, train_perplexity=18.662045, train_loss=2.9264917

Batch 7120, train_perplexity=18.662045, train_loss=2.9264917

Batch 7130, train_perplexity=18.662035, train_loss=2.9264913

Batch 7140, train_perplexity=18.662035, train_loss=2.9264913

Batch 7150, train_perplexity=18.662035, train_loss=2.9264913

Batch 7160, train_perplexity=18.662035, train_loss=2.9264913

Batch 7170, train_perplexity=18.662035, train_loss=2.9264913

Batch 7180, train_perplexity=18.662035, train_loss=2.9264913

Batch 7190, train_perplexity=18.662035, train_loss=2.9264913

Batch 7200, train_perplexity=18.662031, train_loss=2.926491

Batch 7210, train_perplexity=18.662031, train_loss=2.926491

Batch 7220, train_perplexity=18.662025, train_loss=2.9264908

Batch 7230, train_perplexity=18.662025, train_loss=2.9264908

Batch 7240, train_perplexity=18.662022, train_loss=2.9264905

Batch 7250, train_perplexity=18.662018, train_loss=2.9264903

Batch 7260, train_perplexity=18.662022, train_loss=2.9264905

Batch 7270, train_perplexity=18.662018, train_loss=2.9264903

Batch 7280, train_perplexity=18.662012, train_loss=2.92649

Batch 7290, train_perplexity=18.662008, train_loss=2.9264898

Batch 7300, train_perplexity=18.662008, train_loss=2.9264898

Batch 7310, train_perplexity=18.662008, train_loss=2.9264898

Batch 7320, train_perplexity=18.662008, train_loss=2.9264898

Batch 7330, train_perplexity=18.662008, train_loss=2.9264898

Batch 7340, train_perplexity=18.662004, train_loss=2.9264896

Batch 7350, train_perplexity=18.661999, train_loss=2.9264894

Batch 7360, train_perplexity=18.661995, train_loss=2.926489

Batch 7370, train_perplexity=18.661995, train_loss=2.926489

Batch 7380, train_perplexity=18.661991, train_loss=2.9264889

Batch 7390, train_perplexity=18.661985, train_loss=2.9264886

Batch 7400, train_perplexity=18.661991, train_loss=2.9264889

Batch 7410, train_perplexity=18.661985, train_loss=2.9264886

Batch 7420, train_perplexity=18.661985, train_loss=2.9264886

Batch 7430, train_perplexity=18.661982, train_loss=2.9264884

Batch 7440, train_perplexity=18.661982, train_loss=2.9264884

Batch 7450, train_perplexity=18.661982, train_loss=2.9264884

Batch 7460, train_perplexity=18.661982, train_loss=2.9264884

Batch 7470, train_perplexity=18.661978, train_loss=2.9264882

Batch 7480, train_perplexity=18.661978, train_loss=2.9264882

Batch 7490, train_perplexity=18.661978, train_loss=2.9264882

Batch 7500, train_perplexity=18.661972, train_loss=2.926488

Batch 7510, train_perplexity=18.661972, train_loss=2.926488

Batch 7520, train_perplexity=18.661968, train_loss=2.9264877

Batch 7530, train_perplexity=18.661968, train_loss=2.9264877

Batch 7540, train_perplexity=18.661964, train_loss=2.9264874
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7550, train_perplexity=18.661964, train_loss=2.9264874

Batch 7560, train_perplexity=18.661964, train_loss=2.9264874

Batch 7570, train_perplexity=18.661959, train_loss=2.9264872

Batch 7580, train_perplexity=18.661959, train_loss=2.9264872

Batch 7590, train_perplexity=18.661964, train_loss=2.9264874

Batch 7600, train_perplexity=18.661955, train_loss=2.926487

Batch 7610, train_perplexity=18.661955, train_loss=2.926487

Batch 7620, train_perplexity=18.661955, train_loss=2.926487

Batch 7630, train_perplexity=18.661945, train_loss=2.9264865

Batch 7640, train_perplexity=18.661951, train_loss=2.9264867

Batch 7650, train_perplexity=18.661951, train_loss=2.9264867

Batch 7660, train_perplexity=18.661942, train_loss=2.9264863

Batch 7670, train_perplexity=18.661945, train_loss=2.9264865

Batch 7680, train_perplexity=18.661938, train_loss=2.926486

Batch 7690, train_perplexity=18.661938, train_loss=2.926486

Batch 7700, train_perplexity=18.661938, train_loss=2.926486

Batch 7710, train_perplexity=18.661938, train_loss=2.926486

Batch 7720, train_perplexity=18.661928, train_loss=2.9264855

Batch 7730, train_perplexity=18.661928, train_loss=2.9264855

Batch 7740, train_perplexity=18.661928, train_loss=2.9264855

Batch 7750, train_perplexity=18.661928, train_loss=2.9264855

Batch 7760, train_perplexity=18.661924, train_loss=2.9264853

Batch 7770, train_perplexity=18.661928, train_loss=2.9264855

Batch 7780, train_perplexity=18.661924, train_loss=2.9264853

Batch 7790, train_perplexity=18.661919, train_loss=2.926485

Batch 7800, train_perplexity=18.661919, train_loss=2.926485

Batch 7810, train_perplexity=18.661919, train_loss=2.926485

Batch 7820, train_perplexity=18.661915, train_loss=2.9264848

Batch 7830, train_perplexity=18.661911, train_loss=2.9264846

Batch 7840, train_perplexity=18.661911, train_loss=2.9264846

Batch 7850, train_perplexity=18.661911, train_loss=2.9264846

Batch 7860, train_perplexity=18.661911, train_loss=2.9264846

Batch 7870, train_perplexity=18.661911, train_loss=2.9264846

Batch 7880, train_perplexity=18.661905, train_loss=2.9264843

Batch 7890, train_perplexity=18.661911, train_loss=2.9264846

Batch 7900, train_perplexity=18.661911, train_loss=2.9264846

Batch 7910, train_perplexity=18.661905, train_loss=2.9264843

Batch 7920, train_perplexity=18.661901, train_loss=2.926484

Batch 7930, train_perplexity=18.661901, train_loss=2.926484

Batch 7940, train_perplexity=18.661898, train_loss=2.9264839

Batch 7950, train_perplexity=18.661898, train_loss=2.9264839

Batch 7960, train_perplexity=18.661892, train_loss=2.9264836

Batch 7970, train_perplexity=18.661892, train_loss=2.9264836

Batch 7980, train_perplexity=18.661892, train_loss=2.9264836

Batch 7990, train_perplexity=18.661892, train_loss=2.9264836

Batch 8000, train_perplexity=18.661892, train_loss=2.9264836

Batch 8010, train_perplexity=18.661892, train_loss=2.9264836

Batch 8020, train_perplexity=18.661884, train_loss=2.9264832

Batch 8030, train_perplexity=18.661884, train_loss=2.9264832

Batch 8040, train_perplexity=18.661879, train_loss=2.926483

Batch 8050, train_perplexity=18.661879, train_loss=2.926483

Batch 8060, train_perplexity=18.661875, train_loss=2.9264827

Batch 8070, train_perplexity=18.661875, train_loss=2.9264827

Batch 8080, train_perplexity=18.661875, train_loss=2.9264827

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 8090, train_perplexity=18.661875, train_loss=2.9264827

Batch 8100, train_perplexity=18.661871, train_loss=2.9264824

Batch 8110, train_perplexity=18.661871, train_loss=2.9264824

Batch 8120, train_perplexity=18.661871, train_loss=2.9264824

Batch 8130, train_perplexity=18.661867, train_loss=2.9264822

Batch 8140, train_perplexity=18.661867, train_loss=2.9264822

Batch 8150, train_perplexity=18.661858, train_loss=2.9264817

Batch 8160, train_perplexity=18.661858, train_loss=2.9264817

Batch 8170, train_perplexity=18.661858, train_loss=2.9264817

Batch 8180, train_perplexity=18.661858, train_loss=2.9264817

Batch 8190, train_perplexity=18.661858, train_loss=2.9264817

Batch 8200, train_perplexity=18.661854, train_loss=2.9264815

Batch 8210, train_perplexity=18.661858, train_loss=2.9264817

Batch 8220, train_perplexity=18.661854, train_loss=2.9264815

Batch 8230, train_perplexity=18.661854, train_loss=2.9264815

Batch 8240, train_perplexity=18.661858, train_loss=2.9264817

Batch 8250, train_perplexity=18.661848, train_loss=2.9264812

Batch 8260, train_perplexity=18.661848, train_loss=2.9264812

Batch 8270, train_perplexity=18.661848, train_loss=2.9264812

Batch 8280, train_perplexity=18.66184, train_loss=2.9264808

Batch 8290, train_perplexity=18.661844, train_loss=2.926481

Batch 8300, train_perplexity=18.66184, train_loss=2.9264808

Batch 8310, train_perplexity=18.661844, train_loss=2.926481

Batch 8320, train_perplexity=18.66184, train_loss=2.9264808

Batch 8330, train_perplexity=18.66184, train_loss=2.9264808

Batch 8340, train_perplexity=18.66184, train_loss=2.9264808

Batch 8350, train_perplexity=18.66184, train_loss=2.9264808

Batch 8360, train_perplexity=18.66184, train_loss=2.9264808

Batch 8370, train_perplexity=18.661835, train_loss=2.9264805

Batch 8380, train_perplexity=18.66184, train_loss=2.9264808

Batch 8390, train_perplexity=18.661835, train_loss=2.9264805

Batch 8400, train_perplexity=18.66183, train_loss=2.9264803

Batch 8410, train_perplexity=18.66183, train_loss=2.9264803

Batch 8420, train_perplexity=18.66183, train_loss=2.9264803

Batch 8430, train_perplexity=18.661827, train_loss=2.92648

Batch 8440, train_perplexity=18.661827, train_loss=2.92648

Batch 8450, train_perplexity=18.661821, train_loss=2.9264798

Batch 8460, train_perplexity=18.661821, train_loss=2.9264798

Batch 8470, train_perplexity=18.661821, train_loss=2.9264798

Batch 8480, train_perplexity=18.661821, train_loss=2.9264798

Batch 8490, train_perplexity=18.661814, train_loss=2.9264793

Batch 8500, train_perplexity=18.661808, train_loss=2.926479

Batch 8510, train_perplexity=18.661814, train_loss=2.9264793

Batch 8520, train_perplexity=18.661804, train_loss=2.9264789

Batch 8530, train_perplexity=18.661804, train_loss=2.9264789

Batch 8540, train_perplexity=18.661808, train_loss=2.926479

Batch 8550, train_perplexity=18.661804, train_loss=2.9264789

Batch 8560, train_perplexity=18.661804, train_loss=2.9264789

Batch 8570, train_perplexity=18.661804, train_loss=2.9264789

Batch 8580, train_perplexity=18.661804, train_loss=2.9264789

Batch 8590, train_perplexity=18.6618, train_loss=2.9264786

Batch 8600, train_perplexity=18.661795, train_loss=2.9264784

Batch 8610, train_perplexity=18.661795, train_loss=2.9264784

Batch 8620, train_perplexity=18.66179, train_loss=2.9264781

Batch 8630, train_perplexity=18.661795, train_loss=2.9264784

Batch 8640, train_perplexity=18.66179, train_loss=2.9264781

Batch 8650, train_perplexity=18.66179, train_loss=2.9264781

Batch 8660, train_perplexity=18.66179, train_loss=2.9264781

Batch 8670, train_perplexity=18.661787, train_loss=2.926478

Batch 8680, train_perplexity=18.661787, train_loss=2.926478

Batch 8690, train_perplexity=18.661787, train_loss=2.926478

Batch 8700, train_perplexity=18.661787, train_loss=2.926478

Batch 8710, train_perplexity=18.661787, train_loss=2.926478

Batch 8720, train_perplexity=18.661787, train_loss=2.926478

Batch 8730, train_perplexity=18.661781, train_loss=2.9264777

Batch 8740, train_perplexity=18.661781, train_loss=2.9264777

Batch 8750, train_perplexity=18.661781, train_loss=2.9264777

Batch 8760, train_perplexity=18.661777, train_loss=2.9264774

Batch 8770, train_perplexity=18.661777, train_loss=2.9264774

Batch 8780, train_perplexity=18.661777, train_loss=2.9264774

Batch 8790, train_perplexity=18.661774, train_loss=2.9264772

Batch 8800, train_perplexity=18.661777, train_loss=2.9264774

Batch 8810, train_perplexity=18.661777, train_loss=2.9264774
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 8820, train_perplexity=18.661774, train_loss=2.9264772

Batch 8830, train_perplexity=18.661774, train_loss=2.9264772

Batch 8840, train_perplexity=18.661768, train_loss=2.926477

Batch 8850, train_perplexity=18.661768, train_loss=2.926477

Batch 8860, train_perplexity=18.661768, train_loss=2.926477

Batch 8870, train_perplexity=18.661768, train_loss=2.926477

Batch 8880, train_perplexity=18.661768, train_loss=2.926477

Batch 8890, train_perplexity=18.661764, train_loss=2.9264767

Batch 8900, train_perplexity=18.66176, train_loss=2.9264765

Batch 8910, train_perplexity=18.66176, train_loss=2.9264765

Batch 8920, train_perplexity=18.661764, train_loss=2.9264767

Batch 8930, train_perplexity=18.66176, train_loss=2.9264765

Batch 8940, train_perplexity=18.66176, train_loss=2.9264765

Batch 8950, train_perplexity=18.661755, train_loss=2.9264762

Batch 8960, train_perplexity=18.661755, train_loss=2.9264762

Batch 8970, train_perplexity=18.66175, train_loss=2.926476

Batch 8980, train_perplexity=18.661747, train_loss=2.9264758

Batch 8990, train_perplexity=18.66175, train_loss=2.926476

Batch 9000, train_perplexity=18.66175, train_loss=2.926476

Batch 9010, train_perplexity=18.661741, train_loss=2.9264755

Batch 9020, train_perplexity=18.661741, train_loss=2.9264755

Batch 9030, train_perplexity=18.661737, train_loss=2.9264753

Batch 9040, train_perplexity=18.661737, train_loss=2.9264753

Batch 9050, train_perplexity=18.661734, train_loss=2.926475

Batch 9060, train_perplexity=18.661741, train_loss=2.9264755

Batch 9070, train_perplexity=18.661737, train_loss=2.9264753

Batch 9080, train_perplexity=18.661734, train_loss=2.926475

Batch 9090, train_perplexity=18.661734, train_loss=2.926475

Batch 9100, train_perplexity=18.661734, train_loss=2.926475

Batch 9110, train_perplexity=18.661734, train_loss=2.926475

Batch 9120, train_perplexity=18.661728, train_loss=2.9264748

Batch 9130, train_perplexity=18.661728, train_loss=2.9264748

Batch 9140, train_perplexity=18.661724, train_loss=2.9264746

Batch 9150, train_perplexity=18.661734, train_loss=2.926475

Batch 9160, train_perplexity=18.661728, train_loss=2.9264748

Batch 9170, train_perplexity=18.661728, train_loss=2.9264748

Batch 9180, train_perplexity=18.661724, train_loss=2.9264746

Batch 9190, train_perplexity=18.661724, train_loss=2.9264746

Batch 9200, train_perplexity=18.661724, train_loss=2.9264746

Batch 9210, train_perplexity=18.661724, train_loss=2.9264746

Batch 9220, train_perplexity=18.66172, train_loss=2.9264743

Batch 9230, train_perplexity=18.661715, train_loss=2.926474

Batch 9240, train_perplexity=18.66172, train_loss=2.9264743

Batch 9250, train_perplexity=18.661715, train_loss=2.926474

Batch 9260, train_perplexity=18.661724, train_loss=2.9264746

Batch 9270, train_perplexity=18.66172, train_loss=2.9264743

Batch 9280, train_perplexity=18.66172, train_loss=2.9264743

Batch 9290, train_perplexity=18.661715, train_loss=2.926474

Batch 9300, train_perplexity=18.661715, train_loss=2.926474

Batch 9310, train_perplexity=18.661715, train_loss=2.926474

Batch 9320, train_perplexity=18.66171, train_loss=2.9264739

Batch 9330, train_perplexity=18.661734, train_loss=2.926475

Batch 9340, train_perplexity=18.661724, train_loss=2.9264746

Batch 9350, train_perplexity=18.661724, train_loss=2.9264746

Batch 9360, train_perplexity=18.661715, train_loss=2.926474

Batch 9370, train_perplexity=18.661715, train_loss=2.926474

Batch 9380, train_perplexity=18.66171, train_loss=2.9264739

Batch 9390, train_perplexity=18.661707, train_loss=2.9264736

Batch 9400, train_perplexity=18.661707, train_loss=2.9264736

Batch 9410, train_perplexity=18.66171, train_loss=2.9264739

Batch 9420, train_perplexity=18.661707, train_loss=2.9264736

Batch 9430, train_perplexity=18.661701, train_loss=2.9264734

Batch 9440, train_perplexity=18.661701, train_loss=2.9264734

Batch 9450, train_perplexity=18.661697, train_loss=2.9264731

Batch 9460, train_perplexity=18.661697, train_loss=2.9264731

Batch 9470, train_perplexity=18.661697, train_loss=2.9264731

Batch 9480, train_perplexity=18.661694, train_loss=2.926473

Batch 9490, train_perplexity=18.661688, train_loss=2.9264727

Batch 9500, train_perplexity=18.661688, train_loss=2.9264727

Batch 9510, train_perplexity=18.661688, train_loss=2.9264727

Batch 9520, train_perplexity=18.661688, train_loss=2.9264727

Batch 9530, train_perplexity=18.661688, train_loss=2.9264727

Batch 9540, train_perplexity=18.661684, train_loss=2.9264724

Batch 9550, train_perplexity=18.66168, train_loss=2.9264722

Batch 9560, train_perplexity=18.66168, train_loss=2.9264722

Batch 9570, train_perplexity=18.66168, train_loss=2.9264722

Batch 9580, train_perplexity=18.66168, train_loss=2.9264722

Batch 9590, train_perplexity=18.66167, train_loss=2.9264717

Batch 9600, train_perplexity=18.66168, train_loss=2.9264722

Batch 9610, train_perplexity=18.661674, train_loss=2.926472

Batch 9620, train_perplexity=18.661674, train_loss=2.926472

Batch 9630, train_perplexity=18.66167, train_loss=2.9264717

Batch 9640, train_perplexity=18.66167, train_loss=2.9264717

Batch 9650, train_perplexity=18.661667, train_loss=2.9264715

Batch 9660, train_perplexity=18.66167, train_loss=2.9264717

Batch 9670, train_perplexity=18.661667, train_loss=2.9264715

Batch 9680, train_perplexity=18.661667, train_loss=2.9264715

Batch 9690, train_perplexity=18.66167, train_loss=2.9264717

Batch 9700, train_perplexity=18.661661, train_loss=2.9264712

Batch 9710, train_perplexity=18.661661, train_loss=2.9264712

Batch 9720, train_perplexity=18.661661, train_loss=2.9264712

Batch 9730, train_perplexity=18.661661, train_loss=2.9264712

Batch 9740, train_perplexity=18.661661, train_loss=2.9264712

Batch 9750, train_perplexity=18.661661, train_loss=2.9264712

Batch 9760, train_perplexity=18.661661, train_loss=2.9264712

Batch 9770, train_perplexity=18.661661, train_loss=2.9264712

Batch 9780, train_perplexity=18.661661, train_loss=2.9264712

Batch 9790, train_perplexity=18.661654, train_loss=2.9264708

Batch 9800, train_perplexity=18.661661, train_loss=2.9264712

Batch 9810, train_perplexity=18.661661, train_loss=2.9264712

Batch 9820, train_perplexity=18.661654, train_loss=2.9264708

Batch 9830, train_perplexity=18.661657, train_loss=2.926471

Batch 9840, train_perplexity=18.661654, train_loss=2.9264708

Batch 9850, train_perplexity=18.661654, train_loss=2.9264708

Batch 9860, train_perplexity=18.661654, train_loss=2.9264708

Batch 9870, train_perplexity=18.661654, train_loss=2.9264708

Batch 9880, train_perplexity=18.661654, train_loss=2.9264708

Batch 9890, train_perplexity=18.661654, train_loss=2.9264708

Batch 9900, train_perplexity=18.661648, train_loss=2.9264705

Batch 9910, train_perplexity=18.661654, train_loss=2.9264708

Batch 9920, train_perplexity=18.661648, train_loss=2.9264705

Batch 9930, train_perplexity=18.661644, train_loss=2.9264703

Batch 9940, train_perplexity=18.661654, train_loss=2.9264708

Batch 9950, train_perplexity=18.661648, train_loss=2.9264705

Batch 9960, train_perplexity=18.661644, train_loss=2.9264703

Batch 9970, train_perplexity=18.661644, train_loss=2.9264703

Batch 9980, train_perplexity=18.661644, train_loss=2.9264703

Batch 9990, train_perplexity=18.661644, train_loss=2.9264703

Batch 10000, train_perplexity=18.661644, train_loss=2.9264703

Batch 10010, train_perplexity=18.661644, train_loss=2.9264703

Batch 10020, train_perplexity=18.661634, train_loss=2.9264698

Batch 10030, train_perplexity=18.661634, train_loss=2.9264698

Batch 10040, train_perplexity=18.661634, train_loss=2.9264698

Batch 10050, train_perplexity=18.661634, train_loss=2.9264698

Batch 10060, train_perplexity=18.66163, train_loss=2.9264696

Batch 10070, train_perplexity=18.661634, train_loss=2.9264698

Batch 10080, train_perplexity=18.661634, train_loss=2.9264698

Batch 10090, train_perplexity=18.661627, train_loss=2.9264693

Batch 10100, train_perplexity=18.66163, train_loss=2.9264696

Batch 10110, train_perplexity=18.661627, train_loss=2.9264693

Batch 10120, train_perplexity=18.661627, train_loss=2.9264693

Batch 10130, train_perplexity=18.661627, train_loss=2.9264693

Batch 10140, train_perplexity=18.661627, train_loss=2.9264693
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 10150, train_perplexity=18.661617, train_loss=2.9264688

Batch 10160, train_perplexity=18.661617, train_loss=2.9264688

Batch 10170, train_perplexity=18.661617, train_loss=2.9264688

Batch 10180, train_perplexity=18.661617, train_loss=2.9264688

Batch 10190, train_perplexity=18.661617, train_loss=2.9264688

Batch 10200, train_perplexity=18.661613, train_loss=2.9264686

Batch 10210, train_perplexity=18.661617, train_loss=2.9264688

Batch 10220, train_perplexity=18.661617, train_loss=2.9264688

Batch 10230, train_perplexity=18.661608, train_loss=2.9264684

Batch 10240, train_perplexity=18.661613, train_loss=2.9264686

Batch 10250, train_perplexity=18.661608, train_loss=2.9264684

Batch 10260, train_perplexity=18.661613, train_loss=2.9264686

Batch 10270, train_perplexity=18.661608, train_loss=2.9264684

Batch 10280, train_perplexity=18.661608, train_loss=2.9264684

Batch 10290, train_perplexity=18.661604, train_loss=2.9264681

Batch 10300, train_perplexity=18.661608, train_loss=2.9264684

Batch 10310, train_perplexity=18.661604, train_loss=2.9264681

Batch 10320, train_perplexity=18.6616, train_loss=2.926468

Batch 10330, train_perplexity=18.6616, train_loss=2.926468

Batch 10340, train_perplexity=18.6616, train_loss=2.926468

Batch 10350, train_perplexity=18.6616, train_loss=2.926468

Batch 10360, train_perplexity=18.6616, train_loss=2.926468

Batch 10370, train_perplexity=18.6616, train_loss=2.926468

Batch 10380, train_perplexity=18.6616, train_loss=2.926468

Batch 10390, train_perplexity=18.6616, train_loss=2.926468

Batch 10400, train_perplexity=18.661594, train_loss=2.9264677

Batch 10410, train_perplexity=18.661594, train_loss=2.9264677

Batch 10420, train_perplexity=18.661594, train_loss=2.9264677

Batch 10430, train_perplexity=18.661594, train_loss=2.9264677

Batch 10440, train_perplexity=18.66159, train_loss=2.9264674

Batch 10450, train_perplexity=18.66159, train_loss=2.9264674

Batch 10460, train_perplexity=18.661594, train_loss=2.9264677

Batch 10470, train_perplexity=18.66159, train_loss=2.9264674

Batch 10480, train_perplexity=18.66159, train_loss=2.9264674

Batch 10490, train_perplexity=18.661594, train_loss=2.9264677

Batch 10500, train_perplexity=18.66159, train_loss=2.9264674

Batch 10510, train_perplexity=18.66159, train_loss=2.9264674

Batch 10520, train_perplexity=18.66159, train_loss=2.9264674

Batch 10530, train_perplexity=18.661587, train_loss=2.9264672

Batch 10540, train_perplexity=18.661587, train_loss=2.9264672

Batch 10550, train_perplexity=18.661587, train_loss=2.9264672

Batch 10560, train_perplexity=18.661581, train_loss=2.926467

Batch 10570, train_perplexity=18.661581, train_loss=2.926467

Batch 10580, train_perplexity=18.661587, train_loss=2.9264672

Batch 10590, train_perplexity=18.661581, train_loss=2.926467

Batch 10600, train_perplexity=18.661581, train_loss=2.926467

Batch 10610, train_perplexity=18.661581, train_loss=2.926467

Batch 10620, train_perplexity=18.661581, train_loss=2.926467

Batch 10630, train_perplexity=18.661581, train_loss=2.926467

Batch 10640, train_perplexity=18.661581, train_loss=2.926467

Batch 10650, train_perplexity=18.661581, train_loss=2.926467

Batch 10660, train_perplexity=18.661581, train_loss=2.926467

Batch 10670, train_perplexity=18.661581, train_loss=2.926467

Batch 10680, train_perplexity=18.661581, train_loss=2.926467

Batch 10690, train_perplexity=18.661581, train_loss=2.926467

Batch 10700, train_perplexity=18.661573, train_loss=2.9264665

Batch 10710, train_perplexity=18.661577, train_loss=2.9264667

Batch 10720, train_perplexity=18.661573, train_loss=2.9264665

Batch 10730, train_perplexity=18.661573, train_loss=2.9264665

Batch 10740, train_perplexity=18.661573, train_loss=2.9264665

Batch 10750, train_perplexity=18.661573, train_loss=2.9264665

Batch 10760, train_perplexity=18.661564, train_loss=2.926466

Batch 10770, train_perplexity=18.661568, train_loss=2.9264662

Batch 10780, train_perplexity=18.661564, train_loss=2.926466

Batch 10790, train_perplexity=18.661564, train_loss=2.926466

Batch 10800, train_perplexity=18.661564, train_loss=2.926466

Batch 10810, train_perplexity=18.661564, train_loss=2.926466

Batch 10820, train_perplexity=18.661564, train_loss=2.926466

Batch 10830, train_perplexity=18.661564, train_loss=2.926466

Batch 10840, train_perplexity=18.66156, train_loss=2.9264657

Batch 10850, train_perplexity=18.661554, train_loss=2.9264655

Batch 10860, train_perplexity=18.66156, train_loss=2.9264657

Batch 10870, train_perplexity=18.661554, train_loss=2.9264655

Batch 10880, train_perplexity=18.661554, train_loss=2.9264655

Batch 10890, train_perplexity=18.661554, train_loss=2.9264655

Batch 10900, train_perplexity=18.661554, train_loss=2.9264655

Batch 10910, train_perplexity=18.661554, train_loss=2.9264655

Batch 10920, train_perplexity=18.661554, train_loss=2.9264655

Batch 10930, train_perplexity=18.661547, train_loss=2.926465

Batch 10940, train_perplexity=18.661547, train_loss=2.926465

Batch 10950, train_perplexity=18.661547, train_loss=2.926465

Batch 10960, train_perplexity=18.661547, train_loss=2.926465

Batch 10970, train_perplexity=18.661547, train_loss=2.926465

Batch 10980, train_perplexity=18.661547, train_loss=2.926465

Batch 10990, train_perplexity=18.661547, train_loss=2.926465

Batch 11000, train_perplexity=18.661547, train_loss=2.926465

Batch 11010, train_perplexity=18.661547, train_loss=2.926465

Batch 11020, train_perplexity=18.661537, train_loss=2.9264646

Batch 11030, train_perplexity=18.661541, train_loss=2.9264648

Batch 11040, train_perplexity=18.661547, train_loss=2.926465

Batch 11050, train_perplexity=18.661541, train_loss=2.9264648

Batch 11060, train_perplexity=18.661537, train_loss=2.9264646

Batch 11070, train_perplexity=18.661537, train_loss=2.9264646

Batch 11080, train_perplexity=18.661537, train_loss=2.9264646

Batch 11090, train_perplexity=18.661537, train_loss=2.9264646

Batch 11100, train_perplexity=18.661537, train_loss=2.9264646

Batch 11110, train_perplexity=18.661537, train_loss=2.9264646

Batch 11120, train_perplexity=18.661537, train_loss=2.9264646

Batch 11130, train_perplexity=18.661537, train_loss=2.9264646

Batch 11140, train_perplexity=18.661528, train_loss=2.926464

Batch 11150, train_perplexity=18.661528, train_loss=2.926464

Batch 11160, train_perplexity=18.661528, train_loss=2.926464

Batch 11170, train_perplexity=18.661533, train_loss=2.9264643

Batch 11180, train_perplexity=18.661528, train_loss=2.926464

Batch 11190, train_perplexity=18.661528, train_loss=2.926464

Batch 11200, train_perplexity=18.661528, train_loss=2.926464

Batch 11210, train_perplexity=18.661528, train_loss=2.926464

Batch 11220, train_perplexity=18.661528, train_loss=2.926464

Batch 11230, train_perplexity=18.661528, train_loss=2.926464

Batch 11240, train_perplexity=18.661528, train_loss=2.926464

Batch 11250, train_perplexity=18.661528, train_loss=2.926464

Batch 11260, train_perplexity=18.661528, train_loss=2.926464

Batch 11270, train_perplexity=18.661528, train_loss=2.926464

Batch 11280, train_perplexity=18.661528, train_loss=2.926464

Batch 11290, train_perplexity=18.661528, train_loss=2.926464

Batch 11300, train_perplexity=18.661528, train_loss=2.926464

Batch 11310, train_perplexity=18.661524, train_loss=2.9264638

Batch 11320, train_perplexity=18.661524, train_loss=2.9264638

Batch 11330, train_perplexity=18.66152, train_loss=2.9264636

Batch 11340, train_perplexity=18.66152, train_loss=2.9264636

Batch 11350, train_perplexity=18.66152, train_loss=2.9264636

Batch 11360, train_perplexity=18.66152, train_loss=2.9264636

Batch 11370, train_perplexity=18.66152, train_loss=2.9264636

Batch 11380, train_perplexity=18.661514, train_loss=2.9264634

Batch 11390, train_perplexity=18.66152, train_loss=2.9264636

Batch 11400, train_perplexity=18.66152, train_loss=2.9264636

Batch 11410, train_perplexity=18.66151, train_loss=2.9264631

Batch 11420, train_perplexity=18.661514, train_loss=2.9264634

Batch 11430, train_perplexity=18.66151, train_loss=2.9264631

Batch 11440, train_perplexity=18.66151, train_loss=2.9264631

Batch 11450, train_perplexity=18.66151, train_loss=2.9264631

Batch 11460, train_perplexity=18.661514, train_loss=2.9264634
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11470, train_perplexity=18.66151, train_loss=2.9264631

Batch 11480, train_perplexity=18.66151, train_loss=2.9264631

Batch 11490, train_perplexity=18.66151, train_loss=2.9264631

Batch 11500, train_perplexity=18.66151, train_loss=2.9264631

Batch 11510, train_perplexity=18.66151, train_loss=2.9264631

Batch 11520, train_perplexity=18.66151, train_loss=2.9264631

Batch 11530, train_perplexity=18.66151, train_loss=2.9264631

Batch 11540, train_perplexity=18.66151, train_loss=2.9264631

Batch 11550, train_perplexity=18.661507, train_loss=2.926463

Batch 11560, train_perplexity=18.6615, train_loss=2.9264627

Batch 11570, train_perplexity=18.6615, train_loss=2.9264627

Batch 11580, train_perplexity=18.6615, train_loss=2.9264627

Batch 11590, train_perplexity=18.6615, train_loss=2.9264627

Batch 11600, train_perplexity=18.661507, train_loss=2.926463

Batch 11610, train_perplexity=18.6615, train_loss=2.9264627

Batch 11620, train_perplexity=18.6615, train_loss=2.9264627

Batch 11630, train_perplexity=18.6615, train_loss=2.9264627

Batch 11640, train_perplexity=18.661497, train_loss=2.9264624

Batch 11650, train_perplexity=18.661497, train_loss=2.9264624

Batch 11660, train_perplexity=18.661497, train_loss=2.9264624

Batch 11670, train_perplexity=18.661493, train_loss=2.9264622

Batch 11680, train_perplexity=18.661493, train_loss=2.9264622

Batch 11690, train_perplexity=18.661493, train_loss=2.9264622

Batch 11700, train_perplexity=18.661493, train_loss=2.9264622

Batch 11710, train_perplexity=18.661493, train_loss=2.9264622

Batch 11720, train_perplexity=18.661493, train_loss=2.9264622

Batch 11730, train_perplexity=18.661488, train_loss=2.926462

Batch 11740, train_perplexity=18.661493, train_loss=2.9264622

Batch 11750, train_perplexity=18.661488, train_loss=2.926462

Batch 11760, train_perplexity=18.661484, train_loss=2.9264617

Batch 11770, train_perplexity=18.661493, train_loss=2.9264622

Batch 11780, train_perplexity=18.661484, train_loss=2.9264617

Batch 11790, train_perplexity=18.661484, train_loss=2.9264617

Batch 11800, train_perplexity=18.661484, train_loss=2.9264617

Batch 11810, train_perplexity=18.66148, train_loss=2.9264615

Batch 11820, train_perplexity=18.661484, train_loss=2.9264617

Batch 11830, train_perplexity=18.661474, train_loss=2.9264612

Batch 11840, train_perplexity=18.661474, train_loss=2.9264612

Batch 11850, train_perplexity=18.661474, train_loss=2.9264612

Batch 11860, train_perplexity=18.661474, train_loss=2.9264612

Batch 11870, train_perplexity=18.661474, train_loss=2.9264612

Batch 11880, train_perplexity=18.661474, train_loss=2.9264612

Batch 11890, train_perplexity=18.661474, train_loss=2.9264612

Batch 11900, train_perplexity=18.661474, train_loss=2.9264612

Batch 11910, train_perplexity=18.661474, train_loss=2.9264612

Batch 11920, train_perplexity=18.661474, train_loss=2.9264612

Batch 11930, train_perplexity=18.661474, train_loss=2.9264612

Batch 11940, train_perplexity=18.66147, train_loss=2.926461

Batch 11950, train_perplexity=18.661474, train_loss=2.9264612

Batch 11960, train_perplexity=18.661474, train_loss=2.9264612

Batch 11970, train_perplexity=18.661474, train_loss=2.9264612

Batch 11980, train_perplexity=18.661474, train_loss=2.9264612

Batch 11990, train_perplexity=18.66147, train_loss=2.926461

Batch 12000, train_perplexity=18.661467, train_loss=2.9264607

Batch 12010, train_perplexity=18.661474, train_loss=2.9264612

Batch 12020, train_perplexity=18.661474, train_loss=2.9264612

Batch 12030, train_perplexity=18.661467, train_loss=2.9264607

Batch 12040, train_perplexity=18.661467, train_loss=2.9264607

Batch 12050, train_perplexity=18.661467, train_loss=2.9264607

Batch 12060, train_perplexity=18.661467, train_loss=2.9264607

Batch 12070, train_perplexity=18.661467, train_loss=2.9264607

Batch 12080, train_perplexity=18.661467, train_loss=2.9264607

Batch 12090, train_perplexity=18.66146, train_loss=2.9264605

Batch 12100, train_perplexity=18.661467, train_loss=2.9264607

Batch 12110, train_perplexity=18.661467, train_loss=2.9264607

Batch 12120, train_perplexity=18.661467, train_loss=2.9264607

Batch 12130, train_perplexity=18.661467, train_loss=2.9264607

Batch 12140, train_perplexity=18.661457, train_loss=2.9264603

Batch 12150, train_perplexity=18.661457, train_loss=2.9264603

Batch 12160, train_perplexity=18.661457, train_loss=2.9264603

Batch 12170, train_perplexity=18.661457, train_loss=2.9264603

Batch 12180, train_perplexity=18.661457, train_loss=2.9264603

Batch 12190, train_perplexity=18.661457, train_loss=2.9264603

Batch 12200, train_perplexity=18.661457, train_loss=2.9264603

Batch 12210, train_perplexity=18.661457, train_loss=2.9264603

Batch 12220, train_perplexity=18.661457, train_loss=2.9264603

Batch 12230, train_perplexity=18.661457, train_loss=2.9264603

Batch 12240, train_perplexity=18.661457, train_loss=2.9264603

Batch 12250, train_perplexity=18.661457, train_loss=2.9264603

Batch 12260, train_perplexity=18.661457, train_loss=2.9264603

Batch 12270, train_perplexity=18.661457, train_loss=2.9264603

Batch 12280, train_perplexity=18.661457, train_loss=2.9264603

Batch 12290, train_perplexity=18.661453, train_loss=2.92646

Batch 12300, train_perplexity=18.661457, train_loss=2.9264603

Batch 12310, train_perplexity=18.661457, train_loss=2.9264603

Batch 12320, train_perplexity=18.661457, train_loss=2.9264603

Batch 12330, train_perplexity=18.661453, train_loss=2.92646

Batch 12340, train_perplexity=18.661448, train_loss=2.9264598

Batch 12350, train_perplexity=18.661457, train_loss=2.9264603

Batch 12360, train_perplexity=18.661448, train_loss=2.9264598

Batch 12370, train_perplexity=18.661448, train_loss=2.9264598

Batch 12380, train_perplexity=18.661448, train_loss=2.9264598

Batch 12390, train_perplexity=18.661448, train_loss=2.9264598

Batch 12400, train_perplexity=18.661444, train_loss=2.9264596

Batch 12410, train_perplexity=18.661448, train_loss=2.9264598

Batch 12420, train_perplexity=18.661448, train_loss=2.9264598

Batch 12430, train_perplexity=18.661448, train_loss=2.9264598

Batch 12440, train_perplexity=18.661444, train_loss=2.9264596

Batch 12450, train_perplexity=18.661448, train_loss=2.9264598

Batch 12460, train_perplexity=18.661448, train_loss=2.9264598

Batch 12470, train_perplexity=18.66144, train_loss=2.9264593

Batch 12480, train_perplexity=18.66144, train_loss=2.9264593

Batch 12490, train_perplexity=18.66144, train_loss=2.9264593

Batch 12500, train_perplexity=18.66144, train_loss=2.9264593

Batch 12510, train_perplexity=18.66144, train_loss=2.9264593

Batch 12520, train_perplexity=18.661434, train_loss=2.926459

Batch 12530, train_perplexity=18.66144, train_loss=2.9264593

Batch 12540, train_perplexity=18.661434, train_loss=2.926459

Batch 12550, train_perplexity=18.66143, train_loss=2.9264588

Batch 12560, train_perplexity=18.66144, train_loss=2.9264593

Batch 12570, train_perplexity=18.66143, train_loss=2.9264588

Batch 12580, train_perplexity=18.66143, train_loss=2.9264588

Batch 12590, train_perplexity=18.66143, train_loss=2.9264588

Batch 12600, train_perplexity=18.66143, train_loss=2.9264588

Batch 12610, train_perplexity=18.66143, train_loss=2.9264588

Batch 12620, train_perplexity=18.66143, train_loss=2.9264588

Batch 12630, train_perplexity=18.66143, train_loss=2.9264588

Batch 12640, train_perplexity=18.66143, train_loss=2.9264588

Batch 12650, train_perplexity=18.66143, train_loss=2.9264588

Batch 12660, train_perplexity=18.66143, train_loss=2.9264588

Batch 12670, train_perplexity=18.661427, train_loss=2.9264586

Batch 12680, train_perplexity=18.66143, train_loss=2.9264588

Batch 12690, train_perplexity=18.66143, train_loss=2.9264588

Batch 12700, train_perplexity=18.661427, train_loss=2.9264586

Batch 12710, train_perplexity=18.66143, train_loss=2.9264588

Batch 12720, train_perplexity=18.66142, train_loss=2.9264584

Batch 12730, train_perplexity=18.66142, train_loss=2.9264584

Batch 12740, train_perplexity=18.66142, train_loss=2.9264584

Batch 12750, train_perplexity=18.66142, train_loss=2.9264584

Batch 12760, train_perplexity=18.66142, train_loss=2.9264584

Batch 12770, train_perplexity=18.661417, train_loss=2.9264581

Batch 12780, train_perplexity=18.661417, train_loss=2.9264581
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 12790, train_perplexity=18.661413, train_loss=2.926458

Batch 12800, train_perplexity=18.661417, train_loss=2.9264581

Batch 12810, train_perplexity=18.66142, train_loss=2.9264584

Batch 12820, train_perplexity=18.661417, train_loss=2.9264581

Batch 12830, train_perplexity=18.661413, train_loss=2.926458

Batch 12840, train_perplexity=18.66142, train_loss=2.9264584

Batch 12850, train_perplexity=18.661413, train_loss=2.926458

Batch 12860, train_perplexity=18.661413, train_loss=2.926458

Batch 12870, train_perplexity=18.661413, train_loss=2.926458

Batch 12880, train_perplexity=18.661413, train_loss=2.926458

Batch 12890, train_perplexity=18.661413, train_loss=2.926458

Batch 12900, train_perplexity=18.661413, train_loss=2.926458

Batch 12910, train_perplexity=18.661413, train_loss=2.926458

Batch 12920, train_perplexity=18.661413, train_loss=2.926458

Batch 12930, train_perplexity=18.661407, train_loss=2.9264576

Batch 12940, train_perplexity=18.661413, train_loss=2.926458

Batch 12950, train_perplexity=18.661407, train_loss=2.9264576

Batch 12960, train_perplexity=18.661407, train_loss=2.9264576

Batch 12970, train_perplexity=18.661407, train_loss=2.9264576

Batch 12980, train_perplexity=18.661404, train_loss=2.9264574

Batch 12990, train_perplexity=18.661404, train_loss=2.9264574

Batch 13000, train_perplexity=18.661404, train_loss=2.9264574

Batch 13010, train_perplexity=18.661404, train_loss=2.9264574

Batch 13020, train_perplexity=18.661404, train_loss=2.9264574

Batch 13030, train_perplexity=18.661404, train_loss=2.9264574

Batch 13040, train_perplexity=18.661404, train_loss=2.9264574

Batch 13050, train_perplexity=18.661404, train_loss=2.9264574

Batch 13060, train_perplexity=18.661404, train_loss=2.9264574

Batch 13070, train_perplexity=18.661404, train_loss=2.9264574

Batch 13080, train_perplexity=18.6614, train_loss=2.9264572

Batch 13090, train_perplexity=18.661404, train_loss=2.9264574

Batch 13100, train_perplexity=18.661404, train_loss=2.9264574

Batch 13110, train_perplexity=18.661404, train_loss=2.9264574

Batch 13120, train_perplexity=18.661404, train_loss=2.9264574

Batch 13130, train_perplexity=18.661404, train_loss=2.9264574

Batch 13140, train_perplexity=18.6614, train_loss=2.9264572

Batch 13150, train_perplexity=18.661404, train_loss=2.9264574

Batch 13160, train_perplexity=18.661394, train_loss=2.926457

Batch 13170, train_perplexity=18.6614, train_loss=2.9264572

Batch 13180, train_perplexity=18.661394, train_loss=2.926457

Batch 13190, train_perplexity=18.661394, train_loss=2.926457

Batch 13200, train_perplexity=18.6614, train_loss=2.9264572

Batch 13210, train_perplexity=18.661394, train_loss=2.926457

Batch 13220, train_perplexity=18.661394, train_loss=2.926457

Batch 13230, train_perplexity=18.661394, train_loss=2.926457

Batch 13240, train_perplexity=18.661394, train_loss=2.926457

Batch 13250, train_perplexity=18.661394, train_loss=2.926457

Batch 13260, train_perplexity=18.661394, train_loss=2.926457

Batch 13270, train_perplexity=18.661394, train_loss=2.926457

Batch 13280, train_perplexity=18.661386, train_loss=2.9264565

Batch 13290, train_perplexity=18.661394, train_loss=2.926457

Batch 13300, train_perplexity=18.661386, train_loss=2.9264565

Batch 13310, train_perplexity=18.661394, train_loss=2.926457

Batch 13320, train_perplexity=18.661386, train_loss=2.9264565

Batch 13330, train_perplexity=18.66139, train_loss=2.9264567

Batch 13340, train_perplexity=18.661386, train_loss=2.9264565

Batch 13350, train_perplexity=18.661386, train_loss=2.9264565

Batch 13360, train_perplexity=18.661386, train_loss=2.9264565

Batch 13370, train_perplexity=18.661386, train_loss=2.9264565

Batch 13380, train_perplexity=18.661386, train_loss=2.9264565

Batch 13390, train_perplexity=18.661386, train_loss=2.9264565

Batch 13400, train_perplexity=18.661386, train_loss=2.9264565

Batch 13410, train_perplexity=18.661386, train_loss=2.9264565

Batch 13420, train_perplexity=18.661386, train_loss=2.9264565

Batch 13430, train_perplexity=18.661386, train_loss=2.9264565

Batch 13440, train_perplexity=18.661386, train_loss=2.9264565

Batch 13450, train_perplexity=18.661386, train_loss=2.9264565

Batch 13460, train_perplexity=18.661386, train_loss=2.9264565

Batch 13470, train_perplexity=18.661386, train_loss=2.9264565

Batch 13480, train_perplexity=18.661386, train_loss=2.9264565

Batch 13490, train_perplexity=18.66138, train_loss=2.9264562

Batch 13500, train_perplexity=18.661386, train_loss=2.9264565

Batch 13510, train_perplexity=18.661386, train_loss=2.9264565

Batch 13520, train_perplexity=18.661386, train_loss=2.9264565

Batch 13530, train_perplexity=18.661377, train_loss=2.926456

Batch 13540, train_perplexity=18.661377, train_loss=2.926456

Batch 13550, train_perplexity=18.66138, train_loss=2.9264562

Batch 13560, train_perplexity=18.66138, train_loss=2.9264562

Batch 13570, train_perplexity=18.661377, train_loss=2.926456

Batch 13580, train_perplexity=18.661373, train_loss=2.9264557

Batch 13590, train_perplexity=18.661377, train_loss=2.926456

Batch 13600, train_perplexity=18.661377, train_loss=2.926456

Batch 13610, train_perplexity=18.661377, train_loss=2.926456

Batch 13620, train_perplexity=18.661377, train_loss=2.926456

Batch 13630, train_perplexity=18.661373, train_loss=2.9264557

Batch 13640, train_perplexity=18.661373, train_loss=2.9264557

Batch 13650, train_perplexity=18.661367, train_loss=2.9264555

Batch 13660, train_perplexity=18.661373, train_loss=2.9264557

Batch 13670, train_perplexity=18.661373, train_loss=2.9264557

Batch 13680, train_perplexity=18.661367, train_loss=2.9264555

Batch 13690, train_perplexity=18.661367, train_loss=2.9264555

Batch 13700, train_perplexity=18.661373, train_loss=2.9264557

Batch 13710, train_perplexity=18.661367, train_loss=2.9264555

Batch 13720, train_perplexity=18.661367, train_loss=2.9264555

Batch 13730, train_perplexity=18.661367, train_loss=2.9264555

Batch 13740, train_perplexity=18.661367, train_loss=2.9264555

Batch 13750, train_perplexity=18.661367, train_loss=2.9264555

Batch 13760, train_perplexity=18.661367, train_loss=2.9264555

Batch 13770, train_perplexity=18.661367, train_loss=2.9264555

Batch 13780, train_perplexity=18.661364, train_loss=2.9264553

Batch 13790, train_perplexity=18.661367, train_loss=2.9264555

Batch 13800, train_perplexity=18.66136, train_loss=2.926455

Batch 13810, train_perplexity=18.66136, train_loss=2.926455

Batch 13820, train_perplexity=18.66136, train_loss=2.926455

Batch 13830, train_perplexity=18.66136, train_loss=2.926455

Batch 13840, train_perplexity=18.66136, train_loss=2.926455

Batch 13850, train_perplexity=18.66136, train_loss=2.926455

Batch 13860, train_perplexity=18.66136, train_loss=2.926455

Batch 13870, train_perplexity=18.66136, train_loss=2.926455

Batch 13880, train_perplexity=18.66136, train_loss=2.926455

Batch 13890, train_perplexity=18.66136, train_loss=2.926455

Batch 13900, train_perplexity=18.66136, train_loss=2.926455

Batch 13910, train_perplexity=18.66136, train_loss=2.926455

Batch 13920, train_perplexity=18.66136, train_loss=2.926455

Batch 13930, train_perplexity=18.66136, train_loss=2.926455

Batch 13940, train_perplexity=18.66136, train_loss=2.926455

Batch 13950, train_perplexity=18.66135, train_loss=2.9264545

Batch 13960, train_perplexity=18.66135, train_loss=2.9264545

Batch 13970, train_perplexity=18.66136, train_loss=2.926455

Batch 13980, train_perplexity=18.661354, train_loss=2.9264548

Batch 13990, train_perplexity=18.66135, train_loss=2.9264545

Batch 14000, train_perplexity=18.66135, train_loss=2.9264545

Batch 14010, train_perplexity=18.66135, train_loss=2.9264545

Batch 14020, train_perplexity=18.66135, train_loss=2.9264545

Batch 14030, train_perplexity=18.66135, train_loss=2.9264545

Batch 14040, train_perplexity=18.66135, train_loss=2.9264545

Batch 14050, train_perplexity=18.66135, train_loss=2.9264545

Batch 14060, train_perplexity=18.66135, train_loss=2.9264545

Batch 14070, train_perplexity=18.66135, train_loss=2.9264545

Batch 14080, train_perplexity=18.66135, train_loss=2.9264545

Batch 14090, train_perplexity=18.66135, train_loss=2.9264545

Batch 14100, train_perplexity=18.66134, train_loss=2.926454
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 14110, train_perplexity=18.66134, train_loss=2.926454

Batch 14120, train_perplexity=18.66134, train_loss=2.926454

Batch 14130, train_perplexity=18.66134, train_loss=2.926454

Batch 14140, train_perplexity=18.66134, train_loss=2.926454

Batch 14150, train_perplexity=18.661346, train_loss=2.9264543

Batch 14160, train_perplexity=18.66134, train_loss=2.926454

Batch 14170, train_perplexity=18.66134, train_loss=2.926454

Batch 14180, train_perplexity=18.66134, train_loss=2.926454

Batch 14190, train_perplexity=18.66134, train_loss=2.926454

Batch 14200, train_perplexity=18.661337, train_loss=2.9264538

Batch 14210, train_perplexity=18.66134, train_loss=2.926454

Batch 14220, train_perplexity=18.66134, train_loss=2.926454

Batch 14230, train_perplexity=18.66134, train_loss=2.926454

Batch 14240, train_perplexity=18.661337, train_loss=2.9264538

Batch 14250, train_perplexity=18.661337, train_loss=2.9264538

Batch 14260, train_perplexity=18.661337, train_loss=2.9264538

Batch 14270, train_perplexity=18.66134, train_loss=2.926454

Batch 14280, train_perplexity=18.66134, train_loss=2.926454

Batch 14290, train_perplexity=18.66134, train_loss=2.926454

Batch 14300, train_perplexity=18.661333, train_loss=2.9264536

Batch 14310, train_perplexity=18.66134, train_loss=2.926454

Batch 14320, train_perplexity=18.661337, train_loss=2.9264538

Batch 14330, train_perplexity=18.661333, train_loss=2.9264536

Batch 14340, train_perplexity=18.661337, train_loss=2.9264538

Batch 14350, train_perplexity=18.661337, train_loss=2.9264538

Batch 14360, train_perplexity=18.661333, train_loss=2.9264536

Batch 14370, train_perplexity=18.661333, train_loss=2.9264536

Batch 14380, train_perplexity=18.661333, train_loss=2.9264536

Batch 14390, train_perplexity=18.661333, train_loss=2.9264536

Batch 14400, train_perplexity=18.661337, train_loss=2.9264538

Batch 14410, train_perplexity=18.661333, train_loss=2.9264536

Batch 14420, train_perplexity=18.661333, train_loss=2.9264536

Batch 14430, train_perplexity=18.661333, train_loss=2.9264536

Batch 14440, train_perplexity=18.661333, train_loss=2.9264536

Batch 14450, train_perplexity=18.661333, train_loss=2.9264536

Batch 14460, train_perplexity=18.661327, train_loss=2.9264534

Batch 14470, train_perplexity=18.661333, train_loss=2.9264536

Batch 14480, train_perplexity=18.661333, train_loss=2.9264536

Batch 14490, train_perplexity=18.661333, train_loss=2.9264536

Batch 14500, train_perplexity=18.661324, train_loss=2.926453

Batch 14510, train_perplexity=18.661327, train_loss=2.9264534

Batch 14520, train_perplexity=18.661333, train_loss=2.9264536

Batch 14530, train_perplexity=18.661327, train_loss=2.9264534

Batch 14540, train_perplexity=18.661333, train_loss=2.9264536

Batch 14550, train_perplexity=18.661327, train_loss=2.9264534

Batch 14560, train_perplexity=18.661327, train_loss=2.9264534

Batch 14570, train_perplexity=18.661327, train_loss=2.9264534

Batch 14580, train_perplexity=18.661324, train_loss=2.926453

Batch 14590, train_perplexity=18.661324, train_loss=2.926453

Batch 14600, train_perplexity=18.661327, train_loss=2.9264534

Batch 14610, train_perplexity=18.661327, train_loss=2.9264534

Batch 14620, train_perplexity=18.661324, train_loss=2.926453

Batch 14630, train_perplexity=18.661324, train_loss=2.926453

Batch 14640, train_perplexity=18.661324, train_loss=2.926453

Batch 14650, train_perplexity=18.66132, train_loss=2.9264529

Batch 14660, train_perplexity=18.661324, train_loss=2.926453

Batch 14670, train_perplexity=18.661324, train_loss=2.926453

Batch 14680, train_perplexity=18.661324, train_loss=2.926453

Batch 14690, train_perplexity=18.661324, train_loss=2.926453

Batch 14700, train_perplexity=18.661324, train_loss=2.926453

Batch 14710, train_perplexity=18.661324, train_loss=2.926453

Batch 14720, train_perplexity=18.661324, train_loss=2.926453

Batch 14730, train_perplexity=18.661324, train_loss=2.926453

Batch 14740, train_perplexity=18.661324, train_loss=2.926453

Batch 14750, train_perplexity=18.661324, train_loss=2.926453

Batch 14760, train_perplexity=18.66132, train_loss=2.9264529

Batch 14770, train_perplexity=18.661314, train_loss=2.9264526

Batch 14780, train_perplexity=18.661324, train_loss=2.926453

Batch 14790, train_perplexity=18.66132, train_loss=2.9264529

Batch 14800, train_perplexity=18.661314, train_loss=2.9264526

Batch 14810, train_perplexity=18.66132, train_loss=2.9264529

Batch 14820, train_perplexity=18.66132, train_loss=2.9264529

Batch 14830, train_perplexity=18.66132, train_loss=2.9264529

Batch 14840, train_perplexity=18.661314, train_loss=2.9264526

Batch 14850, train_perplexity=18.661314, train_loss=2.9264526

Batch 14860, train_perplexity=18.661314, train_loss=2.9264526

Batch 14870, train_perplexity=18.661314, train_loss=2.9264526

Batch 14880, train_perplexity=18.661314, train_loss=2.9264526

Batch 14890, train_perplexity=18.66131, train_loss=2.9264524

Batch 14900, train_perplexity=18.66131, train_loss=2.9264524

Batch 14910, train_perplexity=18.66131, train_loss=2.9264524

Batch 14920, train_perplexity=18.661306, train_loss=2.9264522

Batch 14930, train_perplexity=18.66131, train_loss=2.9264524

Batch 14940, train_perplexity=18.661306, train_loss=2.9264522

Batch 14950, train_perplexity=18.66131, train_loss=2.9264524

Batch 14960, train_perplexity=18.661306, train_loss=2.9264522

Batch 14970, train_perplexity=18.661306, train_loss=2.9264522

Batch 14980, train_perplexity=18.66131, train_loss=2.9264524

Batch 14990, train_perplexity=18.661306, train_loss=2.9264522

Batch 15000, train_perplexity=18.66131, train_loss=2.9264524

Batch 15010, train_perplexity=18.661306, train_loss=2.9264522

Batch 15020, train_perplexity=18.661306, train_loss=2.9264522

Batch 15030, train_perplexity=18.661306, train_loss=2.9264522

Batch 15040, train_perplexity=18.661306, train_loss=2.9264522

Batch 15050, train_perplexity=18.661306, train_loss=2.9264522

Batch 15060, train_perplexity=18.661306, train_loss=2.9264522

Batch 15070, train_perplexity=18.661306, train_loss=2.9264522

Batch 15080, train_perplexity=18.661306, train_loss=2.9264522

Batch 15090, train_perplexity=18.6613, train_loss=2.926452

Batch 15100, train_perplexity=18.6613, train_loss=2.926452

Batch 15110, train_perplexity=18.661306, train_loss=2.9264522

Batch 15120, train_perplexity=18.661297, train_loss=2.9264517

Batch 15130, train_perplexity=18.661297, train_loss=2.9264517

Batch 15140, train_perplexity=18.6613, train_loss=2.926452

Batch 15150, train_perplexity=18.661297, train_loss=2.9264517

Batch 15160, train_perplexity=18.661306, train_loss=2.9264522

Batch 15170, train_perplexity=18.6613, train_loss=2.926452

Batch 15180, train_perplexity=18.661297, train_loss=2.9264517

Batch 15190, train_perplexity=18.661306, train_loss=2.9264522

Batch 15200, train_perplexity=18.6613, train_loss=2.926452

Batch 15210, train_perplexity=18.6613, train_loss=2.926452

Batch 15220, train_perplexity=18.6613, train_loss=2.926452

Batch 15230, train_perplexity=18.661297, train_loss=2.9264517

Batch 15240, train_perplexity=18.661297, train_loss=2.9264517

Batch 15250, train_perplexity=18.661297, train_loss=2.9264517

Batch 15260, train_perplexity=18.661297, train_loss=2.9264517

Batch 15270, train_perplexity=18.661287, train_loss=2.9264512

Batch 15280, train_perplexity=18.661297, train_loss=2.9264517

Batch 15290, train_perplexity=18.661297, train_loss=2.9264517

Batch 15300, train_perplexity=18.661297, train_loss=2.9264517

Batch 15310, train_perplexity=18.661297, train_loss=2.9264517

Batch 15320, train_perplexity=18.661287, train_loss=2.9264512

Batch 15330, train_perplexity=18.661293, train_loss=2.9264514

Batch 15340, train_perplexity=18.661297, train_loss=2.9264517

Batch 15350, train_perplexity=18.661287, train_loss=2.9264512

Batch 15360, train_perplexity=18.661287, train_loss=2.9264512

Batch 15370, train_perplexity=18.661287, train_loss=2.9264512

Batch 15380, train_perplexity=18.661287, train_loss=2.9264512

Batch 15390, train_perplexity=18.661287, train_loss=2.9264512

Batch 15400, train_perplexity=18.661287, train_loss=2.9264512

Batch 15410, train_perplexity=18.661287, train_loss=2.9264512

Batch 15420, train_perplexity=18.661287, train_loss=2.9264512
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15430, train_perplexity=18.661287, train_loss=2.9264512

Batch 15440, train_perplexity=18.661287, train_loss=2.9264512

Batch 15450, train_perplexity=18.661287, train_loss=2.9264512

Batch 15460, train_perplexity=18.661287, train_loss=2.9264512

Batch 15470, train_perplexity=18.661283, train_loss=2.926451

Batch 15480, train_perplexity=18.661287, train_loss=2.9264512

Batch 15490, train_perplexity=18.661287, train_loss=2.9264512

Batch 15500, train_perplexity=18.661287, train_loss=2.9264512

Batch 15510, train_perplexity=18.66128, train_loss=2.9264507

Batch 15520, train_perplexity=18.66128, train_loss=2.9264507

Batch 15530, train_perplexity=18.66128, train_loss=2.9264507

Batch 15540, train_perplexity=18.661283, train_loss=2.926451

Batch 15550, train_perplexity=18.66128, train_loss=2.9264507

Batch 15560, train_perplexity=18.66128, train_loss=2.9264507

Batch 15570, train_perplexity=18.66128, train_loss=2.9264507

Batch 15580, train_perplexity=18.66128, train_loss=2.9264507

Batch 15590, train_perplexity=18.66128, train_loss=2.9264507

Batch 15600, train_perplexity=18.66128, train_loss=2.9264507

Batch 15610, train_perplexity=18.66128, train_loss=2.9264507

Batch 15620, train_perplexity=18.66128, train_loss=2.9264507

Batch 15630, train_perplexity=18.66128, train_loss=2.9264507

Batch 15640, train_perplexity=18.661274, train_loss=2.9264505

Batch 15650, train_perplexity=18.66128, train_loss=2.9264507

Batch 15660, train_perplexity=18.66128, train_loss=2.9264507

Batch 15670, train_perplexity=18.661274, train_loss=2.9264505

Batch 15680, train_perplexity=18.66128, train_loss=2.9264507

Batch 15690, train_perplexity=18.66128, train_loss=2.9264507

Batch 15700, train_perplexity=18.66128, train_loss=2.9264507

Batch 15710, train_perplexity=18.66128, train_loss=2.9264507

Batch 15720, train_perplexity=18.66128, train_loss=2.9264507

Batch 15730, train_perplexity=18.66128, train_loss=2.9264507

Batch 15740, train_perplexity=18.66128, train_loss=2.9264507

Batch 15750, train_perplexity=18.66128, train_loss=2.9264507

Batch 15760, train_perplexity=18.66127, train_loss=2.9264503

Batch 15770, train_perplexity=18.661274, train_loss=2.9264505

Batch 15780, train_perplexity=18.66128, train_loss=2.9264507

Batch 15790, train_perplexity=18.661274, train_loss=2.9264505

Batch 15800, train_perplexity=18.661274, train_loss=2.9264505

Batch 15810, train_perplexity=18.661274, train_loss=2.9264505

Batch 15820, train_perplexity=18.66127, train_loss=2.9264503

Batch 15830, train_perplexity=18.66127, train_loss=2.9264503

Batch 15840, train_perplexity=18.66127, train_loss=2.9264503

Batch 15850, train_perplexity=18.66127, train_loss=2.9264503

Batch 15860, train_perplexity=18.66127, train_loss=2.9264503

Batch 15870, train_perplexity=18.66127, train_loss=2.9264503

Batch 15880, train_perplexity=18.661266, train_loss=2.92645

Batch 15890, train_perplexity=18.66127, train_loss=2.9264503

Batch 15900, train_perplexity=18.66127, train_loss=2.9264503

Batch 15910, train_perplexity=18.66127, train_loss=2.9264503

Batch 15920, train_perplexity=18.66127, train_loss=2.9264503

Batch 15930, train_perplexity=18.661266, train_loss=2.92645

Batch 15940, train_perplexity=18.66127, train_loss=2.9264503

Batch 15950, train_perplexity=18.661266, train_loss=2.92645

Batch 15960, train_perplexity=18.66127, train_loss=2.9264503

Batch 15970, train_perplexity=18.66127, train_loss=2.9264503

Batch 15980, train_perplexity=18.66127, train_loss=2.9264503

Batch 15990, train_perplexity=18.661266, train_loss=2.92645

Batch 16000, train_perplexity=18.66127, train_loss=2.9264503

Batch 16010, train_perplexity=18.661266, train_loss=2.92645

Batch 16020, train_perplexity=18.66127, train_loss=2.9264503

Batch 16030, train_perplexity=18.66126, train_loss=2.9264498

Batch 16040, train_perplexity=18.66126, train_loss=2.9264498

Batch 16050, train_perplexity=18.661266, train_loss=2.92645

Batch 16060, train_perplexity=18.66126, train_loss=2.9264498

Batch 16070, train_perplexity=18.66126, train_loss=2.9264498

Batch 16080, train_perplexity=18.661266, train_loss=2.92645

Batch 16090, train_perplexity=18.66175, train_loss=2.926476

Batch 16100, train_perplexity=18.66159, train_loss=2.9264674

Batch 16110, train_perplexity=18.661514, train_loss=2.9264634

Batch 16120, train_perplexity=18.661467, train_loss=2.9264607

Batch 16130, train_perplexity=18.66143, train_loss=2.9264588

Batch 16140, train_perplexity=18.6614, train_loss=2.9264572

Batch 16150, train_perplexity=18.661377, train_loss=2.926456

Batch 16160, train_perplexity=18.66135, train_loss=2.9264545

Batch 16170, train_perplexity=18.66134, train_loss=2.926454

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 16180, train_perplexity=18.661324, train_loss=2.926453

Batch 16190, train_perplexity=18.661314, train_loss=2.9264526

Batch 16200, train_perplexity=18.661306, train_loss=2.9264522

Batch 16210, train_perplexity=18.661297, train_loss=2.9264517

Batch 16220, train_perplexity=18.661297, train_loss=2.9264517

Batch 16230, train_perplexity=18.661287, train_loss=2.9264512

Batch 16240, train_perplexity=18.66128, train_loss=2.9264507

Batch 16250, train_perplexity=18.66128, train_loss=2.9264507

Batch 16260, train_perplexity=18.66128, train_loss=2.9264507

Batch 16270, train_perplexity=18.66127, train_loss=2.9264503

Batch 16280, train_perplexity=18.66127, train_loss=2.9264503

Batch 16290, train_perplexity=18.66127, train_loss=2.9264503

Batch 16300, train_perplexity=18.661266, train_loss=2.92645

Batch 16310, train_perplexity=18.661266, train_loss=2.92645

Batch 16320, train_perplexity=18.66126, train_loss=2.9264498

Batch 16330, train_perplexity=18.661257, train_loss=2.9264495

Batch 16340, train_perplexity=18.661253, train_loss=2.9264493

Batch 16350, train_perplexity=18.661257, train_loss=2.9264495

Batch 16360, train_perplexity=18.661253, train_loss=2.9264493

Batch 16370, train_perplexity=18.66126, train_loss=2.9264498

Batch 16380, train_perplexity=18.661257, train_loss=2.9264495

Batch 16390, train_perplexity=18.661253, train_loss=2.9264493

Batch 16400, train_perplexity=18.661253, train_loss=2.9264493

Batch 16410, train_perplexity=18.661253, train_loss=2.9264493

Batch 16420, train_perplexity=18.661253, train_loss=2.9264493

Batch 16430, train_perplexity=18.661253, train_loss=2.9264493

Batch 16440, train_perplexity=18.661253, train_loss=2.9264493

Batch 16450, train_perplexity=18.661253, train_loss=2.9264493

Batch 16460, train_perplexity=18.661253, train_loss=2.9264493

Batch 16470, train_perplexity=18.661253, train_loss=2.9264493

Batch 16480, train_perplexity=18.661253, train_loss=2.9264493

Batch 16490, train_perplexity=18.661253, train_loss=2.9264493

Batch 16500, train_perplexity=18.661247, train_loss=2.926449

Batch 16510, train_perplexity=18.661247, train_loss=2.926449

Batch 16520, train_perplexity=18.661253, train_loss=2.9264493

Batch 16530, train_perplexity=18.661247, train_loss=2.926449

Batch 16540, train_perplexity=18.661247, train_loss=2.926449

Batch 16550, train_perplexity=18.661243, train_loss=2.9264488

Batch 16560, train_perplexity=18.661243, train_loss=2.9264488

Batch 16570, train_perplexity=18.661243, train_loss=2.9264488

Batch 16580, train_perplexity=18.661247, train_loss=2.926449

Batch 16590, train_perplexity=18.661243, train_loss=2.9264488

Batch 16600, train_perplexity=18.661243, train_loss=2.9264488

Batch 16610, train_perplexity=18.661243, train_loss=2.9264488

Batch 16620, train_perplexity=18.661243, train_loss=2.9264488

Batch 16630, train_perplexity=18.66124, train_loss=2.9264486

Batch 16640, train_perplexity=18.661243, train_loss=2.9264488

Batch 16650, train_perplexity=18.661243, train_loss=2.9264488

Batch 16660, train_perplexity=18.661234, train_loss=2.9264483

Batch 16670, train_perplexity=18.661243, train_loss=2.9264488
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 16680, train_perplexity=18.66124, train_loss=2.9264486

Batch 16690, train_perplexity=18.661243, train_loss=2.9264488

Batch 16700, train_perplexity=18.661243, train_loss=2.9264488

Batch 16710, train_perplexity=18.661234, train_loss=2.9264483

Batch 16720, train_perplexity=18.66124, train_loss=2.9264486

Batch 16730, train_perplexity=18.661234, train_loss=2.9264483

Batch 16740, train_perplexity=18.661243, train_loss=2.9264488

Batch 16750, train_perplexity=18.66124, train_loss=2.9264486

Batch 16760, train_perplexity=18.661243, train_loss=2.9264488

Batch 16770, train_perplexity=18.661234, train_loss=2.9264483

Batch 16780, train_perplexity=18.661234, train_loss=2.9264483

Batch 16790, train_perplexity=18.661234, train_loss=2.9264483

Batch 16800, train_perplexity=18.661234, train_loss=2.9264483

Batch 16810, train_perplexity=18.661234, train_loss=2.9264483

Batch 16820, train_perplexity=18.661234, train_loss=2.9264483

Batch 16830, train_perplexity=18.661234, train_loss=2.9264483

Batch 16840, train_perplexity=18.661234, train_loss=2.9264483

Batch 16850, train_perplexity=18.661234, train_loss=2.9264483

Batch 16860, train_perplexity=18.661234, train_loss=2.9264483

Batch 16870, train_perplexity=18.661234, train_loss=2.9264483

Batch 16880, train_perplexity=18.661234, train_loss=2.9264483

Batch 16890, train_perplexity=18.66124, train_loss=2.9264486

Batch 16900, train_perplexity=18.661234, train_loss=2.9264483

Batch 16910, train_perplexity=18.661234, train_loss=2.9264483

Batch 16920, train_perplexity=18.661234, train_loss=2.9264483

Batch 16930, train_perplexity=18.66123, train_loss=2.926448

Batch 16940, train_perplexity=18.661234, train_loss=2.9264483

Batch 16950, train_perplexity=18.661234, train_loss=2.9264483

Batch 16960, train_perplexity=18.66123, train_loss=2.926448

Batch 16970, train_perplexity=18.66123, train_loss=2.926448

Batch 16980, train_perplexity=18.661234, train_loss=2.9264483

Batch 16990, train_perplexity=18.661234, train_loss=2.9264483

Batch 17000, train_perplexity=18.66123, train_loss=2.926448

Batch 17010, train_perplexity=18.661234, train_loss=2.9264483

Batch 17020, train_perplexity=18.661226, train_loss=2.9264479

Batch 17030, train_perplexity=18.66123, train_loss=2.926448

Batch 17040, train_perplexity=18.661226, train_loss=2.9264479

Batch 17050, train_perplexity=18.661226, train_loss=2.9264479

Batch 17060, train_perplexity=18.66123, train_loss=2.926448

Batch 17070, train_perplexity=18.66123, train_loss=2.926448

Batch 17080, train_perplexity=18.66123, train_loss=2.926448

Batch 17090, train_perplexity=18.66123, train_loss=2.926448

Batch 17100, train_perplexity=18.661226, train_loss=2.9264479

Batch 17110, train_perplexity=18.661226, train_loss=2.9264479

Batch 17120, train_perplexity=18.661226, train_loss=2.9264479

Batch 17130, train_perplexity=18.661226, train_loss=2.9264479

Batch 17140, train_perplexity=18.66122, train_loss=2.9264476

Batch 17150, train_perplexity=18.661226, train_loss=2.9264479

Batch 17160, train_perplexity=18.66122, train_loss=2.9264476

Batch 17170, train_perplexity=18.661217, train_loss=2.9264474

Batch 17180, train_perplexity=18.661226, train_loss=2.9264479

Batch 17190, train_perplexity=18.661217, train_loss=2.9264474

Batch 17200, train_perplexity=18.66122, train_loss=2.9264476

Batch 17210, train_perplexity=18.66122, train_loss=2.9264476

Batch 17220, train_perplexity=18.661217, train_loss=2.9264474

Batch 17230, train_perplexity=18.66122, train_loss=2.9264476

Batch 17240, train_perplexity=18.661217, train_loss=2.9264474

Batch 17250, train_perplexity=18.661217, train_loss=2.9264474

Batch 17260, train_perplexity=18.661217, train_loss=2.9264474

Batch 17270, train_perplexity=18.661217, train_loss=2.9264474

Batch 17280, train_perplexity=18.661217, train_loss=2.9264474

Batch 17290, train_perplexity=18.66122, train_loss=2.9264476

Batch 17300, train_perplexity=18.661217, train_loss=2.9264474

Batch 17310, train_perplexity=18.661217, train_loss=2.9264474

Batch 17320, train_perplexity=18.661217, train_loss=2.9264474

Batch 17330, train_perplexity=18.661217, train_loss=2.9264474

Batch 17340, train_perplexity=18.661217, train_loss=2.9264474

Batch 17350, train_perplexity=18.661217, train_loss=2.9264474

Batch 17360, train_perplexity=18.661217, train_loss=2.9264474

Batch 17370, train_perplexity=18.661217, train_loss=2.9264474

Batch 17380, train_perplexity=18.661217, train_loss=2.9264474

Batch 17390, train_perplexity=18.661217, train_loss=2.9264474

Batch 17400, train_perplexity=18.661217, train_loss=2.9264474

Batch 17410, train_perplexity=18.661213, train_loss=2.9264472

Batch 17420, train_perplexity=18.661217, train_loss=2.9264474

Batch 17430, train_perplexity=18.661217, train_loss=2.9264474

Batch 17440, train_perplexity=18.661217, train_loss=2.9264474

Batch 17450, train_perplexity=18.661213, train_loss=2.9264472

Batch 17460, train_perplexity=18.661217, train_loss=2.9264474

Batch 17470, train_perplexity=18.661213, train_loss=2.9264472

Batch 17480, train_perplexity=18.661217, train_loss=2.9264474

Batch 17490, train_perplexity=18.661217, train_loss=2.9264474

Batch 17500, train_perplexity=18.661213, train_loss=2.9264472

Batch 17510, train_perplexity=18.661213, train_loss=2.9264472

Batch 17520, train_perplexity=18.661207, train_loss=2.926447

Batch 17530, train_perplexity=18.661207, train_loss=2.926447

Batch 17540, train_perplexity=18.661217, train_loss=2.9264474

Batch 17550, train_perplexity=18.661217, train_loss=2.9264474

Batch 17560, train_perplexity=18.661207, train_loss=2.926447

Batch 17570, train_perplexity=18.661217, train_loss=2.9264474

Batch 17580, train_perplexity=18.661213, train_loss=2.9264472

Batch 17590, train_perplexity=18.661207, train_loss=2.926447

Batch 17600, train_perplexity=18.661207, train_loss=2.926447

Batch 17610, train_perplexity=18.661207, train_loss=2.926447

Batch 17620, train_perplexity=18.661207, train_loss=2.926447

Batch 17630, train_perplexity=18.661207, train_loss=2.926447

Batch 17640, train_perplexity=18.661207, train_loss=2.926447

Batch 17650, train_perplexity=18.661207, train_loss=2.926447

Batch 17660, train_perplexity=18.661207, train_loss=2.926447

Batch 17670, train_perplexity=18.661207, train_loss=2.926447

Batch 17680, train_perplexity=18.661207, train_loss=2.926447

Batch 17690, train_perplexity=18.661207, train_loss=2.926447

Batch 17700, train_perplexity=18.661207, train_loss=2.926447

Batch 17710, train_perplexity=18.661207, train_loss=2.926447

Batch 17720, train_perplexity=18.661203, train_loss=2.9264467

Batch 17730, train_perplexity=18.661207, train_loss=2.926447

Batch 17740, train_perplexity=18.661203, train_loss=2.9264467

Batch 17750, train_perplexity=18.661207, train_loss=2.926447

Batch 17760, train_perplexity=18.661203, train_loss=2.9264467

Batch 17770, train_perplexity=18.661203, train_loss=2.9264467

Batch 17780, train_perplexity=18.661207, train_loss=2.926447

Batch 17790, train_perplexity=18.6612, train_loss=2.9264464

Batch 17800, train_perplexity=18.661207, train_loss=2.926447

Batch 17810, train_perplexity=18.6612, train_loss=2.9264464

Batch 17820, train_perplexity=18.661203, train_loss=2.9264467

Batch 17830, train_perplexity=18.661207, train_loss=2.926447

Batch 17840, train_perplexity=18.661203, train_loss=2.9264467

Batch 17850, train_perplexity=18.6612, train_loss=2.9264464

Batch 17860, train_perplexity=18.6612, train_loss=2.9264464

Batch 17870, train_perplexity=18.6612, train_loss=2.9264464

Batch 17880, train_perplexity=18.6612, train_loss=2.9264464

Batch 17890, train_perplexity=18.661203, train_loss=2.9264467

Batch 17900, train_perplexity=18.6612, train_loss=2.9264464

Batch 17910, train_perplexity=18.6612, train_loss=2.9264464

Batch 17920, train_perplexity=18.6612, train_loss=2.9264464

Batch 17930, train_perplexity=18.6612, train_loss=2.9264464

Batch 17940, train_perplexity=18.6612, train_loss=2.9264464

Batch 17950, train_perplexity=18.6612, train_loss=2.9264464

Batch 17960, train_perplexity=18.6612, train_loss=2.9264464

Batch 17970, train_perplexity=18.6612, train_loss=2.9264464

Batch 17980, train_perplexity=18.6612, train_loss=2.9264464

Batch 17990, train_perplexity=18.6612, train_loss=2.9264464
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 18000, train_perplexity=18.6612, train_loss=2.9264464

Batch 18010, train_perplexity=18.6612, train_loss=2.9264464

Batch 18020, train_perplexity=18.6612, train_loss=2.9264464

Batch 18030, train_perplexity=18.6612, train_loss=2.9264464

Batch 18040, train_perplexity=18.6612, train_loss=2.9264464

Batch 18050, train_perplexity=18.6612, train_loss=2.9264464

Batch 18060, train_perplexity=18.6612, train_loss=2.9264464

Batch 18070, train_perplexity=18.6612, train_loss=2.9264464

Batch 18080, train_perplexity=18.6612, train_loss=2.9264464

Batch 18090, train_perplexity=18.6612, train_loss=2.9264464

Batch 18100, train_perplexity=18.6612, train_loss=2.9264464

Batch 18110, train_perplexity=18.6612, train_loss=2.9264464

Batch 18120, train_perplexity=18.6612, train_loss=2.9264464

Batch 18130, train_perplexity=18.661194, train_loss=2.9264462

Batch 18140, train_perplexity=18.6612, train_loss=2.9264464

Batch 18150, train_perplexity=18.6612, train_loss=2.9264464

Batch 18160, train_perplexity=18.66119, train_loss=2.926446

Batch 18170, train_perplexity=18.661194, train_loss=2.9264462

Batch 18180, train_perplexity=18.6612, train_loss=2.9264464

Batch 18190, train_perplexity=18.66119, train_loss=2.926446

Batch 18200, train_perplexity=18.66119, train_loss=2.926446

Batch 18210, train_perplexity=18.661194, train_loss=2.9264462

Batch 18220, train_perplexity=18.66119, train_loss=2.926446

Batch 18230, train_perplexity=18.66119, train_loss=2.926446

Batch 18240, train_perplexity=18.661194, train_loss=2.9264462

Batch 18250, train_perplexity=18.6612, train_loss=2.9264464

Batch 18260, train_perplexity=18.661194, train_loss=2.9264462

Batch 18270, train_perplexity=18.66119, train_loss=2.926446

Batch 18280, train_perplexity=18.661194, train_loss=2.9264462

Batch 18290, train_perplexity=18.66119, train_loss=2.926446

Batch 18300, train_perplexity=18.66119, train_loss=2.926446

Batch 18310, train_perplexity=18.661186, train_loss=2.9264457

Batch 18320, train_perplexity=18.66119, train_loss=2.926446

Batch 18330, train_perplexity=18.661186, train_loss=2.9264457

Batch 18340, train_perplexity=18.66118, train_loss=2.9264455

Batch 18350, train_perplexity=18.66119, train_loss=2.926446

Batch 18360, train_perplexity=18.66119, train_loss=2.926446

Batch 18370, train_perplexity=18.66119, train_loss=2.926446

Batch 18380, train_perplexity=18.66118, train_loss=2.9264455

Batch 18390, train_perplexity=18.661186, train_loss=2.9264457

Batch 18400, train_perplexity=18.66118, train_loss=2.9264455

Batch 18410, train_perplexity=18.66118, train_loss=2.9264455

Batch 18420, train_perplexity=18.66118, train_loss=2.9264455

Batch 18430, train_perplexity=18.66118, train_loss=2.9264455

Batch 18440, train_perplexity=18.66118, train_loss=2.9264455

Batch 18450, train_perplexity=18.661186, train_loss=2.9264457

Batch 18460, train_perplexity=18.661186, train_loss=2.9264457

Batch 18470, train_perplexity=18.66118, train_loss=2.9264455

Batch 18480, train_perplexity=18.66118, train_loss=2.9264455

Batch 18490, train_perplexity=18.661186, train_loss=2.9264457

Batch 18500, train_perplexity=18.661186, train_loss=2.9264457

Batch 18510, train_perplexity=18.66118, train_loss=2.9264455

Batch 18520, train_perplexity=18.66118, train_loss=2.9264455

Batch 18530, train_perplexity=18.661186, train_loss=2.9264457

Batch 18540, train_perplexity=18.66118, train_loss=2.9264455

Batch 18550, train_perplexity=18.66118, train_loss=2.9264455

Batch 18560, train_perplexity=18.66118, train_loss=2.9264455

Batch 18570, train_perplexity=18.66118, train_loss=2.9264455

Batch 18580, train_perplexity=18.66118, train_loss=2.9264455

Batch 18590, train_perplexity=18.66118, train_loss=2.9264455

Batch 18600, train_perplexity=18.66118, train_loss=2.9264455

Batch 18610, train_perplexity=18.66118, train_loss=2.9264455

Batch 18620, train_perplexity=18.66118, train_loss=2.9264455

Batch 18630, train_perplexity=18.66118, train_loss=2.9264455

Batch 18640, train_perplexity=18.66118, train_loss=2.9264455

Batch 18650, train_perplexity=18.66118, train_loss=2.9264455

Batch 18660, train_perplexity=18.66118, train_loss=2.9264455

Batch 18670, train_perplexity=18.66118, train_loss=2.9264455

Batch 18680, train_perplexity=18.66118, train_loss=2.9264455

Batch 18690, train_perplexity=18.66118, train_loss=2.9264455

Batch 18700, train_perplexity=18.661173, train_loss=2.926445

Batch 18710, train_perplexity=18.661177, train_loss=2.9264452

Batch 18720, train_perplexity=18.66118, train_loss=2.9264455

Batch 18730, train_perplexity=18.661177, train_loss=2.9264452

Batch 18740, train_perplexity=18.661173, train_loss=2.926445

Batch 18750, train_perplexity=18.661177, train_loss=2.9264452

Batch 18760, train_perplexity=18.661177, train_loss=2.9264452

Batch 18770, train_perplexity=18.661173, train_loss=2.926445

Batch 18780, train_perplexity=18.66118, train_loss=2.9264455

Batch 18790, train_perplexity=18.661173, train_loss=2.926445

Batch 18800, train_perplexity=18.661177, train_loss=2.9264452

Batch 18810, train_perplexity=18.661173, train_loss=2.926445

Batch 18820, train_perplexity=18.661173, train_loss=2.926445

Batch 18830, train_perplexity=18.661177, train_loss=2.9264452

Batch 18840, train_perplexity=18.661173, train_loss=2.926445

Batch 18850, train_perplexity=18.661173, train_loss=2.926445

Batch 18860, train_perplexity=18.661173, train_loss=2.926445

Batch 18870, train_perplexity=18.661173, train_loss=2.926445

Batch 18880, train_perplexity=18.661173, train_loss=2.926445

Batch 18890, train_perplexity=18.661173, train_loss=2.926445

Batch 18900, train_perplexity=18.661173, train_loss=2.926445

Batch 18910, train_perplexity=18.661173, train_loss=2.926445

Batch 18920, train_perplexity=18.661173, train_loss=2.926445

Batch 18930, train_perplexity=18.661173, train_loss=2.926445

Batch 18940, train_perplexity=18.661173, train_loss=2.926445

Batch 18950, train_perplexity=18.661173, train_loss=2.926445

Batch 18960, train_perplexity=18.661173, train_loss=2.926445

Batch 18970, train_perplexity=18.661173, train_loss=2.926445

Batch 18980, train_perplexity=18.661173, train_loss=2.926445

Batch 18990, train_perplexity=18.661173, train_loss=2.926445

Batch 19000, train_perplexity=18.661173, train_loss=2.926445

Batch 19010, train_perplexity=18.661167, train_loss=2.9264448

Batch 19020, train_perplexity=18.661173, train_loss=2.926445

Batch 19030, train_perplexity=18.661173, train_loss=2.926445

Batch 19040, train_perplexity=18.661173, train_loss=2.926445

Batch 19050, train_perplexity=18.661167, train_loss=2.9264448

Batch 19060, train_perplexity=18.661173, train_loss=2.926445

Batch 19070, train_perplexity=18.661173, train_loss=2.926445

Batch 19080, train_perplexity=18.661167, train_loss=2.9264448

Batch 19090, train_perplexity=18.661167, train_loss=2.9264448

Batch 19100, train_perplexity=18.661163, train_loss=2.9264445

Batch 19110, train_perplexity=18.661167, train_loss=2.9264448

Batch 19120, train_perplexity=18.661163, train_loss=2.9264445

Batch 19130, train_perplexity=18.661173, train_loss=2.926445

Batch 19140, train_perplexity=18.661163, train_loss=2.9264445

Batch 19150, train_perplexity=18.661167, train_loss=2.9264448

Batch 19160, train_perplexity=18.661173, train_loss=2.926445

Batch 19170, train_perplexity=18.661167, train_loss=2.9264448

Batch 19180, train_perplexity=18.661163, train_loss=2.9264445

Batch 19190, train_perplexity=18.661167, train_loss=2.9264448

Batch 19200, train_perplexity=18.661163, train_loss=2.9264445

Batch 19210, train_perplexity=18.661163, train_loss=2.9264445

Batch 19220, train_perplexity=18.661163, train_loss=2.9264445

Batch 19230, train_perplexity=18.661167, train_loss=2.9264448

Batch 19240, train_perplexity=18.661163, train_loss=2.9264445

Batch 19250, train_perplexity=18.661163, train_loss=2.9264445

Batch 19260, train_perplexity=18.661163, train_loss=2.9264445

Batch 19270, train_perplexity=18.661163, train_loss=2.9264445

Batch 19280, train_perplexity=18.661163, train_loss=2.9264445

Batch 19290, train_perplexity=18.661163, train_loss=2.9264445

Batch 19300, train_perplexity=18.661163, train_loss=2.9264445

Batch 19310, train_perplexity=18.661163, train_loss=2.9264445
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19320, train_perplexity=18.661163, train_loss=2.9264445

Batch 19330, train_perplexity=18.661163, train_loss=2.9264445

Batch 19340, train_perplexity=18.661163, train_loss=2.9264445

Batch 19350, train_perplexity=18.661163, train_loss=2.9264445

Batch 19360, train_perplexity=18.661163, train_loss=2.9264445

Batch 19370, train_perplexity=18.661163, train_loss=2.9264445

Batch 19380, train_perplexity=18.661163, train_loss=2.9264445

Batch 19390, train_perplexity=18.661163, train_loss=2.9264445

Batch 19400, train_perplexity=18.66116, train_loss=2.9264443

Batch 19410, train_perplexity=18.661163, train_loss=2.9264445

Batch 19420, train_perplexity=18.66116, train_loss=2.9264443

Batch 19430, train_perplexity=18.661154, train_loss=2.926444

Batch 19440, train_perplexity=18.66116, train_loss=2.9264443

Batch 19450, train_perplexity=18.661163, train_loss=2.9264445

Batch 19460, train_perplexity=18.661154, train_loss=2.926444

Batch 19470, train_perplexity=18.66116, train_loss=2.9264443

Batch 19480, train_perplexity=18.661154, train_loss=2.926444

Batch 19490, train_perplexity=18.661154, train_loss=2.926444

Batch 19500, train_perplexity=18.661154, train_loss=2.926444

Batch 19510, train_perplexity=18.661154, train_loss=2.926444

Batch 19520, train_perplexity=18.66116, train_loss=2.9264443

Batch 19530, train_perplexity=18.661154, train_loss=2.926444

Batch 19540, train_perplexity=18.661154, train_loss=2.926444

Batch 19550, train_perplexity=18.661154, train_loss=2.926444

Batch 19560, train_perplexity=18.661154, train_loss=2.926444

Batch 19570, train_perplexity=18.66115, train_loss=2.9264438

Batch 19580, train_perplexity=18.661154, train_loss=2.926444

Batch 19590, train_perplexity=18.661154, train_loss=2.926444

Batch 19600, train_perplexity=18.66115, train_loss=2.9264438

Batch 19610, train_perplexity=18.66115, train_loss=2.9264438

Batch 19620, train_perplexity=18.661154, train_loss=2.926444

Batch 19630, train_perplexity=18.66115, train_loss=2.9264438

Batch 19640, train_perplexity=18.66115, train_loss=2.9264438

Batch 19650, train_perplexity=18.66115, train_loss=2.9264438

Batch 19660, train_perplexity=18.66115, train_loss=2.9264438

Batch 19670, train_perplexity=18.661146, train_loss=2.9264436

Batch 19680, train_perplexity=18.661154, train_loss=2.926444

Batch 19690, train_perplexity=18.661146, train_loss=2.9264436

Batch 19700, train_perplexity=18.661146, train_loss=2.9264436

Batch 19710, train_perplexity=18.66115, train_loss=2.9264438

Batch 19720, train_perplexity=18.661146, train_loss=2.9264436

Batch 19730, train_perplexity=18.66115, train_loss=2.9264438

Batch 19740, train_perplexity=18.661146, train_loss=2.9264436

Batch 19750, train_perplexity=18.661146, train_loss=2.9264436

Batch 19760, train_perplexity=18.661146, train_loss=2.9264436

Batch 19770, train_perplexity=18.661146, train_loss=2.9264436

Batch 19780, train_perplexity=18.661146, train_loss=2.9264436

Batch 19790, train_perplexity=18.661146, train_loss=2.9264436

Batch 19800, train_perplexity=18.66115, train_loss=2.9264438

Batch 19810, train_perplexity=18.661146, train_loss=2.9264436

Batch 19820, train_perplexity=18.661146, train_loss=2.9264436

Batch 19830, train_perplexity=18.66115, train_loss=2.9264438

Batch 19840, train_perplexity=18.661146, train_loss=2.9264436

Batch 19850, train_perplexity=18.661146, train_loss=2.9264436

Batch 19860, train_perplexity=18.661146, train_loss=2.9264436

Batch 19870, train_perplexity=18.661146, train_loss=2.9264436

Batch 19880, train_perplexity=18.661146, train_loss=2.9264436

Batch 19890, train_perplexity=18.661146, train_loss=2.9264436

Batch 19900, train_perplexity=18.661146, train_loss=2.9264436

Batch 19910, train_perplexity=18.661146, train_loss=2.9264436

Batch 19920, train_perplexity=18.661146, train_loss=2.9264436

Batch 19930, train_perplexity=18.661146, train_loss=2.9264436

Batch 19940, train_perplexity=18.66114, train_loss=2.9264433

Batch 19950, train_perplexity=18.661146, train_loss=2.9264436

Batch 19960, train_perplexity=18.661146, train_loss=2.9264436

Batch 19970, train_perplexity=18.661146, train_loss=2.9264436

Batch 19980, train_perplexity=18.661146, train_loss=2.9264436

Batch 19990, train_perplexity=18.661146, train_loss=2.9264436

Batch 20000, train_perplexity=18.661146, train_loss=2.9264436

Batch 20010, train_perplexity=18.661146, train_loss=2.9264436

Batch 20020, train_perplexity=18.661146, train_loss=2.9264436

Batch 20030, train_perplexity=18.661146, train_loss=2.9264436

Batch 20040, train_perplexity=18.66114, train_loss=2.9264433

Batch 20050, train_perplexity=18.661146, train_loss=2.9264436

Batch 20060, train_perplexity=18.661146, train_loss=2.9264436

Batch 20070, train_perplexity=18.661146, train_loss=2.9264436

Batch 20080, train_perplexity=18.66114, train_loss=2.9264433

Batch 20090, train_perplexity=18.661146, train_loss=2.9264436

Batch 20100, train_perplexity=18.66114, train_loss=2.9264433

Batch 20110, train_perplexity=18.66114, train_loss=2.9264433

Batch 20120, train_perplexity=18.661137, train_loss=2.926443

Batch 20130, train_perplexity=18.66114, train_loss=2.9264433

Batch 20140, train_perplexity=18.661137, train_loss=2.926443

Batch 20150, train_perplexity=18.66114, train_loss=2.9264433

Batch 20160, train_perplexity=18.66114, train_loss=2.9264433

Batch 20170, train_perplexity=18.661137, train_loss=2.926443

Batch 20180, train_perplexity=18.661137, train_loss=2.926443

Batch 20190, train_perplexity=18.661137, train_loss=2.926443

Batch 20200, train_perplexity=18.66114, train_loss=2.9264433

Batch 20210, train_perplexity=18.661137, train_loss=2.926443

Batch 20220, train_perplexity=18.661133, train_loss=2.9264429

Batch 20230, train_perplexity=18.661137, train_loss=2.926443

Batch 20240, train_perplexity=18.66114, train_loss=2.9264433

Batch 20250, train_perplexity=18.661137, train_loss=2.926443

Batch 20260, train_perplexity=18.661137, train_loss=2.926443

Batch 20270, train_perplexity=18.661137, train_loss=2.926443

Batch 20280, train_perplexity=18.661137, train_loss=2.926443

Batch 20290, train_perplexity=18.661137, train_loss=2.926443

Batch 20300, train_perplexity=18.661137, train_loss=2.926443

Batch 20310, train_perplexity=18.661133, train_loss=2.9264429

Batch 20320, train_perplexity=18.661137, train_loss=2.926443

Batch 20330, train_perplexity=18.661137, train_loss=2.926443

Batch 20340, train_perplexity=18.661137, train_loss=2.926443

Batch 20350, train_perplexity=18.661133, train_loss=2.9264429

Batch 20360, train_perplexity=18.661133, train_loss=2.9264429

Batch 20370, train_perplexity=18.661137, train_loss=2.926443

Batch 20380, train_perplexity=18.661137, train_loss=2.926443

Batch 20390, train_perplexity=18.661137, train_loss=2.926443

Batch 20400, train_perplexity=18.661137, train_loss=2.926443

Batch 20410, train_perplexity=18.661137, train_loss=2.926443

Batch 20420, train_perplexity=18.661137, train_loss=2.926443

Batch 20430, train_perplexity=18.661137, train_loss=2.926443

Batch 20440, train_perplexity=18.661127, train_loss=2.9264426

Batch 20450, train_perplexity=18.661133, train_loss=2.9264429

Batch 20460, train_perplexity=18.661133, train_loss=2.9264429

Batch 20470, train_perplexity=18.661137, train_loss=2.926443

Batch 20480, train_perplexity=18.661133, train_loss=2.9264429

Batch 20490, train_perplexity=18.661133, train_loss=2.9264429

Batch 20500, train_perplexity=18.661127, train_loss=2.9264426

Batch 20510, train_perplexity=18.661133, train_loss=2.9264429

Batch 20520, train_perplexity=18.661127, train_loss=2.9264426

Batch 20530, train_perplexity=18.661133, train_loss=2.9264429

Batch 20540, train_perplexity=18.661127, train_loss=2.9264426

Batch 20550, train_perplexity=18.661127, train_loss=2.9264426

Batch 20560, train_perplexity=18.661133, train_loss=2.9264429

Batch 20570, train_perplexity=18.661133, train_loss=2.9264429

Batch 20580, train_perplexity=18.661127, train_loss=2.9264426

Batch 20590, train_perplexity=18.661127, train_loss=2.9264426

Batch 20600, train_perplexity=18.661127, train_loss=2.9264426

Batch 20610, train_perplexity=18.661127, train_loss=2.9264426

Batch 20620, train_perplexity=18.661133, train_loss=2.9264429

Batch 20630, train_perplexity=18.661127, train_loss=2.9264426
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 20640, train_perplexity=18.661127, train_loss=2.9264426

Batch 20650, train_perplexity=18.661127, train_loss=2.9264426

Batch 20660, train_perplexity=18.661127, train_loss=2.9264426

Batch 20670, train_perplexity=18.661127, train_loss=2.9264426

Batch 20680, train_perplexity=18.661127, train_loss=2.9264426

Batch 20690, train_perplexity=18.661127, train_loss=2.9264426

Batch 20700, train_perplexity=18.661127, train_loss=2.9264426

Batch 20710, train_perplexity=18.661127, train_loss=2.9264426

Batch 20720, train_perplexity=18.661127, train_loss=2.9264426

Batch 20730, train_perplexity=18.661127, train_loss=2.9264426

Batch 20740, train_perplexity=18.661127, train_loss=2.9264426

Batch 20750, train_perplexity=18.661123, train_loss=2.9264424

Batch 20760, train_perplexity=18.661123, train_loss=2.9264424

Batch 20770, train_perplexity=18.661127, train_loss=2.9264426

Batch 20780, train_perplexity=18.661123, train_loss=2.9264424

Batch 20790, train_perplexity=18.661123, train_loss=2.9264424

Batch 20800, train_perplexity=18.661123, train_loss=2.9264424

Batch 20810, train_perplexity=18.661127, train_loss=2.9264426

Batch 20820, train_perplexity=18.66112, train_loss=2.9264421

Batch 20830, train_perplexity=18.661123, train_loss=2.9264424

Batch 20840, train_perplexity=18.66112, train_loss=2.9264421

Batch 20850, train_perplexity=18.661123, train_loss=2.9264424

Batch 20860, train_perplexity=18.661123, train_loss=2.9264424

Batch 20870, train_perplexity=18.66112, train_loss=2.9264421

Batch 20880, train_perplexity=18.66112, train_loss=2.9264421

Batch 20890, train_perplexity=18.66112, train_loss=2.9264421

Batch 20900, train_perplexity=18.661123, train_loss=2.9264424

Batch 20910, train_perplexity=18.661123, train_loss=2.9264424

Batch 20920, train_perplexity=18.66112, train_loss=2.9264421

Batch 20930, train_perplexity=18.66112, train_loss=2.9264421

Batch 20940, train_perplexity=18.66112, train_loss=2.9264421

Batch 20950, train_perplexity=18.66112, train_loss=2.9264421

Batch 20960, train_perplexity=18.66112, train_loss=2.9264421

Batch 20970, train_perplexity=18.66112, train_loss=2.9264421

Batch 20980, train_perplexity=18.66112, train_loss=2.9264421

Batch 20990, train_perplexity=18.66112, train_loss=2.9264421

Batch 21000, train_perplexity=18.66112, train_loss=2.9264421

Batch 21010, train_perplexity=18.66112, train_loss=2.9264421

Batch 21020, train_perplexity=18.66112, train_loss=2.9264421

Batch 21030, train_perplexity=18.661123, train_loss=2.9264424

Batch 21040, train_perplexity=18.66112, train_loss=2.9264421

Batch 21050, train_perplexity=18.66112, train_loss=2.9264421

Batch 21060, train_perplexity=18.66112, train_loss=2.9264421

Batch 21070, train_perplexity=18.66112, train_loss=2.9264421

Batch 21080, train_perplexity=18.66112, train_loss=2.9264421

Batch 21090, train_perplexity=18.661114, train_loss=2.926442

Batch 21100, train_perplexity=18.66112, train_loss=2.9264421

Batch 21110, train_perplexity=18.66112, train_loss=2.9264421

Batch 21120, train_perplexity=18.66112, train_loss=2.9264421

Batch 21130, train_perplexity=18.661114, train_loss=2.926442

Batch 21140, train_perplexity=18.66112, train_loss=2.9264421

Batch 21150, train_perplexity=18.66112, train_loss=2.9264421

Batch 21160, train_perplexity=18.66112, train_loss=2.9264421

Batch 21170, train_perplexity=18.66112, train_loss=2.9264421

Batch 21180, train_perplexity=18.66112, train_loss=2.9264421

Batch 21190, train_perplexity=18.66111, train_loss=2.9264417

Batch 21200, train_perplexity=18.66112, train_loss=2.9264421

Batch 21210, train_perplexity=18.66112, train_loss=2.9264421

Batch 21220, train_perplexity=18.66112, train_loss=2.9264421

Batch 21230, train_perplexity=18.66112, train_loss=2.9264421

Batch 21240, train_perplexity=18.66112, train_loss=2.9264421

Batch 21250, train_perplexity=18.66112, train_loss=2.9264421

Batch 21260, train_perplexity=18.661114, train_loss=2.926442

Batch 21270, train_perplexity=18.66112, train_loss=2.9264421

Batch 21280, train_perplexity=18.66112, train_loss=2.9264421

Batch 21290, train_perplexity=18.66112, train_loss=2.9264421

Batch 21300, train_perplexity=18.66111, train_loss=2.9264417

Batch 21310, train_perplexity=18.661114, train_loss=2.926442

Batch 21320, train_perplexity=18.66112, train_loss=2.9264421

Batch 21330, train_perplexity=18.661114, train_loss=2.926442

Batch 21340, train_perplexity=18.66112, train_loss=2.9264421

Batch 21350, train_perplexity=18.661114, train_loss=2.926442

Batch 21360, train_perplexity=18.661114, train_loss=2.926442

Batch 21370, train_perplexity=18.66112, train_loss=2.9264421

Batch 21380, train_perplexity=18.66112, train_loss=2.9264421

Batch 21390, train_perplexity=18.66111, train_loss=2.9264417

Batch 21400, train_perplexity=18.66111, train_loss=2.9264417

Batch 21410, train_perplexity=18.66111, train_loss=2.9264417

Batch 21420, train_perplexity=18.66111, train_loss=2.9264417

Batch 21430, train_perplexity=18.66111, train_loss=2.9264417

Batch 21440, train_perplexity=18.661114, train_loss=2.926442

Batch 21450, train_perplexity=18.66111, train_loss=2.9264417

Batch 21460, train_perplexity=18.66111, train_loss=2.9264417

Batch 21470, train_perplexity=18.661114, train_loss=2.926442

Batch 21480, train_perplexity=18.66111, train_loss=2.9264417

Batch 21490, train_perplexity=18.66111, train_loss=2.9264417

Batch 21500, train_perplexity=18.66111, train_loss=2.9264417

Batch 21510, train_perplexity=18.661114, train_loss=2.926442

Batch 21520, train_perplexity=18.66111, train_loss=2.9264417

Batch 21530, train_perplexity=18.66111, train_loss=2.9264417

Batch 21540, train_perplexity=18.66111, train_loss=2.9264417

Batch 21550, train_perplexity=18.66111, train_loss=2.9264417

Batch 21560, train_perplexity=18.66111, train_loss=2.9264417

Batch 21570, train_perplexity=18.66111, train_loss=2.9264417

Batch 21580, train_perplexity=18.66111, train_loss=2.9264417

Batch 21590, train_perplexity=18.66111, train_loss=2.9264417

Batch 21600, train_perplexity=18.66111, train_loss=2.9264417

Batch 21610, train_perplexity=18.661106, train_loss=2.9264414

Batch 21620, train_perplexity=18.66111, train_loss=2.9264417

Batch 21630, train_perplexity=18.661106, train_loss=2.9264414

Batch 21640, train_perplexity=18.66111, train_loss=2.9264417

Batch 21650, train_perplexity=18.661106, train_loss=2.9264414

Batch 21660, train_perplexity=18.66111, train_loss=2.9264417

Batch 21670, train_perplexity=18.6611, train_loss=2.9264412

Batch 21680, train_perplexity=18.6611, train_loss=2.9264412

Batch 21690, train_perplexity=18.661106, train_loss=2.9264414

Batch 21700, train_perplexity=18.6611, train_loss=2.9264412

Batch 21710, train_perplexity=18.661106, train_loss=2.9264414

Batch 21720, train_perplexity=18.661106, train_loss=2.9264414

Batch 21730, train_perplexity=18.6611, train_loss=2.9264412

Batch 21740, train_perplexity=18.6611, train_loss=2.9264412

Batch 21750, train_perplexity=18.6611, train_loss=2.9264412

Batch 21760, train_perplexity=18.6611, train_loss=2.9264412

Batch 21770, train_perplexity=18.6611, train_loss=2.9264412

Batch 21780, train_perplexity=18.661106, train_loss=2.9264414

Batch 21790, train_perplexity=18.661106, train_loss=2.9264414

Batch 21800, train_perplexity=18.6611, train_loss=2.9264412

Batch 21810, train_perplexity=18.6611, train_loss=2.9264412

Batch 21820, train_perplexity=18.66111, train_loss=2.9264417

Batch 21830, train_perplexity=18.6611, train_loss=2.9264412

Batch 21840, train_perplexity=18.6611, train_loss=2.9264412

Batch 21850, train_perplexity=18.6611, train_loss=2.9264412

Batch 21860, train_perplexity=18.6611, train_loss=2.9264412

Batch 21870, train_perplexity=18.6611, train_loss=2.9264412

Batch 21880, train_perplexity=18.6611, train_loss=2.9264412

Batch 21890, train_perplexity=18.66111, train_loss=2.9264417

Batch 21900, train_perplexity=18.6611, train_loss=2.9264412

Batch 21910, train_perplexity=18.6611, train_loss=2.9264412

Batch 21920, train_perplexity=18.6611, train_loss=2.9264412

Batch 21930, train_perplexity=18.6611, train_loss=2.9264412

Batch 21940, train_perplexity=18.6611, train_loss=2.9264412

Batch 21950, train_perplexity=18.661106, train_loss=2.9264414
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21960, train_perplexity=18.6611, train_loss=2.9264412

Batch 21970, train_perplexity=18.6611, train_loss=2.9264412

Batch 21980, train_perplexity=18.6611, train_loss=2.9264412

Batch 21990, train_perplexity=18.6611, train_loss=2.9264412

Batch 22000, train_perplexity=18.6611, train_loss=2.9264412

Batch 22010, train_perplexity=18.6611, train_loss=2.9264412

Batch 22020, train_perplexity=18.6611, train_loss=2.9264412

Batch 22030, train_perplexity=18.661097, train_loss=2.926441

Batch 22040, train_perplexity=18.6611, train_loss=2.9264412

Batch 22050, train_perplexity=18.6611, train_loss=2.9264412

Batch 22060, train_perplexity=18.6611, train_loss=2.9264412

Batch 22070, train_perplexity=18.6611, train_loss=2.9264412

Batch 22080, train_perplexity=18.6611, train_loss=2.9264412

Batch 22090, train_perplexity=18.6611, train_loss=2.9264412

Batch 22100, train_perplexity=18.6611, train_loss=2.9264412

Batch 22110, train_perplexity=18.6611, train_loss=2.9264412

Batch 22120, train_perplexity=18.6611, train_loss=2.9264412

Batch 22130, train_perplexity=18.6611, train_loss=2.9264412

Batch 22140, train_perplexity=18.6611, train_loss=2.9264412

Batch 22150, train_perplexity=18.6611, train_loss=2.9264412

Batch 22160, train_perplexity=18.6611, train_loss=2.9264412

Batch 22170, train_perplexity=18.6611, train_loss=2.9264412

Batch 22180, train_perplexity=18.6611, train_loss=2.9264412

Batch 22190, train_perplexity=18.6611, train_loss=2.9264412

Batch 22200, train_perplexity=18.6611, train_loss=2.9264412

Batch 22210, train_perplexity=18.6611, train_loss=2.9264412

Batch 22220, train_perplexity=18.6611, train_loss=2.9264412

Batch 22230, train_perplexity=18.6611, train_loss=2.9264412

Batch 22240, train_perplexity=18.661093, train_loss=2.9264407

Batch 22250, train_perplexity=18.6611, train_loss=2.9264412

Batch 22260, train_perplexity=18.661097, train_loss=2.926441

Batch 22270, train_perplexity=18.661097, train_loss=2.926441

Batch 22280, train_perplexity=18.661093, train_loss=2.9264407

Batch 22290, train_perplexity=18.661093, train_loss=2.9264407

Batch 22300, train_perplexity=18.661097, train_loss=2.926441

Batch 22310, train_perplexity=18.661093, train_loss=2.9264407

Batch 22320, train_perplexity=18.661097, train_loss=2.926441

Batch 22330, train_perplexity=18.661093, train_loss=2.9264407

Batch 22340, train_perplexity=18.661093, train_loss=2.9264407

Batch 22350, train_perplexity=18.661093, train_loss=2.9264407

Batch 22360, train_perplexity=18.661093, train_loss=2.9264407

Batch 22370, train_perplexity=18.661093, train_loss=2.9264407

Batch 22380, train_perplexity=18.661093, train_loss=2.9264407

Batch 22390, train_perplexity=18.661093, train_loss=2.9264407

Batch 22400, train_perplexity=18.661093, train_loss=2.9264407

Batch 22410, train_perplexity=18.661093, train_loss=2.9264407

Batch 22420, train_perplexity=18.661093, train_loss=2.9264407

Batch 22430, train_perplexity=18.661093, train_loss=2.9264407

Batch 22440, train_perplexity=18.661093, train_loss=2.9264407

Batch 22450, train_perplexity=18.661093, train_loss=2.9264407

Batch 22460, train_perplexity=18.661093, train_loss=2.9264407

Batch 22470, train_perplexity=18.661087, train_loss=2.9264405

Batch 22480, train_perplexity=18.661093, train_loss=2.9264407

Batch 22490, train_perplexity=18.661093, train_loss=2.9264407

Batch 22500, train_perplexity=18.661087, train_loss=2.9264405

Batch 22510, train_perplexity=18.661093, train_loss=2.9264407

Batch 22520, train_perplexity=18.661093, train_loss=2.9264407

Batch 22530, train_perplexity=18.661087, train_loss=2.9264405

Batch 22540, train_perplexity=18.661093, train_loss=2.9264407

Batch 22550, train_perplexity=18.661087, train_loss=2.9264405

Batch 22560, train_perplexity=18.661093, train_loss=2.9264407

Batch 22570, train_perplexity=18.661083, train_loss=2.9264402

Batch 22580, train_perplexity=18.661093, train_loss=2.9264407

Batch 22590, train_perplexity=18.661087, train_loss=2.9264405

Batch 22600, train_perplexity=18.661083, train_loss=2.9264402

Batch 22610, train_perplexity=18.661093, train_loss=2.9264407

Batch 22620, train_perplexity=18.661083, train_loss=2.9264402

Batch 22630, train_perplexity=18.661087, train_loss=2.9264405

Batch 22640, train_perplexity=18.661087, train_loss=2.9264405

Batch 22650, train_perplexity=18.661093, train_loss=2.9264407

Batch 22660, train_perplexity=18.661087, train_loss=2.9264405

Batch 22670, train_perplexity=18.661093, train_loss=2.9264407

Batch 22680, train_perplexity=18.661083, train_loss=2.9264402

Batch 22690, train_perplexity=18.661083, train_loss=2.9264402

Batch 22700, train_perplexity=18.661083, train_loss=2.9264402

Batch 22710, train_perplexity=18.661087, train_loss=2.9264405

Batch 22720, train_perplexity=18.661087, train_loss=2.9264405

Batch 22730, train_perplexity=18.661087, train_loss=2.9264405

Batch 22740, train_perplexity=18.661083, train_loss=2.9264402

Batch 22750, train_perplexity=18.661083, train_loss=2.9264402

Batch 22760, train_perplexity=18.661083, train_loss=2.9264402

Batch 22770, train_perplexity=18.661083, train_loss=2.9264402

Batch 22780, train_perplexity=18.661087, train_loss=2.9264405

Batch 22790, train_perplexity=18.661083, train_loss=2.9264402

Batch 22800, train_perplexity=18.661083, train_loss=2.9264402

Batch 22810, train_perplexity=18.661087, train_loss=2.9264405

Batch 22820, train_perplexity=18.661083, train_loss=2.9264402

Batch 22830, train_perplexity=18.661083, train_loss=2.9264402

Batch 22840, train_perplexity=18.661083, train_loss=2.9264402

Batch 22850, train_perplexity=18.661083, train_loss=2.9264402

Batch 22860, train_perplexity=18.661083, train_loss=2.9264402

Batch 22870, train_perplexity=18.66108, train_loss=2.92644

Batch 22880, train_perplexity=18.661083, train_loss=2.9264402

Batch 22890, train_perplexity=18.661083, train_loss=2.9264402

Batch 22900, train_perplexity=18.66108, train_loss=2.92644

Batch 22910, train_perplexity=18.661083, train_loss=2.9264402

Batch 22920, train_perplexity=18.661083, train_loss=2.9264402

Batch 22930, train_perplexity=18.661083, train_loss=2.9264402

Batch 22940, train_perplexity=18.66108, train_loss=2.92644

Batch 22950, train_perplexity=18.66108, train_loss=2.92644

Batch 22960, train_perplexity=18.66108, train_loss=2.92644

Batch 22970, train_perplexity=18.66108, train_loss=2.92644

Batch 22980, train_perplexity=18.66108, train_loss=2.92644

Batch 22990, train_perplexity=18.661083, train_loss=2.9264402

Batch 23000, train_perplexity=18.66108, train_loss=2.92644

Batch 23010, train_perplexity=18.661083, train_loss=2.9264402

Batch 23020, train_perplexity=18.661083, train_loss=2.9264402

Batch 23030, train_perplexity=18.661074, train_loss=2.9264398

Batch 23040, train_perplexity=18.66108, train_loss=2.92644

Batch 23050, train_perplexity=18.66108, train_loss=2.92644

Batch 23060, train_perplexity=18.661083, train_loss=2.9264402

Batch 23070, train_perplexity=18.661074, train_loss=2.9264398

Batch 23080, train_perplexity=18.661074, train_loss=2.9264398

Batch 23090, train_perplexity=18.661074, train_loss=2.9264398

Batch 23100, train_perplexity=18.661074, train_loss=2.9264398

Batch 23110, train_perplexity=18.661074, train_loss=2.9264398

Batch 23120, train_perplexity=18.661074, train_loss=2.9264398

Batch 23130, train_perplexity=18.661074, train_loss=2.9264398

Batch 23140, train_perplexity=18.66108, train_loss=2.92644

Batch 23150, train_perplexity=18.661074, train_loss=2.9264398

Batch 23160, train_perplexity=18.661074, train_loss=2.9264398

Batch 23170, train_perplexity=18.661074, train_loss=2.9264398

Batch 23180, train_perplexity=18.661074, train_loss=2.9264398

Batch 23190, train_perplexity=18.661074, train_loss=2.9264398

Batch 23200, train_perplexity=18.661074, train_loss=2.9264398

Batch 23210, train_perplexity=18.661074, train_loss=2.9264398

Batch 23220, train_perplexity=18.661074, train_loss=2.9264398

Batch 23230, train_perplexity=18.661074, train_loss=2.9264398

Batch 23240, train_perplexity=18.661074, train_loss=2.9264398

Batch 23250, train_perplexity=18.661074, train_loss=2.9264398

Batch 23260, train_perplexity=18.661074, train_loss=2.9264398

Batch 23270, train_perplexity=18.661074, train_loss=2.9264398
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 23280, train_perplexity=18.661074, train_loss=2.9264398

Batch 23290, train_perplexity=18.661074, train_loss=2.9264398

Batch 23300, train_perplexity=18.661074, train_loss=2.9264398

Batch 23310, train_perplexity=18.661074, train_loss=2.9264398

Batch 23320, train_perplexity=18.661074, train_loss=2.9264398

Batch 23330, train_perplexity=18.661074, train_loss=2.9264398

Batch 23340, train_perplexity=18.661074, train_loss=2.9264398

Batch 23350, train_perplexity=18.661074, train_loss=2.9264398

Batch 23360, train_perplexity=18.661074, train_loss=2.9264398

Batch 23370, train_perplexity=18.661074, train_loss=2.9264398

Batch 23380, train_perplexity=18.661074, train_loss=2.9264398

Batch 23390, train_perplexity=18.661074, train_loss=2.9264398

Batch 23400, train_perplexity=18.661074, train_loss=2.9264398

Batch 23410, train_perplexity=18.661074, train_loss=2.9264398

Batch 23420, train_perplexity=18.661074, train_loss=2.9264398

Batch 23430, train_perplexity=18.661074, train_loss=2.9264398

Batch 23440, train_perplexity=18.66107, train_loss=2.9264395

Batch 23450, train_perplexity=18.66107, train_loss=2.9264395

Batch 23460, train_perplexity=18.66107, train_loss=2.9264395

Batch 23470, train_perplexity=18.661074, train_loss=2.9264398

Batch 23480, train_perplexity=18.661074, train_loss=2.9264398

Batch 23490, train_perplexity=18.661074, train_loss=2.9264398

Batch 23500, train_perplexity=18.661074, train_loss=2.9264398

Batch 23510, train_perplexity=18.661074, train_loss=2.9264398

Batch 23520, train_perplexity=18.661074, train_loss=2.9264398

Batch 23530, train_perplexity=18.661074, train_loss=2.9264398

Batch 23540, train_perplexity=18.66107, train_loss=2.9264395

Batch 23550, train_perplexity=18.66107, train_loss=2.9264395

Batch 23560, train_perplexity=18.661074, train_loss=2.9264398

Batch 23570, train_perplexity=18.66107, train_loss=2.9264395

Batch 23580, train_perplexity=18.66107, train_loss=2.9264395

Batch 23590, train_perplexity=18.661074, train_loss=2.9264398

Batch 23600, train_perplexity=18.66107, train_loss=2.9264395

Batch 23610, train_perplexity=18.66107, train_loss=2.9264395

Batch 23620, train_perplexity=18.66107, train_loss=2.9264395

Batch 23630, train_perplexity=18.661074, train_loss=2.9264398

Batch 23640, train_perplexity=18.661074, train_loss=2.9264398

Batch 23650, train_perplexity=18.661074, train_loss=2.9264398

Batch 23660, train_perplexity=18.66107, train_loss=2.9264395

Batch 23670, train_perplexity=18.661066, train_loss=2.9264393

Batch 23680, train_perplexity=18.661066, train_loss=2.9264393

Batch 23690, train_perplexity=18.661066, train_loss=2.9264393

Batch 23700, train_perplexity=18.661066, train_loss=2.9264393

Batch 23710, train_perplexity=18.661066, train_loss=2.9264393

Batch 23720, train_perplexity=18.661066, train_loss=2.9264393

Batch 23730, train_perplexity=18.661066, train_loss=2.9264393

Batch 23740, train_perplexity=18.661066, train_loss=2.9264393

Batch 23750, train_perplexity=18.661066, train_loss=2.9264393

Batch 23760, train_perplexity=18.661066, train_loss=2.9264393

Batch 23770, train_perplexity=18.661066, train_loss=2.9264393

Batch 23780, train_perplexity=18.661066, train_loss=2.9264393

Batch 23790, train_perplexity=18.661066, train_loss=2.9264393

Batch 23800, train_perplexity=18.661066, train_loss=2.9264393

Batch 23810, train_perplexity=18.661066, train_loss=2.9264393

Batch 23820, train_perplexity=18.661066, train_loss=2.9264393

Batch 23830, train_perplexity=18.661066, train_loss=2.9264393

Batch 23840, train_perplexity=18.661066, train_loss=2.9264393

Batch 23850, train_perplexity=18.661066, train_loss=2.9264393

Batch 23860, train_perplexity=18.661066, train_loss=2.9264393

Batch 23870, train_perplexity=18.661066, train_loss=2.9264393

Batch 23880, train_perplexity=18.661066, train_loss=2.9264393

Batch 23890, train_perplexity=18.66106, train_loss=2.926439

Batch 23900, train_perplexity=18.66106, train_loss=2.926439

Batch 23910, train_perplexity=18.661066, train_loss=2.9264393

Batch 23920, train_perplexity=18.661066, train_loss=2.9264393

Batch 23930, train_perplexity=18.661066, train_loss=2.9264393

Batch 23940, train_perplexity=18.661057, train_loss=2.9264388

Batch 23950, train_perplexity=18.66106, train_loss=2.926439

Batch 23960, train_perplexity=18.661057, train_loss=2.9264388

Batch 23970, train_perplexity=18.66106, train_loss=2.926439

Batch 23980, train_perplexity=18.661057, train_loss=2.9264388

Batch 23990, train_perplexity=18.661057, train_loss=2.9264388

Batch 24000, train_perplexity=18.661066, train_loss=2.9264393

Batch 24010, train_perplexity=18.661057, train_loss=2.9264388

Batch 24020, train_perplexity=18.661057, train_loss=2.9264388

Batch 24030, train_perplexity=18.661057, train_loss=2.9264388

Batch 24040, train_perplexity=18.661057, train_loss=2.9264388

Batch 24050, train_perplexity=18.661066, train_loss=2.9264393

Batch 24060, train_perplexity=18.661057, train_loss=2.9264388

Batch 24070, train_perplexity=18.66106, train_loss=2.926439

Batch 24080, train_perplexity=18.66106, train_loss=2.926439

Batch 24090, train_perplexity=18.661057, train_loss=2.9264388

Batch 24100, train_perplexity=18.66106, train_loss=2.926439

Batch 24110, train_perplexity=18.661057, train_loss=2.9264388

Batch 24120, train_perplexity=18.66106, train_loss=2.926439

Batch 24130, train_perplexity=18.661057, train_loss=2.9264388

Batch 24140, train_perplexity=18.661057, train_loss=2.9264388

Batch 24150, train_perplexity=18.661057, train_loss=2.9264388

Batch 24160, train_perplexity=18.661057, train_loss=2.9264388

Batch 24170, train_perplexity=18.661057, train_loss=2.9264388

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 24180, train_perplexity=18.661057, train_loss=2.9264388

Batch 24190, train_perplexity=18.661057, train_loss=2.9264388

Batch 24200, train_perplexity=18.661053, train_loss=2.9264386

Batch 24210, train_perplexity=18.661057, train_loss=2.9264388

Batch 24220, train_perplexity=18.66106, train_loss=2.926439

Batch 24230, train_perplexity=18.661057, train_loss=2.9264388

Batch 24240, train_perplexity=18.661057, train_loss=2.9264388

Batch 24250, train_perplexity=18.661057, train_loss=2.9264388

Batch 24260, train_perplexity=18.661057, train_loss=2.9264388

Batch 24270, train_perplexity=18.661057, train_loss=2.9264388

Batch 24280, train_perplexity=18.661057, train_loss=2.9264388

Batch 24290, train_perplexity=18.661057, train_loss=2.9264388

Batch 24300, train_perplexity=18.661057, train_loss=2.9264388

Batch 24310, train_perplexity=18.661057, train_loss=2.9264388

Batch 24320, train_perplexity=18.661053, train_loss=2.9264386

Batch 24330, train_perplexity=18.661057, train_loss=2.9264388

Batch 24340, train_perplexity=18.661057, train_loss=2.9264388

Batch 24350, train_perplexity=18.661053, train_loss=2.9264386

Batch 24360, train_perplexity=18.661057, train_loss=2.9264388

Batch 24370, train_perplexity=18.661057, train_loss=2.9264388

Batch 24380, train_perplexity=18.661057, train_loss=2.9264388

Batch 24390, train_perplexity=18.661057, train_loss=2.9264388

Batch 24400, train_perplexity=18.661053, train_loss=2.9264386

Batch 24410, train_perplexity=18.661053, train_loss=2.9264386

Batch 24420, train_perplexity=18.661053, train_loss=2.9264386

Batch 24430, train_perplexity=18.661053, train_loss=2.9264386

Batch 24440, train_perplexity=18.661057, train_loss=2.9264388

Batch 24450, train_perplexity=18.661053, train_loss=2.9264386

Batch 24460, train_perplexity=18.661053, train_loss=2.9264386

Batch 24470, train_perplexity=18.661057, train_loss=2.9264388

Batch 24480, train_perplexity=18.661057, train_loss=2.9264388

Batch 24490, train_perplexity=18.661057, train_loss=2.9264388

Batch 24500, train_perplexity=18.661047, train_loss=2.9264383

Batch 24510, train_perplexity=18.661053, train_loss=2.9264386
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24520, train_perplexity=18.661057, train_loss=2.9264388

Batch 24530, train_perplexity=18.661057, train_loss=2.9264388

Batch 24540, train_perplexity=18.661057, train_loss=2.9264388

Batch 24550, train_perplexity=18.661047, train_loss=2.9264383

Batch 24560, train_perplexity=18.661047, train_loss=2.9264383

Batch 24570, train_perplexity=18.661053, train_loss=2.9264386

Batch 24580, train_perplexity=18.661053, train_loss=2.9264386

Batch 24590, train_perplexity=18.661047, train_loss=2.9264383

Batch 24600, train_perplexity=18.661047, train_loss=2.9264383

Batch 24610, train_perplexity=18.661057, train_loss=2.9264388

Batch 24620, train_perplexity=18.661053, train_loss=2.9264386

Batch 24630, train_perplexity=18.661053, train_loss=2.9264386

Batch 24640, train_perplexity=18.661047, train_loss=2.9264383

Batch 24650, train_perplexity=18.661047, train_loss=2.9264383

Batch 24660, train_perplexity=18.661053, train_loss=2.9264386

Batch 24670, train_perplexity=18.661047, train_loss=2.9264383

Batch 24680, train_perplexity=18.661053, train_loss=2.9264386

Batch 24690, train_perplexity=18.661047, train_loss=2.9264383

Batch 24700, train_perplexity=18.661047, train_loss=2.9264383

Batch 24710, train_perplexity=18.661057, train_loss=2.9264388

Batch 24720, train_perplexity=18.661053, train_loss=2.9264386

Batch 24730, train_perplexity=18.661047, train_loss=2.9264383

Batch 24740, train_perplexity=18.661047, train_loss=2.9264383

Batch 24750, train_perplexity=18.661053, train_loss=2.9264386

Batch 24760, train_perplexity=18.661047, train_loss=2.9264383

Batch 24770, train_perplexity=18.661047, train_loss=2.9264383

Batch 24780, train_perplexity=18.661047, train_loss=2.9264383

Batch 24790, train_perplexity=18.661047, train_loss=2.9264383

Batch 24800, train_perplexity=18.661047, train_loss=2.9264383

Batch 24810, train_perplexity=18.661047, train_loss=2.9264383

Batch 24820, train_perplexity=18.661047, train_loss=2.9264383

Batch 24830, train_perplexity=18.661047, train_loss=2.9264383

Batch 24840, train_perplexity=18.661047, train_loss=2.9264383

Batch 24850, train_perplexity=18.661047, train_loss=2.9264383

Batch 24860, train_perplexity=18.661047, train_loss=2.9264383

Batch 24870, train_perplexity=18.661047, train_loss=2.9264383

Batch 24880, train_perplexity=18.661047, train_loss=2.9264383

Batch 24890, train_perplexity=18.661047, train_loss=2.9264383

Batch 24900, train_perplexity=18.661047, train_loss=2.9264383

Batch 24910, train_perplexity=18.661047, train_loss=2.9264383

Batch 24920, train_perplexity=18.661047, train_loss=2.9264383

Batch 24930, train_perplexity=18.661047, train_loss=2.9264383

Batch 24940, train_perplexity=18.661047, train_loss=2.9264383

Batch 24950, train_perplexity=18.661047, train_loss=2.9264383

Batch 24960, train_perplexity=18.661047, train_loss=2.9264383

Batch 24970, train_perplexity=18.661047, train_loss=2.9264383

Batch 24980, train_perplexity=18.661047, train_loss=2.9264383

Batch 24990, train_perplexity=18.661047, train_loss=2.9264383

Batch 25000, train_perplexity=18.661047, train_loss=2.9264383

Batch 25010, train_perplexity=18.661047, train_loss=2.9264383

Batch 25020, train_perplexity=18.661047, train_loss=2.9264383

Batch 25030, train_perplexity=18.661043, train_loss=2.926438

Batch 25040, train_perplexity=18.661047, train_loss=2.9264383

Batch 25050, train_perplexity=18.661047, train_loss=2.9264383

Batch 25060, train_perplexity=18.661047, train_loss=2.9264383

Batch 25070, train_perplexity=18.661047, train_loss=2.9264383

Batch 25080, train_perplexity=18.661047, train_loss=2.9264383

Batch 25090, train_perplexity=18.661047, train_loss=2.9264383

Batch 25100, train_perplexity=18.661047, train_loss=2.9264383

Batch 25110, train_perplexity=18.661047, train_loss=2.9264383

Batch 25120, train_perplexity=18.661047, train_loss=2.9264383

Batch 25130, train_perplexity=18.661047, train_loss=2.9264383

Batch 25140, train_perplexity=18.661047, train_loss=2.9264383

Batch 25150, train_perplexity=18.661047, train_loss=2.9264383

Batch 25160, train_perplexity=18.661047, train_loss=2.9264383

Batch 25170, train_perplexity=18.661043, train_loss=2.926438

Batch 25180, train_perplexity=18.661043, train_loss=2.926438

Batch 25190, train_perplexity=18.661047, train_loss=2.9264383

Batch 25200, train_perplexity=18.661047, train_loss=2.9264383

Batch 25210, train_perplexity=18.661043, train_loss=2.926438

Batch 25220, train_perplexity=18.661047, train_loss=2.9264383

Batch 25230, train_perplexity=18.661047, train_loss=2.9264383

Batch 25240, train_perplexity=18.661047, train_loss=2.9264383

Batch 25250, train_perplexity=18.661047, train_loss=2.9264383

Batch 25260, train_perplexity=18.66104, train_loss=2.9264379

Batch 25270, train_perplexity=18.661047, train_loss=2.9264383

Batch 25280, train_perplexity=18.661047, train_loss=2.9264383

Batch 25290, train_perplexity=18.661047, train_loss=2.9264383

Batch 25300, train_perplexity=18.661047, train_loss=2.9264383

Batch 25310, train_perplexity=18.661043, train_loss=2.926438

Batch 25320, train_perplexity=18.661043, train_loss=2.926438

Batch 25330, train_perplexity=18.66104, train_loss=2.9264379

Batch 25340, train_perplexity=18.661043, train_loss=2.926438

Batch 25350, train_perplexity=18.66104, train_loss=2.9264379

Batch 25360, train_perplexity=18.66104, train_loss=2.9264379

Batch 25370, train_perplexity=18.661043, train_loss=2.926438

Batch 25380, train_perplexity=18.66104, train_loss=2.9264379

Batch 25390, train_perplexity=18.66104, train_loss=2.9264379

Batch 25400, train_perplexity=18.66104, train_loss=2.9264379

Batch 25410, train_perplexity=18.661043, train_loss=2.926438

Batch 25420, train_perplexity=18.66104, train_loss=2.9264379

Batch 25430, train_perplexity=18.661043, train_loss=2.926438

Batch 25440, train_perplexity=18.661043, train_loss=2.926438

Batch 25450, train_perplexity=18.66104, train_loss=2.9264379

Batch 25460, train_perplexity=18.66104, train_loss=2.9264379

Batch 25470, train_perplexity=18.66104, train_loss=2.9264379

Batch 25480, train_perplexity=18.66104, train_loss=2.9264379

Batch 25490, train_perplexity=18.66104, train_loss=2.9264379

Batch 25500, train_perplexity=18.66104, train_loss=2.9264379

Batch 25510, train_perplexity=18.66104, train_loss=2.9264379

Batch 25520, train_perplexity=18.66104, train_loss=2.9264379

Batch 25530, train_perplexity=18.66104, train_loss=2.9264379

Batch 25540, train_perplexity=18.66104, train_loss=2.9264379

Batch 25550, train_perplexity=18.66104, train_loss=2.9264379

Batch 25560, train_perplexity=18.66104, train_loss=2.9264379

Batch 25570, train_perplexity=18.66104, train_loss=2.9264379

Batch 25580, train_perplexity=18.66104, train_loss=2.9264379

Batch 25590, train_perplexity=18.66104, train_loss=2.9264379

Batch 25600, train_perplexity=18.66104, train_loss=2.9264379

Batch 25610, train_perplexity=18.66104, train_loss=2.9264379

Batch 25620, train_perplexity=18.66104, train_loss=2.9264379

Batch 25630, train_perplexity=18.66104, train_loss=2.9264379

Batch 25640, train_perplexity=18.66104, train_loss=2.9264379

Batch 25650, train_perplexity=18.661034, train_loss=2.9264376

Batch 25660, train_perplexity=18.66104, train_loss=2.9264379

Batch 25670, train_perplexity=18.66104, train_loss=2.9264379

Batch 25680, train_perplexity=18.66104, train_loss=2.9264379

Batch 25690, train_perplexity=18.66103, train_loss=2.9264374

Batch 25700, train_perplexity=18.661034, train_loss=2.9264376

Batch 25710, train_perplexity=18.66104, train_loss=2.9264379

Batch 25720, train_perplexity=18.66104, train_loss=2.9264379

Batch 25730, train_perplexity=18.66104, train_loss=2.9264379

Batch 25740, train_perplexity=18.66104, train_loss=2.9264379

Batch 25750, train_perplexity=18.66104, train_loss=2.9264379

Batch 25760, train_perplexity=18.66104, train_loss=2.9264379

Batch 25770, train_perplexity=18.66103, train_loss=2.9264374

Batch 25780, train_perplexity=18.66104, train_loss=2.9264379

Batch 25790, train_perplexity=18.661034, train_loss=2.9264376

Batch 25800, train_perplexity=18.66104, train_loss=2.9264379

Batch 25810, train_perplexity=18.66104, train_loss=2.9264379

Batch 25820, train_perplexity=18.661034, train_loss=2.9264376
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 25830, train_perplexity=18.661034, train_loss=2.9264376

Batch 25840, train_perplexity=18.66103, train_loss=2.9264374

Batch 25850, train_perplexity=18.66104, train_loss=2.9264379

Batch 25860, train_perplexity=18.66104, train_loss=2.9264379

Batch 25870, train_perplexity=18.66103, train_loss=2.9264374

Batch 25880, train_perplexity=18.661034, train_loss=2.9264376

Batch 25890, train_perplexity=18.66104, train_loss=2.9264379

Batch 25900, train_perplexity=18.66104, train_loss=2.9264379

Batch 25910, train_perplexity=18.66103, train_loss=2.9264374

Batch 25920, train_perplexity=18.661034, train_loss=2.9264376

Batch 25930, train_perplexity=18.661034, train_loss=2.9264376

Batch 25940, train_perplexity=18.661034, train_loss=2.9264376

Batch 25950, train_perplexity=18.661034, train_loss=2.9264376

Batch 25960, train_perplexity=18.66103, train_loss=2.9264374

Batch 25970, train_perplexity=18.661034, train_loss=2.9264376

Batch 25980, train_perplexity=18.66104, train_loss=2.9264379

Batch 25990, train_perplexity=18.661034, train_loss=2.9264376

Batch 26000, train_perplexity=18.66103, train_loss=2.9264374

Batch 26010, train_perplexity=18.66103, train_loss=2.9264374

Batch 26020, train_perplexity=18.66103, train_loss=2.9264374

Batch 26030, train_perplexity=18.66103, train_loss=2.9264374

Batch 26040, train_perplexity=18.66103, train_loss=2.9264374

Batch 26050, train_perplexity=18.66103, train_loss=2.9264374

Batch 26060, train_perplexity=18.66103, train_loss=2.9264374

Batch 26070, train_perplexity=18.66103, train_loss=2.9264374

Batch 26080, train_perplexity=18.66103, train_loss=2.9264374

Batch 26090, train_perplexity=18.66103, train_loss=2.9264374

Batch 26100, train_perplexity=18.661034, train_loss=2.9264376

Batch 26110, train_perplexity=18.66103, train_loss=2.9264374

Batch 26120, train_perplexity=18.66103, train_loss=2.9264374

Batch 26130, train_perplexity=18.661034, train_loss=2.9264376

Batch 26140, train_perplexity=18.661034, train_loss=2.9264376

Batch 26150, train_perplexity=18.66104, train_loss=2.9264379

Batch 26160, train_perplexity=18.66103, train_loss=2.9264374

Batch 26170, train_perplexity=18.66103, train_loss=2.9264374

Batch 26180, train_perplexity=18.66103, train_loss=2.9264374

Batch 26190, train_perplexity=18.66103, train_loss=2.9264374

Batch 26200, train_perplexity=18.66103, train_loss=2.9264374

Batch 26210, train_perplexity=18.66103, train_loss=2.9264374

Batch 26220, train_perplexity=18.661034, train_loss=2.9264376

Batch 26230, train_perplexity=18.66103, train_loss=2.9264374

Batch 26240, train_perplexity=18.661034, train_loss=2.9264376

Batch 26250, train_perplexity=18.66103, train_loss=2.9264374

Batch 26260, train_perplexity=18.66103, train_loss=2.9264374

Batch 26270, train_perplexity=18.66103, train_loss=2.9264374

Batch 26280, train_perplexity=18.66103, train_loss=2.9264374

Batch 26290, train_perplexity=18.66103, train_loss=2.9264374

Batch 26300, train_perplexity=18.66103, train_loss=2.9264374

Batch 26310, train_perplexity=18.66103, train_loss=2.9264374

Batch 26320, train_perplexity=18.66103, train_loss=2.9264374

Batch 26330, train_perplexity=18.66103, train_loss=2.9264374

Batch 26340, train_perplexity=18.66103, train_loss=2.9264374

Batch 26350, train_perplexity=18.66103, train_loss=2.9264374

Batch 26360, train_perplexity=18.66103, train_loss=2.9264374

Batch 26370, train_perplexity=18.661026, train_loss=2.9264371

Batch 26380, train_perplexity=18.66103, train_loss=2.9264374

Batch 26390, train_perplexity=18.66103, train_loss=2.9264374

Batch 26400, train_perplexity=18.661026, train_loss=2.9264371

Batch 26410, train_perplexity=18.66103, train_loss=2.9264374

Batch 26420, train_perplexity=18.66103, train_loss=2.9264374

Batch 26430, train_perplexity=18.66102, train_loss=2.926437

Batch 26440, train_perplexity=18.66102, train_loss=2.926437

Batch 26450, train_perplexity=18.661026, train_loss=2.9264371

Batch 26460, train_perplexity=18.66103, train_loss=2.9264374

Batch 26470, train_perplexity=18.661026, train_loss=2.9264371

Batch 26480, train_perplexity=18.66102, train_loss=2.926437

Batch 26490, train_perplexity=18.66103, train_loss=2.9264374

Batch 26500, train_perplexity=18.661026, train_loss=2.9264371

Batch 26510, train_perplexity=18.66102, train_loss=2.926437

Batch 26520, train_perplexity=18.66102, train_loss=2.926437

Batch 26530, train_perplexity=18.66102, train_loss=2.926437

Batch 26540, train_perplexity=18.66102, train_loss=2.926437

Batch 26550, train_perplexity=18.66102, train_loss=2.926437

Batch 26560, train_perplexity=18.66103, train_loss=2.9264374

Batch 26570, train_perplexity=18.66102, train_loss=2.926437

Batch 26580, train_perplexity=18.66102, train_loss=2.926437

Batch 26590, train_perplexity=18.66102, train_loss=2.926437

Batch 26600, train_perplexity=18.66102, train_loss=2.926437

Batch 26610, train_perplexity=18.66102, train_loss=2.926437

Batch 26620, train_perplexity=18.66102, train_loss=2.926437

Batch 26630, train_perplexity=18.66102, train_loss=2.926437

Batch 26640, train_perplexity=18.66102, train_loss=2.926437

Batch 26650, train_perplexity=18.66102, train_loss=2.926437

Batch 26660, train_perplexity=18.66102, train_loss=2.926437

Batch 26670, train_perplexity=18.66102, train_loss=2.926437

Batch 26680, train_perplexity=18.66102, train_loss=2.926437

Batch 26690, train_perplexity=18.66102, train_loss=2.926437

Batch 26700, train_perplexity=18.66102, train_loss=2.926437

Batch 26710, train_perplexity=18.66102, train_loss=2.926437

Batch 26720, train_perplexity=18.66102, train_loss=2.926437

Batch 26730, train_perplexity=18.661016, train_loss=2.9264367

Batch 26740, train_perplexity=18.66102, train_loss=2.926437

Batch 26750, train_perplexity=18.66102, train_loss=2.926437

Batch 26760, train_perplexity=18.66102, train_loss=2.926437

Batch 26770, train_perplexity=18.66102, train_loss=2.926437

Batch 26780, train_perplexity=18.66102, train_loss=2.926437

Batch 26790, train_perplexity=18.66102, train_loss=2.926437

Batch 26800, train_perplexity=18.66102, train_loss=2.926437

Batch 26810, train_perplexity=18.66102, train_loss=2.926437

Batch 26820, train_perplexity=18.661016, train_loss=2.9264367

Batch 26830, train_perplexity=18.66102, train_loss=2.926437

Batch 26840, train_perplexity=18.66102, train_loss=2.926437

Batch 26850, train_perplexity=18.66102, train_loss=2.926437

Batch 26860, train_perplexity=18.66102, train_loss=2.926437

Batch 26870, train_perplexity=18.66102, train_loss=2.926437

Batch 26880, train_perplexity=18.66102, train_loss=2.926437

Batch 26890, train_perplexity=18.661016, train_loss=2.9264367

Batch 26900, train_perplexity=18.66102, train_loss=2.926437

Batch 26910, train_perplexity=18.66102, train_loss=2.926437

Batch 26920, train_perplexity=18.66102, train_loss=2.926437

Batch 26930, train_perplexity=18.661016, train_loss=2.9264367

Batch 26940, train_perplexity=18.661016, train_loss=2.9264367

Batch 26950, train_perplexity=18.66102, train_loss=2.926437

Batch 26960, train_perplexity=18.66102, train_loss=2.926437

Batch 26970, train_perplexity=18.661016, train_loss=2.9264367

Batch 26980, train_perplexity=18.66102, train_loss=2.926437

Batch 26990, train_perplexity=18.661016, train_loss=2.9264367

Batch 27000, train_perplexity=18.661016, train_loss=2.9264367

Batch 27010, train_perplexity=18.661016, train_loss=2.9264367

Batch 27020, train_perplexity=18.661013, train_loss=2.9264364

Batch 27030, train_perplexity=18.661016, train_loss=2.9264367

Batch 27040, train_perplexity=18.66102, train_loss=2.926437

Batch 27050, train_perplexity=18.661016, train_loss=2.9264367

Batch 27060, train_perplexity=18.661016, train_loss=2.9264367

Batch 27070, train_perplexity=18.66102, train_loss=2.926437

Batch 27080, train_perplexity=18.661016, train_loss=2.9264367

Batch 27090, train_perplexity=18.661013, train_loss=2.9264364

Batch 27100, train_perplexity=18.66102, train_loss=2.926437

Batch 27110, train_perplexity=18.661013, train_loss=2.9264364

Batch 27120, train_perplexity=18.66102, train_loss=2.926437

Batch 27130, train_perplexity=18.661013, train_loss=2.9264364

Batch 27140, train_perplexity=18.661016, train_loss=2.9264367

Batch 27150, train_perplexity=18.661016, train_loss=2.9264367
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 27160, train_perplexity=18.661016, train_loss=2.9264367

Batch 27170, train_perplexity=18.661013, train_loss=2.9264364

Batch 27180, train_perplexity=18.661016, train_loss=2.9264367

Batch 27190, train_perplexity=18.661016, train_loss=2.9264367

Batch 27200, train_perplexity=18.661013, train_loss=2.9264364

Batch 27210, train_perplexity=18.661016, train_loss=2.9264367

Batch 27220, train_perplexity=18.661013, train_loss=2.9264364

Batch 27230, train_perplexity=18.661016, train_loss=2.9264367

Batch 27240, train_perplexity=18.661013, train_loss=2.9264364

Batch 27250, train_perplexity=18.661016, train_loss=2.9264367

Batch 27260, train_perplexity=18.661013, train_loss=2.9264364

Batch 27270, train_perplexity=18.661016, train_loss=2.9264367

Batch 27280, train_perplexity=18.661013, train_loss=2.9264364

Batch 27290, train_perplexity=18.661016, train_loss=2.9264367

Batch 27300, train_perplexity=18.661013, train_loss=2.9264364

Batch 27310, train_perplexity=18.661013, train_loss=2.9264364

Batch 27320, train_perplexity=18.661013, train_loss=2.9264364

Batch 27330, train_perplexity=18.661013, train_loss=2.9264364

Batch 27340, train_perplexity=18.661013, train_loss=2.9264364

Batch 27350, train_perplexity=18.661016, train_loss=2.9264367

Batch 27360, train_perplexity=18.661013, train_loss=2.9264364

Batch 27370, train_perplexity=18.661007, train_loss=2.9264362

Batch 27380, train_perplexity=18.661013, train_loss=2.9264364

Batch 27390, train_perplexity=18.661013, train_loss=2.9264364

Batch 27400, train_perplexity=18.661013, train_loss=2.9264364

Batch 27410, train_perplexity=18.661016, train_loss=2.9264367

Batch 27420, train_perplexity=18.661016, train_loss=2.9264367

Batch 27430, train_perplexity=18.661013, train_loss=2.9264364

Batch 27440, train_perplexity=18.661013, train_loss=2.9264364

Batch 27450, train_perplexity=18.661013, train_loss=2.9264364

Batch 27460, train_perplexity=18.661013, train_loss=2.9264364

Batch 27470, train_perplexity=18.661013, train_loss=2.9264364

Batch 27480, train_perplexity=18.661013, train_loss=2.9264364

Batch 27490, train_perplexity=18.661013, train_loss=2.9264364

Batch 27500, train_perplexity=18.661013, train_loss=2.9264364

Batch 27510, train_perplexity=18.661013, train_loss=2.9264364

Batch 27520, train_perplexity=18.661013, train_loss=2.9264364

Batch 27530, train_perplexity=18.661013, train_loss=2.9264364

Batch 27540, train_perplexity=18.661007, train_loss=2.9264362

Batch 27550, train_perplexity=18.661007, train_loss=2.9264362

Batch 27560, train_perplexity=18.661013, train_loss=2.9264364

Batch 27570, train_perplexity=18.661007, train_loss=2.9264362

Batch 27580, train_perplexity=18.661007, train_loss=2.9264362

Batch 27590, train_perplexity=18.661013, train_loss=2.9264364

Batch 27600, train_perplexity=18.661003, train_loss=2.926436

Batch 27610, train_perplexity=18.661013, train_loss=2.9264364

Batch 27620, train_perplexity=18.661007, train_loss=2.9264362

Batch 27630, train_perplexity=18.661013, train_loss=2.9264364

Batch 27640, train_perplexity=18.661013, train_loss=2.9264364

Batch 27650, train_perplexity=18.661013, train_loss=2.9264364

Batch 27660, train_perplexity=18.661007, train_loss=2.9264362

Batch 27670, train_perplexity=18.661007, train_loss=2.9264362

Batch 27680, train_perplexity=18.661007, train_loss=2.9264362

Batch 27690, train_perplexity=18.661007, train_loss=2.9264362

Batch 27700, train_perplexity=18.661013, train_loss=2.9264364

Batch 27710, train_perplexity=18.661007, train_loss=2.9264362

Batch 27720, train_perplexity=18.661013, train_loss=2.9264364

Batch 27730, train_perplexity=18.661007, train_loss=2.9264362

Batch 27740, train_perplexity=18.661007, train_loss=2.9264362

Batch 27750, train_perplexity=18.661013, train_loss=2.9264364

Batch 27760, train_perplexity=18.661007, train_loss=2.9264362

Batch 27770, train_perplexity=18.661003, train_loss=2.926436

Batch 27780, train_perplexity=18.661007, train_loss=2.9264362

Batch 27790, train_perplexity=18.661007, train_loss=2.9264362

Batch 27800, train_perplexity=18.661013, train_loss=2.9264364

Batch 27810, train_perplexity=18.661013, train_loss=2.9264364

Batch 27820, train_perplexity=18.661007, train_loss=2.9264362

Batch 27830, train_perplexity=18.661003, train_loss=2.926436

Batch 27840, train_perplexity=18.661003, train_loss=2.926436

Batch 27850, train_perplexity=18.661003, train_loss=2.926436

Batch 27860, train_perplexity=18.661003, train_loss=2.926436

Batch 27870, train_perplexity=18.661007, train_loss=2.9264362

Batch 27880, train_perplexity=18.661003, train_loss=2.926436

Batch 27890, train_perplexity=18.661007, train_loss=2.9264362

Batch 27900, train_perplexity=18.661003, train_loss=2.926436

Batch 27910, train_perplexity=18.661003, train_loss=2.926436

Batch 27920, train_perplexity=18.661007, train_loss=2.9264362

Batch 27930, train_perplexity=18.661003, train_loss=2.926436

Batch 27940, train_perplexity=18.661007, train_loss=2.9264362

Batch 27950, train_perplexity=18.661003, train_loss=2.926436

Batch 27960, train_perplexity=18.661003, train_loss=2.926436

Batch 27970, train_perplexity=18.661003, train_loss=2.926436

Batch 27980, train_perplexity=18.661003, train_loss=2.926436

Batch 27990, train_perplexity=18.661003, train_loss=2.926436

Batch 28000, train_perplexity=18.661003, train_loss=2.926436

Batch 28010, train_perplexity=18.661003, train_loss=2.926436

Batch 28020, train_perplexity=18.661003, train_loss=2.926436

Batch 28030, train_perplexity=18.661003, train_loss=2.926436

Batch 28040, train_perplexity=18.661003, train_loss=2.926436

Batch 28050, train_perplexity=18.661003, train_loss=2.926436

Batch 28060, train_perplexity=18.661003, train_loss=2.926436

Batch 28070, train_perplexity=18.661003, train_loss=2.926436

Batch 28080, train_perplexity=18.661003, train_loss=2.926436

Batch 28090, train_perplexity=18.661003, train_loss=2.926436

Batch 28100, train_perplexity=18.661003, train_loss=2.926436

Batch 28110, train_perplexity=18.661003, train_loss=2.926436

Batch 28120, train_perplexity=18.661003, train_loss=2.926436

Batch 28130, train_perplexity=18.661003, train_loss=2.926436

Batch 28140, train_perplexity=18.661003, train_loss=2.926436

Batch 28150, train_perplexity=18.661, train_loss=2.9264357

Batch 28160, train_perplexity=18.661003, train_loss=2.926436

Batch 28170, train_perplexity=18.661003, train_loss=2.926436

Batch 28180, train_perplexity=18.661003, train_loss=2.926436

Batch 28190, train_perplexity=18.661, train_loss=2.9264357

Batch 28200, train_perplexity=18.661, train_loss=2.9264357

Batch 28210, train_perplexity=18.660994, train_loss=2.9264355

Batch 28220, train_perplexity=18.660994, train_loss=2.9264355

Batch 28230, train_perplexity=18.660994, train_loss=2.9264355

Batch 28240, train_perplexity=18.661, train_loss=2.9264357

Batch 28250, train_perplexity=18.660994, train_loss=2.9264355

Batch 28260, train_perplexity=18.661, train_loss=2.9264357

Batch 28270, train_perplexity=18.661, train_loss=2.9264357

Batch 28280, train_perplexity=18.661, train_loss=2.9264357

Batch 28290, train_perplexity=18.660994, train_loss=2.9264355

Batch 28300, train_perplexity=18.660994, train_loss=2.9264355

Batch 28310, train_perplexity=18.660994, train_loss=2.9264355

Batch 28320, train_perplexity=18.660994, train_loss=2.9264355

Batch 28330, train_perplexity=18.660994, train_loss=2.9264355

Batch 28340, train_perplexity=18.660994, train_loss=2.9264355

Batch 28350, train_perplexity=18.660994, train_loss=2.9264355

Batch 28360, train_perplexity=18.660994, train_loss=2.9264355

Batch 28370, train_perplexity=18.660994, train_loss=2.9264355

Batch 28380, train_perplexity=18.660994, train_loss=2.9264355

Batch 28390, train_perplexity=18.660994, train_loss=2.9264355

Batch 28400, train_perplexity=18.660994, train_loss=2.9264355

Batch 28410, train_perplexity=18.660994, train_loss=2.9264355

Batch 28420, train_perplexity=18.660994, train_loss=2.9264355

Batch 28430, train_perplexity=18.660994, train_loss=2.9264355

Batch 28440, train_perplexity=18.660994, train_loss=2.9264355

Batch 28450, train_perplexity=18.660994, train_loss=2.9264355

Batch 28460, train_perplexity=18.660994, train_loss=2.9264355
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28470, train_perplexity=18.660994, train_loss=2.9264355

Batch 28480, train_perplexity=18.660994, train_loss=2.9264355

Batch 28490, train_perplexity=18.660994, train_loss=2.9264355

Batch 28500, train_perplexity=18.660994, train_loss=2.9264355

Batch 28510, train_perplexity=18.660994, train_loss=2.9264355

Batch 28520, train_perplexity=18.66099, train_loss=2.9264352

Batch 28530, train_perplexity=18.660994, train_loss=2.9264355

Batch 28540, train_perplexity=18.660994, train_loss=2.9264355

Batch 28550, train_perplexity=18.660994, train_loss=2.9264355

Batch 28560, train_perplexity=18.660994, train_loss=2.9264355

Batch 28570, train_perplexity=18.660994, train_loss=2.9264355

Batch 28580, train_perplexity=18.660994, train_loss=2.9264355

Batch 28590, train_perplexity=18.660994, train_loss=2.9264355

Batch 28600, train_perplexity=18.660994, train_loss=2.9264355

Batch 28610, train_perplexity=18.660994, train_loss=2.9264355

Batch 28620, train_perplexity=18.660994, train_loss=2.9264355

Batch 28630, train_perplexity=18.660994, train_loss=2.9264355

Batch 28640, train_perplexity=18.660994, train_loss=2.9264355

Batch 28650, train_perplexity=18.660994, train_loss=2.9264355

Batch 28660, train_perplexity=18.660994, train_loss=2.9264355

Batch 28670, train_perplexity=18.660994, train_loss=2.9264355

Batch 28680, train_perplexity=18.660994, train_loss=2.9264355

Batch 28690, train_perplexity=18.660994, train_loss=2.9264355

Batch 28700, train_perplexity=18.660994, train_loss=2.9264355

Batch 28710, train_perplexity=18.66099, train_loss=2.9264352

Batch 28720, train_perplexity=18.660994, train_loss=2.9264355

Batch 28730, train_perplexity=18.660994, train_loss=2.9264355

Batch 28740, train_perplexity=18.660994, train_loss=2.9264355

Batch 28750, train_perplexity=18.66099, train_loss=2.9264352

Batch 28760, train_perplexity=18.660994, train_loss=2.9264355

Batch 28770, train_perplexity=18.660994, train_loss=2.9264355

Batch 28780, train_perplexity=18.660994, train_loss=2.9264355

Batch 28790, train_perplexity=18.660994, train_loss=2.9264355

Batch 28800, train_perplexity=18.66099, train_loss=2.9264352

Batch 28810, train_perplexity=18.660994, train_loss=2.9264355

Batch 28820, train_perplexity=18.660994, train_loss=2.9264355

Batch 28830, train_perplexity=18.660994, train_loss=2.9264355

Batch 28840, train_perplexity=18.660994, train_loss=2.9264355

Batch 28850, train_perplexity=18.66099, train_loss=2.9264352

Batch 28860, train_perplexity=18.660994, train_loss=2.9264355

Batch 28870, train_perplexity=18.660994, train_loss=2.9264355

Batch 28880, train_perplexity=18.66099, train_loss=2.9264352

Batch 28890, train_perplexity=18.660994, train_loss=2.9264355

Batch 28900, train_perplexity=18.660994, train_loss=2.9264355

Batch 28910, train_perplexity=18.660994, train_loss=2.9264355

Batch 28920, train_perplexity=18.660994, train_loss=2.9264355

Batch 28930, train_perplexity=18.660994, train_loss=2.9264355

Batch 28940, train_perplexity=18.660986, train_loss=2.926435

Batch 28950, train_perplexity=18.660994, train_loss=2.9264355

Batch 28960, train_perplexity=18.66099, train_loss=2.9264352

Batch 28970, train_perplexity=18.660994, train_loss=2.9264355

Batch 28980, train_perplexity=18.660994, train_loss=2.9264355

Batch 28990, train_perplexity=18.660994, train_loss=2.9264355

Batch 29000, train_perplexity=18.660994, train_loss=2.9264355

Batch 29010, train_perplexity=18.660994, train_loss=2.9264355

Batch 29020, train_perplexity=18.66099, train_loss=2.9264352

Batch 29030, train_perplexity=18.660994, train_loss=2.9264355

Batch 29040, train_perplexity=18.660994, train_loss=2.9264355

Batch 29050, train_perplexity=18.660986, train_loss=2.926435

Batch 29060, train_perplexity=18.660994, train_loss=2.9264355

Batch 29070, train_perplexity=18.66099, train_loss=2.9264352

Batch 29080, train_perplexity=18.660994, train_loss=2.9264355

Batch 29090, train_perplexity=18.660994, train_loss=2.9264355

Batch 29100, train_perplexity=18.660994, train_loss=2.9264355

Batch 29110, train_perplexity=18.660994, train_loss=2.9264355

Batch 29120, train_perplexity=18.660994, train_loss=2.9264355

Batch 29130, train_perplexity=18.660994, train_loss=2.9264355

Batch 29140, train_perplexity=18.66099, train_loss=2.9264352

Batch 29150, train_perplexity=18.66099, train_loss=2.9264352

Batch 29160, train_perplexity=18.660994, train_loss=2.9264355

Batch 29170, train_perplexity=18.66099, train_loss=2.9264352

Batch 29180, train_perplexity=18.660994, train_loss=2.9264355

Batch 29190, train_perplexity=18.660994, train_loss=2.9264355

Batch 29200, train_perplexity=18.660994, train_loss=2.9264355

Batch 29210, train_perplexity=18.66099, train_loss=2.9264352

Batch 29220, train_perplexity=18.660994, train_loss=2.9264355

Batch 29230, train_perplexity=18.66099, train_loss=2.9264352

Batch 29240, train_perplexity=18.66099, train_loss=2.9264352

Batch 29250, train_perplexity=18.660986, train_loss=2.926435

Batch 29260, train_perplexity=18.660986, train_loss=2.926435

Batch 29270, train_perplexity=18.66099, train_loss=2.9264352

Batch 29280, train_perplexity=18.660986, train_loss=2.926435

Batch 29290, train_perplexity=18.660986, train_loss=2.926435

Batch 29300, train_perplexity=18.660986, train_loss=2.926435

Batch 29310, train_perplexity=18.660986, train_loss=2.926435

Batch 29320, train_perplexity=18.66099, train_loss=2.9264352

Batch 29330, train_perplexity=18.660994, train_loss=2.9264355

Batch 29340, train_perplexity=18.660994, train_loss=2.9264355

Batch 29350, train_perplexity=18.66099, train_loss=2.9264352

Batch 29360, train_perplexity=18.660986, train_loss=2.926435

Batch 29370, train_perplexity=18.660986, train_loss=2.926435

Batch 29380, train_perplexity=18.66099, train_loss=2.9264352

Batch 29390, train_perplexity=18.660986, train_loss=2.926435

Batch 29400, train_perplexity=18.660986, train_loss=2.926435

Batch 29410, train_perplexity=18.660986, train_loss=2.926435

Batch 29420, train_perplexity=18.660986, train_loss=2.926435

Batch 29430, train_perplexity=18.660986, train_loss=2.926435

Batch 29440, train_perplexity=18.660986, train_loss=2.926435

Batch 29450, train_perplexity=18.660986, train_loss=2.926435

Batch 29460, train_perplexity=18.660986, train_loss=2.926435

Batch 29470, train_perplexity=18.660986, train_loss=2.926435

Batch 29480, train_perplexity=18.660986, train_loss=2.926435

Batch 29490, train_perplexity=18.660986, train_loss=2.926435

Batch 29500, train_perplexity=18.66098, train_loss=2.9264348

Batch 29510, train_perplexity=18.660986, train_loss=2.926435

Batch 29520, train_perplexity=18.660986, train_loss=2.926435

Batch 29530, train_perplexity=18.660986, train_loss=2.926435

Batch 29540, train_perplexity=18.660986, train_loss=2.926435

Batch 29550, train_perplexity=18.660986, train_loss=2.926435

Batch 29560, train_perplexity=18.660986, train_loss=2.926435

Batch 29570, train_perplexity=18.660986, train_loss=2.926435

Batch 29580, train_perplexity=18.66098, train_loss=2.9264348

Batch 29590, train_perplexity=18.660986, train_loss=2.926435

Batch 29600, train_perplexity=18.660986, train_loss=2.926435

Batch 29610, train_perplexity=18.660986, train_loss=2.926435

Batch 29620, train_perplexity=18.660986, train_loss=2.926435

Batch 29630, train_perplexity=18.660986, train_loss=2.926435

Batch 29640, train_perplexity=18.660986, train_loss=2.926435

Batch 29650, train_perplexity=18.660986, train_loss=2.926435

Batch 29660, train_perplexity=18.66098, train_loss=2.9264348

Batch 29670, train_perplexity=18.660986, train_loss=2.926435

Batch 29680, train_perplexity=18.66098, train_loss=2.9264348

Batch 29690, train_perplexity=18.660986, train_loss=2.926435

Batch 29700, train_perplexity=18.660986, train_loss=2.926435

Batch 29710, train_perplexity=18.660986, train_loss=2.926435

Batch 29720, train_perplexity=18.660986, train_loss=2.926435

Batch 29730, train_perplexity=18.660986, train_loss=2.926435

Batch 29740, train_perplexity=18.660986, train_loss=2.926435

Batch 29750, train_perplexity=18.660986, train_loss=2.926435

Batch 29760, train_perplexity=18.66098, train_loss=2.9264348

Batch 29770, train_perplexity=18.66098, train_loss=2.9264348

Batch 29780, train_perplexity=18.660986, train_loss=2.926435
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 29790, train_perplexity=18.660986, train_loss=2.926435

Batch 29800, train_perplexity=18.66099, train_loss=2.9264352

Batch 29810, train_perplexity=18.660986, train_loss=2.926435

Batch 29820, train_perplexity=18.66098, train_loss=2.9264348

Batch 29830, train_perplexity=18.660976, train_loss=2.9264345

Batch 29840, train_perplexity=18.660976, train_loss=2.9264345

Batch 29850, train_perplexity=18.660976, train_loss=2.9264345

Batch 29860, train_perplexity=18.660986, train_loss=2.926435

Batch 29870, train_perplexity=18.660986, train_loss=2.926435

Batch 29880, train_perplexity=18.66098, train_loss=2.9264348

Batch 29890, train_perplexity=18.660986, train_loss=2.926435

Batch 29900, train_perplexity=18.66098, train_loss=2.9264348

Batch 29910, train_perplexity=18.660986, train_loss=2.926435

Batch 29920, train_perplexity=18.660986, train_loss=2.926435

Batch 29930, train_perplexity=18.66098, train_loss=2.9264348

Batch 29940, train_perplexity=18.660976, train_loss=2.9264345

Batch 29950, train_perplexity=18.66098, train_loss=2.9264348

Batch 29960, train_perplexity=18.66098, train_loss=2.9264348

Batch 29970, train_perplexity=18.660986, train_loss=2.926435

Batch 29980, train_perplexity=18.660976, train_loss=2.9264345

Batch 29990, train_perplexity=18.660986, train_loss=2.926435

Batch 30000, train_perplexity=18.660976, train_loss=2.9264345

Batch 30010, train_perplexity=18.66098, train_loss=2.9264348

Batch 30020, train_perplexity=18.660976, train_loss=2.9264345

Batch 30030, train_perplexity=18.660986, train_loss=2.926435

Batch 30040, train_perplexity=18.660976, train_loss=2.9264345

Batch 30050, train_perplexity=18.660976, train_loss=2.9264345

Batch 30060, train_perplexity=18.660976, train_loss=2.9264345

Batch 30070, train_perplexity=18.660976, train_loss=2.9264345

Batch 30080, train_perplexity=18.660976, train_loss=2.9264345

Batch 30090, train_perplexity=18.660976, train_loss=2.9264345

Batch 30100, train_perplexity=18.660976, train_loss=2.9264345

Batch 30110, train_perplexity=18.660976, train_loss=2.9264345

Batch 30120, train_perplexity=18.66098, train_loss=2.9264348

Batch 30130, train_perplexity=18.660976, train_loss=2.9264345

Batch 30140, train_perplexity=18.66098, train_loss=2.9264348

Batch 30150, train_perplexity=18.660976, train_loss=2.9264345

Batch 30160, train_perplexity=18.66098, train_loss=2.9264348

Batch 30170, train_perplexity=18.660976, train_loss=2.9264345

Batch 30180, train_perplexity=18.660976, train_loss=2.9264345

Batch 30190, train_perplexity=18.66098, train_loss=2.9264348

Batch 30200, train_perplexity=18.66098, train_loss=2.9264348

Batch 30210, train_perplexity=18.660976, train_loss=2.9264345

Batch 30220, train_perplexity=18.660976, train_loss=2.9264345

Batch 30230, train_perplexity=18.66098, train_loss=2.9264348

Batch 30240, train_perplexity=18.660976, train_loss=2.9264345

Batch 30250, train_perplexity=18.660976, train_loss=2.9264345

Batch 30260, train_perplexity=18.660976, train_loss=2.9264345

Batch 30270, train_perplexity=18.660976, train_loss=2.9264345

Batch 30280, train_perplexity=18.660976, train_loss=2.9264345

Batch 30290, train_perplexity=18.660976, train_loss=2.9264345

Batch 30300, train_perplexity=18.660976, train_loss=2.9264345

Batch 30310, train_perplexity=18.66098, train_loss=2.9264348

Batch 30320, train_perplexity=18.660976, train_loss=2.9264345

Batch 30330, train_perplexity=18.660976, train_loss=2.9264345

Batch 30340, train_perplexity=18.660976, train_loss=2.9264345

Batch 30350, train_perplexity=18.660976, train_loss=2.9264345

Batch 30360, train_perplexity=18.660976, train_loss=2.9264345

Batch 30370, train_perplexity=18.660976, train_loss=2.9264345

Batch 30380, train_perplexity=18.660976, train_loss=2.9264345

Batch 30390, train_perplexity=18.660976, train_loss=2.9264345

Batch 30400, train_perplexity=18.660976, train_loss=2.9264345

Batch 30410, train_perplexity=18.660976, train_loss=2.9264345

Batch 30420, train_perplexity=18.660976, train_loss=2.9264345

Batch 30430, train_perplexity=18.660976, train_loss=2.9264345

Batch 30440, train_perplexity=18.660976, train_loss=2.9264345

Batch 30450, train_perplexity=18.660976, train_loss=2.9264345

Batch 30460, train_perplexity=18.660976, train_loss=2.9264345

Batch 30470, train_perplexity=18.660976, train_loss=2.9264345

Batch 30480, train_perplexity=18.660976, train_loss=2.9264345

Batch 30490, train_perplexity=18.660976, train_loss=2.9264345

Batch 30500, train_perplexity=18.660976, train_loss=2.9264345

Batch 30510, train_perplexity=18.660976, train_loss=2.9264345

Batch 30520, train_perplexity=18.660976, train_loss=2.9264345

Batch 30530, train_perplexity=18.660976, train_loss=2.9264345

Batch 30540, train_perplexity=18.660976, train_loss=2.9264345

Batch 30550, train_perplexity=18.660976, train_loss=2.9264345

Batch 30560, train_perplexity=18.660976, train_loss=2.9264345

Batch 30570, train_perplexity=18.660976, train_loss=2.9264345

Batch 30580, train_perplexity=18.660976, train_loss=2.9264345

Batch 30590, train_perplexity=18.660976, train_loss=2.9264345

Batch 30600, train_perplexity=18.660976, train_loss=2.9264345

Batch 30610, train_perplexity=18.660976, train_loss=2.9264345

Batch 30620, train_perplexity=18.660976, train_loss=2.9264345

Batch 30630, train_perplexity=18.660976, train_loss=2.9264345

Batch 30640, train_perplexity=18.660976, train_loss=2.9264345

Batch 30650, train_perplexity=18.660976, train_loss=2.9264345

Batch 30660, train_perplexity=18.660976, train_loss=2.9264345

Batch 30670, train_perplexity=18.660976, train_loss=2.9264345

Batch 30680, train_perplexity=18.660976, train_loss=2.9264345

Batch 30690, train_perplexity=18.660976, train_loss=2.9264345

Batch 30700, train_perplexity=18.660976, train_loss=2.9264345

Batch 30710, train_perplexity=18.660976, train_loss=2.9264345

Batch 30720, train_perplexity=18.660976, train_loss=2.9264345

Batch 30730, train_perplexity=18.660976, train_loss=2.9264345

Batch 30740, train_perplexity=18.660976, train_loss=2.9264345

Batch 30750, train_perplexity=18.660976, train_loss=2.9264345

Batch 30760, train_perplexity=18.660976, train_loss=2.9264345

Batch 30770, train_perplexity=18.660976, train_loss=2.9264345

Batch 30780, train_perplexity=18.660976, train_loss=2.9264345

Batch 30790, train_perplexity=18.660976, train_loss=2.9264345

Batch 30800, train_perplexity=18.660976, train_loss=2.9264345

Batch 30810, train_perplexity=18.660976, train_loss=2.9264345

Batch 30820, train_perplexity=18.660973, train_loss=2.9264343

Batch 30830, train_perplexity=18.660976, train_loss=2.9264345

Batch 30840, train_perplexity=18.660976, train_loss=2.9264345

Batch 30850, train_perplexity=18.660976, train_loss=2.9264345

Batch 30860, train_perplexity=18.660976, train_loss=2.9264345

Batch 30870, train_perplexity=18.660976, train_loss=2.9264345

Batch 30880, train_perplexity=18.660976, train_loss=2.9264345

Batch 30890, train_perplexity=18.660973, train_loss=2.9264343

Batch 30900, train_perplexity=18.660976, train_loss=2.9264345

Batch 30910, train_perplexity=18.660967, train_loss=2.926434

Batch 30920, train_perplexity=18.660976, train_loss=2.9264345

Batch 30930, train_perplexity=18.660967, train_loss=2.926434

Batch 30940, train_perplexity=18.660973, train_loss=2.9264343

Batch 30950, train_perplexity=18.660976, train_loss=2.9264345

Batch 30960, train_perplexity=18.660967, train_loss=2.926434

Batch 30970, train_perplexity=18.660976, train_loss=2.9264345

Batch 30980, train_perplexity=18.660973, train_loss=2.9264343

Batch 30990, train_perplexity=18.660973, train_loss=2.9264343

Batch 31000, train_perplexity=18.660976, train_loss=2.9264345

Batch 31010, train_perplexity=18.660976, train_loss=2.9264345

Batch 31020, train_perplexity=18.660976, train_loss=2.9264345

Batch 31030, train_perplexity=18.660976, train_loss=2.9264345

Batch 31040, train_perplexity=18.660976, train_loss=2.9264345

Batch 31050, train_perplexity=18.660976, train_loss=2.9264345

Batch 31060, train_perplexity=18.660976, train_loss=2.9264345

Batch 31070, train_perplexity=18.660976, train_loss=2.9264345

Batch 31080, train_perplexity=18.660973, train_loss=2.9264343

Batch 31090, train_perplexity=18.660976, train_loss=2.9264345
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 31100, train_perplexity=18.660976, train_loss=2.9264345

Batch 31110, train_perplexity=18.660967, train_loss=2.926434

Batch 31120, train_perplexity=18.660967, train_loss=2.926434

Batch 31130, train_perplexity=18.660976, train_loss=2.9264345

Batch 31140, train_perplexity=18.660976, train_loss=2.9264345

Batch 31150, train_perplexity=18.660976, train_loss=2.9264345

Batch 31160, train_perplexity=18.660976, train_loss=2.9264345

Batch 31170, train_perplexity=18.660976, train_loss=2.9264345

Batch 31180, train_perplexity=18.660967, train_loss=2.926434

Batch 31190, train_perplexity=18.660973, train_loss=2.9264343

Batch 31200, train_perplexity=18.660976, train_loss=2.9264345

Batch 31210, train_perplexity=18.660976, train_loss=2.9264345

Batch 31220, train_perplexity=18.660976, train_loss=2.9264345

Batch 31230, train_perplexity=18.660973, train_loss=2.9264343

Batch 31240, train_perplexity=18.660976, train_loss=2.9264345

Batch 31250, train_perplexity=18.660967, train_loss=2.926434

Batch 31260, train_perplexity=18.660976, train_loss=2.9264345

Batch 31270, train_perplexity=18.660967, train_loss=2.926434

Batch 31280, train_perplexity=18.660967, train_loss=2.926434

Batch 31290, train_perplexity=18.660976, train_loss=2.9264345

Batch 31300, train_perplexity=18.660967, train_loss=2.926434

Batch 31310, train_perplexity=18.660967, train_loss=2.926434

Batch 31320, train_perplexity=18.660967, train_loss=2.926434

Batch 31330, train_perplexity=18.660976, train_loss=2.9264345

Batch 31340, train_perplexity=18.660967, train_loss=2.926434

Batch 31350, train_perplexity=18.660976, train_loss=2.9264345

Batch 31360, train_perplexity=18.660967, train_loss=2.926434

Batch 31370, train_perplexity=18.660976, train_loss=2.9264345

Batch 31380, train_perplexity=18.660976, train_loss=2.9264345

Batch 31390, train_perplexity=18.660973, train_loss=2.9264343

Batch 31400, train_perplexity=18.660967, train_loss=2.926434

Batch 31410, train_perplexity=18.660976, train_loss=2.9264345

Batch 31420, train_perplexity=18.660967, train_loss=2.926434

Batch 31430, train_perplexity=18.660973, train_loss=2.9264343

Batch 31440, train_perplexity=18.660967, train_loss=2.926434

Batch 31450, train_perplexity=18.660967, train_loss=2.926434

Batch 31460, train_perplexity=18.660973, train_loss=2.9264343

Batch 31470, train_perplexity=18.660967, train_loss=2.926434

Batch 31480, train_perplexity=18.660973, train_loss=2.9264343

Batch 31490, train_perplexity=18.660967, train_loss=2.926434

Batch 31500, train_perplexity=18.660967, train_loss=2.926434

Batch 31510, train_perplexity=18.660967, train_loss=2.926434

Batch 31520, train_perplexity=18.660967, train_loss=2.926434

Batch 31530, train_perplexity=18.660967, train_loss=2.926434

Batch 31540, train_perplexity=18.660967, train_loss=2.926434

Batch 31550, train_perplexity=18.660967, train_loss=2.926434

Batch 31560, train_perplexity=18.660976, train_loss=2.9264345

Batch 31570, train_perplexity=18.660963, train_loss=2.9264338

Batch 31580, train_perplexity=18.660973, train_loss=2.9264343

Batch 31590, train_perplexity=18.660967, train_loss=2.926434

Batch 31600, train_perplexity=18.660967, train_loss=2.926434

Batch 31610, train_perplexity=18.660967, train_loss=2.926434

Batch 31620, train_perplexity=18.660967, train_loss=2.926434

Batch 31630, train_perplexity=18.660967, train_loss=2.926434

Batch 31640, train_perplexity=18.660967, train_loss=2.926434

Batch 31650, train_perplexity=18.660967, train_loss=2.926434

Batch 31660, train_perplexity=18.660967, train_loss=2.926434

Batch 31670, train_perplexity=18.660967, train_loss=2.926434

Batch 31680, train_perplexity=18.660967, train_loss=2.926434

Batch 31690, train_perplexity=18.660967, train_loss=2.926434

Batch 31700, train_perplexity=18.660967, train_loss=2.926434

Batch 31710, train_perplexity=18.660967, train_loss=2.926434

Batch 31720, train_perplexity=18.660967, train_loss=2.926434

Batch 31730, train_perplexity=18.660967, train_loss=2.926434

Batch 31740, train_perplexity=18.660967, train_loss=2.926434

Batch 31750, train_perplexity=18.660967, train_loss=2.926434

Batch 31760, train_perplexity=18.660967, train_loss=2.926434

Batch 31770, train_perplexity=18.660967, train_loss=2.926434

Batch 31780, train_perplexity=18.660967, train_loss=2.926434

Batch 31790, train_perplexity=18.660967, train_loss=2.926434

Batch 31800, train_perplexity=18.660967, train_loss=2.926434

Batch 31810, train_perplexity=18.660967, train_loss=2.926434

Batch 31820, train_perplexity=18.660967, train_loss=2.926434

Batch 31830, train_perplexity=18.660967, train_loss=2.926434

Batch 31840, train_perplexity=18.660967, train_loss=2.926434

Batch 31850, train_perplexity=18.660973, train_loss=2.9264343

Batch 31860, train_perplexity=18.660967, train_loss=2.926434

Batch 31870, train_perplexity=18.660967, train_loss=2.926434

Batch 31880, train_perplexity=18.660967, train_loss=2.926434

Batch 31890, train_perplexity=18.660967, train_loss=2.926434

Batch 31900, train_perplexity=18.660963, train_loss=2.9264338

Batch 31910, train_perplexity=18.660973, train_loss=2.9264343

Batch 31920, train_perplexity=18.660967, train_loss=2.926434

Batch 31930, train_perplexity=18.660963, train_loss=2.9264338

Batch 31940, train_perplexity=18.660967, train_loss=2.926434

Batch 31950, train_perplexity=18.660967, train_loss=2.926434

Batch 31960, train_perplexity=18.660967, train_loss=2.926434

Batch 31970, train_perplexity=18.660967, train_loss=2.926434

Batch 31980, train_perplexity=18.660967, train_loss=2.926434

Batch 31990, train_perplexity=18.660967, train_loss=2.926434

Batch 32000, train_perplexity=18.660967, train_loss=2.926434

Batch 32010, train_perplexity=18.660967, train_loss=2.926434

Batch 32020, train_perplexity=18.660963, train_loss=2.9264338

Batch 32030, train_perplexity=18.660967, train_loss=2.926434

Batch 32040, train_perplexity=18.660967, train_loss=2.926434

Batch 32050, train_perplexity=18.66096, train_loss=2.9264336

Batch 32060, train_perplexity=18.660967, train_loss=2.926434

Batch 32070, train_perplexity=18.66096, train_loss=2.9264336

Batch 32080, train_perplexity=18.660967, train_loss=2.926434

Batch 32090, train_perplexity=18.660963, train_loss=2.9264338

Batch 32100, train_perplexity=18.66096, train_loss=2.9264336

Batch 32110, train_perplexity=18.660967, train_loss=2.926434

Batch 32120, train_perplexity=18.660963, train_loss=2.9264338

Batch 32130, train_perplexity=18.66096, train_loss=2.9264336

Batch 32140, train_perplexity=18.660963, train_loss=2.9264338

Batch 32150, train_perplexity=18.660963, train_loss=2.9264338

Batch 32160, train_perplexity=18.66096, train_loss=2.9264336

Batch 32170, train_perplexity=18.66096, train_loss=2.9264336

Batch 32180, train_perplexity=18.660963, train_loss=2.9264338

Batch 32190, train_perplexity=18.66096, train_loss=2.9264336

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 32200, train_perplexity=18.66096, train_loss=2.9264336

Batch 32210, train_perplexity=18.66096, train_loss=2.9264336

Batch 32220, train_perplexity=18.66096, train_loss=2.9264336

Batch 32230, train_perplexity=18.660963, train_loss=2.9264338

Batch 32240, train_perplexity=18.66096, train_loss=2.9264336

Batch 32250, train_perplexity=18.66096, train_loss=2.9264336

Batch 32260, train_perplexity=18.66096, train_loss=2.9264336

Batch 32270, train_perplexity=18.66096, train_loss=2.9264336

Batch 32280, train_perplexity=18.66096, train_loss=2.9264336

Batch 32290, train_perplexity=18.66096, train_loss=2.9264336

Batch 32300, train_perplexity=18.66096, train_loss=2.9264336

Batch 32310, train_perplexity=18.66096, train_loss=2.9264336

Batch 32320, train_perplexity=18.66096, train_loss=2.9264336

Batch 32330, train_perplexity=18.66096, train_loss=2.9264336

Batch 32340, train_perplexity=18.66096, train_loss=2.9264336
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 32350, train_perplexity=18.66096, train_loss=2.9264336

Batch 32360, train_perplexity=18.66096, train_loss=2.9264336

Batch 32370, train_perplexity=18.66096, train_loss=2.9264336

Batch 32380, train_perplexity=18.66096, train_loss=2.9264336

Batch 32390, train_perplexity=18.66096, train_loss=2.9264336

Batch 32400, train_perplexity=18.66095, train_loss=2.926433

Batch 32410, train_perplexity=18.66096, train_loss=2.9264336

Batch 32420, train_perplexity=18.66096, train_loss=2.9264336

Batch 32430, train_perplexity=18.66096, train_loss=2.9264336

Batch 32440, train_perplexity=18.660954, train_loss=2.9264333

Batch 32450, train_perplexity=18.660954, train_loss=2.9264333

Batch 32460, train_perplexity=18.66096, train_loss=2.9264336

Batch 32470, train_perplexity=18.66096, train_loss=2.9264336

Batch 32480, train_perplexity=18.66096, train_loss=2.9264336

Batch 32490, train_perplexity=18.66096, train_loss=2.9264336

Batch 32500, train_perplexity=18.66096, train_loss=2.9264336

Batch 32510, train_perplexity=18.66095, train_loss=2.926433

Batch 32520, train_perplexity=18.66096, train_loss=2.9264336

Batch 32530, train_perplexity=18.66096, train_loss=2.9264336

Batch 32540, train_perplexity=18.660954, train_loss=2.9264333

Batch 32550, train_perplexity=18.66095, train_loss=2.926433

Batch 32560, train_perplexity=18.660954, train_loss=2.9264333

Batch 32570, train_perplexity=18.66096, train_loss=2.9264336

Batch 32580, train_perplexity=18.660954, train_loss=2.9264333

Batch 32590, train_perplexity=18.66096, train_loss=2.9264336

Batch 32600, train_perplexity=18.66096, train_loss=2.9264336

Batch 32610, train_perplexity=18.66096, train_loss=2.9264336

Batch 32620, train_perplexity=18.66096, train_loss=2.9264336

Batch 32630, train_perplexity=18.660954, train_loss=2.9264333

Batch 32640, train_perplexity=18.66096, train_loss=2.9264336

Batch 32650, train_perplexity=18.66096, train_loss=2.9264336

Batch 32660, train_perplexity=18.660954, train_loss=2.9264333

Batch 32670, train_perplexity=18.66096, train_loss=2.9264336

Batch 32680, train_perplexity=18.66096, train_loss=2.9264336

Batch 32690, train_perplexity=18.66095, train_loss=2.926433

Batch 32700, train_perplexity=18.66096, train_loss=2.9264336

Batch 32710, train_perplexity=18.66096, train_loss=2.9264336

Batch 32720, train_perplexity=18.66096, train_loss=2.9264336

Batch 32730, train_perplexity=18.66095, train_loss=2.926433

Batch 32740, train_perplexity=18.660954, train_loss=2.9264333

Batch 32750, train_perplexity=18.660954, train_loss=2.9264333

Batch 32760, train_perplexity=18.66095, train_loss=2.926433

Batch 32770, train_perplexity=18.660954, train_loss=2.9264333

Batch 32780, train_perplexity=18.66095, train_loss=2.926433

Batch 32790, train_perplexity=18.660954, train_loss=2.9264333

Batch 32800, train_perplexity=18.660954, train_loss=2.9264333

Batch 32810, train_perplexity=18.66096, train_loss=2.9264336

Batch 32820, train_perplexity=18.66095, train_loss=2.926433

Batch 32830, train_perplexity=18.66095, train_loss=2.926433

Batch 32840, train_perplexity=18.660954, train_loss=2.9264333

Batch 32850, train_perplexity=18.66095, train_loss=2.926433

Batch 32860, train_perplexity=18.660954, train_loss=2.9264333

Batch 32870, train_perplexity=18.66095, train_loss=2.926433

Batch 32880, train_perplexity=18.660954, train_loss=2.9264333

Batch 32890, train_perplexity=18.66095, train_loss=2.926433

Batch 32900, train_perplexity=18.660954, train_loss=2.9264333

Batch 32910, train_perplexity=18.66095, train_loss=2.926433

Batch 32920, train_perplexity=18.660954, train_loss=2.9264333

Batch 32930, train_perplexity=18.660954, train_loss=2.9264333

Batch 32940, train_perplexity=18.66095, train_loss=2.926433

Batch 32950, train_perplexity=18.66095, train_loss=2.926433

Batch 32960, train_perplexity=18.66095, train_loss=2.926433

Batch 32970, train_perplexity=18.66095, train_loss=2.926433

Batch 32980, train_perplexity=18.66095, train_loss=2.926433

Batch 32990, train_perplexity=18.660954, train_loss=2.9264333

Batch 33000, train_perplexity=18.66095, train_loss=2.926433

Batch 33010, train_perplexity=18.66095, train_loss=2.926433

Batch 33020, train_perplexity=18.66095, train_loss=2.926433

Batch 33030, train_perplexity=18.66096, train_loss=2.9264336

Batch 33040, train_perplexity=18.66095, train_loss=2.926433

Batch 33050, train_perplexity=18.66095, train_loss=2.926433

Batch 33060, train_perplexity=18.66095, train_loss=2.926433

Batch 33070, train_perplexity=18.66095, train_loss=2.926433

Batch 33080, train_perplexity=18.66095, train_loss=2.926433

Batch 33090, train_perplexity=18.66095, train_loss=2.926433

Batch 33100, train_perplexity=18.66095, train_loss=2.926433

Batch 33110, train_perplexity=18.66095, train_loss=2.926433

Batch 33120, train_perplexity=18.66095, train_loss=2.926433

Batch 33130, train_perplexity=18.66095, train_loss=2.926433

Batch 33140, train_perplexity=18.66095, train_loss=2.926433

Batch 33150, train_perplexity=18.66095, train_loss=2.926433

Batch 33160, train_perplexity=18.66095, train_loss=2.926433

Batch 33170, train_perplexity=18.660954, train_loss=2.9264333

Batch 33180, train_perplexity=18.66095, train_loss=2.926433

Batch 33190, train_perplexity=18.66095, train_loss=2.926433

Batch 33200, train_perplexity=18.66095, train_loss=2.926433

Batch 33210, train_perplexity=18.66095, train_loss=2.926433

Batch 33220, train_perplexity=18.66095, train_loss=2.926433

Batch 33230, train_perplexity=18.66095, train_loss=2.926433

Batch 33240, train_perplexity=18.66095, train_loss=2.926433

Batch 33250, train_perplexity=18.66095, train_loss=2.926433

Batch 33260, train_perplexity=18.660946, train_loss=2.9264328

Batch 33270, train_perplexity=18.66095, train_loss=2.926433

Batch 33280, train_perplexity=18.66095, train_loss=2.926433

Batch 33290, train_perplexity=18.660946, train_loss=2.9264328

Batch 33300, train_perplexity=18.66095, train_loss=2.926433

Batch 33310, train_perplexity=18.66095, train_loss=2.926433

Batch 33320, train_perplexity=18.66095, train_loss=2.926433

Batch 33330, train_perplexity=18.66095, train_loss=2.926433

Batch 33340, train_perplexity=18.66095, train_loss=2.926433

Batch 33350, train_perplexity=18.66095, train_loss=2.926433

Batch 33360, train_perplexity=18.66095, train_loss=2.926433

Batch 33370, train_perplexity=18.66095, train_loss=2.926433

Batch 33380, train_perplexity=18.66095, train_loss=2.926433

Batch 33390, train_perplexity=18.66095, train_loss=2.926433

Batch 33400, train_perplexity=18.660946, train_loss=2.9264328

Batch 33410, train_perplexity=18.660946, train_loss=2.9264328

Batch 33420, train_perplexity=18.660946, train_loss=2.9264328

Batch 33430, train_perplexity=18.66095, train_loss=2.926433

Batch 33440, train_perplexity=18.66095, train_loss=2.926433

Batch 33450, train_perplexity=18.66095, train_loss=2.926433

Batch 33460, train_perplexity=18.66095, train_loss=2.926433

Batch 33470, train_perplexity=18.66095, train_loss=2.926433

Batch 33480, train_perplexity=18.66095, train_loss=2.926433

Batch 33490, train_perplexity=18.66095, train_loss=2.926433

Batch 33500, train_perplexity=18.660946, train_loss=2.9264328

Batch 33510, train_perplexity=18.66095, train_loss=2.926433

Batch 33520, train_perplexity=18.66095, train_loss=2.926433

Batch 33530, train_perplexity=18.66095, train_loss=2.926433

Batch 33540, train_perplexity=18.66095, train_loss=2.926433

Batch 33550, train_perplexity=18.660946, train_loss=2.9264328

Batch 33560, train_perplexity=18.66095, train_loss=2.926433

Batch 33570, train_perplexity=18.66095, train_loss=2.926433

Batch 33580, train_perplexity=18.660946, train_loss=2.9264328

Batch 33590, train_perplexity=18.66095, train_loss=2.926433

Batch 33600, train_perplexity=18.66095, train_loss=2.926433

Batch 33610, train_perplexity=18.660946, train_loss=2.9264328

Batch 33620, train_perplexity=18.66095, train_loss=2.926433

Batch 33630, train_perplexity=18.66095, train_loss=2.926433

Batch 33640, train_perplexity=18.660946, train_loss=2.9264328

Batch 33650, train_perplexity=18.66095, train_loss=2.926433

Batch 33660, train_perplexity=18.66095, train_loss=2.926433

Batch 33670, train_perplexity=18.66095, train_loss=2.926433
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33680, train_perplexity=18.660946, train_loss=2.9264328

Batch 33690, train_perplexity=18.660946, train_loss=2.9264328

Batch 33700, train_perplexity=18.66094, train_loss=2.9264326

Batch 33710, train_perplexity=18.66095, train_loss=2.926433

Batch 33720, train_perplexity=18.660946, train_loss=2.9264328

Batch 33730, train_perplexity=18.66095, train_loss=2.926433

Batch 33740, train_perplexity=18.660946, train_loss=2.9264328

Batch 33750, train_perplexity=18.66095, train_loss=2.926433

Batch 33760, train_perplexity=18.66095, train_loss=2.926433

Batch 33770, train_perplexity=18.660946, train_loss=2.9264328

Batch 33780, train_perplexity=18.66094, train_loss=2.9264326

Batch 33790, train_perplexity=18.660946, train_loss=2.9264328

Batch 33800, train_perplexity=18.66094, train_loss=2.9264326

Batch 33810, train_perplexity=18.660946, train_loss=2.9264328

Batch 33820, train_perplexity=18.660946, train_loss=2.9264328

Batch 33830, train_perplexity=18.660946, train_loss=2.9264328

Batch 33840, train_perplexity=18.660946, train_loss=2.9264328

Batch 33850, train_perplexity=18.660946, train_loss=2.9264328

Batch 33860, train_perplexity=18.660946, train_loss=2.9264328

Batch 33870, train_perplexity=18.660946, train_loss=2.9264328

Batch 33880, train_perplexity=18.66095, train_loss=2.926433

Batch 33890, train_perplexity=18.660946, train_loss=2.9264328

Batch 33900, train_perplexity=18.660946, train_loss=2.9264328

Batch 33910, train_perplexity=18.66094, train_loss=2.9264326

Batch 33920, train_perplexity=18.66094, train_loss=2.9264326

Batch 33930, train_perplexity=18.660946, train_loss=2.9264328

Batch 33940, train_perplexity=18.660946, train_loss=2.9264328

Batch 33950, train_perplexity=18.66094, train_loss=2.9264326

Batch 33960, train_perplexity=18.660946, train_loss=2.9264328

Batch 33970, train_perplexity=18.66094, train_loss=2.9264326

Batch 33980, train_perplexity=18.66095, train_loss=2.926433

Batch 33990, train_perplexity=18.66095, train_loss=2.926433

Batch 34000, train_perplexity=18.660946, train_loss=2.9264328

Batch 34010, train_perplexity=18.660946, train_loss=2.9264328

Batch 34020, train_perplexity=18.66094, train_loss=2.9264326

Batch 34030, train_perplexity=18.66094, train_loss=2.9264326

Batch 34040, train_perplexity=18.66094, train_loss=2.9264326

Batch 34050, train_perplexity=18.66094, train_loss=2.9264326

Batch 34060, train_perplexity=18.660946, train_loss=2.9264328

Batch 34070, train_perplexity=18.66094, train_loss=2.9264326

Batch 34080, train_perplexity=18.66094, train_loss=2.9264326

Batch 34090, train_perplexity=18.66094, train_loss=2.9264326

Batch 34100, train_perplexity=18.66094, train_loss=2.9264326

Batch 34110, train_perplexity=18.66094, train_loss=2.9264326

Batch 34120, train_perplexity=18.66094, train_loss=2.9264326

Batch 34130, train_perplexity=18.66094, train_loss=2.9264326

Batch 34140, train_perplexity=18.66094, train_loss=2.9264326

Batch 34150, train_perplexity=18.66094, train_loss=2.9264326

Batch 34160, train_perplexity=18.66094, train_loss=2.9264326

Batch 34170, train_perplexity=18.66094, train_loss=2.9264326

Batch 34180, train_perplexity=18.66094, train_loss=2.9264326

Batch 34190, train_perplexity=18.66094, train_loss=2.9264326

Batch 34200, train_perplexity=18.66094, train_loss=2.9264326

Batch 34210, train_perplexity=18.66094, train_loss=2.9264326

Batch 34220, train_perplexity=18.66094, train_loss=2.9264326

Batch 34230, train_perplexity=18.66094, train_loss=2.9264326

Batch 34240, train_perplexity=18.66094, train_loss=2.9264326

Batch 34250, train_perplexity=18.66094, train_loss=2.9264326

Batch 34260, train_perplexity=18.66094, train_loss=2.9264326

Batch 34270, train_perplexity=18.660936, train_loss=2.9264324

Batch 34280, train_perplexity=18.66094, train_loss=2.9264326

Batch 34290, train_perplexity=18.66094, train_loss=2.9264326

Batch 34300, train_perplexity=18.66094, train_loss=2.9264326

Batch 34310, train_perplexity=18.66094, train_loss=2.9264326

Batch 34320, train_perplexity=18.66094, train_loss=2.9264326

Batch 34330, train_perplexity=18.660936, train_loss=2.9264324

Batch 34340, train_perplexity=18.66094, train_loss=2.9264326

Batch 34350, train_perplexity=18.660936, train_loss=2.9264324

Batch 34360, train_perplexity=18.66094, train_loss=2.9264326

Batch 34370, train_perplexity=18.660933, train_loss=2.9264321

Batch 34380, train_perplexity=18.660936, train_loss=2.9264324

Batch 34390, train_perplexity=18.66094, train_loss=2.9264326

Batch 34400, train_perplexity=18.660936, train_loss=2.9264324

Batch 34410, train_perplexity=18.66094, train_loss=2.9264326

Batch 34420, train_perplexity=18.660933, train_loss=2.9264321

Batch 34430, train_perplexity=18.66094, train_loss=2.9264326

Batch 34440, train_perplexity=18.660936, train_loss=2.9264324

Batch 34450, train_perplexity=18.660933, train_loss=2.9264321

Batch 34460, train_perplexity=18.660933, train_loss=2.9264321

Batch 34470, train_perplexity=18.660936, train_loss=2.9264324

Batch 34480, train_perplexity=18.660933, train_loss=2.9264321

Batch 34490, train_perplexity=18.660933, train_loss=2.9264321

Batch 34500, train_perplexity=18.660933, train_loss=2.9264321

Batch 34510, train_perplexity=18.66094, train_loss=2.9264326

Batch 34520, train_perplexity=18.660933, train_loss=2.9264321

Batch 34530, train_perplexity=18.66094, train_loss=2.9264326

Batch 34540, train_perplexity=18.660933, train_loss=2.9264321

Batch 34550, train_perplexity=18.660933, train_loss=2.9264321

Batch 34560, train_perplexity=18.660933, train_loss=2.9264321

Batch 34570, train_perplexity=18.660936, train_loss=2.9264324

Batch 34580, train_perplexity=18.660933, train_loss=2.9264321

Batch 34590, train_perplexity=18.660936, train_loss=2.9264324

Batch 34600, train_perplexity=18.660936, train_loss=2.9264324

Batch 34610, train_perplexity=18.660936, train_loss=2.9264324

Batch 34620, train_perplexity=18.660933, train_loss=2.9264321

Batch 34630, train_perplexity=18.660936, train_loss=2.9264324

Batch 34640, train_perplexity=18.660936, train_loss=2.9264324

Batch 34650, train_perplexity=18.660933, train_loss=2.9264321

Batch 34660, train_perplexity=18.660936, train_loss=2.9264324

Batch 34670, train_perplexity=18.660933, train_loss=2.9264321

Batch 34680, train_perplexity=18.660933, train_loss=2.9264321

Batch 34690, train_perplexity=18.660933, train_loss=2.9264321

Batch 34700, train_perplexity=18.660936, train_loss=2.9264324

Batch 34710, train_perplexity=18.660933, train_loss=2.9264321

Batch 34720, train_perplexity=18.660933, train_loss=2.9264321

Batch 34730, train_perplexity=18.660936, train_loss=2.9264324

Batch 34740, train_perplexity=18.660933, train_loss=2.9264321

Batch 34750, train_perplexity=18.660933, train_loss=2.9264321

Batch 34760, train_perplexity=18.660933, train_loss=2.9264321

Batch 34770, train_perplexity=18.660936, train_loss=2.9264324

Batch 34780, train_perplexity=18.660933, train_loss=2.9264321

Batch 34790, train_perplexity=18.660933, train_loss=2.9264321

Batch 34800, train_perplexity=18.660936, train_loss=2.9264324

Batch 34810, train_perplexity=18.660933, train_loss=2.9264321

Batch 34820, train_perplexity=18.660933, train_loss=2.9264321

Batch 34830, train_perplexity=18.660933, train_loss=2.9264321

Batch 34840, train_perplexity=18.660933, train_loss=2.9264321

Batch 34850, train_perplexity=18.660933, train_loss=2.9264321

Batch 34860, train_perplexity=18.660933, train_loss=2.9264321

Batch 34870, train_perplexity=18.660933, train_loss=2.9264321

Batch 34880, train_perplexity=18.660933, train_loss=2.9264321

Batch 34890, train_perplexity=18.660933, train_loss=2.9264321

Batch 34900, train_perplexity=18.660933, train_loss=2.9264321

Batch 34910, train_perplexity=18.660933, train_loss=2.9264321

Batch 34920, train_perplexity=18.660936, train_loss=2.9264324

Batch 34930, train_perplexity=18.660933, train_loss=2.9264321

Batch 34940, train_perplexity=18.660933, train_loss=2.9264321

Batch 34950, train_perplexity=18.660933, train_loss=2.9264321

Batch 34960, train_perplexity=18.660933, train_loss=2.9264321

Batch 34970, train_perplexity=18.660933, train_loss=2.9264321

Batch 34980, train_perplexity=18.660923, train_loss=2.9264317
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34990, train_perplexity=18.660933, train_loss=2.9264321

Batch 35000, train_perplexity=18.660933, train_loss=2.9264321

Batch 35010, train_perplexity=18.660933, train_loss=2.9264321

Batch 35020, train_perplexity=18.660933, train_loss=2.9264321

Batch 35030, train_perplexity=18.660927, train_loss=2.926432

Batch 35040, train_perplexity=18.660923, train_loss=2.9264317

Batch 35050, train_perplexity=18.660923, train_loss=2.9264317

Batch 35060, train_perplexity=18.660933, train_loss=2.9264321

Batch 35070, train_perplexity=18.660933, train_loss=2.9264321

Batch 35080, train_perplexity=18.660923, train_loss=2.9264317

Batch 35090, train_perplexity=18.660933, train_loss=2.9264321

Batch 35100, train_perplexity=18.660933, train_loss=2.9264321

Batch 35110, train_perplexity=18.660933, train_loss=2.9264321

Batch 35120, train_perplexity=18.660933, train_loss=2.9264321

Batch 35130, train_perplexity=18.660927, train_loss=2.926432

Batch 35140, train_perplexity=18.660933, train_loss=2.9264321

Batch 35150, train_perplexity=18.660933, train_loss=2.9264321

Batch 35160, train_perplexity=18.660927, train_loss=2.926432

Batch 35170, train_perplexity=18.660933, train_loss=2.9264321

Batch 35180, train_perplexity=18.660923, train_loss=2.9264317

Batch 35190, train_perplexity=18.660933, train_loss=2.9264321

Batch 35200, train_perplexity=18.660933, train_loss=2.9264321

Batch 35210, train_perplexity=18.660927, train_loss=2.926432

Batch 35220, train_perplexity=18.660933, train_loss=2.9264321

Batch 35230, train_perplexity=18.660933, train_loss=2.9264321

Batch 35240, train_perplexity=18.660927, train_loss=2.926432

Batch 35250, train_perplexity=18.660927, train_loss=2.926432

Batch 35260, train_perplexity=18.660933, train_loss=2.9264321

Batch 35270, train_perplexity=18.660927, train_loss=2.926432

Batch 35280, train_perplexity=18.660923, train_loss=2.9264317

Batch 35290, train_perplexity=18.660933, train_loss=2.9264321

Batch 35300, train_perplexity=18.660933, train_loss=2.9264321

Batch 35310, train_perplexity=18.660933, train_loss=2.9264321

Batch 35320, train_perplexity=18.660933, train_loss=2.9264321

Batch 35330, train_perplexity=18.660923, train_loss=2.9264317

Batch 35340, train_perplexity=18.660927, train_loss=2.926432

Batch 35350, train_perplexity=18.660927, train_loss=2.926432

Batch 35360, train_perplexity=18.660933, train_loss=2.9264321

Batch 35370, train_perplexity=18.660933, train_loss=2.9264321

Batch 35380, train_perplexity=18.660927, train_loss=2.926432

Batch 35390, train_perplexity=18.660933, train_loss=2.9264321

Batch 35400, train_perplexity=18.660927, train_loss=2.926432

Batch 35410, train_perplexity=18.660933, train_loss=2.9264321

Batch 35420, train_perplexity=18.660923, train_loss=2.9264317

Batch 35430, train_perplexity=18.660923, train_loss=2.9264317

Batch 35440, train_perplexity=18.660927, train_loss=2.926432

Batch 35450, train_perplexity=18.660933, train_loss=2.9264321

Batch 35460, train_perplexity=18.660933, train_loss=2.9264321

Batch 35470, train_perplexity=18.660923, train_loss=2.9264317

Batch 35480, train_perplexity=18.660933, train_loss=2.9264321

Batch 35490, train_perplexity=18.660933, train_loss=2.9264321

Batch 35500, train_perplexity=18.660927, train_loss=2.926432

Batch 35510, train_perplexity=18.660923, train_loss=2.9264317

Batch 35520, train_perplexity=18.660923, train_loss=2.9264317

Batch 35530, train_perplexity=18.660923, train_loss=2.9264317

Batch 35540, train_perplexity=18.660923, train_loss=2.9264317

Batch 35550, train_perplexity=18.660923, train_loss=2.9264317

Batch 35560, train_perplexity=18.660923, train_loss=2.9264317

Batch 35570, train_perplexity=18.660923, train_loss=2.9264317

Batch 35580, train_perplexity=18.660933, train_loss=2.9264321

Batch 35590, train_perplexity=18.660927, train_loss=2.926432

Batch 35600, train_perplexity=18.660927, train_loss=2.926432

Batch 35610, train_perplexity=18.660927, train_loss=2.926432

Batch 35620, train_perplexity=18.660933, train_loss=2.9264321

Batch 35630, train_perplexity=18.660923, train_loss=2.9264317

Batch 35640, train_perplexity=18.660927, train_loss=2.926432

Batch 35650, train_perplexity=18.660923, train_loss=2.9264317

Batch 35660, train_perplexity=18.660933, train_loss=2.9264321

Batch 35670, train_perplexity=18.660933, train_loss=2.9264321

Batch 35680, train_perplexity=18.660923, train_loss=2.9264317

Batch 35690, train_perplexity=18.660923, train_loss=2.9264317

Batch 35700, train_perplexity=18.660923, train_loss=2.9264317

Batch 35710, train_perplexity=18.660927, train_loss=2.926432

Batch 35720, train_perplexity=18.660923, train_loss=2.9264317

Batch 35730, train_perplexity=18.660923, train_loss=2.9264317

Batch 35740, train_perplexity=18.660933, train_loss=2.9264321

Batch 35750, train_perplexity=18.660923, train_loss=2.9264317

Batch 35760, train_perplexity=18.660927, train_loss=2.926432

Batch 35770, train_perplexity=18.660927, train_loss=2.926432

Batch 35780, train_perplexity=18.660923, train_loss=2.9264317

Batch 35790, train_perplexity=18.660923, train_loss=2.9264317

Batch 35800, train_perplexity=18.660927, train_loss=2.926432

Batch 35810, train_perplexity=18.660923, train_loss=2.9264317

Batch 35820, train_perplexity=18.660923, train_loss=2.9264317

Batch 35830, train_perplexity=18.660923, train_loss=2.9264317

Batch 35840, train_perplexity=18.660923, train_loss=2.9264317

Batch 35850, train_perplexity=18.660923, train_loss=2.9264317

Batch 35860, train_perplexity=18.660923, train_loss=2.9264317

Batch 35870, train_perplexity=18.660923, train_loss=2.9264317

Batch 35880, train_perplexity=18.660923, train_loss=2.9264317

Batch 35890, train_perplexity=18.660923, train_loss=2.9264317

Batch 35900, train_perplexity=18.660923, train_loss=2.9264317

Batch 35910, train_perplexity=18.660923, train_loss=2.9264317

Batch 35920, train_perplexity=18.660927, train_loss=2.926432

Batch 35930, train_perplexity=18.660927, train_loss=2.926432

Batch 35940, train_perplexity=18.660923, train_loss=2.9264317

Batch 35950, train_perplexity=18.660923, train_loss=2.9264317

Batch 35960, train_perplexity=18.660927, train_loss=2.926432

Batch 35970, train_perplexity=18.660923, train_loss=2.9264317

Batch 35980, train_perplexity=18.660923, train_loss=2.9264317

Batch 35990, train_perplexity=18.660923, train_loss=2.9264317

Batch 36000, train_perplexity=18.660927, train_loss=2.926432

Batch 36010, train_perplexity=18.660923, train_loss=2.9264317

Batch 36020, train_perplexity=18.660923, train_loss=2.9264317

Batch 36030, train_perplexity=18.660927, train_loss=2.926432

Batch 36040, train_perplexity=18.660923, train_loss=2.9264317

Batch 36050, train_perplexity=18.660923, train_loss=2.9264317

Batch 36060, train_perplexity=18.660923, train_loss=2.9264317

Batch 36070, train_perplexity=18.660923, train_loss=2.9264317

Batch 36080, train_perplexity=18.660923, train_loss=2.9264317

Batch 36090, train_perplexity=18.660927, train_loss=2.926432

Batch 36100, train_perplexity=18.660923, train_loss=2.9264317

Batch 36110, train_perplexity=18.660923, train_loss=2.9264317

Batch 36120, train_perplexity=18.660923, train_loss=2.9264317

Batch 36130, train_perplexity=18.660923, train_loss=2.9264317

Batch 36140, train_perplexity=18.660923, train_loss=2.9264317

Batch 36150, train_perplexity=18.660923, train_loss=2.9264317

Batch 36160, train_perplexity=18.660923, train_loss=2.9264317

Batch 36170, train_perplexity=18.660923, train_loss=2.9264317

Batch 36180, train_perplexity=18.660923, train_loss=2.9264317

Batch 36190, train_perplexity=18.660923, train_loss=2.9264317

Batch 36200, train_perplexity=18.660923, train_loss=2.9264317

Batch 36210, train_perplexity=18.660923, train_loss=2.9264317

Batch 36220, train_perplexity=18.660923, train_loss=2.9264317

Batch 36230, train_perplexity=18.660923, train_loss=2.9264317

Batch 36240, train_perplexity=18.660923, train_loss=2.9264317

Batch 36250, train_perplexity=18.660923, train_loss=2.9264317

Batch 36260, train_perplexity=18.660923, train_loss=2.9264317

Batch 36270, train_perplexity=18.660923, train_loss=2.9264317

Batch 36280, train_perplexity=18.660923, train_loss=2.9264317

Batch 36290, train_perplexity=18.660923, train_loss=2.9264317
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 36300, train_perplexity=18.660923, train_loss=2.9264317

Batch 36310, train_perplexity=18.660923, train_loss=2.9264317

Batch 36320, train_perplexity=18.660923, train_loss=2.9264317

Batch 36330, train_perplexity=18.660923, train_loss=2.9264317

Batch 36340, train_perplexity=18.660923, train_loss=2.9264317

Batch 36350, train_perplexity=18.660923, train_loss=2.9264317

Batch 36360, train_perplexity=18.660923, train_loss=2.9264317

Batch 36370, train_perplexity=18.660923, train_loss=2.9264317

Batch 36380, train_perplexity=18.660923, train_loss=2.9264317

Batch 36390, train_perplexity=18.660923, train_loss=2.9264317

Batch 36400, train_perplexity=18.660923, train_loss=2.9264317

Batch 36410, train_perplexity=18.660923, train_loss=2.9264317

Batch 36420, train_perplexity=18.660923, train_loss=2.9264317

Batch 36430, train_perplexity=18.660923, train_loss=2.9264317

Batch 36440, train_perplexity=18.660923, train_loss=2.9264317

Batch 36450, train_perplexity=18.660923, train_loss=2.9264317

Batch 36460, train_perplexity=18.660923, train_loss=2.9264317

Batch 36470, train_perplexity=18.660923, train_loss=2.9264317

Batch 36480, train_perplexity=18.660923, train_loss=2.9264317

Batch 36490, train_perplexity=18.66092, train_loss=2.9264314

Batch 36500, train_perplexity=18.66092, train_loss=2.9264314

Batch 36510, train_perplexity=18.660923, train_loss=2.9264317

Batch 36520, train_perplexity=18.66092, train_loss=2.9264314

Batch 36530, train_perplexity=18.660923, train_loss=2.9264317

Batch 36540, train_perplexity=18.66092, train_loss=2.9264314

Batch 36550, train_perplexity=18.660923, train_loss=2.9264317

Batch 36560, train_perplexity=18.660923, train_loss=2.9264317

Batch 36570, train_perplexity=18.66092, train_loss=2.9264314

Batch 36580, train_perplexity=18.660923, train_loss=2.9264317

Batch 36590, train_perplexity=18.660923, train_loss=2.9264317

Batch 36600, train_perplexity=18.660923, train_loss=2.9264317

Batch 36610, train_perplexity=18.66092, train_loss=2.9264314

Batch 36620, train_perplexity=18.66092, train_loss=2.9264314

Batch 36630, train_perplexity=18.66092, train_loss=2.9264314

Batch 36640, train_perplexity=18.660923, train_loss=2.9264317

Batch 36650, train_perplexity=18.660923, train_loss=2.9264317

Batch 36660, train_perplexity=18.660923, train_loss=2.9264317

Batch 36670, train_perplexity=18.660923, train_loss=2.9264317

Batch 36680, train_perplexity=18.660923, train_loss=2.9264317

Batch 36690, train_perplexity=18.660923, train_loss=2.9264317

Batch 36700, train_perplexity=18.66092, train_loss=2.9264314

Batch 36710, train_perplexity=18.660923, train_loss=2.9264317

Batch 36720, train_perplexity=18.66092, train_loss=2.9264314

Batch 36730, train_perplexity=18.660923, train_loss=2.9264317

Batch 36740, train_perplexity=18.660923, train_loss=2.9264317

Batch 36750, train_perplexity=18.660923, train_loss=2.9264317

Batch 36760, train_perplexity=18.660923, train_loss=2.9264317

Batch 36770, train_perplexity=18.660923, train_loss=2.9264317

Batch 36780, train_perplexity=18.66092, train_loss=2.9264314

Batch 36790, train_perplexity=18.66092, train_loss=2.9264314

Batch 36800, train_perplexity=18.660923, train_loss=2.9264317

Batch 36810, train_perplexity=18.660923, train_loss=2.9264317

Batch 36820, train_perplexity=18.660923, train_loss=2.9264317

Batch 36830, train_perplexity=18.66092, train_loss=2.9264314

Batch 36840, train_perplexity=18.66092, train_loss=2.9264314

Batch 36850, train_perplexity=18.66092, train_loss=2.9264314

Batch 36860, train_perplexity=18.660923, train_loss=2.9264317

Batch 36870, train_perplexity=18.66092, train_loss=2.9264314

Batch 36880, train_perplexity=18.66092, train_loss=2.9264314

Batch 36890, train_perplexity=18.660923, train_loss=2.9264317

Batch 36900, train_perplexity=18.66092, train_loss=2.9264314

Batch 36910, train_perplexity=18.660923, train_loss=2.9264317

Batch 36920, train_perplexity=18.66092, train_loss=2.9264314

Batch 36930, train_perplexity=18.66092, train_loss=2.9264314

Batch 36940, train_perplexity=18.660923, train_loss=2.9264317

Batch 36950, train_perplexity=18.660923, train_loss=2.9264317

Batch 36960, train_perplexity=18.660923, train_loss=2.9264317

Batch 36970, train_perplexity=18.66092, train_loss=2.9264314

Batch 36980, train_perplexity=18.66092, train_loss=2.9264314

Batch 36990, train_perplexity=18.66092, train_loss=2.9264314

Batch 37000, train_perplexity=18.66092, train_loss=2.9264314

Batch 37010, train_perplexity=18.66092, train_loss=2.9264314

Batch 37020, train_perplexity=18.660923, train_loss=2.9264317

Batch 37030, train_perplexity=18.66092, train_loss=2.9264314

Batch 37040, train_perplexity=18.66092, train_loss=2.9264314

Batch 37050, train_perplexity=18.660923, train_loss=2.9264317

Batch 37060, train_perplexity=18.66092, train_loss=2.9264314

Batch 37070, train_perplexity=18.66092, train_loss=2.9264314

Batch 37080, train_perplexity=18.66092, train_loss=2.9264314

Batch 37090, train_perplexity=18.660923, train_loss=2.9264317

Batch 37100, train_perplexity=18.660913, train_loss=2.9264312

Batch 37110, train_perplexity=18.66092, train_loss=2.9264314

Batch 37120, train_perplexity=18.66092, train_loss=2.9264314

Batch 37130, train_perplexity=18.66092, train_loss=2.9264314

Batch 37140, train_perplexity=18.660913, train_loss=2.9264312

Batch 37150, train_perplexity=18.660913, train_loss=2.9264312

Batch 37160, train_perplexity=18.66092, train_loss=2.9264314

Batch 37170, train_perplexity=18.660923, train_loss=2.9264317

Batch 37180, train_perplexity=18.66092, train_loss=2.9264314

Batch 37190, train_perplexity=18.660923, train_loss=2.9264317

Batch 37200, train_perplexity=18.66092, train_loss=2.9264314

Batch 37210, train_perplexity=18.66092, train_loss=2.9264314

Batch 37220, train_perplexity=18.66092, train_loss=2.9264314

Batch 37230, train_perplexity=18.660913, train_loss=2.9264312

Batch 37240, train_perplexity=18.66092, train_loss=2.9264314

Batch 37250, train_perplexity=18.66092, train_loss=2.9264314

Batch 37260, train_perplexity=18.66092, train_loss=2.9264314

Batch 37270, train_perplexity=18.66092, train_loss=2.9264314

Batch 37280, train_perplexity=18.66092, train_loss=2.9264314

Batch 37290, train_perplexity=18.66092, train_loss=2.9264314

Batch 37300, train_perplexity=18.66092, train_loss=2.9264314

Batch 37310, train_perplexity=18.66092, train_loss=2.9264314

Batch 37320, train_perplexity=18.660913, train_loss=2.9264312

Batch 37330, train_perplexity=18.66092, train_loss=2.9264314

Batch 37340, train_perplexity=18.660913, train_loss=2.9264312

Batch 37350, train_perplexity=18.66092, train_loss=2.9264314

Batch 37360, train_perplexity=18.66092, train_loss=2.9264314

Batch 37370, train_perplexity=18.66092, train_loss=2.9264314

Batch 37380, train_perplexity=18.660923, train_loss=2.9264317

Batch 37390, train_perplexity=18.660913, train_loss=2.9264312

Batch 37400, train_perplexity=18.660913, train_loss=2.9264312

Batch 37410, train_perplexity=18.660913, train_loss=2.9264312

Batch 37420, train_perplexity=18.660913, train_loss=2.9264312

Batch 37430, train_perplexity=18.66092, train_loss=2.9264314

Batch 37440, train_perplexity=18.660913, train_loss=2.9264312

Batch 37450, train_perplexity=18.660913, train_loss=2.9264312

Batch 37460, train_perplexity=18.66092, train_loss=2.9264314

Batch 37470, train_perplexity=18.660913, train_loss=2.9264312

Batch 37480, train_perplexity=18.660913, train_loss=2.9264312

Batch 37490, train_perplexity=18.66092, train_loss=2.9264314

Batch 37500, train_perplexity=18.660913, train_loss=2.9264312

Batch 37510, train_perplexity=18.66092, train_loss=2.9264314

Batch 37520, train_perplexity=18.66092, train_loss=2.9264314

Batch 37530, train_perplexity=18.660913, train_loss=2.9264312

Batch 37540, train_perplexity=18.660913, train_loss=2.9264312

Batch 37550, train_perplexity=18.66092, train_loss=2.9264314

Batch 37560, train_perplexity=18.660913, train_loss=2.9264312

Batch 37570, train_perplexity=18.66092, train_loss=2.9264314

Batch 37580, train_perplexity=18.66092, train_loss=2.9264314

Batch 37590, train_perplexity=18.660913, train_loss=2.9264312

Batch 37600, train_perplexity=18.660913, train_loss=2.9264312
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37610, train_perplexity=18.660913, train_loss=2.9264312

Batch 37620, train_perplexity=18.66092, train_loss=2.9264314

Batch 37630, train_perplexity=18.660913, train_loss=2.9264312

Batch 37640, train_perplexity=18.66092, train_loss=2.9264314

Batch 37650, train_perplexity=18.66092, train_loss=2.9264314

Batch 37660, train_perplexity=18.66092, train_loss=2.9264314

Batch 37670, train_perplexity=18.660913, train_loss=2.9264312

Batch 37680, train_perplexity=18.660913, train_loss=2.9264312

Batch 37690, train_perplexity=18.66092, train_loss=2.9264314

Batch 37700, train_perplexity=18.66092, train_loss=2.9264314

Batch 37710, train_perplexity=18.66092, train_loss=2.9264314

Batch 37720, train_perplexity=18.66092, train_loss=2.9264314

Batch 37730, train_perplexity=18.66092, train_loss=2.9264314

Batch 37740, train_perplexity=18.66092, train_loss=2.9264314

Batch 37750, train_perplexity=18.66092, train_loss=2.9264314

Batch 37760, train_perplexity=18.660913, train_loss=2.9264312

Batch 37770, train_perplexity=18.660913, train_loss=2.9264312

Batch 37780, train_perplexity=18.660913, train_loss=2.9264312

Batch 37790, train_perplexity=18.66092, train_loss=2.9264314

Batch 37800, train_perplexity=18.660913, train_loss=2.9264312

Batch 37810, train_perplexity=18.660913, train_loss=2.9264312

Batch 37820, train_perplexity=18.66092, train_loss=2.9264314

Batch 37830, train_perplexity=18.660913, train_loss=2.9264312

Batch 37840, train_perplexity=18.660913, train_loss=2.9264312

Batch 37850, train_perplexity=18.660913, train_loss=2.9264312

Batch 37860, train_perplexity=18.660913, train_loss=2.9264312

Batch 37870, train_perplexity=18.66092, train_loss=2.9264314

Batch 37880, train_perplexity=18.660913, train_loss=2.9264312

Batch 37890, train_perplexity=18.660913, train_loss=2.9264312

Batch 37900, train_perplexity=18.660913, train_loss=2.9264312

Batch 37910, train_perplexity=18.660913, train_loss=2.9264312

Batch 37920, train_perplexity=18.660913, train_loss=2.9264312

Batch 37930, train_perplexity=18.660913, train_loss=2.9264312

Batch 37940, train_perplexity=18.660913, train_loss=2.9264312

Batch 37950, train_perplexity=18.660913, train_loss=2.9264312

Batch 37960, train_perplexity=18.660913, train_loss=2.9264312

Batch 37970, train_perplexity=18.660906, train_loss=2.9264307

Batch 37980, train_perplexity=18.660913, train_loss=2.9264312

Batch 37990, train_perplexity=18.660913, train_loss=2.9264312

Batch 38000, train_perplexity=18.660913, train_loss=2.9264312

Batch 38010, train_perplexity=18.660913, train_loss=2.9264312

Batch 38020, train_perplexity=18.660913, train_loss=2.9264312

Batch 38030, train_perplexity=18.660913, train_loss=2.9264312

Batch 38040, train_perplexity=18.660913, train_loss=2.9264312

Batch 38050, train_perplexity=18.660913, train_loss=2.9264312

Batch 38060, train_perplexity=18.660913, train_loss=2.9264312

Batch 38070, train_perplexity=18.660913, train_loss=2.9264312

Batch 38080, train_perplexity=18.660913, train_loss=2.9264312

Batch 38090, train_perplexity=18.660913, train_loss=2.9264312

Batch 38100, train_perplexity=18.660913, train_loss=2.9264312

Batch 38110, train_perplexity=18.66092, train_loss=2.9264314

Batch 38120, train_perplexity=18.660913, train_loss=2.9264312

Batch 38130, train_perplexity=18.660913, train_loss=2.9264312

Batch 38140, train_perplexity=18.660913, train_loss=2.9264312

Batch 38150, train_perplexity=18.66092, train_loss=2.9264314

Batch 38160, train_perplexity=18.660913, train_loss=2.9264312

Batch 38170, train_perplexity=18.660913, train_loss=2.9264312

Batch 38180, train_perplexity=18.660913, train_loss=2.9264312

Batch 38190, train_perplexity=18.660913, train_loss=2.9264312

Batch 38200, train_perplexity=18.660913, train_loss=2.9264312

Batch 38210, train_perplexity=18.660913, train_loss=2.9264312

Batch 38220, train_perplexity=18.660913, train_loss=2.9264312

Batch 38230, train_perplexity=18.660913, train_loss=2.9264312

Batch 38240, train_perplexity=18.660913, train_loss=2.9264312

Batch 38250, train_perplexity=18.660913, train_loss=2.9264312

Batch 38260, train_perplexity=18.660913, train_loss=2.9264312

Batch 38270, train_perplexity=18.660913, train_loss=2.9264312

Batch 38280, train_perplexity=18.660913, train_loss=2.9264312

Batch 38290, train_perplexity=18.660913, train_loss=2.9264312

Batch 38300, train_perplexity=18.660913, train_loss=2.9264312

Batch 38310, train_perplexity=18.660913, train_loss=2.9264312

Batch 38320, train_perplexity=18.660913, train_loss=2.9264312

Batch 38330, train_perplexity=18.660913, train_loss=2.9264312

Batch 38340, train_perplexity=18.660913, train_loss=2.9264312

Batch 38350, train_perplexity=18.660913, train_loss=2.9264312

Batch 38360, train_perplexity=18.660913, train_loss=2.9264312

Batch 38370, train_perplexity=18.660913, train_loss=2.9264312

Batch 38380, train_perplexity=18.660913, train_loss=2.9264312

Batch 38390, train_perplexity=18.660913, train_loss=2.9264312

Batch 38400, train_perplexity=18.660913, train_loss=2.9264312

Batch 38410, train_perplexity=18.660913, train_loss=2.9264312

Batch 38420, train_perplexity=18.66091, train_loss=2.926431

Batch 38430, train_perplexity=18.660913, train_loss=2.9264312

Batch 38440, train_perplexity=18.660913, train_loss=2.9264312

Batch 38450, train_perplexity=18.660913, train_loss=2.9264312

Batch 38460, train_perplexity=18.660913, train_loss=2.9264312

Batch 38470, train_perplexity=18.660913, train_loss=2.9264312

Batch 38480, train_perplexity=18.660913, train_loss=2.9264312

Batch 38490, train_perplexity=18.660913, train_loss=2.9264312

Batch 38500, train_perplexity=18.660913, train_loss=2.9264312

Batch 38510, train_perplexity=18.660913, train_loss=2.9264312

Batch 38520, train_perplexity=18.660913, train_loss=2.9264312

Batch 38530, train_perplexity=18.660913, train_loss=2.9264312

Batch 38540, train_perplexity=18.660906, train_loss=2.9264307

Batch 38550, train_perplexity=18.660913, train_loss=2.9264312

Batch 38560, train_perplexity=18.66091, train_loss=2.926431

Batch 38570, train_perplexity=18.660913, train_loss=2.9264312

Batch 38580, train_perplexity=18.660906, train_loss=2.9264307

Batch 38590, train_perplexity=18.66091, train_loss=2.926431

Batch 38600, train_perplexity=18.660906, train_loss=2.9264307

Batch 38610, train_perplexity=18.660913, train_loss=2.9264312

Batch 38620, train_perplexity=18.660906, train_loss=2.9264307

Batch 38630, train_perplexity=18.660913, train_loss=2.9264312

Batch 38640, train_perplexity=18.66091, train_loss=2.926431

Batch 38650, train_perplexity=18.660906, train_loss=2.9264307

Batch 38660, train_perplexity=18.660913, train_loss=2.9264312

Batch 38670, train_perplexity=18.660913, train_loss=2.9264312

Batch 38680, train_perplexity=18.660913, train_loss=2.9264312

Batch 38690, train_perplexity=18.66091, train_loss=2.926431

Batch 38700, train_perplexity=18.660913, train_loss=2.9264312

Batch 38710, train_perplexity=18.660913, train_loss=2.9264312

Batch 38720, train_perplexity=18.660906, train_loss=2.9264307

Batch 38730, train_perplexity=18.660906, train_loss=2.9264307

Batch 38740, train_perplexity=18.660906, train_loss=2.9264307

Batch 38750, train_perplexity=18.660906, train_loss=2.9264307

Batch 38760, train_perplexity=18.66091, train_loss=2.926431

Batch 38770, train_perplexity=18.660906, train_loss=2.9264307

Batch 38780, train_perplexity=18.660906, train_loss=2.9264307

Batch 38790, train_perplexity=18.660906, train_loss=2.9264307

Batch 38800, train_perplexity=18.660906, train_loss=2.9264307

Batch 38810, train_perplexity=18.66091, train_loss=2.926431

Batch 38820, train_perplexity=18.660913, train_loss=2.9264312

Batch 38830, train_perplexity=18.66091, train_loss=2.926431

Batch 38840, train_perplexity=18.66091, train_loss=2.926431

Batch 38850, train_perplexity=18.660906, train_loss=2.9264307

Batch 38860, train_perplexity=18.66091, train_loss=2.926431

Batch 38870, train_perplexity=18.66091, train_loss=2.926431

Batch 38880, train_perplexity=18.660906, train_loss=2.9264307

Batch 38890, train_perplexity=18.660906, train_loss=2.9264307

Batch 38900, train_perplexity=18.66091, train_loss=2.926431

Batch 38910, train_perplexity=18.660906, train_loss=2.9264307
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38920, train_perplexity=18.660906, train_loss=2.9264307

Batch 38930, train_perplexity=18.66091, train_loss=2.926431

Batch 38940, train_perplexity=18.660906, train_loss=2.9264307

Batch 38950, train_perplexity=18.660906, train_loss=2.9264307

Batch 38960, train_perplexity=18.660906, train_loss=2.9264307

Batch 38970, train_perplexity=18.66091, train_loss=2.926431

Batch 38980, train_perplexity=18.660906, train_loss=2.9264307

Batch 38990, train_perplexity=18.66091, train_loss=2.926431

Batch 39000, train_perplexity=18.66091, train_loss=2.926431

Batch 39010, train_perplexity=18.660906, train_loss=2.9264307

Batch 39020, train_perplexity=18.660906, train_loss=2.9264307

Batch 39030, train_perplexity=18.660906, train_loss=2.9264307

Batch 39040, train_perplexity=18.660906, train_loss=2.9264307

Batch 39050, train_perplexity=18.660906, train_loss=2.9264307

Batch 39060, train_perplexity=18.660906, train_loss=2.9264307

Batch 39070, train_perplexity=18.66091, train_loss=2.926431

Batch 39080, train_perplexity=18.660906, train_loss=2.9264307

Batch 39090, train_perplexity=18.660906, train_loss=2.9264307

Batch 39100, train_perplexity=18.660906, train_loss=2.9264307

Batch 39110, train_perplexity=18.660906, train_loss=2.9264307

Batch 39120, train_perplexity=18.660906, train_loss=2.9264307

Batch 39130, train_perplexity=18.66091, train_loss=2.926431

Batch 39140, train_perplexity=18.660906, train_loss=2.9264307

Batch 39150, train_perplexity=18.660906, train_loss=2.9264307

Batch 39160, train_perplexity=18.660906, train_loss=2.9264307

Batch 39170, train_perplexity=18.66091, train_loss=2.926431

Batch 39180, train_perplexity=18.660906, train_loss=2.9264307

Batch 39190, train_perplexity=18.660906, train_loss=2.9264307

Batch 39200, train_perplexity=18.660906, train_loss=2.9264307

Batch 39210, train_perplexity=18.660906, train_loss=2.9264307

Batch 39220, train_perplexity=18.660906, train_loss=2.9264307

Batch 39230, train_perplexity=18.660906, train_loss=2.9264307

Batch 39240, train_perplexity=18.660906, train_loss=2.9264307

Batch 39250, train_perplexity=18.660906, train_loss=2.9264307

Batch 39260, train_perplexity=18.660906, train_loss=2.9264307

Batch 39270, train_perplexity=18.660906, train_loss=2.9264307

Batch 39280, train_perplexity=18.66091, train_loss=2.926431

Batch 39290, train_perplexity=18.660906, train_loss=2.9264307

Batch 39300, train_perplexity=18.660906, train_loss=2.9264307

Batch 39310, train_perplexity=18.660906, train_loss=2.9264307

Batch 39320, train_perplexity=18.660906, train_loss=2.9264307

Batch 39330, train_perplexity=18.660906, train_loss=2.9264307

Batch 39340, train_perplexity=18.660906, train_loss=2.9264307

Batch 39350, train_perplexity=18.660906, train_loss=2.9264307

Batch 39360, train_perplexity=18.660906, train_loss=2.9264307

Batch 39370, train_perplexity=18.660906, train_loss=2.9264307

Batch 39380, train_perplexity=18.660906, train_loss=2.9264307

Batch 39390, train_perplexity=18.66091, train_loss=2.926431

Batch 39400, train_perplexity=18.660906, train_loss=2.9264307

Batch 39410, train_perplexity=18.660906, train_loss=2.9264307

Batch 39420, train_perplexity=18.660906, train_loss=2.9264307

Batch 39430, train_perplexity=18.660906, train_loss=2.9264307

Batch 39440, train_perplexity=18.660906, train_loss=2.9264307

Batch 39450, train_perplexity=18.660906, train_loss=2.9264307

Batch 39460, train_perplexity=18.660906, train_loss=2.9264307

Batch 39470, train_perplexity=18.660906, train_loss=2.9264307

Batch 39480, train_perplexity=18.660906, train_loss=2.9264307

Batch 39490, train_perplexity=18.660906, train_loss=2.9264307

Batch 39500, train_perplexity=18.660906, train_loss=2.9264307

Batch 39510, train_perplexity=18.660906, train_loss=2.9264307

Batch 39520, train_perplexity=18.66091, train_loss=2.926431

Batch 39530, train_perplexity=18.660906, train_loss=2.9264307

Batch 39540, train_perplexity=18.660906, train_loss=2.9264307

Batch 39550, train_perplexity=18.660906, train_loss=2.9264307

Batch 39560, train_perplexity=18.660906, train_loss=2.9264307

Batch 39570, train_perplexity=18.660906, train_loss=2.9264307

Batch 39580, train_perplexity=18.660906, train_loss=2.9264307

Batch 39590, train_perplexity=18.660906, train_loss=2.9264307

Batch 39600, train_perplexity=18.660906, train_loss=2.9264307

Batch 39610, train_perplexity=18.660906, train_loss=2.9264307

Batch 39620, train_perplexity=18.660906, train_loss=2.9264307

Batch 39630, train_perplexity=18.660906, train_loss=2.9264307

Batch 39640, train_perplexity=18.660906, train_loss=2.9264307

Batch 39650, train_perplexity=18.660906, train_loss=2.9264307

Batch 39660, train_perplexity=18.660906, train_loss=2.9264307

Batch 39670, train_perplexity=18.660906, train_loss=2.9264307

Batch 39680, train_perplexity=18.660906, train_loss=2.9264307

Batch 39690, train_perplexity=18.660906, train_loss=2.9264307

Batch 39700, train_perplexity=18.660906, train_loss=2.9264307

Batch 39710, train_perplexity=18.660906, train_loss=2.9264307

Batch 39720, train_perplexity=18.660906, train_loss=2.9264307

Batch 39730, train_perplexity=18.660906, train_loss=2.9264307

Batch 39740, train_perplexity=18.660906, train_loss=2.9264307

Batch 39750, train_perplexity=18.660906, train_loss=2.9264307

Batch 39760, train_perplexity=18.660906, train_loss=2.9264307

Batch 39770, train_perplexity=18.660906, train_loss=2.9264307

Batch 39780, train_perplexity=18.660906, train_loss=2.9264307

Batch 39790, train_perplexity=18.660906, train_loss=2.9264307

Batch 39800, train_perplexity=18.660906, train_loss=2.9264307

Batch 39810, train_perplexity=18.660906, train_loss=2.9264307

Batch 39820, train_perplexity=18.660883, train_loss=2.9264295

Batch 39830, train_perplexity=18.660887, train_loss=2.9264297

Batch 39840, train_perplexity=18.660887, train_loss=2.9264297

Batch 39850, train_perplexity=18.660883, train_loss=2.9264295

Batch 39860, train_perplexity=18.66088, train_loss=2.9264293

Batch 39870, train_perplexity=18.660883, train_loss=2.9264295

Batch 39880, train_perplexity=18.660883, train_loss=2.9264295

Batch 39890, train_perplexity=18.66088, train_loss=2.9264293

Batch 39900, train_perplexity=18.66088, train_loss=2.9264293

Batch 39910, train_perplexity=18.66088, train_loss=2.9264293

Batch 39920, train_perplexity=18.660873, train_loss=2.926429

Batch 39930, train_perplexity=18.66088, train_loss=2.9264293

Batch 39940, train_perplexity=18.66088, train_loss=2.9264293

Batch 39950, train_perplexity=18.66088, train_loss=2.9264293

Batch 39960, train_perplexity=18.660873, train_loss=2.926429

Batch 39970, train_perplexity=18.66088, train_loss=2.9264293

Batch 39980, train_perplexity=18.66088, train_loss=2.9264293

Batch 39990, train_perplexity=18.66087, train_loss=2.9264288

Batch 40000, train_perplexity=18.660873, train_loss=2.926429

Batch 40010, train_perplexity=18.66088, train_loss=2.9264293

Batch 40020, train_perplexity=18.660873, train_loss=2.926429

Batch 40030, train_perplexity=18.66087, train_loss=2.9264288

Batch 40040, train_perplexity=18.660873, train_loss=2.926429

Batch 40050, train_perplexity=18.66087, train_loss=2.9264288

Batch 40060, train_perplexity=18.660873, train_loss=2.926429

Batch 40070, train_perplexity=18.66088, train_loss=2.9264293

Batch 40080, train_perplexity=18.660873, train_loss=2.926429

Batch 40090, train_perplexity=18.66087, train_loss=2.9264288

Batch 40100, train_perplexity=18.66087, train_loss=2.9264288

Batch 40110, train_perplexity=18.66087, train_loss=2.9264288

Batch 40120, train_perplexity=18.66087, train_loss=2.9264288

Batch 40130, train_perplexity=18.660873, train_loss=2.926429

Batch 40140, train_perplexity=18.66087, train_loss=2.9264288

Batch 40150, train_perplexity=18.66087, train_loss=2.9264288

Batch 40160, train_perplexity=18.660866, train_loss=2.9264286

Batch 40170, train_perplexity=18.660866, train_loss=2.9264286

Batch 40180, train_perplexity=18.66087, train_loss=2.9264288

Batch 40190, train_perplexity=18.660866, train_loss=2.9264286

Batch 40200, train_perplexity=18.66087, train_loss=2.9264288

Batch 40210, train_perplexity=18.66087, train_loss=2.9264288

Batch 40220, train_perplexity=18.660866, train_loss=2.9264286
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40230, train_perplexity=18.66086, train_loss=2.9264283

Batch 40240, train_perplexity=18.66086, train_loss=2.9264283

Batch 40250, train_perplexity=18.66086, train_loss=2.9264283

Batch 40260, train_perplexity=18.66086, train_loss=2.9264283

Batch 40270, train_perplexity=18.660866, train_loss=2.9264286

Batch 40280, train_perplexity=18.66086, train_loss=2.9264283

Batch 40290, train_perplexity=18.66086, train_loss=2.9264283

Batch 40300, train_perplexity=18.660866, train_loss=2.9264286

Batch 40310, train_perplexity=18.660852, train_loss=2.9264278

Batch 40320, train_perplexity=18.66086, train_loss=2.9264283

Batch 40330, train_perplexity=18.66086, train_loss=2.9264283

Batch 40340, train_perplexity=18.66086, train_loss=2.9264283

Batch 40350, train_perplexity=18.66086, train_loss=2.9264283

Batch 40360, train_perplexity=18.66086, train_loss=2.9264283

Batch 40370, train_perplexity=18.66086, train_loss=2.9264283

Batch 40380, train_perplexity=18.66086, train_loss=2.9264283

Batch 40390, train_perplexity=18.66086, train_loss=2.9264283

Batch 40400, train_perplexity=18.66086, train_loss=2.9264283

Batch 40410, train_perplexity=18.66086, train_loss=2.9264283

Batch 40420, train_perplexity=18.660856, train_loss=2.926428

Batch 40430, train_perplexity=18.660856, train_loss=2.926428

Batch 40440, train_perplexity=18.660852, train_loss=2.9264278

Batch 40450, train_perplexity=18.66086, train_loss=2.9264283

Batch 40460, train_perplexity=18.660856, train_loss=2.926428

Batch 40470, train_perplexity=18.660852, train_loss=2.9264278

Batch 40480, train_perplexity=18.66086, train_loss=2.9264283

Batch 40490, train_perplexity=18.660856, train_loss=2.926428

Batch 40500, train_perplexity=18.660856, train_loss=2.926428

Batch 40510, train_perplexity=18.660852, train_loss=2.9264278

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 40520, train_perplexity=18.660852, train_loss=2.9264278

Batch 40530, train_perplexity=18.660852, train_loss=2.9264278

Batch 40540, train_perplexity=18.660852, train_loss=2.9264278

Batch 40550, train_perplexity=18.660852, train_loss=2.9264278

Batch 40560, train_perplexity=18.660852, train_loss=2.9264278

Batch 40570, train_perplexity=18.660852, train_loss=2.9264278

Batch 40580, train_perplexity=18.660852, train_loss=2.9264278

Batch 40590, train_perplexity=18.66086, train_loss=2.9264283

Batch 40600, train_perplexity=18.660852, train_loss=2.9264278

Batch 40610, train_perplexity=18.660852, train_loss=2.9264278

Batch 40620, train_perplexity=18.660852, train_loss=2.9264278

Batch 40630, train_perplexity=18.660852, train_loss=2.9264278

Batch 40640, train_perplexity=18.660852, train_loss=2.9264278

Batch 40650, train_perplexity=18.660847, train_loss=2.9264276

Batch 40660, train_perplexity=18.660852, train_loss=2.9264278

Batch 40670, train_perplexity=18.660843, train_loss=2.9264274

Batch 40680, train_perplexity=18.660852, train_loss=2.9264278

Batch 40690, train_perplexity=18.660852, train_loss=2.9264278

Batch 40700, train_perplexity=18.660852, train_loss=2.9264278

Batch 40710, train_perplexity=18.660847, train_loss=2.9264276

Batch 40720, train_perplexity=18.660852, train_loss=2.9264278

Batch 40730, train_perplexity=18.660843, train_loss=2.9264274

Batch 40740, train_perplexity=18.660843, train_loss=2.9264274

Batch 40750, train_perplexity=18.660852, train_loss=2.9264278

Batch 40760, train_perplexity=18.660843, train_loss=2.9264274

Batch 40770, train_perplexity=18.660843, train_loss=2.9264274

Batch 40780, train_perplexity=18.660843, train_loss=2.9264274

Batch 40790, train_perplexity=18.660847, train_loss=2.9264276

Batch 40800, train_perplexity=18.660843, train_loss=2.9264274

Batch 40810, train_perplexity=18.660843, train_loss=2.9264274

Batch 40820, train_perplexity=18.660843, train_loss=2.9264274

Batch 40830, train_perplexity=18.660847, train_loss=2.9264276

Batch 40840, train_perplexity=18.660843, train_loss=2.9264274

Batch 40850, train_perplexity=18.660843, train_loss=2.9264274

Batch 40860, train_perplexity=18.660843, train_loss=2.9264274

Batch 40870, train_perplexity=18.660843, train_loss=2.9264274

Batch 40880, train_perplexity=18.660833, train_loss=2.926427

Batch 40890, train_perplexity=18.660847, train_loss=2.9264276

Batch 40900, train_perplexity=18.660843, train_loss=2.9264274

Batch 40910, train_perplexity=18.660843, train_loss=2.9264274

Batch 40920, train_perplexity=18.660843, train_loss=2.9264274

Batch 40930, train_perplexity=18.660843, train_loss=2.9264274

Batch 40940, train_perplexity=18.660843, train_loss=2.9264274

Batch 40950, train_perplexity=18.660843, train_loss=2.9264274

Batch 40960, train_perplexity=18.660843, train_loss=2.9264274

Batch 40970, train_perplexity=18.660843, train_loss=2.9264274

Batch 40980, train_perplexity=18.660843, train_loss=2.9264274

Batch 40990, train_perplexity=18.660852, train_loss=2.9264278

Batch 41000, train_perplexity=18.660843, train_loss=2.9264274

Batch 41010, train_perplexity=18.660843, train_loss=2.9264274

Batch 41020, train_perplexity=18.660847, train_loss=2.9264276

Batch 41030, train_perplexity=18.660852, train_loss=2.9264278

Batch 41040, train_perplexity=18.660852, train_loss=2.9264278

Batch 41050, train_perplexity=18.660843, train_loss=2.9264274

Batch 41060, train_perplexity=18.660852, train_loss=2.9264278

Batch 41070, train_perplexity=18.660843, train_loss=2.9264274

Batch 41080, train_perplexity=18.660852, train_loss=2.9264278

Batch 41090, train_perplexity=18.660852, train_loss=2.9264278

Batch 41100, train_perplexity=18.660847, train_loss=2.9264276

Batch 41110, train_perplexity=18.660847, train_loss=2.9264276

Batch 41120, train_perplexity=18.660852, train_loss=2.9264278

Batch 41130, train_perplexity=18.660852, train_loss=2.9264278

Batch 41140, train_perplexity=18.660852, train_loss=2.9264278

Batch 41150, train_perplexity=18.660852, train_loss=2.9264278

Batch 41160, train_perplexity=18.660852, train_loss=2.9264278

Batch 41170, train_perplexity=18.660847, train_loss=2.9264276

Batch 41180, train_perplexity=18.660852, train_loss=2.9264278

Batch 41190, train_perplexity=18.660852, train_loss=2.9264278

Batch 41200, train_perplexity=18.660852, train_loss=2.9264278

Batch 41210, train_perplexity=18.660852, train_loss=2.9264278

Batch 41220, train_perplexity=18.660852, train_loss=2.9264278

Batch 41230, train_perplexity=18.660852, train_loss=2.9264278

Batch 41240, train_perplexity=18.660852, train_loss=2.9264278

Batch 41250, train_perplexity=18.660852, train_loss=2.9264278

Batch 41260, train_perplexity=18.660852, train_loss=2.9264278

Batch 41270, train_perplexity=18.660852, train_loss=2.9264278

Batch 41280, train_perplexity=18.660852, train_loss=2.9264278

Batch 41290, train_perplexity=18.660852, train_loss=2.9264278

Batch 41300, train_perplexity=18.660852, train_loss=2.9264278

Batch 41310, train_perplexity=18.660852, train_loss=2.9264278

Batch 41320, train_perplexity=18.660852, train_loss=2.9264278

Batch 41330, train_perplexity=18.660852, train_loss=2.9264278

Batch 41340, train_perplexity=18.660852, train_loss=2.9264278

Batch 41350, train_perplexity=18.660852, train_loss=2.9264278

Batch 41360, train_perplexity=18.660843, train_loss=2.9264274

Batch 41370, train_perplexity=18.660852, train_loss=2.9264278

Batch 41380, train_perplexity=18.660847, train_loss=2.9264276

Batch 41390, train_perplexity=18.660852, train_loss=2.9264278

Batch 41400, train_perplexity=18.660852, train_loss=2.9264278

Batch 41410, train_perplexity=18.660852, train_loss=2.9264278

Batch 41420, train_perplexity=18.660852, train_loss=2.9264278

Batch 41430, train_perplexity=18.660843, train_loss=2.9264274

Batch 41440, train_perplexity=18.660852, train_loss=2.9264278

Batch 41450, train_perplexity=18.660847, train_loss=2.9264276

Batch 41460, train_perplexity=18.660852, train_loss=2.9264278
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 41470, train_perplexity=18.660852, train_loss=2.9264278

Batch 41480, train_perplexity=18.660843, train_loss=2.9264274

Batch 41490, train_perplexity=18.660847, train_loss=2.9264276

Batch 41500, train_perplexity=18.660843, train_loss=2.9264274

Batch 41510, train_perplexity=18.660843, train_loss=2.9264274

Batch 41520, train_perplexity=18.660843, train_loss=2.9264274

Batch 41530, train_perplexity=18.660843, train_loss=2.9264274

Batch 41540, train_perplexity=18.660852, train_loss=2.9264278

Batch 41550, train_perplexity=18.660847, train_loss=2.9264276

Batch 41560, train_perplexity=18.660843, train_loss=2.9264274

Batch 41570, train_perplexity=18.660847, train_loss=2.9264276

Batch 41580, train_perplexity=18.660843, train_loss=2.9264274

Batch 41590, train_perplexity=18.660843, train_loss=2.9264274

Batch 41600, train_perplexity=18.660843, train_loss=2.9264274

Batch 41610, train_perplexity=18.660843, train_loss=2.9264274

Batch 41620, train_perplexity=18.660843, train_loss=2.9264274

Batch 41630, train_perplexity=18.660843, train_loss=2.9264274

Batch 41640, train_perplexity=18.660843, train_loss=2.9264274

Batch 41650, train_perplexity=18.66084, train_loss=2.9264271

Batch 41660, train_perplexity=18.66084, train_loss=2.9264271

Batch 41670, train_perplexity=18.660843, train_loss=2.9264274

Batch 41680, train_perplexity=18.66084, train_loss=2.9264271

Batch 41690, train_perplexity=18.660843, train_loss=2.9264274

Batch 41700, train_perplexity=18.660843, train_loss=2.9264274

Batch 41710, train_perplexity=18.660843, train_loss=2.9264274

Batch 41720, train_perplexity=18.66084, train_loss=2.9264271

Batch 41730, train_perplexity=18.660833, train_loss=2.926427

Batch 41740, train_perplexity=18.660833, train_loss=2.926427

Batch 41750, train_perplexity=18.660843, train_loss=2.9264274

Batch 41760, train_perplexity=18.660833, train_loss=2.926427

Batch 41770, train_perplexity=18.660833, train_loss=2.926427

Batch 41780, train_perplexity=18.660833, train_loss=2.926427

Batch 41790, train_perplexity=18.660833, train_loss=2.926427

Batch 41800, train_perplexity=18.660833, train_loss=2.926427

Batch 41810, train_perplexity=18.660833, train_loss=2.926427

Batch 41820, train_perplexity=18.660833, train_loss=2.926427

Batch 41830, train_perplexity=18.660833, train_loss=2.926427

Batch 41840, train_perplexity=18.66083, train_loss=2.9264266

Batch 41850, train_perplexity=18.660833, train_loss=2.926427

Batch 41860, train_perplexity=18.660826, train_loss=2.9264264

Batch 41870, train_perplexity=18.660833, train_loss=2.926427

Batch 41880, train_perplexity=18.66083, train_loss=2.9264266

Batch 41890, train_perplexity=18.660833, train_loss=2.926427

Batch 41900, train_perplexity=18.66083, train_loss=2.9264266

Batch 41910, train_perplexity=18.660833, train_loss=2.926427

Batch 41920, train_perplexity=18.66083, train_loss=2.9264266

Batch 41930, train_perplexity=18.660833, train_loss=2.926427

Batch 41940, train_perplexity=18.660826, train_loss=2.9264264

Batch 41950, train_perplexity=18.660826, train_loss=2.9264264

Batch 41960, train_perplexity=18.660826, train_loss=2.9264264

Batch 41970, train_perplexity=18.660826, train_loss=2.9264264

Batch 41980, train_perplexity=18.660826, train_loss=2.9264264

Batch 41990, train_perplexity=18.66082, train_loss=2.9264262

Batch 42000, train_perplexity=18.66082, train_loss=2.9264262

Batch 42010, train_perplexity=18.660816, train_loss=2.926426

Batch 42020, train_perplexity=18.660826, train_loss=2.9264264

Batch 42030, train_perplexity=18.660816, train_loss=2.926426

Batch 42040, train_perplexity=18.660826, train_loss=2.9264264

Batch 42050, train_perplexity=18.66082, train_loss=2.9264262

Batch 42060, train_perplexity=18.66082, train_loss=2.9264262

Batch 42070, train_perplexity=18.66082, train_loss=2.9264262

Batch 42080, train_perplexity=18.660816, train_loss=2.926426

Batch 42090, train_perplexity=18.660816, train_loss=2.926426

Batch 42100, train_perplexity=18.660816, train_loss=2.926426

Batch 42110, train_perplexity=18.660816, train_loss=2.926426

Batch 42120, train_perplexity=18.660816, train_loss=2.926426

Batch 42130, train_perplexity=18.660816, train_loss=2.926426

Batch 42140, train_perplexity=18.660807, train_loss=2.9264255

Batch 42150, train_perplexity=18.660812, train_loss=2.9264257

Batch 42160, train_perplexity=18.660807, train_loss=2.9264255

Batch 42170, train_perplexity=18.660812, train_loss=2.9264257

Batch 42180, train_perplexity=18.660807, train_loss=2.9264255

Batch 42190, train_perplexity=18.660807, train_loss=2.9264255

Batch 42200, train_perplexity=18.660807, train_loss=2.9264255

Batch 42210, train_perplexity=18.660812, train_loss=2.9264257

Batch 42220, train_perplexity=18.660807, train_loss=2.9264255

Batch 42230, train_perplexity=18.660807, train_loss=2.9264255

Batch 42240, train_perplexity=18.660807, train_loss=2.9264255

Batch 42250, train_perplexity=18.660807, train_loss=2.9264255

Batch 42260, train_perplexity=18.660807, train_loss=2.9264255

Batch 42270, train_perplexity=18.660799, train_loss=2.926425

Batch 42280, train_perplexity=18.660807, train_loss=2.9264255

Batch 42290, train_perplexity=18.660807, train_loss=2.9264255

Batch 42300, train_perplexity=18.660799, train_loss=2.926425

Batch 42310, train_perplexity=18.660799, train_loss=2.926425

Batch 42320, train_perplexity=18.660799, train_loss=2.926425

Batch 42330, train_perplexity=18.660799, train_loss=2.926425

Batch 42340, train_perplexity=18.660799, train_loss=2.926425

Batch 42350, train_perplexity=18.660799, train_loss=2.926425

Batch 42360, train_perplexity=18.660799, train_loss=2.926425

Batch 42370, train_perplexity=18.660799, train_loss=2.926425

Batch 42380, train_perplexity=18.660799, train_loss=2.926425

Batch 42390, train_perplexity=18.66079, train_loss=2.9264245

Batch 42400, train_perplexity=18.660793, train_loss=2.9264247

Batch 42410, train_perplexity=18.66079, train_loss=2.9264245

Batch 42420, train_perplexity=18.660799, train_loss=2.926425

Batch 42430, train_perplexity=18.660799, train_loss=2.926425

Batch 42440, train_perplexity=18.66079, train_loss=2.9264245

Batch 42450, train_perplexity=18.66079, train_loss=2.9264245

Batch 42460, train_perplexity=18.66079, train_loss=2.9264245

Batch 42470, train_perplexity=18.66079, train_loss=2.9264245

Batch 42480, train_perplexity=18.66079, train_loss=2.9264245

Batch 42490, train_perplexity=18.66078, train_loss=2.926424

Batch 42500, train_perplexity=18.66079, train_loss=2.9264245

Batch 42510, train_perplexity=18.66079, train_loss=2.9264245

Batch 42520, train_perplexity=18.660786, train_loss=2.9264243

Batch 42530, train_perplexity=18.66079, train_loss=2.9264245

Batch 42540, train_perplexity=18.66079, train_loss=2.9264245

Batch 42550, train_perplexity=18.660786, train_loss=2.9264243

Batch 42560, train_perplexity=18.660786, train_loss=2.9264243

Batch 42570, train_perplexity=18.660786, train_loss=2.9264243

Batch 42580, train_perplexity=18.660786, train_loss=2.9264243

Batch 42590, train_perplexity=18.66079, train_loss=2.9264245

Batch 42600, train_perplexity=18.66079, train_loss=2.9264245

Batch 42610, train_perplexity=18.66078, train_loss=2.926424

Batch 42620, train_perplexity=18.660786, train_loss=2.9264243

Batch 42630, train_perplexity=18.74968, train_loss=2.9311767

Batch 42640, train_perplexity=18.669094, train_loss=2.9268694

Batch 42650, train_perplexity=18.664408, train_loss=2.9266183

Batch 42660, train_perplexity=18.662622, train_loss=2.9265227

Batch 42670, train_perplexity=18.66184, train_loss=2.9264808

Batch 42680, train_perplexity=18.661457, train_loss=2.9264603

Batch 42690, train_perplexity=18.661253, train_loss=2.9264493

Batch 42700, train_perplexity=18.661127, train_loss=2.9264426

Batch 42710, train_perplexity=18.661047, train_loss=2.9264383

Batch 42720, train_perplexity=18.660986, train_loss=2.926435

Batch 42730, train_perplexity=18.66094, train_loss=2.9264326

Batch 42740, train_perplexity=18.66092, train_loss=2.9264314

Batch 42750, train_perplexity=18.660892, train_loss=2.92643

Batch 42760, train_perplexity=18.66087, train_loss=2.9264288

Batch 42770, train_perplexity=18.66086, train_loss=2.9264283

Batch 42780, train_perplexity=18.660847, train_loss=2.9264276
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42790, train_perplexity=18.660833, train_loss=2.926427

Batch 42800, train_perplexity=18.66083, train_loss=2.9264266

Batch 42810, train_perplexity=18.660826, train_loss=2.9264264

Batch 42820, train_perplexity=18.66082, train_loss=2.9264262

Batch 42830, train_perplexity=18.660816, train_loss=2.926426

Batch 42840, train_perplexity=18.660816, train_loss=2.926426

Batch 42850, train_perplexity=18.660812, train_loss=2.9264257

Batch 42860, train_perplexity=18.660807, train_loss=2.9264255

Batch 42870, train_perplexity=18.660807, train_loss=2.9264255

Batch 42880, train_perplexity=18.660812, train_loss=2.9264257

Batch 42890, train_perplexity=18.660807, train_loss=2.9264255

Batch 42900, train_perplexity=18.660799, train_loss=2.926425

Batch 42910, train_perplexity=18.66079, train_loss=2.9264245

Batch 42920, train_perplexity=18.660799, train_loss=2.926425

Batch 42930, train_perplexity=18.660803, train_loss=2.9264252

Batch 42940, train_perplexity=18.660799, train_loss=2.926425

Batch 42950, train_perplexity=18.660799, train_loss=2.926425

Batch 42960, train_perplexity=18.660799, train_loss=2.926425

Batch 42970, train_perplexity=18.660799, train_loss=2.926425

Batch 42980, train_perplexity=18.66079, train_loss=2.9264245

Batch 42990, train_perplexity=18.660799, train_loss=2.926425

Batch 43000, train_perplexity=18.660799, train_loss=2.926425

Batch 43010, train_perplexity=18.660793, train_loss=2.9264247

Batch 43020, train_perplexity=18.66079, train_loss=2.9264245

Batch 43030, train_perplexity=18.660799, train_loss=2.926425

Batch 43040, train_perplexity=18.660799, train_loss=2.926425

Batch 43050, train_perplexity=18.660793, train_loss=2.9264247

Batch 43060, train_perplexity=18.66079, train_loss=2.9264245

Batch 43070, train_perplexity=18.66079, train_loss=2.9264245

Batch 43080, train_perplexity=18.66079, train_loss=2.9264245

Batch 43090, train_perplexity=18.660793, train_loss=2.9264247

Batch 43100, train_perplexity=18.66079, train_loss=2.9264245

Batch 43110, train_perplexity=18.66079, train_loss=2.9264245

Batch 43120, train_perplexity=18.66079, train_loss=2.9264245

Batch 43130, train_perplexity=18.660793, train_loss=2.9264247

Batch 43140, train_perplexity=18.66079, train_loss=2.9264245

Batch 43150, train_perplexity=18.66079, train_loss=2.9264245

Batch 43160, train_perplexity=18.66079, train_loss=2.9264245

Batch 43170, train_perplexity=18.660793, train_loss=2.9264247

Batch 43180, train_perplexity=18.66079, train_loss=2.9264245

Batch 43190, train_perplexity=18.66079, train_loss=2.9264245

Batch 43200, train_perplexity=18.66079, train_loss=2.9264245

Batch 43210, train_perplexity=18.66079, train_loss=2.9264245

Batch 43220, train_perplexity=18.66079, train_loss=2.9264245

Batch 43230, train_perplexity=18.66079, train_loss=2.9264245

Batch 43240, train_perplexity=18.66079, train_loss=2.9264245

Batch 43250, train_perplexity=18.66079, train_loss=2.9264245

Batch 43260, train_perplexity=18.66078, train_loss=2.926424

Batch 43270, train_perplexity=18.66079, train_loss=2.9264245

Batch 43280, train_perplexity=18.66079, train_loss=2.9264245

Batch 43290, train_perplexity=18.66079, train_loss=2.9264245

Batch 43300, train_perplexity=18.660786, train_loss=2.9264243

Batch 43310, train_perplexity=18.66078, train_loss=2.926424

Batch 43320, train_perplexity=18.660786, train_loss=2.9264243

Batch 43330, train_perplexity=18.66079, train_loss=2.9264245

Batch 43340, train_perplexity=18.66078, train_loss=2.926424

Batch 43350, train_perplexity=18.66078, train_loss=2.926424

Batch 43360, train_perplexity=18.66078, train_loss=2.926424

Batch 43370, train_perplexity=18.66078, train_loss=2.926424

Batch 43380, train_perplexity=18.66079, train_loss=2.9264245

Batch 43390, train_perplexity=18.66078, train_loss=2.926424

Batch 43400, train_perplexity=18.660776, train_loss=2.9264238

Batch 43410, train_perplexity=18.66078, train_loss=2.926424

Batch 43420, train_perplexity=18.66078, train_loss=2.926424

Batch 43430, train_perplexity=18.66078, train_loss=2.926424

Batch 43440, train_perplexity=18.66078, train_loss=2.926424

Batch 43450, train_perplexity=18.66078, train_loss=2.926424

Batch 43460, train_perplexity=18.66078, train_loss=2.926424

Batch 43470, train_perplexity=18.66078, train_loss=2.926424

Batch 43480, train_perplexity=18.66078, train_loss=2.926424

Batch 43490, train_perplexity=18.66078, train_loss=2.926424

Batch 43500, train_perplexity=18.66078, train_loss=2.926424

Batch 43510, train_perplexity=18.66078, train_loss=2.926424

Batch 43520, train_perplexity=18.66078, train_loss=2.926424

Batch 43530, train_perplexity=18.66078, train_loss=2.926424

Batch 43540, train_perplexity=18.66078, train_loss=2.926424

Batch 43550, train_perplexity=18.660776, train_loss=2.9264238

Batch 43560, train_perplexity=18.66078, train_loss=2.926424

Batch 43570, train_perplexity=18.660786, train_loss=2.9264243

Batch 43580, train_perplexity=18.66078, train_loss=2.926424

Batch 43590, train_perplexity=18.66078, train_loss=2.926424

Batch 43600, train_perplexity=18.660776, train_loss=2.9264238

Batch 43610, train_perplexity=18.660772, train_loss=2.9264235

Batch 43620, train_perplexity=18.660776, train_loss=2.9264238

Batch 43630, train_perplexity=18.660776, train_loss=2.9264238

Batch 43640, train_perplexity=18.660776, train_loss=2.9264238

Batch 43650, train_perplexity=18.660776, train_loss=2.9264238

Batch 43660, train_perplexity=18.66078, train_loss=2.926424

Batch 43670, train_perplexity=18.660776, train_loss=2.9264238

Batch 43680, train_perplexity=18.660776, train_loss=2.9264238

Batch 43690, train_perplexity=18.66078, train_loss=2.926424

Batch 43700, train_perplexity=18.660776, train_loss=2.9264238

Batch 43710, train_perplexity=18.660772, train_loss=2.9264235

Batch 43720, train_perplexity=18.660772, train_loss=2.9264235

Batch 43730, train_perplexity=18.660772, train_loss=2.9264235

Batch 43740, train_perplexity=18.660776, train_loss=2.9264238

Batch 43750, train_perplexity=18.660772, train_loss=2.9264235

Batch 43760, train_perplexity=18.660776, train_loss=2.9264238

Batch 43770, train_perplexity=18.660772, train_loss=2.9264235

Batch 43780, train_perplexity=18.660776, train_loss=2.9264238

Batch 43790, train_perplexity=18.660772, train_loss=2.9264235

Batch 43800, train_perplexity=18.660772, train_loss=2.9264235

Batch 43810, train_perplexity=18.660772, train_loss=2.9264235

Batch 43820, train_perplexity=18.660772, train_loss=2.9264235

Batch 43830, train_perplexity=18.660772, train_loss=2.9264235

Batch 43840, train_perplexity=18.660772, train_loss=2.9264235

Batch 43850, train_perplexity=18.660772, train_loss=2.9264235

Batch 43860, train_perplexity=18.660772, train_loss=2.9264235

Batch 43870, train_perplexity=18.660772, train_loss=2.9264235

Batch 43880, train_perplexity=18.660767, train_loss=2.9264233

Batch 43890, train_perplexity=18.660772, train_loss=2.9264235

Batch 43900, train_perplexity=18.660767, train_loss=2.9264233

Batch 43910, train_perplexity=18.660772, train_loss=2.9264235

Batch 43920, train_perplexity=18.660772, train_loss=2.9264235

Batch 43930, train_perplexity=18.660772, train_loss=2.9264235

Batch 43940, train_perplexity=18.660772, train_loss=2.9264235

Batch 43950, train_perplexity=18.660767, train_loss=2.9264233

Batch 43960, train_perplexity=18.660772, train_loss=2.9264235

Batch 43970, train_perplexity=18.660772, train_loss=2.9264235

Batch 43980, train_perplexity=18.660763, train_loss=2.926423

Batch 43990, train_perplexity=18.660772, train_loss=2.9264235

Batch 44000, train_perplexity=18.660767, train_loss=2.9264233

Batch 44010, train_perplexity=18.660763, train_loss=2.926423

Batch 44020, train_perplexity=18.660767, train_loss=2.9264233

Batch 44030, train_perplexity=18.660772, train_loss=2.9264235

Batch 44040, train_perplexity=18.660763, train_loss=2.926423

Batch 44050, train_perplexity=18.660772, train_loss=2.9264235

Batch 44060, train_perplexity=18.660767, train_loss=2.9264233

Batch 44070, train_perplexity=18.660772, train_loss=2.9264235

Batch 44080, train_perplexity=18.660763, train_loss=2.926423

Batch 44090, train_perplexity=18.660763, train_loss=2.926423

Batch 44100, train_perplexity=18.660763, train_loss=2.926423
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44110, train_perplexity=18.660772, train_loss=2.9264235

Batch 44120, train_perplexity=18.660763, train_loss=2.926423

Batch 44130, train_perplexity=18.660763, train_loss=2.926423

Batch 44140, train_perplexity=18.660763, train_loss=2.926423

Batch 44150, train_perplexity=18.660767, train_loss=2.9264233

Batch 44160, train_perplexity=18.660763, train_loss=2.926423

Batch 44170, train_perplexity=18.660763, train_loss=2.926423

Batch 44180, train_perplexity=18.660763, train_loss=2.926423

Batch 44190, train_perplexity=18.660763, train_loss=2.926423

Batch 44200, train_perplexity=18.660763, train_loss=2.926423

Batch 44210, train_perplexity=18.660763, train_loss=2.926423

Batch 44220, train_perplexity=18.660767, train_loss=2.9264233

Batch 44230, train_perplexity=18.660763, train_loss=2.926423

Batch 44240, train_perplexity=18.660763, train_loss=2.926423

Batch 44250, train_perplexity=18.660763, train_loss=2.926423

Batch 44260, train_perplexity=18.660763, train_loss=2.926423

Batch 44270, train_perplexity=18.660763, train_loss=2.926423

Batch 44280, train_perplexity=18.660763, train_loss=2.926423

Batch 44290, train_perplexity=18.660763, train_loss=2.926423

Batch 44300, train_perplexity=18.660763, train_loss=2.926423

Batch 44310, train_perplexity=18.660763, train_loss=2.926423

Batch 44320, train_perplexity=18.660753, train_loss=2.9264226

Batch 44330, train_perplexity=18.660763, train_loss=2.926423

Batch 44340, train_perplexity=18.660753, train_loss=2.9264226

Batch 44350, train_perplexity=18.660763, train_loss=2.926423

Batch 44360, train_perplexity=18.660759, train_loss=2.9264228

Batch 44370, train_perplexity=18.660753, train_loss=2.9264226

Batch 44380, train_perplexity=18.660759, train_loss=2.9264228

Batch 44390, train_perplexity=18.660753, train_loss=2.9264226

Batch 44400, train_perplexity=18.660753, train_loss=2.9264226

Batch 44410, train_perplexity=18.660753, train_loss=2.9264226

Batch 44420, train_perplexity=18.660753, train_loss=2.9264226

Batch 44430, train_perplexity=18.660753, train_loss=2.9264226

Batch 44440, train_perplexity=18.660753, train_loss=2.9264226

Batch 44450, train_perplexity=18.660753, train_loss=2.9264226

Batch 44460, train_perplexity=18.660759, train_loss=2.9264228

Batch 44470, train_perplexity=18.660753, train_loss=2.9264226

Batch 44480, train_perplexity=18.660753, train_loss=2.9264226

Batch 44490, train_perplexity=18.660753, train_loss=2.9264226

Batch 44500, train_perplexity=18.660753, train_loss=2.9264226

Batch 44510, train_perplexity=18.660753, train_loss=2.9264226

Batch 44520, train_perplexity=18.660753, train_loss=2.9264226

Batch 44530, train_perplexity=18.660753, train_loss=2.9264226

Batch 44540, train_perplexity=18.66075, train_loss=2.9264224

Batch 44550, train_perplexity=18.66075, train_loss=2.9264224

Batch 44560, train_perplexity=18.660746, train_loss=2.926422

Batch 44570, train_perplexity=18.660753, train_loss=2.9264226

Batch 44580, train_perplexity=18.660753, train_loss=2.9264226

Batch 44590, train_perplexity=18.660753, train_loss=2.9264226

Batch 44600, train_perplexity=18.66075, train_loss=2.9264224

Batch 44610, train_perplexity=18.660753, train_loss=2.9264226

Batch 44620, train_perplexity=18.66075, train_loss=2.9264224

Batch 44630, train_perplexity=18.660746, train_loss=2.926422

Batch 44640, train_perplexity=18.66075, train_loss=2.9264224

Batch 44650, train_perplexity=18.660753, train_loss=2.9264226

Batch 44660, train_perplexity=18.660746, train_loss=2.926422

Batch 44670, train_perplexity=18.660753, train_loss=2.9264226

Batch 44680, train_perplexity=18.660753, train_loss=2.9264226

Batch 44690, train_perplexity=18.66075, train_loss=2.9264224

Batch 44700, train_perplexity=18.660746, train_loss=2.926422

Batch 44710, train_perplexity=18.660753, train_loss=2.9264226

Batch 44720, train_perplexity=18.66075, train_loss=2.9264224

Batch 44730, train_perplexity=18.66075, train_loss=2.9264224

Batch 44740, train_perplexity=18.660753, train_loss=2.9264226

Batch 44750, train_perplexity=18.66075, train_loss=2.9264224

Batch 44760, train_perplexity=18.660753, train_loss=2.9264226

Batch 44770, train_perplexity=18.660753, train_loss=2.9264226

Batch 44780, train_perplexity=18.660753, train_loss=2.9264226

Batch 44790, train_perplexity=18.660753, train_loss=2.9264226

Batch 44800, train_perplexity=18.660753, train_loss=2.9264226

Batch 44810, train_perplexity=18.66075, train_loss=2.9264224

Batch 44820, train_perplexity=18.660753, train_loss=2.9264226

Batch 44830, train_perplexity=18.660753, train_loss=2.9264226

Batch 44840, train_perplexity=18.660753, train_loss=2.9264226

Batch 44850, train_perplexity=18.66075, train_loss=2.9264224

Batch 44860, train_perplexity=18.660753, train_loss=2.9264226

Batch 44870, train_perplexity=18.660753, train_loss=2.9264226

Batch 44880, train_perplexity=18.66075, train_loss=2.9264224

Batch 44890, train_perplexity=18.66075, train_loss=2.9264224

Batch 44900, train_perplexity=18.66075, train_loss=2.9264224

Batch 44910, train_perplexity=18.660753, train_loss=2.9264226

Batch 44920, train_perplexity=18.660753, train_loss=2.9264226

Batch 44930, train_perplexity=18.66075, train_loss=2.9264224

Batch 44940, train_perplexity=18.660753, train_loss=2.9264226

Batch 44950, train_perplexity=18.660753, train_loss=2.9264226

Batch 44960, train_perplexity=18.660753, train_loss=2.9264226

Batch 44970, train_perplexity=18.660753, train_loss=2.9264226

Batch 44980, train_perplexity=18.660753, train_loss=2.9264226

Batch 44990, train_perplexity=18.660753, train_loss=2.9264226

Batch 45000, train_perplexity=18.660753, train_loss=2.9264226

Batch 45010, train_perplexity=18.660753, train_loss=2.9264226

Batch 45020, train_perplexity=18.660753, train_loss=2.9264226

Batch 45030, train_perplexity=18.660753, train_loss=2.9264226

Batch 45040, train_perplexity=18.660753, train_loss=2.9264226

Batch 45050, train_perplexity=18.660753, train_loss=2.9264226

Batch 45060, train_perplexity=18.660753, train_loss=2.9264226

Batch 45070, train_perplexity=18.660746, train_loss=2.926422

Batch 45080, train_perplexity=18.660753, train_loss=2.9264226

Batch 45090, train_perplexity=18.660753, train_loss=2.9264226

Batch 45100, train_perplexity=18.660753, train_loss=2.9264226

Batch 45110, train_perplexity=18.660753, train_loss=2.9264226

Batch 45120, train_perplexity=18.660753, train_loss=2.9264226

Batch 45130, train_perplexity=18.660753, train_loss=2.9264226

Batch 45140, train_perplexity=18.660746, train_loss=2.926422

Batch 45150, train_perplexity=18.660753, train_loss=2.9264226

Batch 45160, train_perplexity=18.660746, train_loss=2.926422

Batch 45170, train_perplexity=18.660753, train_loss=2.9264226

Batch 45180, train_perplexity=18.660753, train_loss=2.9264226

Batch 45190, train_perplexity=18.660753, train_loss=2.9264226

Batch 45200, train_perplexity=18.660753, train_loss=2.9264226

Batch 45210, train_perplexity=18.660753, train_loss=2.9264226

Batch 45220, train_perplexity=18.660753, train_loss=2.9264226

Batch 45230, train_perplexity=18.660753, train_loss=2.9264226

Batch 45240, train_perplexity=18.660753, train_loss=2.9264226

Batch 45250, train_perplexity=18.660753, train_loss=2.9264226

Batch 45260, train_perplexity=18.660753, train_loss=2.9264226

Batch 45270, train_perplexity=18.66075, train_loss=2.9264224

Batch 45280, train_perplexity=18.66075, train_loss=2.9264224

Batch 45290, train_perplexity=18.660753, train_loss=2.9264226

Batch 45300, train_perplexity=18.660753, train_loss=2.9264226

Batch 45310, train_perplexity=18.66075, train_loss=2.9264224

Batch 45320, train_perplexity=18.66075, train_loss=2.9264224

Batch 45330, train_perplexity=18.66075, train_loss=2.9264224

Batch 45340, train_perplexity=18.660746, train_loss=2.926422

Batch 45350, train_perplexity=18.660746, train_loss=2.926422

Batch 45360, train_perplexity=18.660746, train_loss=2.926422

Batch 45370, train_perplexity=18.660753, train_loss=2.9264226

Batch 45380, train_perplexity=18.66075, train_loss=2.9264224

Batch 45390, train_perplexity=18.66075, train_loss=2.9264224

Batch 45400, train_perplexity=18.66075, train_loss=2.9264224

Batch 45410, train_perplexity=18.66075, train_loss=2.9264224
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 45420, train_perplexity=18.66075, train_loss=2.9264224

Batch 45430, train_perplexity=18.66075, train_loss=2.9264224

Batch 45440, train_perplexity=18.660753, train_loss=2.9264226

Batch 45450, train_perplexity=18.660746, train_loss=2.926422

Batch 45460, train_perplexity=18.660746, train_loss=2.926422

Batch 45470, train_perplexity=18.660746, train_loss=2.926422

Batch 45480, train_perplexity=18.660746, train_loss=2.926422

Batch 45490, train_perplexity=18.660746, train_loss=2.926422

Batch 45500, train_perplexity=18.660746, train_loss=2.926422

Batch 45510, train_perplexity=18.660746, train_loss=2.926422

Batch 45520, train_perplexity=18.660746, train_loss=2.926422

Batch 45530, train_perplexity=18.66075, train_loss=2.9264224

Batch 45540, train_perplexity=18.66075, train_loss=2.9264224

Batch 45550, train_perplexity=18.660746, train_loss=2.926422

Batch 45560, train_perplexity=18.66075, train_loss=2.9264224

Batch 45570, train_perplexity=18.66075, train_loss=2.9264224

Batch 45580, train_perplexity=18.660746, train_loss=2.926422

Batch 45590, train_perplexity=18.66075, train_loss=2.9264224

Batch 45600, train_perplexity=18.660746, train_loss=2.926422

Batch 45610, train_perplexity=18.66075, train_loss=2.9264224

Batch 45620, train_perplexity=18.660753, train_loss=2.9264226

Batch 45630, train_perplexity=18.660746, train_loss=2.926422

Batch 45640, train_perplexity=18.660746, train_loss=2.926422

Batch 45650, train_perplexity=18.66075, train_loss=2.9264224

Batch 45660, train_perplexity=18.66075, train_loss=2.9264224

Batch 45670, train_perplexity=18.660753, train_loss=2.9264226

Batch 45680, train_perplexity=18.660746, train_loss=2.926422

Batch 45690, train_perplexity=18.660746, train_loss=2.926422

Batch 45700, train_perplexity=18.660746, train_loss=2.926422

Batch 45710, train_perplexity=18.660746, train_loss=2.926422

Batch 45720, train_perplexity=18.660746, train_loss=2.926422

Batch 45730, train_perplexity=18.660746, train_loss=2.926422

Batch 45740, train_perplexity=18.660746, train_loss=2.926422

Batch 45750, train_perplexity=18.660746, train_loss=2.926422

Batch 45760, train_perplexity=18.660746, train_loss=2.926422

Batch 45770, train_perplexity=18.660746, train_loss=2.926422

Batch 45780, train_perplexity=18.66074, train_loss=2.9264219

Batch 45790, train_perplexity=18.660746, train_loss=2.926422

Batch 45800, train_perplexity=18.660746, train_loss=2.926422

Batch 45810, train_perplexity=18.66074, train_loss=2.9264219

Batch 45820, train_perplexity=18.660746, train_loss=2.926422

Batch 45830, train_perplexity=18.66074, train_loss=2.9264219

Batch 45840, train_perplexity=18.660746, train_loss=2.926422

Batch 45850, train_perplexity=18.660746, train_loss=2.926422

Batch 45860, train_perplexity=18.660746, train_loss=2.926422

Batch 45870, train_perplexity=18.66074, train_loss=2.9264219

Batch 45880, train_perplexity=18.660746, train_loss=2.926422

Batch 45890, train_perplexity=18.66074, train_loss=2.9264219

Batch 45900, train_perplexity=18.660746, train_loss=2.926422

Batch 45910, train_perplexity=18.660746, train_loss=2.926422

Batch 45920, train_perplexity=18.660746, train_loss=2.926422

Batch 45930, train_perplexity=18.660746, train_loss=2.926422

Batch 45940, train_perplexity=18.66074, train_loss=2.9264219

Batch 45950, train_perplexity=18.660746, train_loss=2.926422

Batch 45960, train_perplexity=18.66074, train_loss=2.9264219

Batch 45970, train_perplexity=18.66074, train_loss=2.9264219

Batch 45980, train_perplexity=18.66074, train_loss=2.9264219

Batch 45990, train_perplexity=18.66074, train_loss=2.9264219

Batch 46000, train_perplexity=18.66074, train_loss=2.9264219

Batch 46010, train_perplexity=18.660746, train_loss=2.926422

Batch 46020, train_perplexity=18.660736, train_loss=2.9264216

Batch 46030, train_perplexity=18.660746, train_loss=2.926422

Batch 46040, train_perplexity=18.66074, train_loss=2.9264219

Batch 46050, train_perplexity=18.660746, train_loss=2.926422

Batch 46060, train_perplexity=18.660746, train_loss=2.926422

Batch 46070, train_perplexity=18.660736, train_loss=2.9264216

Batch 46080, train_perplexity=18.660736, train_loss=2.9264216

Batch 46090, train_perplexity=18.660736, train_loss=2.9264216

Batch 46100, train_perplexity=18.66074, train_loss=2.9264219

Batch 46110, train_perplexity=18.66074, train_loss=2.9264219

Batch 46120, train_perplexity=18.66074, train_loss=2.9264219

Batch 46130, train_perplexity=18.66074, train_loss=2.9264219

Batch 46140, train_perplexity=18.66074, train_loss=2.9264219

Batch 46150, train_perplexity=18.66074, train_loss=2.9264219

Batch 46160, train_perplexity=18.66074, train_loss=2.9264219

Batch 46170, train_perplexity=18.66074, train_loss=2.9264219

Batch 46180, train_perplexity=18.66074, train_loss=2.9264219

Batch 46190, train_perplexity=18.66074, train_loss=2.9264219

Batch 46200, train_perplexity=18.66074, train_loss=2.9264219

Batch 46210, train_perplexity=18.66074, train_loss=2.9264219

Batch 46220, train_perplexity=18.660736, train_loss=2.9264216

Batch 46230, train_perplexity=18.66074, train_loss=2.9264219

Batch 46240, train_perplexity=18.660736, train_loss=2.9264216

Batch 46250, train_perplexity=18.66074, train_loss=2.9264219

Batch 46260, train_perplexity=18.66074, train_loss=2.9264219

Batch 46270, train_perplexity=18.66074, train_loss=2.9264219

Batch 46280, train_perplexity=18.660736, train_loss=2.9264216

Batch 46290, train_perplexity=18.660736, train_loss=2.9264216

Batch 46300, train_perplexity=18.66074, train_loss=2.9264219

Batch 46310, train_perplexity=18.660736, train_loss=2.9264216

Batch 46320, train_perplexity=18.660736, train_loss=2.9264216

Batch 46330, train_perplexity=18.66074, train_loss=2.9264219

Batch 46340, train_perplexity=18.660736, train_loss=2.9264216

Batch 46350, train_perplexity=18.66074, train_loss=2.9264219

Batch 46360, train_perplexity=18.660736, train_loss=2.9264216

Batch 46370, train_perplexity=18.660736, train_loss=2.9264216

Batch 46380, train_perplexity=18.660736, train_loss=2.9264216

Batch 46390, train_perplexity=18.660736, train_loss=2.9264216

Batch 46400, train_perplexity=18.660736, train_loss=2.9264216

Batch 46410, train_perplexity=18.660736, train_loss=2.9264216

Batch 46420, train_perplexity=18.660736, train_loss=2.9264216

Batch 46430, train_perplexity=18.660736, train_loss=2.9264216

Batch 46440, train_perplexity=18.660736, train_loss=2.9264216

Batch 46450, train_perplexity=18.660736, train_loss=2.9264216

Batch 46460, train_perplexity=18.660736, train_loss=2.9264216

Batch 46470, train_perplexity=18.660736, train_loss=2.9264216

Batch 46480, train_perplexity=18.660736, train_loss=2.9264216

Batch 46490, train_perplexity=18.660732, train_loss=2.9264214

Batch 46500, train_perplexity=18.660736, train_loss=2.9264216

Batch 46510, train_perplexity=18.660736, train_loss=2.9264216

Batch 46520, train_perplexity=18.660727, train_loss=2.9264212

Batch 46530, train_perplexity=18.660736, train_loss=2.9264216

Batch 46540, train_perplexity=18.660732, train_loss=2.9264214

Batch 46550, train_perplexity=18.660736, train_loss=2.9264216

Batch 46560, train_perplexity=18.660736, train_loss=2.9264216

Batch 46570, train_perplexity=18.660732, train_loss=2.9264214

Batch 46580, train_perplexity=18.660732, train_loss=2.9264214

Batch 46590, train_perplexity=18.660736, train_loss=2.9264216

Batch 46600, train_perplexity=18.660732, train_loss=2.9264214

Batch 46610, train_perplexity=18.660736, train_loss=2.9264216

Batch 46620, train_perplexity=18.660736, train_loss=2.9264216

Batch 46630, train_perplexity=18.660732, train_loss=2.9264214

Batch 46640, train_perplexity=18.660736, train_loss=2.9264216

Batch 46650, train_perplexity=18.660736, train_loss=2.9264216

Batch 46660, train_perplexity=18.660736, train_loss=2.9264216

Batch 46670, train_perplexity=18.660732, train_loss=2.9264214

Batch 46680, train_perplexity=18.660727, train_loss=2.9264212

Batch 46690, train_perplexity=18.660732, train_loss=2.9264214

Batch 46700, train_perplexity=18.660732, train_loss=2.9264214

Batch 46710, train_perplexity=18.660732, train_loss=2.9264214

Batch 46720, train_perplexity=18.660727, train_loss=2.9264212

Batch 46730, train_perplexity=18.660736, train_loss=2.9264216
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46740, train_perplexity=18.660736, train_loss=2.9264216

Batch 46750, train_perplexity=18.660727, train_loss=2.9264212

Batch 46760, train_perplexity=18.660727, train_loss=2.9264212

Batch 46770, train_perplexity=18.660727, train_loss=2.9264212

Batch 46780, train_perplexity=18.660727, train_loss=2.9264212

Batch 46790, train_perplexity=18.660727, train_loss=2.9264212

Batch 46800, train_perplexity=18.660732, train_loss=2.9264214

Batch 46810, train_perplexity=18.660727, train_loss=2.9264212

Batch 46820, train_perplexity=18.660727, train_loss=2.9264212

Batch 46830, train_perplexity=18.660727, train_loss=2.9264212

Batch 46840, train_perplexity=18.660727, train_loss=2.9264212

Batch 46850, train_perplexity=18.660727, train_loss=2.9264212

Batch 46860, train_perplexity=18.660732, train_loss=2.9264214

Batch 46870, train_perplexity=18.660732, train_loss=2.9264214

Batch 46880, train_perplexity=18.660727, train_loss=2.9264212

Batch 46890, train_perplexity=18.660723, train_loss=2.926421

Batch 46900, train_perplexity=18.660732, train_loss=2.9264214

Batch 46910, train_perplexity=18.660727, train_loss=2.9264212

Batch 46920, train_perplexity=18.660727, train_loss=2.9264212

Batch 46930, train_perplexity=18.660727, train_loss=2.9264212

Batch 46940, train_perplexity=18.660727, train_loss=2.9264212

Batch 46950, train_perplexity=18.660732, train_loss=2.9264214

Batch 46960, train_perplexity=18.660727, train_loss=2.9264212

Batch 46970, train_perplexity=18.660723, train_loss=2.926421

Batch 46980, train_perplexity=18.660727, train_loss=2.9264212

Batch 46990, train_perplexity=18.660723, train_loss=2.926421

Batch 47000, train_perplexity=18.660727, train_loss=2.9264212

Batch 47010, train_perplexity=18.660727, train_loss=2.9264212

Batch 47020, train_perplexity=18.660732, train_loss=2.9264214

Batch 47030, train_perplexity=18.660727, train_loss=2.9264212

Batch 47040, train_perplexity=18.660727, train_loss=2.9264212

Batch 47050, train_perplexity=18.660727, train_loss=2.9264212

Batch 47060, train_perplexity=18.660727, train_loss=2.9264212

Batch 47070, train_perplexity=18.660727, train_loss=2.9264212

Batch 47080, train_perplexity=18.660723, train_loss=2.926421

Batch 47090, train_perplexity=18.660727, train_loss=2.9264212

Batch 47100, train_perplexity=18.660719, train_loss=2.9264207

Batch 47110, train_perplexity=18.660719, train_loss=2.9264207

Batch 47120, train_perplexity=18.660727, train_loss=2.9264212

Batch 47130, train_perplexity=18.660727, train_loss=2.9264212

Batch 47140, train_perplexity=18.660723, train_loss=2.926421

Batch 47150, train_perplexity=18.660723, train_loss=2.926421

Batch 47160, train_perplexity=18.660723, train_loss=2.926421

Batch 47170, train_perplexity=18.660719, train_loss=2.9264207

Batch 47180, train_perplexity=18.660727, train_loss=2.9264212

Batch 47190, train_perplexity=18.660723, train_loss=2.926421

Batch 47200, train_perplexity=18.660719, train_loss=2.9264207

Batch 47210, train_perplexity=18.660719, train_loss=2.9264207

Batch 47220, train_perplexity=18.660719, train_loss=2.9264207

Batch 47230, train_perplexity=18.660719, train_loss=2.9264207

Batch 47240, train_perplexity=18.660719, train_loss=2.9264207

Batch 47250, train_perplexity=18.660727, train_loss=2.9264212

Batch 47260, train_perplexity=18.660723, train_loss=2.926421

Batch 47270, train_perplexity=18.660719, train_loss=2.9264207

Batch 47280, train_perplexity=18.660719, train_loss=2.9264207

Batch 47290, train_perplexity=18.660719, train_loss=2.9264207

Batch 47300, train_perplexity=18.660719, train_loss=2.9264207

Batch 47310, train_perplexity=18.660719, train_loss=2.9264207

Batch 47320, train_perplexity=18.660719, train_loss=2.9264207

Batch 47330, train_perplexity=18.660719, train_loss=2.9264207

Batch 47340, train_perplexity=18.660723, train_loss=2.926421

Batch 47350, train_perplexity=18.660719, train_loss=2.9264207

Batch 47360, train_perplexity=18.660719, train_loss=2.9264207

Batch 47370, train_perplexity=18.660719, train_loss=2.9264207

Batch 47380, train_perplexity=18.660719, train_loss=2.9264207

Batch 47390, train_perplexity=18.660719, train_loss=2.9264207

Batch 47400, train_perplexity=18.660719, train_loss=2.9264207

Batch 47410, train_perplexity=18.660719, train_loss=2.9264207

Batch 47420, train_perplexity=18.660719, train_loss=2.9264207

Batch 47430, train_perplexity=18.660723, train_loss=2.926421

Batch 47440, train_perplexity=18.660719, train_loss=2.9264207

Batch 47450, train_perplexity=18.660719, train_loss=2.9264207

Batch 47460, train_perplexity=18.660719, train_loss=2.9264207

Batch 47470, train_perplexity=18.660713, train_loss=2.9264205

Batch 47480, train_perplexity=18.660713, train_loss=2.9264205

Batch 47490, train_perplexity=18.66071, train_loss=2.9264202

Batch 47500, train_perplexity=18.660719, train_loss=2.9264207

Batch 47510, train_perplexity=18.660719, train_loss=2.9264207

Batch 47520, train_perplexity=18.660719, train_loss=2.9264207

Batch 47530, train_perplexity=18.660713, train_loss=2.9264205

Batch 47540, train_perplexity=18.660719, train_loss=2.9264207

Batch 47550, train_perplexity=18.660713, train_loss=2.9264205

Batch 47560, train_perplexity=18.660713, train_loss=2.9264205

Batch 47570, train_perplexity=18.660713, train_loss=2.9264205

Batch 47580, train_perplexity=18.66071, train_loss=2.9264202

Batch 47590, train_perplexity=18.660713, train_loss=2.9264205

Batch 47600, train_perplexity=18.660713, train_loss=2.9264205

Batch 47610, train_perplexity=18.660713, train_loss=2.9264205

Batch 47620, train_perplexity=18.660713, train_loss=2.9264205

Batch 47630, train_perplexity=18.660713, train_loss=2.9264205

Batch 47640, train_perplexity=18.66071, train_loss=2.9264202

Batch 47650, train_perplexity=18.66071, train_loss=2.9264202

Batch 47660, train_perplexity=18.66071, train_loss=2.9264202

Batch 47670, train_perplexity=18.66071, train_loss=2.9264202

Batch 47680, train_perplexity=18.66071, train_loss=2.9264202

Batch 47690, train_perplexity=18.66071, train_loss=2.9264202

Batch 47700, train_perplexity=18.660713, train_loss=2.9264205

Batch 47710, train_perplexity=18.66071, train_loss=2.9264202

Batch 47720, train_perplexity=18.66071, train_loss=2.9264202

Batch 47730, train_perplexity=18.66071, train_loss=2.9264202

Batch 47740, train_perplexity=18.66071, train_loss=2.9264202

Batch 47750, train_perplexity=18.66071, train_loss=2.9264202

Batch 47760, train_perplexity=18.660706, train_loss=2.92642

Batch 47770, train_perplexity=18.660706, train_loss=2.92642

Batch 47780, train_perplexity=18.66071, train_loss=2.9264202

Batch 47790, train_perplexity=18.66071, train_loss=2.9264202

Batch 47800, train_perplexity=18.66071, train_loss=2.9264202

Batch 47810, train_perplexity=18.660706, train_loss=2.92642

Batch 47820, train_perplexity=18.66071, train_loss=2.9264202

Batch 47830, train_perplexity=18.66071, train_loss=2.9264202

Batch 47840, train_perplexity=18.66071, train_loss=2.9264202

Batch 47850, train_perplexity=18.6607, train_loss=2.9264197

Batch 47860, train_perplexity=18.66071, train_loss=2.9264202

Batch 47870, train_perplexity=18.66071, train_loss=2.9264202

Batch 47880, train_perplexity=18.660706, train_loss=2.92642

Batch 47890, train_perplexity=18.660706, train_loss=2.92642

Batch 47900, train_perplexity=18.660706, train_loss=2.92642

Batch 47910, train_perplexity=18.66071, train_loss=2.9264202

Batch 47920, train_perplexity=18.660706, train_loss=2.92642

Batch 47930, train_perplexity=18.6607, train_loss=2.9264197

Batch 47940, train_perplexity=18.6607, train_loss=2.9264197

Batch 47950, train_perplexity=18.6607, train_loss=2.9264197

Batch 47960, train_perplexity=18.6607, train_loss=2.9264197

Batch 47970, train_perplexity=18.6607, train_loss=2.9264197

Batch 47980, train_perplexity=18.6607, train_loss=2.9264197

Batch 47990, train_perplexity=18.6607, train_loss=2.9264197

Batch 48000, train_perplexity=18.6607, train_loss=2.9264197

Batch 48010, train_perplexity=18.6607, train_loss=2.9264197

Batch 48020, train_perplexity=18.6607, train_loss=2.9264197

Batch 48030, train_perplexity=18.6607, train_loss=2.9264197

Batch 48040, train_perplexity=18.6607, train_loss=2.9264197

Batch 48050, train_perplexity=18.6607, train_loss=2.9264197
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48060, train_perplexity=18.6607, train_loss=2.9264197

Batch 48070, train_perplexity=18.6607, train_loss=2.9264197

Batch 48080, train_perplexity=18.6607, train_loss=2.9264197

Batch 48090, train_perplexity=18.6607, train_loss=2.9264197

Batch 48100, train_perplexity=18.660696, train_loss=2.9264195

Batch 48110, train_perplexity=18.660696, train_loss=2.9264195

Batch 48120, train_perplexity=18.6607, train_loss=2.9264197

Batch 48130, train_perplexity=18.660696, train_loss=2.9264195

Batch 48140, train_perplexity=18.6607, train_loss=2.9264197

Batch 48150, train_perplexity=18.660692, train_loss=2.9264193

Batch 48160, train_perplexity=18.660692, train_loss=2.9264193

Batch 48170, train_perplexity=18.660692, train_loss=2.9264193

Batch 48180, train_perplexity=18.660696, train_loss=2.9264195

Batch 48190, train_perplexity=18.660692, train_loss=2.9264193

Batch 48200, train_perplexity=18.660696, train_loss=2.9264195

Batch 48210, train_perplexity=18.660692, train_loss=2.9264193

Batch 48220, train_perplexity=18.660692, train_loss=2.9264193

Batch 48230, train_perplexity=18.660692, train_loss=2.9264193

Batch 48240, train_perplexity=18.660692, train_loss=2.9264193

Batch 48250, train_perplexity=18.660692, train_loss=2.9264193

Batch 48260, train_perplexity=18.660692, train_loss=2.9264193

Batch 48270, train_perplexity=18.660686, train_loss=2.926419

Batch 48280, train_perplexity=18.660692, train_loss=2.9264193

Batch 48290, train_perplexity=18.660683, train_loss=2.9264188

Batch 48300, train_perplexity=18.660686, train_loss=2.926419

Batch 48310, train_perplexity=18.660686, train_loss=2.926419

Batch 48320, train_perplexity=18.660683, train_loss=2.9264188

Batch 48330, train_perplexity=18.660683, train_loss=2.9264188

Batch 48340, train_perplexity=18.660683, train_loss=2.9264188

Batch 48350, train_perplexity=18.660686, train_loss=2.926419

Batch 48360, train_perplexity=18.660683, train_loss=2.9264188

Batch 48370, train_perplexity=18.660683, train_loss=2.9264188

Batch 48380, train_perplexity=18.660683, train_loss=2.9264188

Batch 48390, train_perplexity=18.660686, train_loss=2.926419

Batch 48400, train_perplexity=18.660683, train_loss=2.9264188

Batch 48410, train_perplexity=18.660683, train_loss=2.9264188

Batch 48420, train_perplexity=18.660683, train_loss=2.9264188

Batch 48430, train_perplexity=18.660683, train_loss=2.9264188

Batch 48440, train_perplexity=18.660679, train_loss=2.9264185

Batch 48450, train_perplexity=18.660683, train_loss=2.9264188

Batch 48460, train_perplexity=18.660679, train_loss=2.9264185

Batch 48470, train_perplexity=18.660683, train_loss=2.9264188

Batch 48480, train_perplexity=18.660679, train_loss=2.9264185

Batch 48490, train_perplexity=18.660679, train_loss=2.9264185

Batch 48500, train_perplexity=18.660673, train_loss=2.9264183

Batch 48510, train_perplexity=18.660673, train_loss=2.9264183

Batch 48520, train_perplexity=18.660673, train_loss=2.9264183

Batch 48530, train_perplexity=18.660673, train_loss=2.9264183

Batch 48540, train_perplexity=18.660679, train_loss=2.9264185

Batch 48550, train_perplexity=18.660673, train_loss=2.9264183

Batch 48560, train_perplexity=18.660673, train_loss=2.9264183

Batch 48570, train_perplexity=18.660673, train_loss=2.9264183

Batch 48580, train_perplexity=18.66067, train_loss=2.926418

Batch 48590, train_perplexity=18.66067, train_loss=2.926418

Batch 48600, train_perplexity=18.66067, train_loss=2.926418

Batch 48610, train_perplexity=18.660666, train_loss=2.9264178

Batch 48620, train_perplexity=18.66067, train_loss=2.926418

Batch 48630, train_perplexity=18.660673, train_loss=2.9264183

Batch 48640, train_perplexity=18.660666, train_loss=2.9264178

Batch 48650, train_perplexity=18.66067, train_loss=2.926418

Batch 48660, train_perplexity=18.660666, train_loss=2.9264178

Batch 48670, train_perplexity=18.660666, train_loss=2.9264178

Batch 48680, train_perplexity=18.660666, train_loss=2.9264178

Batch 48690, train_perplexity=18.660666, train_loss=2.9264178

Batch 48700, train_perplexity=18.660666, train_loss=2.9264178

Batch 48710, train_perplexity=18.66066, train_loss=2.9264176

Batch 48720, train_perplexity=18.66066, train_loss=2.9264176

Batch 48730, train_perplexity=18.660666, train_loss=2.9264178

Batch 48740, train_perplexity=18.660656, train_loss=2.9264174

Batch 48750, train_perplexity=18.660656, train_loss=2.9264174

Batch 48760, train_perplexity=18.660656, train_loss=2.9264174

Batch 48770, train_perplexity=18.660656, train_loss=2.9264174

Batch 48780, train_perplexity=18.660656, train_loss=2.9264174

Batch 48790, train_perplexity=18.660656, train_loss=2.9264174

Batch 48800, train_perplexity=18.660656, train_loss=2.9264174

Batch 48810, train_perplexity=18.660656, train_loss=2.9264174

Batch 48820, train_perplexity=18.660656, train_loss=2.9264174

Batch 48830, train_perplexity=18.660656, train_loss=2.9264174

Batch 48840, train_perplexity=18.660656, train_loss=2.9264174

Batch 48850, train_perplexity=18.660656, train_loss=2.9264174

Batch 48860, train_perplexity=18.660646, train_loss=2.9264169

Batch 48870, train_perplexity=18.660656, train_loss=2.9264174

Batch 48880, train_perplexity=18.660656, train_loss=2.9264174

Batch 48890, train_perplexity=18.660646, train_loss=2.9264169

Batch 48900, train_perplexity=18.660646, train_loss=2.9264169

Batch 48910, train_perplexity=18.660656, train_loss=2.9264174

Batch 48920, train_perplexity=18.660646, train_loss=2.9264169

Batch 48930, train_perplexity=18.660652, train_loss=2.926417

Batch 48940, train_perplexity=18.660646, train_loss=2.9264169

Batch 48950, train_perplexity=18.660646, train_loss=2.9264169

Batch 48960, train_perplexity=18.660646, train_loss=2.9264169

Batch 48970, train_perplexity=18.660646, train_loss=2.9264169

Batch 48980, train_perplexity=18.660646, train_loss=2.9264169

Batch 48990, train_perplexity=18.660646, train_loss=2.9264169

Batch 49000, train_perplexity=18.660646, train_loss=2.9264169

Batch 49010, train_perplexity=18.660643, train_loss=2.9264166

Batch 49020, train_perplexity=18.660639, train_loss=2.9264164

Batch 49030, train_perplexity=18.660643, train_loss=2.9264166

Batch 49040, train_perplexity=18.660646, train_loss=2.9264169

Batch 49050, train_perplexity=18.660639, train_loss=2.9264164

Batch 49060, train_perplexity=18.660646, train_loss=2.9264169

Batch 49070, train_perplexity=18.660643, train_loss=2.9264166

Batch 49080, train_perplexity=18.660643, train_loss=2.9264166

Batch 49090, train_perplexity=18.660639, train_loss=2.9264164

Batch 49100, train_perplexity=18.660639, train_loss=2.9264164

Batch 49110, train_perplexity=18.660639, train_loss=2.9264164

Batch 49120, train_perplexity=18.660639, train_loss=2.9264164

Batch 49130, train_perplexity=18.660639, train_loss=2.9264164

Batch 49140, train_perplexity=18.660639, train_loss=2.9264164

Batch 49150, train_perplexity=18.660639, train_loss=2.9264164

Batch 49160, train_perplexity=18.660639, train_loss=2.9264164

Batch 49170, train_perplexity=18.660639, train_loss=2.9264164

Batch 49180, train_perplexity=18.660639, train_loss=2.9264164

Batch 49190, train_perplexity=18.660639, train_loss=2.9264164

Batch 49200, train_perplexity=18.660639, train_loss=2.9264164

Batch 49210, train_perplexity=18.660639, train_loss=2.9264164

Batch 49220, train_perplexity=18.660633, train_loss=2.9264162

Batch 49230, train_perplexity=18.660639, train_loss=2.9264164

Batch 49240, train_perplexity=18.660639, train_loss=2.9264164

Batch 49250, train_perplexity=18.660639, train_loss=2.9264164

Batch 49260, train_perplexity=18.660639, train_loss=2.9264164

Batch 49270, train_perplexity=18.660639, train_loss=2.9264164

Batch 49280, train_perplexity=18.660639, train_loss=2.9264164

Batch 49290, train_perplexity=18.660639, train_loss=2.9264164

Batch 49300, train_perplexity=18.66063, train_loss=2.926416

Batch 49310, train_perplexity=18.660639, train_loss=2.9264164

Batch 49320, train_perplexity=18.66063, train_loss=2.926416

Batch 49330, train_perplexity=18.660633, train_loss=2.9264162

Batch 49340, train_perplexity=18.66063, train_loss=2.926416

Batch 49350, train_perplexity=18.660633, train_loss=2.9264162

Batch 49360, train_perplexity=18.66063, train_loss=2.926416
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 49370, train_perplexity=18.66063, train_loss=2.926416

Batch 49380, train_perplexity=18.660639, train_loss=2.9264164

Batch 49390, train_perplexity=18.66063, train_loss=2.926416

Batch 49400, train_perplexity=18.660625, train_loss=2.9264157

Batch 49410, train_perplexity=18.66063, train_loss=2.926416

Batch 49420, train_perplexity=18.66063, train_loss=2.926416

Batch 49430, train_perplexity=18.66063, train_loss=2.926416

Batch 49440, train_perplexity=18.66063, train_loss=2.926416

Batch 49450, train_perplexity=18.66063, train_loss=2.926416

Batch 49460, train_perplexity=18.66063, train_loss=2.926416

Batch 49470, train_perplexity=18.66063, train_loss=2.926416

Batch 49480, train_perplexity=18.66063, train_loss=2.926416

Batch 49490, train_perplexity=18.660633, train_loss=2.9264162

Batch 49500, train_perplexity=18.66062, train_loss=2.9264154

Batch 49510, train_perplexity=18.660625, train_loss=2.9264157

Batch 49520, train_perplexity=18.66062, train_loss=2.9264154

Batch 49530, train_perplexity=18.660625, train_loss=2.9264157

Batch 49540, train_perplexity=18.660625, train_loss=2.9264157

Batch 49550, train_perplexity=18.66063, train_loss=2.926416

Batch 49560, train_perplexity=18.66063, train_loss=2.926416

Batch 49570, train_perplexity=18.660625, train_loss=2.9264157

Batch 49580, train_perplexity=18.66063, train_loss=2.926416

Batch 49590, train_perplexity=18.66062, train_loss=2.9264154

Batch 49600, train_perplexity=18.660625, train_loss=2.9264157

Batch 49610, train_perplexity=18.66062, train_loss=2.9264154

Batch 49620, train_perplexity=18.66062, train_loss=2.9264154

Batch 49630, train_perplexity=18.66062, train_loss=2.9264154

Batch 49640, train_perplexity=18.660616, train_loss=2.9264152

Batch 49650, train_perplexity=18.66062, train_loss=2.9264154

Batch 49660, train_perplexity=18.66062, train_loss=2.9264154

Batch 49670, train_perplexity=18.66062, train_loss=2.9264154

Batch 49680, train_perplexity=18.660616, train_loss=2.9264152

Batch 49690, train_perplexity=18.66062, train_loss=2.9264154

Batch 49700, train_perplexity=18.66062, train_loss=2.9264154

Batch 49710, train_perplexity=18.66062, train_loss=2.9264154

Batch 49720, train_perplexity=18.66062, train_loss=2.9264154

Batch 49730, train_perplexity=18.660616, train_loss=2.9264152

Batch 49740, train_perplexity=18.660616, train_loss=2.9264152

Batch 49750, train_perplexity=18.660612, train_loss=2.926415

Batch 49760, train_perplexity=18.66062, train_loss=2.9264154

Batch 49770, train_perplexity=18.660612, train_loss=2.926415

Batch 49780, train_perplexity=18.660616, train_loss=2.9264152

Batch 49790, train_perplexity=18.66062, train_loss=2.9264154

Batch 49800, train_perplexity=18.660616, train_loss=2.9264152

Batch 49810, train_perplexity=18.660616, train_loss=2.9264152

Batch 49820, train_perplexity=18.660616, train_loss=2.9264152

Batch 49830, train_perplexity=18.660616, train_loss=2.9264152

Batch 49840, train_perplexity=18.66062, train_loss=2.9264154

Batch 49850, train_perplexity=18.66062, train_loss=2.9264154

Batch 49860, train_perplexity=18.660612, train_loss=2.926415

Batch 49870, train_perplexity=18.66062, train_loss=2.9264154

Batch 49880, train_perplexity=18.660612, train_loss=2.926415

Batch 49890, train_perplexity=18.660612, train_loss=2.926415

Batch 49900, train_perplexity=18.660612, train_loss=2.926415

Batch 49910, train_perplexity=18.660612, train_loss=2.926415

Batch 49920, train_perplexity=18.660612, train_loss=2.926415

Batch 49930, train_perplexity=18.660612, train_loss=2.926415

Batch 49940, train_perplexity=18.660612, train_loss=2.926415

Batch 49950, train_perplexity=18.660612, train_loss=2.926415

Batch 49960, train_perplexity=18.660616, train_loss=2.9264152

Batch 49970, train_perplexity=18.660612, train_loss=2.926415

Batch 49980, train_perplexity=18.660612, train_loss=2.926415

Batch 49990, train_perplexity=18.660612, train_loss=2.926415

Batch 50000, train_perplexity=18.660612, train_loss=2.926415

Batch 50010, train_perplexity=18.660612, train_loss=2.926415

Batch 50020, train_perplexity=18.660606, train_loss=2.9264147

Batch 50030, train_perplexity=18.660612, train_loss=2.926415

Batch 50040, train_perplexity=18.660612, train_loss=2.926415

Batch 50050, train_perplexity=18.660612, train_loss=2.926415

Batch 50060, train_perplexity=18.660612, train_loss=2.926415

Batch 50070, train_perplexity=18.660606, train_loss=2.9264147

Batch 50080, train_perplexity=18.660606, train_loss=2.9264147

Batch 50090, train_perplexity=18.660606, train_loss=2.9264147

Batch 50100, train_perplexity=18.660606, train_loss=2.9264147

Batch 50110, train_perplexity=18.660612, train_loss=2.926415

Batch 50120, train_perplexity=18.660612, train_loss=2.926415

Batch 50130, train_perplexity=18.660603, train_loss=2.9264145

Batch 50140, train_perplexity=18.660612, train_loss=2.926415

Batch 50150, train_perplexity=18.660603, train_loss=2.9264145

Batch 50160, train_perplexity=18.660606, train_loss=2.9264147

Batch 50170, train_perplexity=18.660603, train_loss=2.9264145

Batch 50180, train_perplexity=18.660603, train_loss=2.9264145

Batch 50190, train_perplexity=18.660603, train_loss=2.9264145

Batch 50200, train_perplexity=18.660603, train_loss=2.9264145

Batch 50210, train_perplexity=18.660603, train_loss=2.9264145

Batch 50220, train_perplexity=18.660603, train_loss=2.9264145

Batch 50230, train_perplexity=18.660603, train_loss=2.9264145

Batch 50240, train_perplexity=18.660603, train_loss=2.9264145

Batch 50250, train_perplexity=18.660599, train_loss=2.9264143

Batch 50260, train_perplexity=18.660603, train_loss=2.9264145

Batch 50270, train_perplexity=18.660603, train_loss=2.9264145

Batch 50280, train_perplexity=18.660599, train_loss=2.9264143

Batch 50290, train_perplexity=18.660603, train_loss=2.9264145

Batch 50300, train_perplexity=18.660599, train_loss=2.9264143

Batch 50310, train_perplexity=18.660599, train_loss=2.9264143

Batch 50320, train_perplexity=18.660603, train_loss=2.9264145

Batch 50330, train_perplexity=18.660599, train_loss=2.9264143

Batch 50340, train_perplexity=18.660603, train_loss=2.9264145

Batch 50350, train_perplexity=18.660603, train_loss=2.9264145

Batch 50360, train_perplexity=18.660599, train_loss=2.9264143

Batch 50370, train_perplexity=18.660603, train_loss=2.9264145

Batch 50380, train_perplexity=18.660603, train_loss=2.9264145

Batch 50390, train_perplexity=18.660593, train_loss=2.926414

Batch 50400, train_perplexity=18.660593, train_loss=2.926414

Batch 50410, train_perplexity=18.660593, train_loss=2.926414

Batch 50420, train_perplexity=18.660599, train_loss=2.9264143

Batch 50430, train_perplexity=18.660593, train_loss=2.926414

Batch 50440, train_perplexity=18.660603, train_loss=2.9264145

Batch 50450, train_perplexity=18.660599, train_loss=2.9264143

Batch 50460, train_perplexity=18.660599, train_loss=2.9264143

Batch 50470, train_perplexity=18.660593, train_loss=2.926414

Batch 50480, train_perplexity=18.660599, train_loss=2.9264143

Batch 50490, train_perplexity=18.660593, train_loss=2.926414

Batch 50500, train_perplexity=18.660593, train_loss=2.926414

Batch 50510, train_perplexity=18.660593, train_loss=2.926414

Batch 50520, train_perplexity=18.660593, train_loss=2.926414

Batch 50530, train_perplexity=18.660593, train_loss=2.926414

Batch 50540, train_perplexity=18.660593, train_loss=2.926414

Batch 50550, train_perplexity=18.660593, train_loss=2.926414

Batch 50560, train_perplexity=18.660593, train_loss=2.926414

Batch 50570, train_perplexity=18.660593, train_loss=2.926414

Batch 50580, train_perplexity=18.66059, train_loss=2.9264138

Batch 50590, train_perplexity=18.660599, train_loss=2.9264143

Batch 50600, train_perplexity=18.660593, train_loss=2.926414

Batch 50610, train_perplexity=18.660593, train_loss=2.926414

Batch 50620, train_perplexity=18.66059, train_loss=2.9264138

Batch 50630, train_perplexity=18.660593, train_loss=2.926414

Batch 50640, train_perplexity=18.660593, train_loss=2.926414

Batch 50650, train_perplexity=18.66059, train_loss=2.9264138

Batch 50660, train_perplexity=18.660593, train_loss=2.926414

Batch 50670, train_perplexity=18.660593, train_loss=2.926414

Batch 50680, train_perplexity=18.660593, train_loss=2.926414
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 50690, train_perplexity=18.660585, train_loss=2.9264135

Batch 50700, train_perplexity=18.660585, train_loss=2.9264135

Batch 50710, train_perplexity=18.660585, train_loss=2.9264135

Batch 50720, train_perplexity=18.660593, train_loss=2.926414

Batch 50730, train_perplexity=18.66059, train_loss=2.9264138

Batch 50740, train_perplexity=18.66059, train_loss=2.9264138

Batch 50750, train_perplexity=18.66059, train_loss=2.9264138

Batch 50760, train_perplexity=18.660585, train_loss=2.9264135

Batch 50770, train_perplexity=18.66059, train_loss=2.9264138

Batch 50780, train_perplexity=18.660585, train_loss=2.9264135

Batch 50790, train_perplexity=18.660585, train_loss=2.9264135

Batch 50800, train_perplexity=18.660585, train_loss=2.9264135

Batch 50810, train_perplexity=18.660585, train_loss=2.9264135

Batch 50820, train_perplexity=18.660585, train_loss=2.9264135

Batch 50830, train_perplexity=18.660585, train_loss=2.9264135

Batch 50840, train_perplexity=18.660593, train_loss=2.926414

Batch 50850, train_perplexity=18.660585, train_loss=2.9264135

Batch 50860, train_perplexity=18.660585, train_loss=2.9264135

Batch 50870, train_perplexity=18.660585, train_loss=2.9264135

Batch 50880, train_perplexity=18.660585, train_loss=2.9264135

Batch 50890, train_perplexity=18.660585, train_loss=2.9264135

Batch 50900, train_perplexity=18.660585, train_loss=2.9264135

Batch 50910, train_perplexity=18.660585, train_loss=2.9264135

Batch 50920, train_perplexity=18.66058, train_loss=2.9264133

Batch 50930, train_perplexity=18.660585, train_loss=2.9264135

Batch 50940, train_perplexity=18.660585, train_loss=2.9264135

Batch 50950, train_perplexity=18.660585, train_loss=2.9264135

Batch 50960, train_perplexity=18.660585, train_loss=2.9264135

Batch 50970, train_perplexity=18.660585, train_loss=2.9264135

Batch 50980, train_perplexity=18.660585, train_loss=2.9264135

Batch 50990, train_perplexity=18.66058, train_loss=2.9264133

Batch 51000, train_perplexity=18.660585, train_loss=2.9264135

Batch 51010, train_perplexity=18.660585, train_loss=2.9264135

Batch 51020, train_perplexity=18.66058, train_loss=2.9264133

Batch 51030, train_perplexity=18.660576, train_loss=2.926413

Batch 51040, train_perplexity=18.660576, train_loss=2.926413

Batch 51050, train_perplexity=18.660585, train_loss=2.9264135

Batch 51060, train_perplexity=18.660576, train_loss=2.926413

Batch 51070, train_perplexity=18.660585, train_loss=2.9264135

Batch 51080, train_perplexity=18.660576, train_loss=2.926413

Batch 51090, train_perplexity=18.660576, train_loss=2.926413

Batch 51100, train_perplexity=18.660576, train_loss=2.926413

Batch 51110, train_perplexity=18.660576, train_loss=2.926413

Batch 51120, train_perplexity=18.660576, train_loss=2.926413

Batch 51130, train_perplexity=18.660576, train_loss=2.926413

Batch 51140, train_perplexity=18.660576, train_loss=2.926413

Batch 51150, train_perplexity=18.660576, train_loss=2.926413

Batch 51160, train_perplexity=18.660576, train_loss=2.926413

Batch 51170, train_perplexity=18.660576, train_loss=2.926413

Batch 51180, train_perplexity=18.660576, train_loss=2.926413

Batch 51190, train_perplexity=18.660576, train_loss=2.926413

Batch 51200, train_perplexity=18.660576, train_loss=2.926413

Batch 51210, train_perplexity=18.660576, train_loss=2.926413

Batch 51220, train_perplexity=18.660576, train_loss=2.926413

Batch 51230, train_perplexity=18.660576, train_loss=2.926413

Batch 51240, train_perplexity=18.660572, train_loss=2.9264128

Batch 51250, train_perplexity=18.660576, train_loss=2.926413

Batch 51260, train_perplexity=18.660576, train_loss=2.926413

Batch 51270, train_perplexity=18.660572, train_loss=2.9264128

Batch 51280, train_perplexity=18.660576, train_loss=2.926413

Batch 51290, train_perplexity=18.660572, train_loss=2.9264128

Batch 51300, train_perplexity=18.660572, train_loss=2.9264128

Batch 51310, train_perplexity=18.660572, train_loss=2.9264128

Batch 51320, train_perplexity=18.660572, train_loss=2.9264128

Batch 51330, train_perplexity=18.660572, train_loss=2.9264128

Batch 51340, train_perplexity=18.660566, train_loss=2.9264126

Batch 51350, train_perplexity=18.660566, train_loss=2.9264126

Batch 51360, train_perplexity=18.660576, train_loss=2.926413

Batch 51370, train_perplexity=18.660566, train_loss=2.9264126

Batch 51380, train_perplexity=18.660566, train_loss=2.9264126

Batch 51390, train_perplexity=18.660566, train_loss=2.9264126

Batch 51400, train_perplexity=18.660566, train_loss=2.9264126

Batch 51410, train_perplexity=18.660566, train_loss=2.9264126

Batch 51420, train_perplexity=18.660566, train_loss=2.9264126

Batch 51430, train_perplexity=18.660566, train_loss=2.9264126

Batch 51440, train_perplexity=18.660566, train_loss=2.9264126

Batch 51450, train_perplexity=18.660572, train_loss=2.9264128

Batch 51460, train_perplexity=18.660566, train_loss=2.9264126

Batch 51470, train_perplexity=18.660566, train_loss=2.9264126

Batch 51480, train_perplexity=18.660566, train_loss=2.9264126

Batch 51490, train_perplexity=18.660566, train_loss=2.9264126

Batch 51500, train_perplexity=18.660566, train_loss=2.9264126

Batch 51510, train_perplexity=18.660566, train_loss=2.9264126

Batch 51520, train_perplexity=18.660566, train_loss=2.9264126

Batch 51530, train_perplexity=18.660566, train_loss=2.9264126

Batch 51540, train_perplexity=18.660566, train_loss=2.9264126

Batch 51550, train_perplexity=18.660566, train_loss=2.9264126

Batch 51560, train_perplexity=18.660566, train_loss=2.9264126

Batch 51570, train_perplexity=18.660563, train_loss=2.9264123

Batch 51580, train_perplexity=18.660566, train_loss=2.9264126

Batch 51590, train_perplexity=18.660566, train_loss=2.9264126

Batch 51600, train_perplexity=18.660559, train_loss=2.926412

Batch 51610, train_perplexity=18.660559, train_loss=2.926412

Batch 51620, train_perplexity=18.660566, train_loss=2.9264126

Batch 51630, train_perplexity=18.660566, train_loss=2.9264126

Batch 51640, train_perplexity=18.660559, train_loss=2.926412

Batch 51650, train_perplexity=18.660566, train_loss=2.9264126

Batch 51660, train_perplexity=18.660566, train_loss=2.9264126

Batch 51670, train_perplexity=18.660559, train_loss=2.926412

Batch 51680, train_perplexity=18.660566, train_loss=2.9264126

Batch 51690, train_perplexity=18.660559, train_loss=2.926412

Batch 51700, train_perplexity=18.660559, train_loss=2.926412

Batch 51710, train_perplexity=18.660559, train_loss=2.926412

Batch 51720, train_perplexity=18.660559, train_loss=2.926412

Batch 51730, train_perplexity=18.660563, train_loss=2.9264123

Batch 51740, train_perplexity=18.660559, train_loss=2.926412

Batch 51750, train_perplexity=18.660559, train_loss=2.926412

Batch 51760, train_perplexity=18.660559, train_loss=2.926412

Batch 51770, train_perplexity=18.660559, train_loss=2.926412

Batch 51780, train_perplexity=18.660563, train_loss=2.9264123

Batch 51790, train_perplexity=18.660559, train_loss=2.926412

Batch 51800, train_perplexity=18.660559, train_loss=2.926412

Batch 51810, train_perplexity=18.660559, train_loss=2.926412

Batch 51820, train_perplexity=18.660559, train_loss=2.926412

Batch 51830, train_perplexity=18.660559, train_loss=2.926412

Batch 51840, train_perplexity=18.660559, train_loss=2.926412

Batch 51850, train_perplexity=18.660553, train_loss=2.9264119

Batch 51860, train_perplexity=18.660553, train_loss=2.9264119

Batch 51870, train_perplexity=18.660559, train_loss=2.926412

Batch 51880, train_perplexity=18.660559, train_loss=2.926412

Batch 51890, train_perplexity=18.660559, train_loss=2.926412

Batch 51900, train_perplexity=18.660553, train_loss=2.9264119

Batch 51910, train_perplexity=18.660559, train_loss=2.926412

Batch 51920, train_perplexity=18.66055, train_loss=2.9264116

Batch 51930, train_perplexity=18.660559, train_loss=2.926412

Batch 51940, train_perplexity=18.660559, train_loss=2.926412

Batch 51950, train_perplexity=18.66055, train_loss=2.9264116

Batch 51960, train_perplexity=18.66055, train_loss=2.9264116

Batch 51970, train_perplexity=18.660553, train_loss=2.9264119

Batch 51980, train_perplexity=18.66055, train_loss=2.9264116

Batch 51990, train_perplexity=18.66055, train_loss=2.9264116

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 52000, train_perplexity=18.66055, train_loss=2.9264116

Batch 52010, train_perplexity=18.66055, train_loss=2.9264116

Batch 52020, train_perplexity=18.660553, train_loss=2.9264119

Batch 52030, train_perplexity=18.66055, train_loss=2.9264116

Batch 52040, train_perplexity=18.66055, train_loss=2.9264116

Batch 52050, train_perplexity=18.66055, train_loss=2.9264116

Batch 52060, train_perplexity=18.66055, train_loss=2.9264116

Batch 52070, train_perplexity=18.66055, train_loss=2.9264116

Batch 52080, train_perplexity=18.66055, train_loss=2.9264116

Batch 52090, train_perplexity=18.66055, train_loss=2.9264116

Batch 52100, train_perplexity=18.660545, train_loss=2.9264114

Batch 52110, train_perplexity=18.660545, train_loss=2.9264114

Batch 52120, train_perplexity=18.66055, train_loss=2.9264116

Batch 52130, train_perplexity=18.660545, train_loss=2.9264114

Batch 52140, train_perplexity=18.66054, train_loss=2.9264112

Batch 52150, train_perplexity=18.66054, train_loss=2.9264112

Batch 52160, train_perplexity=18.66055, train_loss=2.9264116

Batch 52170, train_perplexity=18.66054, train_loss=2.9264112

Batch 52180, train_perplexity=18.660545, train_loss=2.9264114

Batch 52190, train_perplexity=18.66055, train_loss=2.9264116

Batch 52200, train_perplexity=18.66055, train_loss=2.9264116

Batch 52210, train_perplexity=18.66054, train_loss=2.9264112

Batch 52220, train_perplexity=18.660545, train_loss=2.9264114

Batch 52230, train_perplexity=18.66054, train_loss=2.9264112

Batch 52240, train_perplexity=18.66054, train_loss=2.9264112

Batch 52250, train_perplexity=18.66054, train_loss=2.9264112

Batch 52260, train_perplexity=18.66054, train_loss=2.9264112

Batch 52270, train_perplexity=18.660545, train_loss=2.9264114

Batch 52280, train_perplexity=18.66054, train_loss=2.9264112

Batch 52290, train_perplexity=18.66054, train_loss=2.9264112

Batch 52300, train_perplexity=18.660532, train_loss=2.9264107

Batch 52310, train_perplexity=18.66054, train_loss=2.9264112

Batch 52320, train_perplexity=18.66054, train_loss=2.9264112

Batch 52330, train_perplexity=18.660536, train_loss=2.926411

Batch 52340, train_perplexity=18.66054, train_loss=2.9264112

Batch 52350, train_perplexity=18.66054, train_loss=2.9264112

Batch 52360, train_perplexity=18.66054, train_loss=2.9264112

Batch 52370, train_perplexity=18.660536, train_loss=2.926411

Batch 52380, train_perplexity=18.66054, train_loss=2.9264112

Batch 52390, train_perplexity=18.660532, train_loss=2.9264107

Batch 52400, train_perplexity=18.660532, train_loss=2.9264107

Batch 52410, train_perplexity=18.660532, train_loss=2.9264107

Batch 52420, train_perplexity=18.660532, train_loss=2.9264107

Batch 52430, train_perplexity=18.660532, train_loss=2.9264107

Batch 52440, train_perplexity=18.660532, train_loss=2.9264107

Batch 52450, train_perplexity=18.660532, train_loss=2.9264107

Batch 52460, train_perplexity=18.660532, train_loss=2.9264107

Batch 52470, train_perplexity=18.660532, train_loss=2.9264107

Batch 52480, train_perplexity=18.660532, train_loss=2.9264107

Batch 52490, train_perplexity=18.660532, train_loss=2.9264107

Batch 52500, train_perplexity=18.660532, train_loss=2.9264107

Batch 52510, train_perplexity=18.660532, train_loss=2.9264107

Batch 52520, train_perplexity=18.660532, train_loss=2.9264107

Batch 52530, train_perplexity=18.660522, train_loss=2.9264102

Batch 52540, train_perplexity=18.660526, train_loss=2.9264104

Batch 52550, train_perplexity=18.660532, train_loss=2.9264107

Batch 52560, train_perplexity=18.660526, train_loss=2.9264104

Batch 52570, train_perplexity=18.660526, train_loss=2.9264104

Batch 52580, train_perplexity=18.660522, train_loss=2.9264102

Batch 52590, train_perplexity=18.660532, train_loss=2.9264107

Batch 52600, train_perplexity=18.660526, train_loss=2.9264104

Batch 52610, train_perplexity=18.660532, train_loss=2.9264107

Batch 52620, train_perplexity=18.660526, train_loss=2.9264104

Batch 52630, train_perplexity=18.660522, train_loss=2.9264102

Batch 52640, train_perplexity=18.660532, train_loss=2.9264107

Batch 52650, train_perplexity=18.660532, train_loss=2.9264107

Batch 52660, train_perplexity=18.660522, train_loss=2.9264102

Batch 52670, train_perplexity=18.660522, train_loss=2.9264102

Batch 52680, train_perplexity=18.660522, train_loss=2.9264102

Batch 52690, train_perplexity=18.660522, train_loss=2.9264102

Batch 52700, train_perplexity=18.660522, train_loss=2.9264102

Batch 52710, train_perplexity=18.660522, train_loss=2.9264102

Batch 52720, train_perplexity=18.660526, train_loss=2.9264104

Batch 52730, train_perplexity=18.660526, train_loss=2.9264104

Batch 52740, train_perplexity=18.660522, train_loss=2.9264102

Batch 52750, train_perplexity=18.660522, train_loss=2.9264102

Batch 52760, train_perplexity=18.660522, train_loss=2.9264102

Batch 52770, train_perplexity=18.660522, train_loss=2.9264102

Batch 52780, train_perplexity=18.660522, train_loss=2.9264102

Batch 52790, train_perplexity=18.660522, train_loss=2.9264102

Batch 52800, train_perplexity=18.660522, train_loss=2.9264102

Batch 52810, train_perplexity=18.660519, train_loss=2.92641

Batch 52820, train_perplexity=18.660522, train_loss=2.9264102

Batch 52830, train_perplexity=18.660522, train_loss=2.9264102

Batch 52840, train_perplexity=18.660522, train_loss=2.9264102

Batch 52850, train_perplexity=18.660513, train_loss=2.9264097

Batch 52860, train_perplexity=18.660522, train_loss=2.9264102

Batch 52870, train_perplexity=18.660519, train_loss=2.92641

Batch 52880, train_perplexity=18.660513, train_loss=2.9264097

Batch 52890, train_perplexity=18.660513, train_loss=2.9264097

Batch 52900, train_perplexity=18.660522, train_loss=2.9264102

Batch 52910, train_perplexity=18.660513, train_loss=2.9264097

Batch 52920, train_perplexity=18.660519, train_loss=2.92641

Batch 52930, train_perplexity=18.660519, train_loss=2.92641

Batch 52940, train_perplexity=18.660513, train_loss=2.9264097

Batch 52950, train_perplexity=18.660513, train_loss=2.9264097

Batch 52960, train_perplexity=18.660513, train_loss=2.9264097

Batch 52970, train_perplexity=18.660513, train_loss=2.9264097

Batch 52980, train_perplexity=18.660513, train_loss=2.9264097

Batch 52990, train_perplexity=18.660519, train_loss=2.92641

Batch 53000, train_perplexity=18.660513, train_loss=2.9264097

Batch 53010, train_perplexity=18.660513, train_loss=2.9264097

Batch 53020, train_perplexity=18.660513, train_loss=2.9264097

Batch 53030, train_perplexity=18.660513, train_loss=2.9264097

Batch 53040, train_perplexity=18.660513, train_loss=2.9264097

Batch 53050, train_perplexity=18.660513, train_loss=2.9264097

Batch 53060, train_perplexity=18.660513, train_loss=2.9264097

Batch 53070, train_perplexity=18.66051, train_loss=2.9264095

Batch 53080, train_perplexity=18.660513, train_loss=2.9264097

Batch 53090, train_perplexity=18.660513, train_loss=2.9264097

Batch 53100, train_perplexity=18.660513, train_loss=2.9264097

Batch 53110, train_perplexity=18.660513, train_loss=2.9264097

Batch 53120, train_perplexity=18.66051, train_loss=2.9264095

Batch 53130, train_perplexity=18.660513, train_loss=2.9264097

Batch 53140, train_perplexity=18.660513, train_loss=2.9264097

Batch 53150, train_perplexity=18.660513, train_loss=2.9264097

Batch 53160, train_perplexity=18.660513, train_loss=2.9264097

Batch 53170, train_perplexity=18.660513, train_loss=2.9264097

Batch 53180, train_perplexity=18.660505, train_loss=2.9264092

Batch 53190, train_perplexity=18.660513, train_loss=2.9264097

Batch 53200, train_perplexity=18.660513, train_loss=2.9264097

Batch 53210, train_perplexity=18.66051, train_loss=2.9264095

Batch 53220, train_perplexity=18.66051, train_loss=2.9264095

Batch 53230, train_perplexity=18.660513, train_loss=2.9264097

Batch 53240, train_perplexity=18.660513, train_loss=2.9264097

Batch 53250, train_perplexity=18.660505, train_loss=2.9264092

Batch 53260, train_perplexity=18.66051, train_loss=2.9264095

Batch 53270, train_perplexity=18.660505, train_loss=2.9264092

Batch 53280, train_perplexity=18.66051, train_loss=2.9264095

Batch 53290, train_perplexity=18.660505, train_loss=2.9264092

Batch 53300, train_perplexity=18.660505, train_loss=2.9264092
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53310, train_perplexity=18.66051, train_loss=2.9264095

Batch 53320, train_perplexity=18.660505, train_loss=2.9264092

Batch 53330, train_perplexity=18.660505, train_loss=2.9264092

Batch 53340, train_perplexity=18.660505, train_loss=2.9264092

Batch 53350, train_perplexity=18.660505, train_loss=2.9264092

Batch 53360, train_perplexity=18.660505, train_loss=2.9264092

Batch 53370, train_perplexity=18.660505, train_loss=2.9264092

Batch 53380, train_perplexity=18.660505, train_loss=2.9264092

Batch 53390, train_perplexity=18.660505, train_loss=2.9264092

Batch 53400, train_perplexity=18.660505, train_loss=2.9264092

Batch 53410, train_perplexity=18.660505, train_loss=2.9264092

Batch 53420, train_perplexity=18.660505, train_loss=2.9264092

Batch 53430, train_perplexity=18.660505, train_loss=2.9264092

Batch 53440, train_perplexity=18.660505, train_loss=2.9264092

Batch 53450, train_perplexity=18.660505, train_loss=2.9264092

Batch 53460, train_perplexity=18.660505, train_loss=2.9264092

Batch 53470, train_perplexity=18.660505, train_loss=2.9264092

Batch 53480, train_perplexity=18.660505, train_loss=2.9264092

Batch 53490, train_perplexity=18.660496, train_loss=2.9264088

Batch 53500, train_perplexity=18.660505, train_loss=2.9264092

Batch 53510, train_perplexity=18.660505, train_loss=2.9264092

Batch 53520, train_perplexity=18.660496, train_loss=2.9264088

Batch 53530, train_perplexity=18.660505, train_loss=2.9264092

Batch 53540, train_perplexity=18.660505, train_loss=2.9264092

Batch 53550, train_perplexity=18.6605, train_loss=2.926409

Batch 53560, train_perplexity=18.660505, train_loss=2.9264092

Batch 53570, train_perplexity=18.660505, train_loss=2.9264092

Batch 53580, train_perplexity=18.660505, train_loss=2.9264092

Batch 53590, train_perplexity=18.6605, train_loss=2.926409

Batch 53600, train_perplexity=18.6605, train_loss=2.926409

Batch 53610, train_perplexity=18.6605, train_loss=2.926409

Batch 53620, train_perplexity=18.660496, train_loss=2.9264088

Batch 53630, train_perplexity=18.660496, train_loss=2.9264088

Batch 53640, train_perplexity=18.660496, train_loss=2.9264088

Batch 53650, train_perplexity=18.660496, train_loss=2.9264088

Batch 53660, train_perplexity=18.6605, train_loss=2.926409

Batch 53670, train_perplexity=18.6605, train_loss=2.926409

Batch 53680, train_perplexity=18.660496, train_loss=2.9264088

Batch 53690, train_perplexity=18.660496, train_loss=2.9264088

Batch 53700, train_perplexity=18.660496, train_loss=2.9264088

Batch 53710, train_perplexity=18.660496, train_loss=2.9264088

Batch 53720, train_perplexity=18.660496, train_loss=2.9264088

Batch 53730, train_perplexity=18.660492, train_loss=2.9264085

Batch 53740, train_perplexity=18.660496, train_loss=2.9264088

Batch 53750, train_perplexity=18.660496, train_loss=2.9264088

Batch 53760, train_perplexity=18.660496, train_loss=2.9264088

Batch 53770, train_perplexity=18.660496, train_loss=2.9264088

Batch 53780, train_perplexity=18.660496, train_loss=2.9264088

Batch 53790, train_perplexity=18.660496, train_loss=2.9264088

Batch 53800, train_perplexity=18.660496, train_loss=2.9264088

Batch 53810, train_perplexity=18.660496, train_loss=2.9264088

Batch 53820, train_perplexity=18.660496, train_loss=2.9264088

Batch 53830, train_perplexity=18.660486, train_loss=2.9264083

Batch 53840, train_perplexity=18.660486, train_loss=2.9264083

Batch 53850, train_perplexity=18.660496, train_loss=2.9264088

Batch 53860, train_perplexity=18.660486, train_loss=2.9264083

Batch 53870, train_perplexity=18.660486, train_loss=2.9264083

Batch 53880, train_perplexity=18.660486, train_loss=2.9264083

Batch 53890, train_perplexity=18.660486, train_loss=2.9264083

Batch 53900, train_perplexity=18.660492, train_loss=2.9264085

Batch 53910, train_perplexity=18.660486, train_loss=2.9264083

Batch 53920, train_perplexity=18.660496, train_loss=2.9264088

Batch 53930, train_perplexity=18.660486, train_loss=2.9264083

Batch 53940, train_perplexity=18.660486, train_loss=2.9264083

Batch 53950, train_perplexity=18.660486, train_loss=2.9264083

Batch 53960, train_perplexity=18.660492, train_loss=2.9264085

Batch 53970, train_perplexity=18.660486, train_loss=2.9264083

Batch 53980, train_perplexity=18.660486, train_loss=2.9264083

Batch 53990, train_perplexity=18.660486, train_loss=2.9264083

Batch 54000, train_perplexity=18.660486, train_loss=2.9264083

Batch 54010, train_perplexity=18.660486, train_loss=2.9264083

Batch 54020, train_perplexity=18.660486, train_loss=2.9264083

Batch 54030, train_perplexity=18.660486, train_loss=2.9264083

Batch 54040, train_perplexity=18.660486, train_loss=2.9264083

Batch 54050, train_perplexity=18.660486, train_loss=2.9264083

Batch 54060, train_perplexity=18.660486, train_loss=2.9264083

Batch 54070, train_perplexity=18.660486, train_loss=2.9264083

Batch 54080, train_perplexity=18.660486, train_loss=2.9264083

Batch 54090, train_perplexity=18.660486, train_loss=2.9264083

Batch 54100, train_perplexity=18.660486, train_loss=2.9264083

Batch 54110, train_perplexity=18.660486, train_loss=2.9264083

Batch 54120, train_perplexity=18.660482, train_loss=2.926408

Batch 54130, train_perplexity=18.660486, train_loss=2.9264083

Batch 54140, train_perplexity=18.660486, train_loss=2.9264083

Batch 54150, train_perplexity=18.660486, train_loss=2.9264083

Batch 54160, train_perplexity=18.660486, train_loss=2.9264083

Batch 54170, train_perplexity=18.660482, train_loss=2.926408

Batch 54180, train_perplexity=18.660486, train_loss=2.9264083

Batch 54190, train_perplexity=18.660486, train_loss=2.9264083

Batch 54200, train_perplexity=18.660479, train_loss=2.9264078

Batch 54210, train_perplexity=18.660486, train_loss=2.9264083

Batch 54220, train_perplexity=18.660479, train_loss=2.9264078

Batch 54230, train_perplexity=18.660479, train_loss=2.9264078

Batch 54240, train_perplexity=18.660486, train_loss=2.9264083

Batch 54250, train_perplexity=18.660479, train_loss=2.9264078

Batch 54260, train_perplexity=18.660473, train_loss=2.9264076

Batch 54270, train_perplexity=18.660479, train_loss=2.9264078

Batch 54280, train_perplexity=18.660479, train_loss=2.9264078

Batch 54290, train_perplexity=18.660479, train_loss=2.9264078

Batch 54300, train_perplexity=18.660479, train_loss=2.9264078

Batch 54310, train_perplexity=18.660479, train_loss=2.9264078

Batch 54320, train_perplexity=18.660479, train_loss=2.9264078

Batch 54330, train_perplexity=18.660479, train_loss=2.9264078

Batch 54340, train_perplexity=18.660479, train_loss=2.9264078

Batch 54350, train_perplexity=18.660479, train_loss=2.9264078

Batch 54360, train_perplexity=18.660479, train_loss=2.9264078

Batch 54370, train_perplexity=18.660479, train_loss=2.9264078

Batch 54380, train_perplexity=18.660479, train_loss=2.9264078

Batch 54390, train_perplexity=18.660479, train_loss=2.9264078

Batch 54400, train_perplexity=18.660479, train_loss=2.9264078

Batch 54410, train_perplexity=18.660479, train_loss=2.9264078

Batch 54420, train_perplexity=18.660479, train_loss=2.9264078

Batch 54430, train_perplexity=18.660479, train_loss=2.9264078

Batch 54440, train_perplexity=18.660479, train_loss=2.9264078

Batch 54450, train_perplexity=18.660479, train_loss=2.9264078

Batch 54460, train_perplexity=18.660479, train_loss=2.9264078

Batch 54470, train_perplexity=18.660479, train_loss=2.9264078

Batch 54480, train_perplexity=18.660479, train_loss=2.9264078

Batch 54490, train_perplexity=18.660473, train_loss=2.9264076

Batch 54500, train_perplexity=18.66047, train_loss=2.9264073

Batch 54510, train_perplexity=18.660473, train_loss=2.9264076

Batch 54520, train_perplexity=18.660473, train_loss=2.9264076

Batch 54530, train_perplexity=18.660479, train_loss=2.9264078

Batch 54540, train_perplexity=18.660465, train_loss=2.926407

Batch 54550, train_perplexity=18.66047, train_loss=2.9264073

Batch 54560, train_perplexity=18.66047, train_loss=2.9264073

Batch 54570, train_perplexity=18.66047, train_loss=2.9264073

Batch 54580, train_perplexity=18.66047, train_loss=2.9264073

Batch 54590, train_perplexity=18.66047, train_loss=2.9264073

Batch 54600, train_perplexity=18.66047, train_loss=2.9264073

Batch 54610, train_perplexity=18.66047, train_loss=2.9264073
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 54620, train_perplexity=18.660473, train_loss=2.9264076

Batch 54630, train_perplexity=18.66047, train_loss=2.9264073

Batch 54640, train_perplexity=18.66047, train_loss=2.9264073

Batch 54650, train_perplexity=18.660465, train_loss=2.926407

Batch 54660, train_perplexity=18.66047, train_loss=2.9264073

Batch 54670, train_perplexity=18.66047, train_loss=2.9264073

Batch 54680, train_perplexity=18.66047, train_loss=2.9264073

Batch 54690, train_perplexity=18.66047, train_loss=2.9264073

Batch 54700, train_perplexity=18.660465, train_loss=2.926407

Batch 54710, train_perplexity=18.66047, train_loss=2.9264073

Batch 54720, train_perplexity=18.66046, train_loss=2.9264069

Batch 54730, train_perplexity=18.660465, train_loss=2.926407

Batch 54740, train_perplexity=18.660465, train_loss=2.926407

Batch 54750, train_perplexity=18.660465, train_loss=2.926407

Batch 54760, train_perplexity=18.660465, train_loss=2.926407

Batch 54770, train_perplexity=18.660465, train_loss=2.926407

Batch 54780, train_perplexity=18.66047, train_loss=2.9264073

Batch 54790, train_perplexity=18.660465, train_loss=2.926407

Batch 54800, train_perplexity=18.660465, train_loss=2.926407

Batch 54810, train_perplexity=18.660465, train_loss=2.926407

Batch 54820, train_perplexity=18.660465, train_loss=2.926407

Batch 54830, train_perplexity=18.66046, train_loss=2.9264069

Batch 54840, train_perplexity=18.66046, train_loss=2.9264069

Batch 54850, train_perplexity=18.660465, train_loss=2.926407

Batch 54860, train_perplexity=18.66046, train_loss=2.9264069

Batch 54870, train_perplexity=18.66046, train_loss=2.9264069

Batch 54880, train_perplexity=18.66046, train_loss=2.9264069

Batch 54890, train_perplexity=18.66047, train_loss=2.9264073

Batch 54900, train_perplexity=18.66046, train_loss=2.9264069

Batch 54910, train_perplexity=18.66046, train_loss=2.9264069

Batch 54920, train_perplexity=18.66046, train_loss=2.9264069

Batch 54930, train_perplexity=18.66046, train_loss=2.9264069

Batch 54940, train_perplexity=18.66046, train_loss=2.9264069

Batch 54950, train_perplexity=18.6603, train_loss=2.9263983

Batch 54960, train_perplexity=18.661407, train_loss=2.9264576

Batch 54970, train_perplexity=18.7157, train_loss=2.9293628

Batch 54980, train_perplexity=18.68435, train_loss=2.9276862

Batch 54990, train_perplexity=18.67406, train_loss=2.9271355

Batch 55000, train_perplexity=18.669067, train_loss=2.926868

Batch 55010, train_perplexity=18.666267, train_loss=2.926718

Batch 55020, train_perplexity=18.664555, train_loss=2.9266262

Batch 55030, train_perplexity=18.66342, train_loss=2.9265654

Batch 55040, train_perplexity=18.66264, train_loss=2.9265237

Batch 55050, train_perplexity=18.661991, train_loss=2.9264889

Batch 55060, train_perplexity=18.661573, train_loss=2.9264665

Batch 55070, train_perplexity=18.66128, train_loss=2.9264507

Batch 55080, train_perplexity=18.66106, train_loss=2.926439

Batch 55090, train_perplexity=18.6609, train_loss=2.9264305

Batch 55100, train_perplexity=18.66079, train_loss=2.9264245

Batch 55110, train_perplexity=18.660706, train_loss=2.92642

Batch 55120, train_perplexity=18.660639, train_loss=2.9264164

Batch 55130, train_perplexity=18.660593, train_loss=2.926414

Batch 55140, train_perplexity=18.660563, train_loss=2.9264123

Batch 55150, train_perplexity=18.660532, train_loss=2.9264107

Batch 55160, train_perplexity=18.660513, train_loss=2.9264097

Batch 55170, train_perplexity=18.660496, train_loss=2.9264088

Batch 55180, train_perplexity=18.660486, train_loss=2.9264083

Batch 55190, train_perplexity=18.660479, train_loss=2.9264078

Batch 55200, train_perplexity=18.66047, train_loss=2.9264073

Batch 55210, train_perplexity=18.66046, train_loss=2.9264069

Batch 55220, train_perplexity=18.66046, train_loss=2.9264069

Batch 55230, train_perplexity=18.660452, train_loss=2.9264064

Batch 55240, train_perplexity=18.660452, train_loss=2.9264064

Batch 55250, train_perplexity=18.660446, train_loss=2.9264061

Batch 55260, train_perplexity=18.660433, train_loss=2.9264054

Batch 55270, train_perplexity=18.660433, train_loss=2.9264054

Batch 55280, train_perplexity=18.660442, train_loss=2.926406

Batch 55290, train_perplexity=18.660433, train_loss=2.9264054

Batch 55300, train_perplexity=18.660433, train_loss=2.9264054

Batch 55310, train_perplexity=18.660433, train_loss=2.9264054

Batch 55320, train_perplexity=18.660433, train_loss=2.9264054

Batch 55330, train_perplexity=18.660433, train_loss=2.9264054

Batch 55340, train_perplexity=18.660433, train_loss=2.9264054

Batch 55350, train_perplexity=18.660425, train_loss=2.926405

Batch 55360, train_perplexity=18.660425, train_loss=2.926405

Batch 55370, train_perplexity=18.660425, train_loss=2.926405

Batch 55380, train_perplexity=18.660433, train_loss=2.9264054

Batch 55390, train_perplexity=18.660425, train_loss=2.926405

Batch 55400, train_perplexity=18.660429, train_loss=2.9264052

Batch 55410, train_perplexity=18.660429, train_loss=2.9264052

Batch 55420, train_perplexity=18.660425, train_loss=2.926405

Batch 55430, train_perplexity=18.660425, train_loss=2.926405

Batch 55440, train_perplexity=18.660425, train_loss=2.926405

Batch 55450, train_perplexity=18.660433, train_loss=2.9264054

Batch 55460, train_perplexity=18.660429, train_loss=2.9264052

Batch 55470, train_perplexity=18.660429, train_loss=2.9264052

Batch 55480, train_perplexity=18.660425, train_loss=2.926405

Batch 55490, train_perplexity=18.660433, train_loss=2.9264054

Batch 55500, train_perplexity=18.660425, train_loss=2.926405

Batch 55510, train_perplexity=18.66042, train_loss=2.9264047

Batch 55520, train_perplexity=18.660425, train_loss=2.926405

Batch 55530, train_perplexity=18.660425, train_loss=2.926405

Batch 55540, train_perplexity=18.660425, train_loss=2.926405

Batch 55550, train_perplexity=18.660429, train_loss=2.9264052

Batch 55560, train_perplexity=18.660429, train_loss=2.9264052

Batch 55570, train_perplexity=18.660425, train_loss=2.926405

Batch 55580, train_perplexity=18.660425, train_loss=2.926405

Batch 55590, train_perplexity=18.660425, train_loss=2.926405

Batch 55600, train_perplexity=18.660425, train_loss=2.926405

Batch 55610, train_perplexity=18.660425, train_loss=2.926405

Batch 55620, train_perplexity=18.660425, train_loss=2.926405

Batch 55630, train_perplexity=18.660425, train_loss=2.926405

Batch 55640, train_perplexity=18.660425, train_loss=2.926405

Batch 55650, train_perplexity=18.66042, train_loss=2.9264047

Batch 55660, train_perplexity=18.66042, train_loss=2.9264047

Batch 55670, train_perplexity=18.660425, train_loss=2.926405

Batch 55680, train_perplexity=18.660416, train_loss=2.9264045

Batch 55690, train_perplexity=18.66042, train_loss=2.9264047

Batch 55700, train_perplexity=18.66042, train_loss=2.9264047

Batch 55710, train_perplexity=18.66042, train_loss=2.9264047

Batch 55720, train_perplexity=18.66042, train_loss=2.9264047

Batch 55730, train_perplexity=18.660416, train_loss=2.9264045

Batch 55740, train_perplexity=18.660416, train_loss=2.9264045

Batch 55750, train_perplexity=18.66042, train_loss=2.9264047

Batch 55760, train_perplexity=18.660425, train_loss=2.926405

Batch 55770, train_perplexity=18.660416, train_loss=2.9264045

Batch 55780, train_perplexity=18.660416, train_loss=2.9264045

Batch 55790, train_perplexity=18.660416, train_loss=2.9264045

Batch 55800, train_perplexity=18.660416, train_loss=2.9264045

Batch 55810, train_perplexity=18.660416, train_loss=2.9264045

Batch 55820, train_perplexity=18.660416, train_loss=2.9264045

Batch 55830, train_perplexity=18.66042, train_loss=2.9264047

Batch 55840, train_perplexity=18.660416, train_loss=2.9264045

Batch 55850, train_perplexity=18.660416, train_loss=2.9264045

Batch 55860, train_perplexity=18.66042, train_loss=2.9264047

Batch 55870, train_perplexity=18.66042, train_loss=2.9264047

Batch 55880, train_perplexity=18.660416, train_loss=2.9264045

Batch 55890, train_perplexity=18.660416, train_loss=2.9264045

Batch 55900, train_perplexity=18.66042, train_loss=2.9264047

Batch 55910, train_perplexity=18.660416, train_loss=2.9264045

Batch 55920, train_perplexity=18.660416, train_loss=2.9264045

Batch 55930, train_perplexity=18.660412, train_loss=2.9264042
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55940, train_perplexity=18.660412, train_loss=2.9264042

Batch 55950, train_perplexity=18.660416, train_loss=2.9264045

Batch 55960, train_perplexity=18.660416, train_loss=2.9264045

Batch 55970, train_perplexity=18.660416, train_loss=2.9264045

Batch 55980, train_perplexity=18.660416, train_loss=2.9264045

Batch 55990, train_perplexity=18.660416, train_loss=2.9264045

Batch 56000, train_perplexity=18.660416, train_loss=2.9264045

Batch 56010, train_perplexity=18.660416, train_loss=2.9264045

Batch 56020, train_perplexity=18.660416, train_loss=2.9264045

Batch 56030, train_perplexity=18.660406, train_loss=2.926404

Batch 56040, train_perplexity=18.660412, train_loss=2.9264042

Batch 56050, train_perplexity=18.660416, train_loss=2.9264045

Batch 56060, train_perplexity=18.660406, train_loss=2.926404

Batch 56070, train_perplexity=18.660416, train_loss=2.9264045

Batch 56080, train_perplexity=18.660416, train_loss=2.9264045

Batch 56090, train_perplexity=18.660406, train_loss=2.926404

Batch 56100, train_perplexity=18.660406, train_loss=2.926404

Batch 56110, train_perplexity=18.660416, train_loss=2.9264045

Batch 56120, train_perplexity=18.660412, train_loss=2.9264042

Batch 56130, train_perplexity=18.660412, train_loss=2.9264042

Batch 56140, train_perplexity=18.660416, train_loss=2.9264045

Batch 56150, train_perplexity=18.660406, train_loss=2.926404

Batch 56160, train_perplexity=18.660406, train_loss=2.926404

Batch 56170, train_perplexity=18.660412, train_loss=2.9264042

Batch 56180, train_perplexity=18.660406, train_loss=2.926404

Batch 56190, train_perplexity=18.660406, train_loss=2.926404

Batch 56200, train_perplexity=18.660416, train_loss=2.9264045

Batch 56210, train_perplexity=18.660406, train_loss=2.926404

Batch 56220, train_perplexity=18.660406, train_loss=2.926404

Batch 56230, train_perplexity=18.660412, train_loss=2.9264042

Batch 56240, train_perplexity=18.660406, train_loss=2.926404

Batch 56250, train_perplexity=18.660406, train_loss=2.926404

Batch 56260, train_perplexity=18.660402, train_loss=2.9264038

Batch 56270, train_perplexity=18.660406, train_loss=2.926404

Batch 56280, train_perplexity=18.660406, train_loss=2.926404

Batch 56290, train_perplexity=18.660406, train_loss=2.926404

Batch 56300, train_perplexity=18.660406, train_loss=2.926404

Batch 56310, train_perplexity=18.660406, train_loss=2.926404

Batch 56320, train_perplexity=18.660406, train_loss=2.926404

Batch 56330, train_perplexity=18.660406, train_loss=2.926404

Batch 56340, train_perplexity=18.660406, train_loss=2.926404

Batch 56350, train_perplexity=18.660402, train_loss=2.9264038

Batch 56360, train_perplexity=18.660406, train_loss=2.926404

Batch 56370, train_perplexity=18.660406, train_loss=2.926404

Batch 56380, train_perplexity=18.660406, train_loss=2.926404

Batch 56390, train_perplexity=18.660398, train_loss=2.9264035

Batch 56400, train_perplexity=18.660406, train_loss=2.926404

Batch 56410, train_perplexity=18.660406, train_loss=2.926404

Batch 56420, train_perplexity=18.660398, train_loss=2.9264035

Batch 56430, train_perplexity=18.660402, train_loss=2.9264038

Batch 56440, train_perplexity=18.660402, train_loss=2.9264038

Batch 56450, train_perplexity=18.660398, train_loss=2.9264035

Batch 56460, train_perplexity=18.660398, train_loss=2.9264035

Batch 56470, train_perplexity=18.660398, train_loss=2.9264035

Batch 56480, train_perplexity=18.660402, train_loss=2.9264038

Batch 56490, train_perplexity=18.660398, train_loss=2.9264035

Batch 56500, train_perplexity=18.660398, train_loss=2.9264035

Batch 56510, train_perplexity=18.660406, train_loss=2.926404

Batch 56520, train_perplexity=18.660398, train_loss=2.9264035

Batch 56530, train_perplexity=18.660398, train_loss=2.9264035

Batch 56540, train_perplexity=18.660398, train_loss=2.9264035

Batch 56550, train_perplexity=18.660398, train_loss=2.9264035

Batch 56560, train_perplexity=18.660398, train_loss=2.9264035

Batch 56570, train_perplexity=18.660406, train_loss=2.926404

Batch 56580, train_perplexity=18.660398, train_loss=2.9264035

Batch 56590, train_perplexity=18.660398, train_loss=2.9264035

Batch 56600, train_perplexity=18.660398, train_loss=2.9264035

Batch 56610, train_perplexity=18.660398, train_loss=2.9264035

Batch 56620, train_perplexity=18.660393, train_loss=2.9264033

Batch 56630, train_perplexity=18.660398, train_loss=2.9264035

Batch 56640, train_perplexity=18.660398, train_loss=2.9264035

Batch 56650, train_perplexity=18.660398, train_loss=2.9264035

Batch 56660, train_perplexity=18.660398, train_loss=2.9264035

Batch 56670, train_perplexity=18.660398, train_loss=2.9264035

Batch 56680, train_perplexity=18.660393, train_loss=2.9264033

Batch 56690, train_perplexity=18.660398, train_loss=2.9264035

Batch 56700, train_perplexity=18.660389, train_loss=2.926403

Batch 56710, train_perplexity=18.660398, train_loss=2.9264035

Batch 56720, train_perplexity=18.660393, train_loss=2.9264033

Batch 56730, train_perplexity=18.660398, train_loss=2.9264035

Batch 56740, train_perplexity=18.660393, train_loss=2.9264033

Batch 56750, train_perplexity=18.660398, train_loss=2.9264035

Batch 56760, train_perplexity=18.660393, train_loss=2.9264033

Batch 56770, train_perplexity=18.660389, train_loss=2.926403

Batch 56780, train_perplexity=18.660398, train_loss=2.9264035

Batch 56790, train_perplexity=18.660389, train_loss=2.926403

Batch 56800, train_perplexity=18.660389, train_loss=2.926403

Batch 56810, train_perplexity=18.660393, train_loss=2.9264033

Batch 56820, train_perplexity=18.660389, train_loss=2.926403

Batch 56830, train_perplexity=18.660389, train_loss=2.926403

Batch 56840, train_perplexity=18.660389, train_loss=2.926403

Batch 56850, train_perplexity=18.660385, train_loss=2.9264028

Batch 56860, train_perplexity=18.660389, train_loss=2.926403

Batch 56870, train_perplexity=18.660389, train_loss=2.926403

Batch 56880, train_perplexity=18.660389, train_loss=2.926403

Batch 56890, train_perplexity=18.660389, train_loss=2.926403

Batch 56900, train_perplexity=18.660389, train_loss=2.926403

Batch 56910, train_perplexity=18.660389, train_loss=2.926403

Batch 56920, train_perplexity=18.660389, train_loss=2.926403

Batch 56930, train_perplexity=18.660389, train_loss=2.926403

Batch 56940, train_perplexity=18.660389, train_loss=2.926403

Batch 56950, train_perplexity=18.660385, train_loss=2.9264028

Batch 56960, train_perplexity=18.660389, train_loss=2.926403

Batch 56970, train_perplexity=18.660389, train_loss=2.926403

Batch 56980, train_perplexity=18.660389, train_loss=2.926403

Batch 56990, train_perplexity=18.660389, train_loss=2.926403

Batch 57000, train_perplexity=18.66038, train_loss=2.9264026

Batch 57010, train_perplexity=18.660389, train_loss=2.926403

Batch 57020, train_perplexity=18.660389, train_loss=2.926403

Batch 57030, train_perplexity=18.66038, train_loss=2.9264026

Batch 57040, train_perplexity=18.660385, train_loss=2.9264028

Batch 57050, train_perplexity=18.66038, train_loss=2.9264026

Batch 57060, train_perplexity=18.660385, train_loss=2.9264028

Batch 57070, train_perplexity=18.66038, train_loss=2.9264026

Batch 57080, train_perplexity=18.66038, train_loss=2.9264026

Batch 57090, train_perplexity=18.66038, train_loss=2.9264026

Batch 57100, train_perplexity=18.66038, train_loss=2.9264026

Batch 57110, train_perplexity=18.66038, train_loss=2.9264026

Batch 57120, train_perplexity=18.66038, train_loss=2.9264026

Batch 57130, train_perplexity=18.66038, train_loss=2.9264026

Batch 57140, train_perplexity=18.66038, train_loss=2.9264026

Batch 57150, train_perplexity=18.66038, train_loss=2.9264026

Batch 57160, train_perplexity=18.66038, train_loss=2.9264026

Batch 57170, train_perplexity=18.66038, train_loss=2.9264026

Batch 57180, train_perplexity=18.66038, train_loss=2.9264026

Batch 57190, train_perplexity=18.66038, train_loss=2.9264026

Batch 57200, train_perplexity=18.66038, train_loss=2.9264026

Batch 57210, train_perplexity=18.66038, train_loss=2.9264026

Batch 57220, train_perplexity=18.66038, train_loss=2.9264026

Batch 57230, train_perplexity=18.66038, train_loss=2.9264026

Batch 57240, train_perplexity=18.66038, train_loss=2.9264026

Batch 57250, train_perplexity=18.66038, train_loss=2.9264026
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 57260, train_perplexity=18.66038, train_loss=2.9264026

Batch 57270, train_perplexity=18.660376, train_loss=2.9264023

Batch 57280, train_perplexity=18.660376, train_loss=2.9264023

Batch 57290, train_perplexity=18.66038, train_loss=2.9264026

Batch 57300, train_perplexity=18.660372, train_loss=2.926402

Batch 57310, train_perplexity=18.66038, train_loss=2.9264026

Batch 57320, train_perplexity=18.660372, train_loss=2.926402

Batch 57330, train_perplexity=18.660372, train_loss=2.926402

Batch 57340, train_perplexity=18.660376, train_loss=2.9264023

Batch 57350, train_perplexity=18.660372, train_loss=2.926402

Batch 57360, train_perplexity=18.660372, train_loss=2.926402

Batch 57370, train_perplexity=18.660372, train_loss=2.926402

Batch 57380, train_perplexity=18.660372, train_loss=2.926402

Batch 57390, train_perplexity=18.660372, train_loss=2.926402

Batch 57400, train_perplexity=18.660372, train_loss=2.926402

Batch 57410, train_perplexity=18.660372, train_loss=2.926402

Batch 57420, train_perplexity=18.660372, train_loss=2.926402

Batch 57430, train_perplexity=18.660372, train_loss=2.926402

Batch 57440, train_perplexity=18.660372, train_loss=2.926402

Batch 57450, train_perplexity=18.660372, train_loss=2.926402

Batch 57460, train_perplexity=18.660372, train_loss=2.926402

Batch 57470, train_perplexity=18.660372, train_loss=2.926402

Batch 57480, train_perplexity=18.660372, train_loss=2.926402

Batch 57490, train_perplexity=18.660372, train_loss=2.926402

Batch 57500, train_perplexity=18.660372, train_loss=2.926402

Batch 57510, train_perplexity=18.660372, train_loss=2.926402

Batch 57520, train_perplexity=18.660366, train_loss=2.9264019

Batch 57530, train_perplexity=18.660366, train_loss=2.9264019

Batch 57540, train_perplexity=18.660362, train_loss=2.9264016

Batch 57550, train_perplexity=18.660362, train_loss=2.9264016

Batch 57560, train_perplexity=18.660362, train_loss=2.9264016

Batch 57570, train_perplexity=18.660366, train_loss=2.9264019

Batch 57580, train_perplexity=18.660362, train_loss=2.9264016

Batch 57590, train_perplexity=18.660362, train_loss=2.9264016

Batch 57600, train_perplexity=18.660366, train_loss=2.9264019

Batch 57610, train_perplexity=18.660362, train_loss=2.9264016

Batch 57620, train_perplexity=18.660362, train_loss=2.9264016

Batch 57630, train_perplexity=18.660362, train_loss=2.9264016

Batch 57640, train_perplexity=18.660362, train_loss=2.9264016

Batch 57650, train_perplexity=18.660362, train_loss=2.9264016

Batch 57660, train_perplexity=18.660362, train_loss=2.9264016

Batch 57670, train_perplexity=18.660362, train_loss=2.9264016

Batch 57680, train_perplexity=18.660362, train_loss=2.9264016

Batch 57690, train_perplexity=18.660362, train_loss=2.9264016

Batch 57700, train_perplexity=18.660353, train_loss=2.9264011

Batch 57710, train_perplexity=18.660362, train_loss=2.9264016

Batch 57720, train_perplexity=18.660358, train_loss=2.9264014

Batch 57730, train_perplexity=18.660362, train_loss=2.9264016

Batch 57740, train_perplexity=18.660362, train_loss=2.9264016

Batch 57750, train_perplexity=18.660353, train_loss=2.9264011

Batch 57760, train_perplexity=18.660358, train_loss=2.9264014

Batch 57770, train_perplexity=18.660358, train_loss=2.9264014

Batch 57780, train_perplexity=18.660362, train_loss=2.9264016

Batch 57790, train_perplexity=18.660353, train_loss=2.9264011

Batch 57800, train_perplexity=18.660362, train_loss=2.9264016

Batch 57810, train_perplexity=18.660353, train_loss=2.9264011

Batch 57820, train_perplexity=18.660353, train_loss=2.9264011

Batch 57830, train_perplexity=18.660358, train_loss=2.9264014

Batch 57840, train_perplexity=18.660353, train_loss=2.9264011

Batch 57850, train_perplexity=18.660358, train_loss=2.9264014

Batch 57860, train_perplexity=18.660349, train_loss=2.926401

Batch 57870, train_perplexity=18.660353, train_loss=2.9264011

Batch 57880, train_perplexity=18.660353, train_loss=2.9264011

Batch 57890, train_perplexity=18.660255, train_loss=2.926396

Batch 57900, train_perplexity=18.698591, train_loss=2.9284482

Batch 57910, train_perplexity=18.681293, train_loss=2.9275227

Batch 57920, train_perplexity=18.673037, train_loss=2.9270806

Batch 57930, train_perplexity=18.668488, train_loss=2.926837

Batch 57940, train_perplexity=18.665737, train_loss=2.9266896

Batch 57950, train_perplexity=18.663998, train_loss=2.9265964

Batch 57960, train_perplexity=18.662846, train_loss=2.9265347

Batch 57970, train_perplexity=18.662079, train_loss=2.9264936

Batch 57980, train_perplexity=18.661554, train_loss=2.9264655

Batch 57990, train_perplexity=18.66119, train_loss=2.926446

Batch 58000, train_perplexity=18.66095, train_loss=2.926433

Batch 58010, train_perplexity=18.660772, train_loss=2.9264235

Batch 58020, train_perplexity=18.660646, train_loss=2.9264169

Batch 58030, train_perplexity=18.660559, train_loss=2.926412

Batch 58040, train_perplexity=18.660496, train_loss=2.9264088

Batch 58050, train_perplexity=18.66046, train_loss=2.9264069

Batch 58060, train_perplexity=18.660425, train_loss=2.926405

Batch 58070, train_perplexity=18.660402, train_loss=2.9264038

Batch 58080, train_perplexity=18.660389, train_loss=2.926403

Batch 58090, train_perplexity=18.66038, train_loss=2.9264026

Batch 58100, train_perplexity=18.660366, train_loss=2.9264019

Batch 58110, train_perplexity=18.660353, train_loss=2.9264011

Batch 58120, train_perplexity=18.660353, train_loss=2.9264011

Batch 58130, train_perplexity=18.660349, train_loss=2.926401

Batch 58140, train_perplexity=18.660345, train_loss=2.9264007

Batch 58150, train_perplexity=18.660345, train_loss=2.9264007

Batch 58160, train_perplexity=18.66034, train_loss=2.9264004

Batch 58170, train_perplexity=18.66034, train_loss=2.9264004

Batch 58180, train_perplexity=18.66034, train_loss=2.9264004

Batch 58190, train_perplexity=18.660336, train_loss=2.9264002

Batch 58200, train_perplexity=18.660345, train_loss=2.9264007

Batch 58210, train_perplexity=18.660345, train_loss=2.9264007

Batch 58220, train_perplexity=18.660336, train_loss=2.9264002

Batch 58230, train_perplexity=18.660336, train_loss=2.9264002

Batch 58240, train_perplexity=18.660336, train_loss=2.9264002

Batch 58250, train_perplexity=18.660336, train_loss=2.9264002

Batch 58260, train_perplexity=18.66034, train_loss=2.9264004

Batch 58270, train_perplexity=18.66034, train_loss=2.9264004

Batch 58280, train_perplexity=18.660336, train_loss=2.9264002

Batch 58290, train_perplexity=18.660332, train_loss=2.9264

Batch 58300, train_perplexity=18.660336, train_loss=2.9264002

Batch 58310, train_perplexity=18.660336, train_loss=2.9264002

Batch 58320, train_perplexity=18.660336, train_loss=2.9264002

Batch 58330, train_perplexity=18.660336, train_loss=2.9264002

Batch 58340, train_perplexity=18.660336, train_loss=2.9264002

Batch 58350, train_perplexity=18.660336, train_loss=2.9264002

Batch 58360, train_perplexity=18.660336, train_loss=2.9264002

Batch 58370, train_perplexity=18.660336, train_loss=2.9264002

Batch 58380, train_perplexity=18.660336, train_loss=2.9264002

Batch 58390, train_perplexity=18.660336, train_loss=2.9264002

Batch 58400, train_perplexity=18.660332, train_loss=2.9264

Batch 58410, train_perplexity=18.660336, train_loss=2.9264002

Batch 58420, train_perplexity=18.660326, train_loss=2.9263997

Batch 58430, train_perplexity=18.660326, train_loss=2.9263997

Batch 58440, train_perplexity=18.660336, train_loss=2.9264002

Batch 58450, train_perplexity=18.660326, train_loss=2.9263997

Batch 58460, train_perplexity=18.660326, train_loss=2.9263997

Batch 58470, train_perplexity=18.660336, train_loss=2.9264002

Batch 58480, train_perplexity=18.660336, train_loss=2.9264002

Batch 58490, train_perplexity=18.660336, train_loss=2.9264002

Batch 58500, train_perplexity=18.660326, train_loss=2.9263997

Batch 58510, train_perplexity=18.660326, train_loss=2.9263997

Batch 58520, train_perplexity=18.660326, train_loss=2.9263997

Batch 58530, train_perplexity=18.660326, train_loss=2.9263997

Batch 58540, train_perplexity=18.660326, train_loss=2.9263997

Batch 58550, train_perplexity=18.660326, train_loss=2.9263997

Batch 58560, train_perplexity=18.660326, train_loss=2.9263997
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 58570, train_perplexity=18.660326, train_loss=2.9263997

Batch 58580, train_perplexity=18.660326, train_loss=2.9263997

Batch 58590, train_perplexity=18.660322, train_loss=2.9263995

Batch 58600, train_perplexity=18.660326, train_loss=2.9263997

Batch 58610, train_perplexity=18.660326, train_loss=2.9263997

Batch 58620, train_perplexity=18.660326, train_loss=2.9263997

Batch 58630, train_perplexity=18.660326, train_loss=2.9263997

Batch 58640, train_perplexity=18.660326, train_loss=2.9263997

Batch 58650, train_perplexity=18.660326, train_loss=2.9263997

Batch 58660, train_perplexity=18.660318, train_loss=2.9263992

Batch 58670, train_perplexity=18.660326, train_loss=2.9263997

Batch 58680, train_perplexity=18.660322, train_loss=2.9263995

Batch 58690, train_perplexity=18.660326, train_loss=2.9263997

Batch 58700, train_perplexity=18.660326, train_loss=2.9263997

Batch 58710, train_perplexity=18.660326, train_loss=2.9263997

Batch 58720, train_perplexity=18.660322, train_loss=2.9263995

Batch 58730, train_perplexity=18.660318, train_loss=2.9263992

Batch 58740, train_perplexity=18.660326, train_loss=2.9263997

Batch 58750, train_perplexity=18.660322, train_loss=2.9263995

Batch 58760, train_perplexity=18.660318, train_loss=2.9263992

Batch 58770, train_perplexity=18.660326, train_loss=2.9263997

Batch 58780, train_perplexity=18.660322, train_loss=2.9263995

Batch 58790, train_perplexity=18.660318, train_loss=2.9263992

Batch 58800, train_perplexity=18.660318, train_loss=2.9263992

Batch 58810, train_perplexity=18.660322, train_loss=2.9263995

Batch 58820, train_perplexity=18.660322, train_loss=2.9263995

Batch 58830, train_perplexity=18.660322, train_loss=2.9263995

Batch 58840, train_perplexity=18.660322, train_loss=2.9263995

Batch 58850, train_perplexity=18.660318, train_loss=2.9263992

Batch 58860, train_perplexity=18.660318, train_loss=2.9263992

Batch 58870, train_perplexity=18.660318, train_loss=2.9263992

Batch 58880, train_perplexity=18.660318, train_loss=2.9263992

Batch 58890, train_perplexity=18.660313, train_loss=2.926399

Batch 58900, train_perplexity=18.660318, train_loss=2.9263992

Batch 58910, train_perplexity=18.660309, train_loss=2.9263988

Batch 58920, train_perplexity=18.660318, train_loss=2.9263992

Batch 58930, train_perplexity=18.660318, train_loss=2.9263992

Batch 58940, train_perplexity=18.660309, train_loss=2.9263988

Batch 58950, train_perplexity=18.660313, train_loss=2.926399

Batch 58960, train_perplexity=18.660313, train_loss=2.926399

Batch 58970, train_perplexity=18.660309, train_loss=2.9263988

Batch 58980, train_perplexity=18.660318, train_loss=2.9263992

Batch 58990, train_perplexity=18.660318, train_loss=2.9263992

Batch 59000, train_perplexity=18.660309, train_loss=2.9263988

Batch 59010, train_perplexity=18.660309, train_loss=2.9263988

Batch 59020, train_perplexity=18.660309, train_loss=2.9263988

Batch 59030, train_perplexity=18.660309, train_loss=2.9263988

Batch 59040, train_perplexity=18.660309, train_loss=2.9263988

Batch 59050, train_perplexity=18.660309, train_loss=2.9263988

Batch 59060, train_perplexity=18.660309, train_loss=2.9263988

Batch 59070, train_perplexity=18.660313, train_loss=2.926399

Batch 59080, train_perplexity=18.660309, train_loss=2.9263988

Batch 59090, train_perplexity=18.660313, train_loss=2.926399

Batch 59100, train_perplexity=18.660309, train_loss=2.9263988

Batch 59110, train_perplexity=18.660309, train_loss=2.9263988

Batch 59120, train_perplexity=18.660309, train_loss=2.9263988

Batch 59130, train_perplexity=18.660309, train_loss=2.9263988

Batch 59140, train_perplexity=18.660309, train_loss=2.9263988

Batch 59150, train_perplexity=18.660305, train_loss=2.9263985

Batch 59160, train_perplexity=18.660309, train_loss=2.9263988

Batch 59170, train_perplexity=18.660309, train_loss=2.9263988

Batch 59180, train_perplexity=18.660309, train_loss=2.9263988

Batch 59190, train_perplexity=18.660309, train_loss=2.9263988

Batch 59200, train_perplexity=18.660305, train_loss=2.9263985

Batch 59210, train_perplexity=18.660305, train_loss=2.9263985

Batch 59220, train_perplexity=18.660305, train_loss=2.9263985

Batch 59230, train_perplexity=18.660305, train_loss=2.9263985

Batch 59240, train_perplexity=18.6603, train_loss=2.9263983

Batch 59250, train_perplexity=18.6603, train_loss=2.9263983

Batch 59260, train_perplexity=18.660309, train_loss=2.9263988

Batch 59270, train_perplexity=18.660309, train_loss=2.9263988

Batch 59280, train_perplexity=18.660309, train_loss=2.9263988

Batch 59290, train_perplexity=18.6603, train_loss=2.9263983

Batch 59300, train_perplexity=18.660305, train_loss=2.9263985

Batch 59310, train_perplexity=18.660305, train_loss=2.9263985

Batch 59320, train_perplexity=18.6603, train_loss=2.9263983

Batch 59330, train_perplexity=18.6603, train_loss=2.9263983

Batch 59340, train_perplexity=18.6603, train_loss=2.9263983

Batch 59350, train_perplexity=18.6603, train_loss=2.9263983

Batch 59360, train_perplexity=18.6603, train_loss=2.9263983

Batch 59370, train_perplexity=18.6603, train_loss=2.9263983

Batch 59380, train_perplexity=18.660305, train_loss=2.9263985

Batch 59390, train_perplexity=18.660295, train_loss=2.926398

Batch 59400, train_perplexity=18.660295, train_loss=2.926398

Batch 59410, train_perplexity=18.6603, train_loss=2.9263983

Batch 59420, train_perplexity=18.6603, train_loss=2.9263983

Batch 59430, train_perplexity=18.6603, train_loss=2.9263983

Batch 59440, train_perplexity=18.6603, train_loss=2.9263983

Batch 59450, train_perplexity=18.660295, train_loss=2.926398

Batch 59460, train_perplexity=18.6603, train_loss=2.9263983

Batch 59470, train_perplexity=18.6603, train_loss=2.9263983

Batch 59480, train_perplexity=18.6603, train_loss=2.9263983

Batch 59490, train_perplexity=18.660295, train_loss=2.926398

Batch 59500, train_perplexity=18.6603, train_loss=2.9263983

Batch 59510, train_perplexity=18.660292, train_loss=2.9263978

Batch 59520, train_perplexity=18.660292, train_loss=2.9263978

Batch 59530, train_perplexity=18.660292, train_loss=2.9263978

Batch 59540, train_perplexity=18.660295, train_loss=2.926398

Batch 59550, train_perplexity=18.6603, train_loss=2.9263983

Batch 59560, train_perplexity=18.6603, train_loss=2.9263983

Batch 59570, train_perplexity=18.660292, train_loss=2.9263978

Batch 59580, train_perplexity=18.660292, train_loss=2.9263978

Batch 59590, train_perplexity=18.660292, train_loss=2.9263978

Batch 59600, train_perplexity=18.660292, train_loss=2.9263978

Batch 59610, train_perplexity=18.660292, train_loss=2.9263978

Batch 59620, train_perplexity=18.660292, train_loss=2.9263978

Batch 59630, train_perplexity=18.660292, train_loss=2.9263978

Batch 59640, train_perplexity=18.660292, train_loss=2.9263978

Batch 59650, train_perplexity=18.660292, train_loss=2.9263978

Batch 59660, train_perplexity=18.660292, train_loss=2.9263978

Batch 59670, train_perplexity=18.660292, train_loss=2.9263978

Batch 59680, train_perplexity=18.660295, train_loss=2.926398

Batch 59690, train_perplexity=18.660292, train_loss=2.9263978

Batch 59700, train_perplexity=18.660292, train_loss=2.9263978

Batch 59710, train_perplexity=18.660292, train_loss=2.9263978

Batch 59720, train_perplexity=18.660292, train_loss=2.9263978

Batch 59730, train_perplexity=18.660292, train_loss=2.9263978

Batch 59740, train_perplexity=18.660286, train_loss=2.9263976

Batch 59750, train_perplexity=18.660286, train_loss=2.9263976

Batch 59760, train_perplexity=18.660286, train_loss=2.9263976

Batch 59770, train_perplexity=18.660282, train_loss=2.9263973

Batch 59780, train_perplexity=18.660286, train_loss=2.9263976

Batch 59790, train_perplexity=18.660286, train_loss=2.9263976

Batch 59800, train_perplexity=18.660286, train_loss=2.9263976

Batch 59810, train_perplexity=18.660282, train_loss=2.9263973

Batch 59820, train_perplexity=18.660282, train_loss=2.9263973

Batch 59830, train_perplexity=18.660292, train_loss=2.9263978

Batch 59840, train_perplexity=18.660292, train_loss=2.9263978

Batch 59850, train_perplexity=18.660282, train_loss=2.9263973

Batch 59860, train_perplexity=18.660282, train_loss=2.9263973

Batch 59870, train_perplexity=18.660282, train_loss=2.9263973
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 59880, train_perplexity=18.660282, train_loss=2.9263973

Batch 59890, train_perplexity=18.660278, train_loss=2.926397

Batch 59900, train_perplexity=18.660282, train_loss=2.9263973

Batch 59910, train_perplexity=18.660278, train_loss=2.926397

Batch 59920, train_perplexity=18.660282, train_loss=2.9263973

Batch 59930, train_perplexity=18.660282, train_loss=2.9263973

Batch 59940, train_perplexity=18.660282, train_loss=2.9263973

Batch 59950, train_perplexity=18.660282, train_loss=2.9263973

Batch 59960, train_perplexity=18.660282, train_loss=2.9263973

Batch 59970, train_perplexity=18.660282, train_loss=2.9263973

Batch 59980, train_perplexity=18.660282, train_loss=2.9263973

Batch 59990, train_perplexity=18.660278, train_loss=2.926397

Batch 60000, train_perplexity=18.660278, train_loss=2.926397

Batch 60010, train_perplexity=18.660273, train_loss=2.9263968

Batch 60020, train_perplexity=18.660278, train_loss=2.926397

Batch 60030, train_perplexity=18.660282, train_loss=2.9263973

Batch 60040, train_perplexity=18.660282, train_loss=2.9263973

Batch 60050, train_perplexity=18.660273, train_loss=2.9263968

Batch 60060, train_perplexity=18.660278, train_loss=2.926397

Batch 60070, train_perplexity=18.660273, train_loss=2.9263968

Batch 60080, train_perplexity=18.660273, train_loss=2.9263968

Batch 60090, train_perplexity=18.660278, train_loss=2.926397

Batch 60100, train_perplexity=18.660273, train_loss=2.9263968

Batch 60110, train_perplexity=18.660273, train_loss=2.9263968

Batch 60120, train_perplexity=18.660278, train_loss=2.926397

Batch 60130, train_perplexity=18.660273, train_loss=2.9263968

Batch 60140, train_perplexity=18.660273, train_loss=2.9263968

Batch 60150, train_perplexity=18.660273, train_loss=2.9263968

Batch 60160, train_perplexity=18.660273, train_loss=2.9263968

Batch 60170, train_perplexity=18.660273, train_loss=2.9263968

Batch 60180, train_perplexity=18.660273, train_loss=2.9263968

Batch 60190, train_perplexity=18.660273, train_loss=2.9263968

Batch 60200, train_perplexity=18.660273, train_loss=2.9263968

Batch 60210, train_perplexity=18.660273, train_loss=2.9263968

Batch 60220, train_perplexity=18.660273, train_loss=2.9263968

Batch 60230, train_perplexity=18.660273, train_loss=2.9263968

Batch 60240, train_perplexity=18.660269, train_loss=2.9263966

Batch 60250, train_perplexity=18.660273, train_loss=2.9263968

Batch 60260, train_perplexity=18.660269, train_loss=2.9263966

Batch 60270, train_perplexity=18.660265, train_loss=2.9263964

Batch 60280, train_perplexity=18.660265, train_loss=2.9263964

Batch 60290, train_perplexity=18.660273, train_loss=2.9263968

Batch 60300, train_perplexity=18.660273, train_loss=2.9263968

Batch 60310, train_perplexity=18.660265, train_loss=2.9263964

Batch 60320, train_perplexity=18.660269, train_loss=2.9263966

Batch 60330, train_perplexity=18.660269, train_loss=2.9263966

Batch 60340, train_perplexity=18.660269, train_loss=2.9263966

Batch 60350, train_perplexity=18.660265, train_loss=2.9263964

Batch 60360, train_perplexity=18.660269, train_loss=2.9263966

Batch 60370, train_perplexity=18.660265, train_loss=2.9263964

Batch 60380, train_perplexity=18.660269, train_loss=2.9263966

Batch 60390, train_perplexity=18.660269, train_loss=2.9263966

Batch 60400, train_perplexity=18.660265, train_loss=2.9263964

Batch 60410, train_perplexity=18.660265, train_loss=2.9263964

Batch 60420, train_perplexity=18.660265, train_loss=2.9263964

Batch 60430, train_perplexity=18.66026, train_loss=2.9263961

Batch 60440, train_perplexity=18.660265, train_loss=2.9263964

Batch 60450, train_perplexity=18.660265, train_loss=2.9263964

Batch 60460, train_perplexity=18.660265, train_loss=2.9263964

Batch 60470, train_perplexity=18.660265, train_loss=2.9263964

Batch 60480, train_perplexity=18.66026, train_loss=2.9263961

Batch 60490, train_perplexity=18.660265, train_loss=2.9263964

Batch 60500, train_perplexity=18.660265, train_loss=2.9263964

Batch 60510, train_perplexity=18.66026, train_loss=2.9263961

Batch 60520, train_perplexity=18.660265, train_loss=2.9263964

Batch 60530, train_perplexity=18.660265, train_loss=2.9263964

Batch 60540, train_perplexity=18.660255, train_loss=2.926396

Batch 60550, train_perplexity=18.660255, train_loss=2.926396

Batch 60560, train_perplexity=18.66026, train_loss=2.9263961

Batch 60570, train_perplexity=18.66026, train_loss=2.9263961

Batch 60580, train_perplexity=18.660255, train_loss=2.926396

Batch 60590, train_perplexity=18.660265, train_loss=2.9263964

Batch 60600, train_perplexity=18.660255, train_loss=2.926396

Batch 60610, train_perplexity=18.660255, train_loss=2.926396

Batch 60620, train_perplexity=18.660255, train_loss=2.926396

Batch 60630, train_perplexity=18.660255, train_loss=2.926396

Batch 60640, train_perplexity=18.660255, train_loss=2.926396

Batch 60650, train_perplexity=18.660255, train_loss=2.926396

Batch 60660, train_perplexity=18.660252, train_loss=2.9263957

Batch 60670, train_perplexity=18.660252, train_loss=2.9263957

Batch 60680, train_perplexity=18.660255, train_loss=2.926396

Batch 60690, train_perplexity=18.660255, train_loss=2.926396

Batch 60700, train_perplexity=18.660252, train_loss=2.9263957

Batch 60710, train_perplexity=18.660252, train_loss=2.9263957

Batch 60720, train_perplexity=18.660255, train_loss=2.926396

Batch 60730, train_perplexity=18.660255, train_loss=2.926396

Batch 60740, train_perplexity=18.660255, train_loss=2.926396

Batch 60750, train_perplexity=18.660255, train_loss=2.926396

Batch 60760, train_perplexity=18.660252, train_loss=2.9263957

Batch 60770, train_perplexity=18.660252, train_loss=2.9263957

Batch 60780, train_perplexity=18.660246, train_loss=2.9263954

Batch 60790, train_perplexity=18.660255, train_loss=2.926396

Batch 60800, train_perplexity=18.660252, train_loss=2.9263957

Batch 60810, train_perplexity=18.660252, train_loss=2.9263957

Batch 60820, train_perplexity=18.660252, train_loss=2.9263957

Batch 60830, train_perplexity=18.660252, train_loss=2.9263957

Batch 60840, train_perplexity=18.660252, train_loss=2.9263957

Batch 60850, train_perplexity=18.660252, train_loss=2.9263957

Batch 60860, train_perplexity=18.660246, train_loss=2.9263954

Batch 60870, train_perplexity=18.660252, train_loss=2.9263957

Batch 60880, train_perplexity=18.660252, train_loss=2.9263957

Batch 60890, train_perplexity=18.660246, train_loss=2.9263954

Batch 60900, train_perplexity=18.660246, train_loss=2.9263954

Batch 60910, train_perplexity=18.660246, train_loss=2.9263954

Batch 60920, train_perplexity=18.660242, train_loss=2.9263952

Batch 60930, train_perplexity=18.660246, train_loss=2.9263954

Batch 60940, train_perplexity=18.660246, train_loss=2.9263954

Batch 60950, train_perplexity=18.660242, train_loss=2.9263952

Batch 60960, train_perplexity=18.660246, train_loss=2.9263954

Batch 60970, train_perplexity=18.660242, train_loss=2.9263952

Batch 60980, train_perplexity=18.660246, train_loss=2.9263954

Batch 60990, train_perplexity=18.660242, train_loss=2.9263952

Batch 61000, train_perplexity=18.660242, train_loss=2.9263952

Batch 61010, train_perplexity=18.660238, train_loss=2.926395

Batch 61020, train_perplexity=18.660242, train_loss=2.9263952

Batch 61030, train_perplexity=18.660242, train_loss=2.9263952

Batch 61040, train_perplexity=18.660242, train_loss=2.9263952

Batch 61050, train_perplexity=18.660242, train_loss=2.9263952

Batch 61060, train_perplexity=18.660238, train_loss=2.926395

Batch 61070, train_perplexity=18.660242, train_loss=2.9263952

Batch 61080, train_perplexity=18.660238, train_loss=2.926395

Batch 61090, train_perplexity=18.660238, train_loss=2.926395

Batch 61100, train_perplexity=18.660238, train_loss=2.926395

Batch 61110, train_perplexity=18.660238, train_loss=2.926395

Batch 61120, train_perplexity=18.660238, train_loss=2.926395

Batch 61130, train_perplexity=18.660238, train_loss=2.926395

Batch 61140, train_perplexity=18.660238, train_loss=2.926395

Batch 61150, train_perplexity=18.660242, train_loss=2.9263952

Batch 61160, train_perplexity=18.660238, train_loss=2.926395

Batch 61170, train_perplexity=18.660238, train_loss=2.926395

Batch 61180, train_perplexity=18.660233, train_loss=2.9263947
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61190, train_perplexity=18.660238, train_loss=2.926395

Batch 61200, train_perplexity=18.660229, train_loss=2.9263945

Batch 61210, train_perplexity=18.660229, train_loss=2.9263945

Batch 61220, train_perplexity=18.660238, train_loss=2.926395

Batch 61230, train_perplexity=18.660229, train_loss=2.9263945

Batch 61240, train_perplexity=18.660233, train_loss=2.9263947

Batch 61250, train_perplexity=18.660238, train_loss=2.926395

Batch 61260, train_perplexity=18.660238, train_loss=2.926395

Batch 61270, train_perplexity=18.660233, train_loss=2.9263947

Batch 61280, train_perplexity=18.660229, train_loss=2.9263945

Batch 61290, train_perplexity=18.660229, train_loss=2.9263945

Batch 61300, train_perplexity=18.660238, train_loss=2.926395

Batch 61310, train_perplexity=18.660233, train_loss=2.9263947

Batch 61320, train_perplexity=18.660229, train_loss=2.9263945

Batch 61330, train_perplexity=18.660229, train_loss=2.9263945

Batch 61340, train_perplexity=18.660229, train_loss=2.9263945

Batch 61350, train_perplexity=18.660229, train_loss=2.9263945

Batch 61360, train_perplexity=18.660229, train_loss=2.9263945

Batch 61370, train_perplexity=18.660229, train_loss=2.9263945

Batch 61380, train_perplexity=18.660225, train_loss=2.9263942

Batch 61390, train_perplexity=18.660225, train_loss=2.9263942

Batch 61400, train_perplexity=18.66022, train_loss=2.926394

Batch 61410, train_perplexity=18.66022, train_loss=2.926394

Batch 61420, train_perplexity=18.66022, train_loss=2.926394

Batch 61430, train_perplexity=18.660225, train_loss=2.9263942

Batch 61440, train_perplexity=18.660229, train_loss=2.9263945

Batch 61450, train_perplexity=18.660229, train_loss=2.9263945

Batch 61460, train_perplexity=18.660225, train_loss=2.9263942

Batch 61470, train_perplexity=18.660225, train_loss=2.9263942

Batch 61480, train_perplexity=18.66022, train_loss=2.926394

Batch 61490, train_perplexity=18.66022, train_loss=2.926394

Batch 61500, train_perplexity=18.66022, train_loss=2.926394

Batch 61510, train_perplexity=18.66022, train_loss=2.926394

Batch 61520, train_perplexity=18.660225, train_loss=2.9263942

Batch 61530, train_perplexity=18.66022, train_loss=2.926394

Batch 61540, train_perplexity=18.66022, train_loss=2.926394

Batch 61550, train_perplexity=18.66022, train_loss=2.926394

Batch 61560, train_perplexity=18.66022, train_loss=2.926394

Batch 61570, train_perplexity=18.66022, train_loss=2.926394

Batch 61580, train_perplexity=18.66022, train_loss=2.926394

Batch 61590, train_perplexity=18.660215, train_loss=2.9263937

Batch 61600, train_perplexity=18.66022, train_loss=2.926394

Batch 61610, train_perplexity=18.66022, train_loss=2.926394

Batch 61620, train_perplexity=18.66022, train_loss=2.926394

Batch 61630, train_perplexity=18.66022, train_loss=2.926394

Batch 61640, train_perplexity=18.66022, train_loss=2.926394

Batch 61650, train_perplexity=18.66022, train_loss=2.926394

Batch 61660, train_perplexity=18.66022, train_loss=2.926394

Batch 61670, train_perplexity=18.660212, train_loss=2.9263935

Batch 61680, train_perplexity=18.660212, train_loss=2.9263935

Batch 61690, train_perplexity=18.660215, train_loss=2.9263937

Batch 61700, train_perplexity=18.660215, train_loss=2.9263937

Batch 61710, train_perplexity=18.660215, train_loss=2.9263937

Batch 61720, train_perplexity=18.660215, train_loss=2.9263937

Batch 61730, train_perplexity=18.660212, train_loss=2.9263935

Batch 61740, train_perplexity=18.660212, train_loss=2.9263935

Batch 61750, train_perplexity=18.660212, train_loss=2.9263935

Batch 61760, train_perplexity=18.660212, train_loss=2.9263935

Batch 61770, train_perplexity=18.660212, train_loss=2.9263935

Batch 61780, train_perplexity=18.660212, train_loss=2.9263935

Batch 61790, train_perplexity=18.660212, train_loss=2.9263935

Batch 61800, train_perplexity=18.660206, train_loss=2.9263933

Batch 61810, train_perplexity=18.660212, train_loss=2.9263935

Batch 61820, train_perplexity=18.660212, train_loss=2.9263935

Batch 61830, train_perplexity=18.660206, train_loss=2.9263933

Batch 61840, train_perplexity=18.660202, train_loss=2.926393

Batch 61850, train_perplexity=18.660212, train_loss=2.9263935

Batch 61860, train_perplexity=18.660202, train_loss=2.926393

Batch 61870, train_perplexity=18.660202, train_loss=2.926393

Batch 61880, train_perplexity=18.660202, train_loss=2.926393

Batch 61890, train_perplexity=18.660202, train_loss=2.926393

Batch 61900, train_perplexity=18.660202, train_loss=2.926393

Batch 61910, train_perplexity=18.660206, train_loss=2.9263933

Batch 61920, train_perplexity=18.660202, train_loss=2.926393

Batch 61930, train_perplexity=18.660206, train_loss=2.9263933

Batch 61940, train_perplexity=18.660202, train_loss=2.926393

Batch 61950, train_perplexity=18.660206, train_loss=2.9263933

Batch 61960, train_perplexity=18.660202, train_loss=2.926393

Batch 61970, train_perplexity=18.660206, train_loss=2.9263933

Batch 61980, train_perplexity=18.660202, train_loss=2.926393

Batch 61990, train_perplexity=18.660198, train_loss=2.9263928

Batch 62000, train_perplexity=18.660202, train_loss=2.926393

Batch 62010, train_perplexity=18.660202, train_loss=2.926393

Batch 62020, train_perplexity=18.660202, train_loss=2.926393

Batch 62030, train_perplexity=18.660202, train_loss=2.926393

Batch 62040, train_perplexity=18.660202, train_loss=2.926393

Batch 62050, train_perplexity=18.660198, train_loss=2.9263928

Batch 62060, train_perplexity=18.660198, train_loss=2.9263928

Batch 62070, train_perplexity=18.660202, train_loss=2.926393

Batch 62080, train_perplexity=18.660192, train_loss=2.9263926

Batch 62090, train_perplexity=18.660192, train_loss=2.9263926

Batch 62100, train_perplexity=18.660192, train_loss=2.9263926

Batch 62110, train_perplexity=18.660198, train_loss=2.9263928

Batch 62120, train_perplexity=18.660192, train_loss=2.9263926

Batch 62130, train_perplexity=18.660202, train_loss=2.926393

Batch 62140, train_perplexity=18.660189, train_loss=2.9263923

Batch 62150, train_perplexity=18.660198, train_loss=2.9263928

Batch 62160, train_perplexity=18.660192, train_loss=2.9263926

Batch 62170, train_perplexity=18.660189, train_loss=2.9263923

Batch 62180, train_perplexity=18.660192, train_loss=2.9263926

Batch 62190, train_perplexity=18.660192, train_loss=2.9263926

Batch 62200, train_perplexity=18.660192, train_loss=2.9263926

Batch 62210, train_perplexity=18.660192, train_loss=2.9263926

Batch 62220, train_perplexity=18.660189, train_loss=2.9263923

Batch 62230, train_perplexity=18.660189, train_loss=2.9263923

Batch 62240, train_perplexity=18.660189, train_loss=2.9263923

Batch 62250, train_perplexity=18.660189, train_loss=2.9263923

Batch 62260, train_perplexity=18.660185, train_loss=2.926392

Batch 62270, train_perplexity=18.660189, train_loss=2.9263923

Batch 62280, train_perplexity=18.660185, train_loss=2.926392

Batch 62290, train_perplexity=18.660185, train_loss=2.926392

Batch 62300, train_perplexity=18.660185, train_loss=2.926392

Batch 62310, train_perplexity=18.660189, train_loss=2.9263923

Batch 62320, train_perplexity=18.660185, train_loss=2.926392

Batch 62330, train_perplexity=18.660185, train_loss=2.926392

Batch 62340, train_perplexity=18.660185, train_loss=2.926392

Batch 62350, train_perplexity=18.660185, train_loss=2.926392

Batch 62360, train_perplexity=18.66018, train_loss=2.9263918

Batch 62370, train_perplexity=18.660185, train_loss=2.926392

Batch 62380, train_perplexity=18.660185, train_loss=2.926392

Batch 62390, train_perplexity=18.660175, train_loss=2.9263916

Batch 62400, train_perplexity=18.660185, train_loss=2.926392

Batch 62410, train_perplexity=18.66018, train_loss=2.9263918

Batch 62420, train_perplexity=18.660185, train_loss=2.926392

Batch 62430, train_perplexity=18.660175, train_loss=2.9263916

Batch 62440, train_perplexity=18.660175, train_loss=2.9263916

Batch 62450, train_perplexity=18.660175, train_loss=2.9263916

Batch 62460, train_perplexity=18.660175, train_loss=2.9263916

Batch 62470, train_perplexity=18.660175, train_loss=2.9263916

Batch 62480, train_perplexity=18.660175, train_loss=2.9263916

Batch 62490, train_perplexity=18.660175, train_loss=2.9263916

Batch 62500, train_perplexity=18.660175, train_loss=2.9263916
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 62510, train_perplexity=18.660175, train_loss=2.9263916

Batch 62520, train_perplexity=18.660175, train_loss=2.9263916

Batch 62530, train_perplexity=18.660175, train_loss=2.9263916

Batch 62540, train_perplexity=18.660175, train_loss=2.9263916

Batch 62550, train_perplexity=18.660166, train_loss=2.9263911

Batch 62560, train_perplexity=18.660175, train_loss=2.9263916

Batch 62570, train_perplexity=18.660172, train_loss=2.9263914

Batch 62580, train_perplexity=18.660175, train_loss=2.9263916

Batch 62590, train_perplexity=18.660166, train_loss=2.9263911

Batch 62600, train_perplexity=18.660166, train_loss=2.9263911

Batch 62610, train_perplexity=18.660166, train_loss=2.9263911

Batch 62620, train_perplexity=18.660166, train_loss=2.9263911

Batch 62630, train_perplexity=18.660166, train_loss=2.9263911

Batch 62640, train_perplexity=18.660172, train_loss=2.9263914

Batch 62650, train_perplexity=18.660166, train_loss=2.9263911

Batch 62660, train_perplexity=18.660166, train_loss=2.9263911

Batch 62670, train_perplexity=18.660166, train_loss=2.9263911

Batch 62680, train_perplexity=18.660166, train_loss=2.9263911

Batch 62690, train_perplexity=18.660158, train_loss=2.9263906

Batch 62700, train_perplexity=18.660166, train_loss=2.9263911

Batch 62710, train_perplexity=18.660158, train_loss=2.9263906

Batch 62720, train_perplexity=18.660166, train_loss=2.9263911

Batch 62730, train_perplexity=18.660162, train_loss=2.926391

Batch 62740, train_perplexity=18.660158, train_loss=2.9263906

Batch 62750, train_perplexity=18.660158, train_loss=2.9263906

Batch 62760, train_perplexity=18.660158, train_loss=2.9263906

Batch 62770, train_perplexity=18.660158, train_loss=2.9263906

Batch 62780, train_perplexity=18.660158, train_loss=2.9263906

Batch 62790, train_perplexity=18.660158, train_loss=2.9263906

Batch 62800, train_perplexity=18.660158, train_loss=2.9263906

Batch 62810, train_perplexity=18.660158, train_loss=2.9263906

Batch 62820, train_perplexity=18.660158, train_loss=2.9263906

Batch 62830, train_perplexity=18.660158, train_loss=2.9263906

Batch 62840, train_perplexity=18.660158, train_loss=2.9263906

Batch 62850, train_perplexity=18.660149, train_loss=2.9263902

Batch 62860, train_perplexity=18.660152, train_loss=2.9263904

Batch 62870, train_perplexity=18.660152, train_loss=2.9263904

Batch 62880, train_perplexity=18.660152, train_loss=2.9263904

Batch 62890, train_perplexity=18.660149, train_loss=2.9263902

Batch 62900, train_perplexity=18.660149, train_loss=2.9263902

Batch 62910, train_perplexity=18.660152, train_loss=2.9263904

Batch 62920, train_perplexity=18.660149, train_loss=2.9263902

Batch 62930, train_perplexity=18.660149, train_loss=2.9263902

Batch 62940, train_perplexity=18.660149, train_loss=2.9263902

Batch 62950, train_perplexity=18.660149, train_loss=2.9263902

Batch 62960, train_perplexity=18.660149, train_loss=2.9263902

Batch 62970, train_perplexity=18.660149, train_loss=2.9263902

Batch 62980, train_perplexity=18.660149, train_loss=2.9263902

Batch 62990, train_perplexity=18.660149, train_loss=2.9263902

Batch 63000, train_perplexity=18.660149, train_loss=2.9263902

Batch 63010, train_perplexity=18.660149, train_loss=2.9263902

Batch 63020, train_perplexity=18.66014, train_loss=2.9263897

Batch 63030, train_perplexity=18.660145, train_loss=2.92639

Batch 63040, train_perplexity=18.660149, train_loss=2.9263902

Batch 63050, train_perplexity=18.66014, train_loss=2.9263897

Batch 63060, train_perplexity=18.66014, train_loss=2.9263897

Batch 63070, train_perplexity=18.66014, train_loss=2.9263897

Batch 63080, train_perplexity=18.660145, train_loss=2.92639

Batch 63090, train_perplexity=18.66014, train_loss=2.9263897

Batch 63100, train_perplexity=18.66014, train_loss=2.9263897

Batch 63110, train_perplexity=18.66014, train_loss=2.9263897

Batch 63120, train_perplexity=18.66014, train_loss=2.9263897

Batch 63130, train_perplexity=18.660145, train_loss=2.92639

Batch 63140, train_perplexity=18.66014, train_loss=2.9263897

Batch 63150, train_perplexity=18.66014, train_loss=2.9263897

Batch 63160, train_perplexity=18.66014, train_loss=2.9263897

Batch 63170, train_perplexity=18.66014, train_loss=2.9263897

Batch 63180, train_perplexity=18.66014, train_loss=2.9263897

Batch 63190, train_perplexity=18.66014, train_loss=2.9263897

Batch 63200, train_perplexity=18.66014, train_loss=2.9263897

Batch 63210, train_perplexity=18.66014, train_loss=2.9263897

Batch 63220, train_perplexity=18.66014, train_loss=2.9263897

Batch 63230, train_perplexity=18.66014, train_loss=2.9263897

Batch 63240, train_perplexity=18.66014, train_loss=2.9263897

Batch 63250, train_perplexity=18.66014, train_loss=2.9263897

Batch 63260, train_perplexity=18.66014, train_loss=2.9263897

Batch 63270, train_perplexity=18.66014, train_loss=2.9263897

Batch 63280, train_perplexity=18.66014, train_loss=2.9263897

Batch 63290, train_perplexity=18.660131, train_loss=2.9263892

Batch 63300, train_perplexity=18.66014, train_loss=2.9263897

Batch 63310, train_perplexity=18.660135, train_loss=2.9263895

Batch 63320, train_perplexity=18.66014, train_loss=2.9263897

Batch 63330, train_perplexity=18.660135, train_loss=2.9263895

Batch 63340, train_perplexity=18.660135, train_loss=2.9263895

Batch 63350, train_perplexity=18.660131, train_loss=2.9263892

Batch 63360, train_perplexity=18.660135, train_loss=2.9263895

Batch 63370, train_perplexity=18.660135, train_loss=2.9263895

Batch 63380, train_perplexity=18.660131, train_loss=2.9263892

Batch 63390, train_perplexity=18.660131, train_loss=2.9263892

Batch 63400, train_perplexity=18.660135, train_loss=2.9263895

Batch 63410, train_perplexity=18.660131, train_loss=2.9263892

Batch 63420, train_perplexity=18.660135, train_loss=2.9263895

Batch 63430, train_perplexity=18.660131, train_loss=2.9263892

Batch 63440, train_perplexity=18.660131, train_loss=2.9263892

Batch 63450, train_perplexity=18.660131, train_loss=2.9263892

Batch 63460, train_perplexity=18.660131, train_loss=2.9263892

Batch 63470, train_perplexity=18.660135, train_loss=2.9263895

Batch 63480, train_perplexity=18.660131, train_loss=2.9263892

Batch 63490, train_perplexity=18.660131, train_loss=2.9263892

Batch 63500, train_perplexity=18.660131, train_loss=2.9263892

Batch 63510, train_perplexity=18.660131, train_loss=2.9263892

Batch 63520, train_perplexity=18.660126, train_loss=2.926389

Batch 63530, train_perplexity=18.660131, train_loss=2.9263892

Batch 63540, train_perplexity=18.660131, train_loss=2.9263892

Batch 63550, train_perplexity=18.660131, train_loss=2.9263892

Batch 63560, train_perplexity=18.660131, train_loss=2.9263892

Batch 63570, train_perplexity=18.660122, train_loss=2.9263887

Batch 63580, train_perplexity=18.660126, train_loss=2.926389

Batch 63590, train_perplexity=18.660126, train_loss=2.926389

Batch 63600, train_perplexity=18.660131, train_loss=2.9263892

Batch 63610, train_perplexity=18.660122, train_loss=2.9263887

Batch 63620, train_perplexity=18.660122, train_loss=2.9263887

Batch 63630, train_perplexity=18.660122, train_loss=2.9263887

Batch 63640, train_perplexity=18.660122, train_loss=2.9263887

Batch 63650, train_perplexity=18.660122, train_loss=2.9263887

Batch 63660, train_perplexity=18.660126, train_loss=2.926389

Batch 63670, train_perplexity=18.660122, train_loss=2.9263887

Batch 63680, train_perplexity=18.660131, train_loss=2.9263892

Batch 63690, train_perplexity=18.660122, train_loss=2.9263887

Batch 63700, train_perplexity=18.660122, train_loss=2.9263887

Batch 63710, train_perplexity=18.660122, train_loss=2.9263887

Batch 63720, train_perplexity=18.660122, train_loss=2.9263887

Batch 63730, train_perplexity=18.660122, train_loss=2.9263887

Batch 63740, train_perplexity=18.660122, train_loss=2.9263887

Batch 63750, train_perplexity=18.660122, train_loss=2.9263887

Batch 63760, train_perplexity=18.660118, train_loss=2.9263885

Batch 63770, train_perplexity=18.660122, train_loss=2.9263887

Batch 63780, train_perplexity=18.660118, train_loss=2.9263885

Batch 63790, train_perplexity=18.660122, train_loss=2.9263887

Batch 63800, train_perplexity=18.660122, train_loss=2.9263887

Batch 63810, train_perplexity=18.660118, train_loss=2.9263885
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 63820, train_perplexity=18.660122, train_loss=2.9263887

Batch 63830, train_perplexity=18.660118, train_loss=2.9263885

Batch 63840, train_perplexity=18.660122, train_loss=2.9263887

Batch 63850, train_perplexity=18.660122, train_loss=2.9263887

Batch 63860, train_perplexity=18.660118, train_loss=2.9263885

Batch 63870, train_perplexity=18.660112, train_loss=2.9263883

Batch 63880, train_perplexity=18.660112, train_loss=2.9263883

Batch 63890, train_perplexity=18.660112, train_loss=2.9263883

Batch 63900, train_perplexity=18.660118, train_loss=2.9263885

Batch 63910, train_perplexity=18.660112, train_loss=2.9263883

Batch 63920, train_perplexity=18.660112, train_loss=2.9263883

Batch 63930, train_perplexity=18.660112, train_loss=2.9263883

Batch 63940, train_perplexity=18.660112, train_loss=2.9263883

Batch 63950, train_perplexity=18.660118, train_loss=2.9263885

Batch 63960, train_perplexity=18.660112, train_loss=2.9263883

Batch 63970, train_perplexity=18.660112, train_loss=2.9263883

Batch 63980, train_perplexity=18.660112, train_loss=2.9263883

Batch 63990, train_perplexity=18.660112, train_loss=2.9263883

Batch 64000, train_perplexity=18.660112, train_loss=2.9263883

Batch 64010, train_perplexity=18.660112, train_loss=2.9263883

Batch 64020, train_perplexity=18.660112, train_loss=2.9263883

Batch 64030, train_perplexity=18.660112, train_loss=2.9263883

Batch 64040, train_perplexity=18.660112, train_loss=2.9263883

Batch 64050, train_perplexity=18.660112, train_loss=2.9263883

Batch 64060, train_perplexity=18.660109, train_loss=2.926388

Batch 64070, train_perplexity=18.660109, train_loss=2.926388

Batch 64080, train_perplexity=18.660112, train_loss=2.9263883

Batch 64090, train_perplexity=18.660109, train_loss=2.926388

Batch 64100, train_perplexity=18.660112, train_loss=2.9263883

Batch 64110, train_perplexity=18.660105, train_loss=2.9263878

Batch 64120, train_perplexity=18.660105, train_loss=2.9263878

Batch 64130, train_perplexity=18.660109, train_loss=2.926388

Batch 64140, train_perplexity=18.660105, train_loss=2.9263878

Batch 64150, train_perplexity=18.660112, train_loss=2.9263883

Batch 64160, train_perplexity=18.660105, train_loss=2.9263878

Batch 64170, train_perplexity=18.660105, train_loss=2.9263878

Batch 64180, train_perplexity=18.660109, train_loss=2.926388

Batch 64190, train_perplexity=18.660105, train_loss=2.9263878

Batch 64200, train_perplexity=18.660105, train_loss=2.9263878

Batch 64210, train_perplexity=18.660109, train_loss=2.926388

Batch 64220, train_perplexity=18.660105, train_loss=2.9263878

Batch 64230, train_perplexity=18.660105, train_loss=2.9263878

Batch 64240, train_perplexity=18.660105, train_loss=2.9263878

Batch 64250, train_perplexity=18.660105, train_loss=2.9263878

Batch 64260, train_perplexity=18.660099, train_loss=2.9263875

Batch 64270, train_perplexity=18.660105, train_loss=2.9263878

Batch 64280, train_perplexity=18.660099, train_loss=2.9263875

Batch 64290, train_perplexity=18.660105, train_loss=2.9263878

Batch 64300, train_perplexity=18.660105, train_loss=2.9263878

Batch 64310, train_perplexity=18.660105, train_loss=2.9263878

Batch 64320, train_perplexity=18.660099, train_loss=2.9263875

Batch 64330, train_perplexity=18.660105, train_loss=2.9263878

Batch 64340, train_perplexity=18.660105, train_loss=2.9263878

Batch 64350, train_perplexity=18.660105, train_loss=2.9263878

Batch 64360, train_perplexity=18.660105, train_loss=2.9263878

Batch 64370, train_perplexity=18.660099, train_loss=2.9263875

Batch 64380, train_perplexity=18.660105, train_loss=2.9263878

Batch 64390, train_perplexity=18.660095, train_loss=2.9263873

Batch 64400, train_perplexity=18.660095, train_loss=2.9263873

Batch 64410, train_perplexity=18.660105, train_loss=2.9263878

Batch 64420, train_perplexity=18.660099, train_loss=2.9263875

Batch 64430, train_perplexity=18.660095, train_loss=2.9263873

Batch 64440, train_perplexity=18.660095, train_loss=2.9263873

Batch 64450, train_perplexity=18.660099, train_loss=2.9263875

Batch 64460, train_perplexity=18.660105, train_loss=2.9263878

Batch 64470, train_perplexity=18.660095, train_loss=2.9263873

Batch 64480, train_perplexity=18.660095, train_loss=2.9263873

Batch 64490, train_perplexity=18.660095, train_loss=2.9263873

Batch 64500, train_perplexity=18.660095, train_loss=2.9263873

Batch 64510, train_perplexity=18.660095, train_loss=2.9263873

Batch 64520, train_perplexity=18.660095, train_loss=2.9263873

Batch 64530, train_perplexity=18.660095, train_loss=2.9263873

Batch 64540, train_perplexity=18.660095, train_loss=2.9263873

Batch 64550, train_perplexity=18.660099, train_loss=2.9263875

Batch 64560, train_perplexity=18.660095, train_loss=2.9263873

Batch 64570, train_perplexity=18.660095, train_loss=2.9263873

Batch 64580, train_perplexity=18.660095, train_loss=2.9263873

Batch 64590, train_perplexity=18.660091, train_loss=2.926387

Batch 64600, train_perplexity=18.660091, train_loss=2.926387

Batch 64610, train_perplexity=18.660091, train_loss=2.926387

Batch 64620, train_perplexity=18.660095, train_loss=2.9263873

Batch 64630, train_perplexity=18.660091, train_loss=2.926387

Batch 64640, train_perplexity=18.660095, train_loss=2.9263873

Batch 64650, train_perplexity=18.660091, train_loss=2.926387

Batch 64660, train_perplexity=18.660086, train_loss=2.9263868

Batch 64670, train_perplexity=18.660086, train_loss=2.9263868

Batch 64680, train_perplexity=18.660095, train_loss=2.9263873

Batch 64690, train_perplexity=18.660095, train_loss=2.9263873

Batch 64700, train_perplexity=18.660086, train_loss=2.9263868

Batch 64710, train_perplexity=18.660086, train_loss=2.9263868

Batch 64720, train_perplexity=18.660086, train_loss=2.9263868

Batch 64730, train_perplexity=18.660086, train_loss=2.9263868

Batch 64740, train_perplexity=18.660091, train_loss=2.926387

Batch 64750, train_perplexity=18.660086, train_loss=2.9263868

Batch 64760, train_perplexity=18.660086, train_loss=2.9263868

Batch 64770, train_perplexity=18.660086, train_loss=2.9263868

Batch 64780, train_perplexity=18.660086, train_loss=2.9263868

Batch 64790, train_perplexity=18.660086, train_loss=2.9263868

Batch 64800, train_perplexity=18.660091, train_loss=2.926387

Batch 64810, train_perplexity=18.660082, train_loss=2.9263866

Batch 64820, train_perplexity=18.660086, train_loss=2.9263868

Batch 64830, train_perplexity=18.660086, train_loss=2.9263868

Batch 64840, train_perplexity=18.660086, train_loss=2.9263868

Batch 64850, train_perplexity=18.660086, train_loss=2.9263868

Batch 64860, train_perplexity=18.660086, train_loss=2.9263868

Batch 64870, train_perplexity=18.660086, train_loss=2.9263868

Batch 64880, train_perplexity=18.660086, train_loss=2.9263868

Batch 64890, train_perplexity=18.660078, train_loss=2.9263864

Batch 64900, train_perplexity=18.660078, train_loss=2.9263864

Batch 64910, train_perplexity=18.660086, train_loss=2.9263868

Batch 64920, train_perplexity=18.660086, train_loss=2.9263868

Batch 64930, train_perplexity=18.660078, train_loss=2.9263864

Batch 64940, train_perplexity=18.660082, train_loss=2.9263866

Batch 64950, train_perplexity=18.660082, train_loss=2.9263866

Batch 64960, train_perplexity=18.660078, train_loss=2.9263864

Batch 64970, train_perplexity=18.660078, train_loss=2.9263864

Batch 64980, train_perplexity=18.660078, train_loss=2.9263864

Batch 64990, train_perplexity=18.660078, train_loss=2.9263864

Batch 65000, train_perplexity=18.660078, train_loss=2.9263864

Batch 65010, train_perplexity=18.660078, train_loss=2.9263864

Batch 65020, train_perplexity=18.660078, train_loss=2.9263864

Batch 65030, train_perplexity=18.660082, train_loss=2.9263866

Batch 65040, train_perplexity=18.660072, train_loss=2.926386

Batch 65050, train_perplexity=18.660078, train_loss=2.9263864

Batch 65060, train_perplexity=18.660078, train_loss=2.9263864

Batch 65070, train_perplexity=18.660078, train_loss=2.9263864

Batch 65080, train_perplexity=18.660078, train_loss=2.9263864

Batch 65090, train_perplexity=18.660078, train_loss=2.9263864

Batch 65100, train_perplexity=18.660069, train_loss=2.9263859

Batch 65110, train_perplexity=18.660078, train_loss=2.9263864

Batch 65120, train_perplexity=18.660078, train_loss=2.9263864
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65130, train_perplexity=18.660078, train_loss=2.9263864

Batch 65140, train_perplexity=18.660078, train_loss=2.9263864

Batch 65150, train_perplexity=18.660072, train_loss=2.926386

Batch 65160, train_perplexity=18.660078, train_loss=2.9263864

Batch 65170, train_perplexity=18.660072, train_loss=2.926386

Batch 65180, train_perplexity=18.660069, train_loss=2.9263859

Batch 65190, train_perplexity=18.660069, train_loss=2.9263859

Batch 65200, train_perplexity=18.660072, train_loss=2.926386

Batch 65210, train_perplexity=18.660078, train_loss=2.9263864

Batch 65220, train_perplexity=18.660078, train_loss=2.9263864

Batch 65230, train_perplexity=18.660078, train_loss=2.9263864

Batch 65240, train_perplexity=18.660072, train_loss=2.926386

Batch 65250, train_perplexity=18.660072, train_loss=2.926386

Batch 65260, train_perplexity=18.660072, train_loss=2.926386

Batch 65270, train_perplexity=18.660069, train_loss=2.9263859

Batch 65280, train_perplexity=18.660069, train_loss=2.9263859

Batch 65290, train_perplexity=18.660069, train_loss=2.9263859

Batch 65300, train_perplexity=18.660072, train_loss=2.926386

Batch 65310, train_perplexity=18.660069, train_loss=2.9263859

Batch 65320, train_perplexity=18.660069, train_loss=2.9263859

Batch 65330, train_perplexity=18.660065, train_loss=2.9263856

Batch 65340, train_perplexity=18.660069, train_loss=2.9263859

Batch 65350, train_perplexity=18.660065, train_loss=2.9263856

Batch 65360, train_perplexity=18.660069, train_loss=2.9263859

Batch 65370, train_perplexity=18.660069, train_loss=2.9263859

Batch 65380, train_perplexity=18.660065, train_loss=2.9263856

Batch 65390, train_perplexity=18.660069, train_loss=2.9263859

Batch 65400, train_perplexity=18.660069, train_loss=2.9263859

Batch 65410, train_perplexity=18.660065, train_loss=2.9263856

Batch 65420, train_perplexity=18.660069, train_loss=2.9263859

Batch 65430, train_perplexity=18.660059, train_loss=2.9263854

Batch 65440, train_perplexity=18.660069, train_loss=2.9263859

Batch 65450, train_perplexity=18.660065, train_loss=2.9263856

Batch 65460, train_perplexity=18.660069, train_loss=2.9263859

Batch 65470, train_perplexity=18.660065, train_loss=2.9263856

Batch 65480, train_perplexity=18.660059, train_loss=2.9263854

Batch 65490, train_perplexity=18.660065, train_loss=2.9263856

Batch 65500, train_perplexity=18.660069, train_loss=2.9263859

Batch 65510, train_perplexity=18.660065, train_loss=2.9263856

Batch 65520, train_perplexity=18.660065, train_loss=2.9263856

Batch 65530, train_perplexity=18.660059, train_loss=2.9263854

Batch 65540, train_perplexity=18.660069, train_loss=2.9263859

Batch 65550, train_perplexity=18.660059, train_loss=2.9263854

Batch 65560, train_perplexity=18.660065, train_loss=2.9263856

Batch 65570, train_perplexity=18.660065, train_loss=2.9263856

Batch 65580, train_perplexity=18.660059, train_loss=2.9263854

Batch 65590, train_perplexity=18.660059, train_loss=2.9263854

Batch 65600, train_perplexity=18.660069, train_loss=2.9263859

Batch 65610, train_perplexity=18.660059, train_loss=2.9263854

Batch 65620, train_perplexity=18.660059, train_loss=2.9263854

Batch 65630, train_perplexity=18.660059, train_loss=2.9263854

Batch 65640, train_perplexity=18.660059, train_loss=2.9263854

Batch 65650, train_perplexity=18.660059, train_loss=2.9263854

Batch 65660, train_perplexity=18.660059, train_loss=2.9263854

Batch 65670, train_perplexity=18.660059, train_loss=2.9263854

Batch 65680, train_perplexity=18.660055, train_loss=2.9263852

Batch 65690, train_perplexity=18.660059, train_loss=2.9263854

Batch 65700, train_perplexity=18.660059, train_loss=2.9263854

Batch 65710, train_perplexity=18.660055, train_loss=2.9263852

Batch 65720, train_perplexity=18.660059, train_loss=2.9263854

Batch 65730, train_perplexity=18.660051, train_loss=2.926385

Batch 65740, train_perplexity=18.660059, train_loss=2.9263854

Batch 65750, train_perplexity=18.660055, train_loss=2.9263852

Batch 65760, train_perplexity=18.660055, train_loss=2.9263852

Batch 65770, train_perplexity=18.660055, train_loss=2.9263852

Batch 65780, train_perplexity=18.660059, train_loss=2.9263854

Batch 65790, train_perplexity=18.660055, train_loss=2.9263852

Batch 65800, train_perplexity=18.660051, train_loss=2.926385

Batch 65810, train_perplexity=18.660051, train_loss=2.926385

Batch 65820, train_perplexity=18.660051, train_loss=2.926385

Batch 65830, train_perplexity=18.660051, train_loss=2.926385

Batch 65840, train_perplexity=18.660051, train_loss=2.926385

Batch 65850, train_perplexity=18.660051, train_loss=2.926385

Batch 65860, train_perplexity=18.660051, train_loss=2.926385

Batch 65870, train_perplexity=18.660051, train_loss=2.926385

Batch 65880, train_perplexity=18.660051, train_loss=2.926385

Batch 65890, train_perplexity=18.660055, train_loss=2.9263852

Batch 65900, train_perplexity=18.660051, train_loss=2.926385

Batch 65910, train_perplexity=18.660051, train_loss=2.926385

Batch 65920, train_perplexity=18.660046, train_loss=2.9263847

Batch 65930, train_perplexity=18.660051, train_loss=2.926385

Batch 65940, train_perplexity=18.660051, train_loss=2.926385

Batch 65950, train_perplexity=18.660042, train_loss=2.9263844

Batch 65960, train_perplexity=18.660046, train_loss=2.9263847

Batch 65970, train_perplexity=18.660051, train_loss=2.926385

Batch 65980, train_perplexity=18.660046, train_loss=2.9263847

Batch 65990, train_perplexity=18.660051, train_loss=2.926385

Batch 66000, train_perplexity=18.660051, train_loss=2.926385

Batch 66010, train_perplexity=18.660051, train_loss=2.926385

Batch 66020, train_perplexity=18.660051, train_loss=2.926385

Batch 66030, train_perplexity=18.660042, train_loss=2.9263844

Batch 66040, train_perplexity=18.660051, train_loss=2.926385

Batch 66050, train_perplexity=18.660051, train_loss=2.926385

Batch 66060, train_perplexity=18.660042, train_loss=2.9263844

Batch 66070, train_perplexity=18.660042, train_loss=2.9263844

Batch 66080, train_perplexity=18.660042, train_loss=2.9263844

Batch 66090, train_perplexity=18.660046, train_loss=2.9263847

Batch 66100, train_perplexity=18.660046, train_loss=2.9263847

Batch 66110, train_perplexity=18.660042, train_loss=2.9263844

Batch 66120, train_perplexity=18.660042, train_loss=2.9263844

Batch 66130, train_perplexity=18.660042, train_loss=2.9263844

Batch 66140, train_perplexity=18.660042, train_loss=2.9263844

Batch 66150, train_perplexity=18.660042, train_loss=2.9263844

Batch 66160, train_perplexity=18.660042, train_loss=2.9263844

Batch 66170, train_perplexity=18.660042, train_loss=2.9263844

Batch 66180, train_perplexity=18.660042, train_loss=2.9263844

Batch 66190, train_perplexity=18.660042, train_loss=2.9263844

Batch 66200, train_perplexity=18.660042, train_loss=2.9263844

Batch 66210, train_perplexity=18.660042, train_loss=2.9263844

Batch 66220, train_perplexity=18.660042, train_loss=2.9263844

Batch 66230, train_perplexity=18.660038, train_loss=2.9263842

Batch 66240, train_perplexity=18.660042, train_loss=2.9263844

Batch 66250, train_perplexity=18.660042, train_loss=2.9263844

Batch 66260, train_perplexity=18.660038, train_loss=2.9263842

Batch 66270, train_perplexity=18.660042, train_loss=2.9263844

Batch 66280, train_perplexity=18.660042, train_loss=2.9263844

Batch 66290, train_perplexity=18.660042, train_loss=2.9263844

Batch 66300, train_perplexity=18.660032, train_loss=2.926384

Batch 66310, train_perplexity=18.660038, train_loss=2.9263842

Batch 66320, train_perplexity=18.660032, train_loss=2.926384

Batch 66330, train_perplexity=18.660042, train_loss=2.9263844

Batch 66340, train_perplexity=18.660038, train_loss=2.9263842

Batch 66350, train_perplexity=18.660042, train_loss=2.9263844

Batch 66360, train_perplexity=18.660038, train_loss=2.9263842

Batch 66370, train_perplexity=18.660032, train_loss=2.926384

Batch 66380, train_perplexity=18.660032, train_loss=2.926384

Batch 66390, train_perplexity=18.660032, train_loss=2.926384

Batch 66400, train_perplexity=18.660032, train_loss=2.926384

Batch 66410, train_perplexity=18.660032, train_loss=2.926384

Batch 66420, train_perplexity=18.660032, train_loss=2.926384

Batch 66430, train_perplexity=18.660032, train_loss=2.926384
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66440, train_perplexity=18.660032, train_loss=2.926384

Batch 66450, train_perplexity=18.660032, train_loss=2.926384

Batch 66460, train_perplexity=18.660028, train_loss=2.9263837

Batch 66470, train_perplexity=18.660032, train_loss=2.926384

Batch 66480, train_perplexity=18.660032, train_loss=2.926384

Batch 66490, train_perplexity=18.660032, train_loss=2.926384

Batch 66500, train_perplexity=18.660032, train_loss=2.926384

Batch 66510, train_perplexity=18.660025, train_loss=2.9263835

Batch 66520, train_perplexity=18.660032, train_loss=2.926384

Batch 66530, train_perplexity=18.660025, train_loss=2.9263835

Batch 66540, train_perplexity=18.660028, train_loss=2.9263837

Batch 66550, train_perplexity=18.660032, train_loss=2.926384

Batch 66560, train_perplexity=18.660028, train_loss=2.9263837

Batch 66570, train_perplexity=18.660032, train_loss=2.926384

Batch 66580, train_perplexity=18.660028, train_loss=2.9263837

Batch 66590, train_perplexity=18.660032, train_loss=2.926384

Batch 66600, train_perplexity=18.660028, train_loss=2.9263837

Batch 66610, train_perplexity=18.660025, train_loss=2.9263835

Batch 66620, train_perplexity=18.660028, train_loss=2.9263837

Batch 66630, train_perplexity=18.660025, train_loss=2.9263835

Batch 66640, train_perplexity=18.660025, train_loss=2.9263835

Batch 66650, train_perplexity=18.660025, train_loss=2.9263835

Batch 66660, train_perplexity=18.660032, train_loss=2.926384

Batch 66670, train_perplexity=18.660025, train_loss=2.9263835

Batch 66680, train_perplexity=18.660025, train_loss=2.9263835

Batch 66690, train_perplexity=18.660025, train_loss=2.9263835

Batch 66700, train_perplexity=18.660025, train_loss=2.9263835

Batch 66710, train_perplexity=18.660025, train_loss=2.9263835

Batch 66720, train_perplexity=18.660025, train_loss=2.9263835

Batch 66730, train_perplexity=18.660025, train_loss=2.9263835

Batch 66740, train_perplexity=18.660025, train_loss=2.9263835

Batch 66750, train_perplexity=18.660025, train_loss=2.9263835

Batch 66760, train_perplexity=18.660025, train_loss=2.9263835

Batch 66770, train_perplexity=18.660028, train_loss=2.9263837

Batch 66780, train_perplexity=18.660025, train_loss=2.9263835

Batch 66790, train_perplexity=18.660015, train_loss=2.926383

Batch 66800, train_perplexity=18.660028, train_loss=2.9263837

Batch 66810, train_perplexity=18.660019, train_loss=2.9263833

Batch 66820, train_perplexity=18.660025, train_loss=2.9263835

Batch 66830, train_perplexity=18.660019, train_loss=2.9263833

Batch 66840, train_perplexity=18.660015, train_loss=2.926383

Batch 66850, train_perplexity=18.660025, train_loss=2.9263835

Batch 66860, train_perplexity=18.660019, train_loss=2.9263833

Batch 66870, train_perplexity=18.660019, train_loss=2.9263833

Batch 66880, train_perplexity=18.660015, train_loss=2.926383

Batch 66890, train_perplexity=18.660015, train_loss=2.926383

Batch 66900, train_perplexity=18.660025, train_loss=2.9263835

Batch 66910, train_perplexity=18.660015, train_loss=2.926383

Batch 66920, train_perplexity=18.660019, train_loss=2.9263833

Batch 66930, train_perplexity=18.660019, train_loss=2.9263833

Batch 66940, train_perplexity=18.660015, train_loss=2.926383

Batch 66950, train_perplexity=18.660019, train_loss=2.9263833

Batch 66960, train_perplexity=18.660015, train_loss=2.926383

Batch 66970, train_perplexity=18.660019, train_loss=2.9263833

Batch 66980, train_perplexity=18.660015, train_loss=2.926383

Batch 66990, train_perplexity=18.660015, train_loss=2.926383

Batch 67000, train_perplexity=18.660015, train_loss=2.926383

Batch 67010, train_perplexity=18.660015, train_loss=2.926383

Batch 67020, train_perplexity=18.660011, train_loss=2.9263828

Batch 67030, train_perplexity=18.660015, train_loss=2.926383

Batch 67040, train_perplexity=18.660015, train_loss=2.926383

Batch 67050, train_perplexity=18.660015, train_loss=2.926383

Batch 67060, train_perplexity=18.660015, train_loss=2.926383

Batch 67070, train_perplexity=18.660011, train_loss=2.9263828

Batch 67080, train_perplexity=18.660015, train_loss=2.926383

Batch 67090, train_perplexity=18.660015, train_loss=2.926383

Batch 67100, train_perplexity=18.660011, train_loss=2.9263828

Batch 67110, train_perplexity=18.660011, train_loss=2.9263828

Batch 67120, train_perplexity=18.660006, train_loss=2.9263825

Batch 67130, train_perplexity=18.660006, train_loss=2.9263825

Batch 67140, train_perplexity=18.660011, train_loss=2.9263828

Batch 67150, train_perplexity=18.660006, train_loss=2.9263825

Batch 67160, train_perplexity=18.660006, train_loss=2.9263825

Batch 67170, train_perplexity=18.660015, train_loss=2.926383

Batch 67180, train_perplexity=18.660015, train_loss=2.926383

Batch 67190, train_perplexity=18.660011, train_loss=2.9263828

Batch 67200, train_perplexity=18.660011, train_loss=2.9263828

Batch 67210, train_perplexity=18.660011, train_loss=2.9263828

Batch 67220, train_perplexity=18.660006, train_loss=2.9263825

Batch 67230, train_perplexity=18.660006, train_loss=2.9263825

Batch 67240, train_perplexity=18.660006, train_loss=2.9263825

Batch 67250, train_perplexity=18.660006, train_loss=2.9263825

Batch 67260, train_perplexity=18.660006, train_loss=2.9263825

Batch 67270, train_perplexity=18.660006, train_loss=2.9263825

Batch 67280, train_perplexity=18.660006, train_loss=2.9263825

Batch 67290, train_perplexity=18.660006, train_loss=2.9263825

Batch 67300, train_perplexity=18.660002, train_loss=2.9263823

Batch 67310, train_perplexity=18.660006, train_loss=2.9263825

Batch 67320, train_perplexity=18.660006, train_loss=2.9263825

Batch 67330, train_perplexity=18.660006, train_loss=2.9263825

Batch 67340, train_perplexity=18.660006, train_loss=2.9263825

Batch 67350, train_perplexity=18.660006, train_loss=2.9263825

Batch 67360, train_perplexity=18.660002, train_loss=2.9263823

Batch 67370, train_perplexity=18.660006, train_loss=2.9263825

Batch 67380, train_perplexity=18.660006, train_loss=2.9263825

Batch 67390, train_perplexity=18.660006, train_loss=2.9263825

Batch 67400, train_perplexity=18.660006, train_loss=2.9263825

Batch 67410, train_perplexity=18.660006, train_loss=2.9263825

Batch 67420, train_perplexity=18.660006, train_loss=2.9263825

Batch 67430, train_perplexity=18.660002, train_loss=2.9263823

Batch 67440, train_perplexity=18.660006, train_loss=2.9263825

Batch 67450, train_perplexity=18.660006, train_loss=2.9263825

Batch 67460, train_perplexity=18.660002, train_loss=2.9263823

Batch 67470, train_perplexity=18.659998, train_loss=2.926382

Batch 67480, train_perplexity=18.660006, train_loss=2.9263825

Batch 67490, train_perplexity=18.659998, train_loss=2.926382

Batch 67500, train_perplexity=18.659998, train_loss=2.926382

Batch 67510, train_perplexity=18.659998, train_loss=2.926382

Batch 67520, train_perplexity=18.660006, train_loss=2.9263825

Batch 67530, train_perplexity=18.659998, train_loss=2.926382

Batch 67540, train_perplexity=18.659998, train_loss=2.926382

Batch 67550, train_perplexity=18.659998, train_loss=2.926382

Batch 67560, train_perplexity=18.659998, train_loss=2.926382

Batch 67570, train_perplexity=18.660002, train_loss=2.9263823

Batch 67580, train_perplexity=18.659998, train_loss=2.926382

Batch 67590, train_perplexity=18.659998, train_loss=2.926382

Batch 67600, train_perplexity=18.659998, train_loss=2.926382

Batch 67610, train_perplexity=18.659998, train_loss=2.926382

Batch 67620, train_perplexity=18.659998, train_loss=2.926382

Batch 67630, train_perplexity=18.659998, train_loss=2.926382

Batch 67640, train_perplexity=18.659998, train_loss=2.926382

Batch 67650, train_perplexity=18.659992, train_loss=2.9263818

Batch 67660, train_perplexity=18.659998, train_loss=2.926382

Batch 67670, train_perplexity=18.659998, train_loss=2.926382

Batch 67680, train_perplexity=18.659998, train_loss=2.926382

Batch 67690, train_perplexity=18.659998, train_loss=2.926382

Batch 67700, train_perplexity=18.659992, train_loss=2.9263818

Batch 67710, train_perplexity=18.659988, train_loss=2.9263816

Batch 67720, train_perplexity=18.659998, train_loss=2.926382

Batch 67730, train_perplexity=18.659992, train_loss=2.9263818

Batch 67740, train_perplexity=18.659998, train_loss=2.926382
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 67750, train_perplexity=18.659988, train_loss=2.9263816

Batch 67760, train_perplexity=18.659998, train_loss=2.926382

Batch 67770, train_perplexity=18.659992, train_loss=2.9263818

Batch 67780, train_perplexity=18.659992, train_loss=2.9263818

Batch 67790, train_perplexity=18.659988, train_loss=2.9263816

Batch 67800, train_perplexity=18.659992, train_loss=2.9263818

Batch 67810, train_perplexity=18.659992, train_loss=2.9263818

Batch 67820, train_perplexity=18.659988, train_loss=2.9263816

Batch 67830, train_perplexity=18.659988, train_loss=2.9263816

Batch 67840, train_perplexity=18.659988, train_loss=2.9263816

Batch 67850, train_perplexity=18.659988, train_loss=2.9263816

Batch 67860, train_perplexity=18.659988, train_loss=2.9263816

Batch 67870, train_perplexity=18.659988, train_loss=2.9263816

Batch 67880, train_perplexity=18.659988, train_loss=2.9263816

Batch 67890, train_perplexity=18.659988, train_loss=2.9263816

Batch 67900, train_perplexity=18.659988, train_loss=2.9263816

Batch 67910, train_perplexity=18.659988, train_loss=2.9263816

Batch 67920, train_perplexity=18.659985, train_loss=2.9263813

Batch 67930, train_perplexity=18.659988, train_loss=2.9263816

Batch 67940, train_perplexity=18.659988, train_loss=2.9263816

Batch 67950, train_perplexity=18.659988, train_loss=2.9263816

Batch 67960, train_perplexity=18.659988, train_loss=2.9263816

Batch 67970, train_perplexity=18.659988, train_loss=2.9263816

Batch 67980, train_perplexity=18.659985, train_loss=2.9263813

Batch 67990, train_perplexity=18.659985, train_loss=2.9263813

Batch 68000, train_perplexity=18.659988, train_loss=2.9263816

Batch 68010, train_perplexity=18.659979, train_loss=2.926381

Batch 68020, train_perplexity=18.659979, train_loss=2.926381

Batch 68030, train_perplexity=18.659985, train_loss=2.9263813

Batch 68040, train_perplexity=18.659985, train_loss=2.9263813

Batch 68050, train_perplexity=18.659988, train_loss=2.9263816

Batch 68060, train_perplexity=18.659985, train_loss=2.9263813

Batch 68070, train_perplexity=18.659979, train_loss=2.926381

Batch 68080, train_perplexity=18.659979, train_loss=2.926381

Batch 68090, train_perplexity=18.659985, train_loss=2.9263813

Batch 68100, train_perplexity=18.659979, train_loss=2.926381

Batch 68110, train_perplexity=18.659979, train_loss=2.926381

Batch 68120, train_perplexity=18.659979, train_loss=2.926381

Batch 68130, train_perplexity=18.659979, train_loss=2.926381

Batch 68140, train_perplexity=18.659985, train_loss=2.9263813

Batch 68150, train_perplexity=18.659975, train_loss=2.9263809

Batch 68160, train_perplexity=18.659979, train_loss=2.926381

Batch 68170, train_perplexity=18.659979, train_loss=2.926381

Batch 68180, train_perplexity=18.659979, train_loss=2.926381

Batch 68190, train_perplexity=18.659979, train_loss=2.926381

Batch 68200, train_perplexity=18.659979, train_loss=2.926381

Batch 68210, train_perplexity=18.659979, train_loss=2.926381

Batch 68220, train_perplexity=18.659979, train_loss=2.926381

Batch 68230, train_perplexity=18.659979, train_loss=2.926381

Batch 68240, train_perplexity=18.659979, train_loss=2.926381

Batch 68250, train_perplexity=18.659979, train_loss=2.926381

Batch 68260, train_perplexity=18.659979, train_loss=2.926381

Batch 68270, train_perplexity=18.659971, train_loss=2.9263806

Batch 68280, train_perplexity=18.659975, train_loss=2.9263809

Batch 68290, train_perplexity=18.659979, train_loss=2.926381

Batch 68300, train_perplexity=18.659975, train_loss=2.9263809

Batch 68310, train_perplexity=18.659975, train_loss=2.9263809

Batch 68320, train_perplexity=18.659971, train_loss=2.9263806

Batch 68330, train_perplexity=18.659971, train_loss=2.9263806

Batch 68340, train_perplexity=18.659975, train_loss=2.9263809

Batch 68350, train_perplexity=18.659979, train_loss=2.926381

Batch 68360, train_perplexity=18.659971, train_loss=2.9263806

Batch 68370, train_perplexity=18.659971, train_loss=2.9263806

Batch 68380, train_perplexity=18.659971, train_loss=2.9263806

Batch 68390, train_perplexity=18.659971, train_loss=2.9263806

Batch 68400, train_perplexity=18.659971, train_loss=2.9263806

Batch 68410, train_perplexity=18.659971, train_loss=2.9263806

Batch 68420, train_perplexity=18.659971, train_loss=2.9263806

Batch 68430, train_perplexity=18.659975, train_loss=2.9263809

Batch 68440, train_perplexity=18.659979, train_loss=2.926381

Batch 68450, train_perplexity=18.659971, train_loss=2.9263806

Batch 68460, train_perplexity=18.659971, train_loss=2.9263806

Batch 68470, train_perplexity=18.659971, train_loss=2.9263806

Batch 68480, train_perplexity=18.659971, train_loss=2.9263806

Batch 68490, train_perplexity=18.659971, train_loss=2.9263806

Batch 68500, train_perplexity=18.659966, train_loss=2.9263804

Batch 68510, train_perplexity=18.659971, train_loss=2.9263806

Batch 68520, train_perplexity=18.659971, train_loss=2.9263806

Batch 68530, train_perplexity=18.659971, train_loss=2.9263806

Batch 68540, train_perplexity=18.659966, train_loss=2.9263804

Batch 68550, train_perplexity=18.659971, train_loss=2.9263806

Batch 68560, train_perplexity=18.659971, train_loss=2.9263806

Batch 68570, train_perplexity=18.659966, train_loss=2.9263804

Batch 68580, train_perplexity=18.659962, train_loss=2.9263802

Batch 68590, train_perplexity=18.659971, train_loss=2.9263806

Batch 68600, train_perplexity=18.659966, train_loss=2.9263804

Batch 68610, train_perplexity=18.659966, train_loss=2.9263804

Batch 68620, train_perplexity=18.659971, train_loss=2.9263806

Batch 68630, train_perplexity=18.659971, train_loss=2.9263806

Batch 68640, train_perplexity=18.659962, train_loss=2.9263802

Batch 68650, train_perplexity=18.659962, train_loss=2.9263802

Batch 68660, train_perplexity=18.659962, train_loss=2.9263802

Batch 68670, train_perplexity=18.659966, train_loss=2.9263804

Batch 68680, train_perplexity=18.659962, train_loss=2.9263802

Batch 68690, train_perplexity=18.659962, train_loss=2.9263802

Batch 68700, train_perplexity=18.659962, train_loss=2.9263802

Batch 68710, train_perplexity=18.659962, train_loss=2.9263802

Batch 68720, train_perplexity=18.659962, train_loss=2.9263802

Batch 68730, train_perplexity=18.659962, train_loss=2.9263802

Batch 68740, train_perplexity=18.659962, train_loss=2.9263802

Batch 68750, train_perplexity=18.659962, train_loss=2.9263802

Batch 68760, train_perplexity=18.659962, train_loss=2.9263802

Batch 68770, train_perplexity=18.659962, train_loss=2.9263802

Batch 68780, train_perplexity=18.659962, train_loss=2.9263802

Batch 68790, train_perplexity=18.659962, train_loss=2.9263802

Batch 68800, train_perplexity=18.659962, train_loss=2.9263802

Batch 68810, train_perplexity=18.659962, train_loss=2.9263802

Batch 68820, train_perplexity=18.659962, train_loss=2.9263802

Batch 68830, train_perplexity=18.659962, train_loss=2.9263802

Batch 68840, train_perplexity=18.659962, train_loss=2.9263802

Batch 68850, train_perplexity=18.659962, train_loss=2.9263802

Batch 68860, train_perplexity=18.659962, train_loss=2.9263802

Batch 68870, train_perplexity=18.659962, train_loss=2.9263802

Batch 68880, train_perplexity=18.659952, train_loss=2.9263797

Batch 68890, train_perplexity=18.659958, train_loss=2.92638

Batch 68900, train_perplexity=18.659958, train_loss=2.92638

Batch 68910, train_perplexity=18.659958, train_loss=2.92638

Batch 68920, train_perplexity=18.659962, train_loss=2.9263802

Batch 68930, train_perplexity=18.659952, train_loss=2.9263797

Batch 68940, train_perplexity=18.659958, train_loss=2.92638

Batch 68950, train_perplexity=18.659952, train_loss=2.9263797

Batch 68960, train_perplexity=18.659952, train_loss=2.9263797

Batch 68970, train_perplexity=18.659952, train_loss=2.9263797

Batch 68980, train_perplexity=18.659952, train_loss=2.9263797

Batch 68990, train_perplexity=18.659952, train_loss=2.9263797

Batch 69000, train_perplexity=18.659952, train_loss=2.9263797

Batch 69010, train_perplexity=18.659952, train_loss=2.9263797

Batch 69020, train_perplexity=18.659952, train_loss=2.9263797

Batch 69030, train_perplexity=18.659952, train_loss=2.9263797

Batch 69040, train_perplexity=18.659952, train_loss=2.9263797

Batch 69050, train_perplexity=18.659952, train_loss=2.9263797
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69060, train_perplexity=18.659952, train_loss=2.9263797

Batch 69070, train_perplexity=18.659952, train_loss=2.9263797

Batch 69080, train_perplexity=18.659952, train_loss=2.9263797

Batch 69090, train_perplexity=18.659952, train_loss=2.9263797

Batch 69100, train_perplexity=18.659952, train_loss=2.9263797

Batch 69110, train_perplexity=18.659952, train_loss=2.9263797

Batch 69120, train_perplexity=18.659948, train_loss=2.9263794

Batch 69130, train_perplexity=18.659948, train_loss=2.9263794

Batch 69140, train_perplexity=18.659952, train_loss=2.9263797

Batch 69150, train_perplexity=18.659945, train_loss=2.9263792

Batch 69160, train_perplexity=18.659945, train_loss=2.9263792

Batch 69170, train_perplexity=18.659945, train_loss=2.9263792

Batch 69180, train_perplexity=18.659945, train_loss=2.9263792

Batch 69190, train_perplexity=18.659948, train_loss=2.9263794

Batch 69200, train_perplexity=18.659945, train_loss=2.9263792

Batch 69210, train_perplexity=18.659948, train_loss=2.9263794

Batch 69220, train_perplexity=18.659945, train_loss=2.9263792

Batch 69230, train_perplexity=18.659945, train_loss=2.9263792

Batch 69240, train_perplexity=18.659952, train_loss=2.9263797

Batch 69250, train_perplexity=18.659945, train_loss=2.9263792

Batch 69260, train_perplexity=18.659945, train_loss=2.9263792

Batch 69270, train_perplexity=18.659945, train_loss=2.9263792

Batch 69280, train_perplexity=18.659945, train_loss=2.9263792

Batch 69290, train_perplexity=18.659945, train_loss=2.9263792

Batch 69300, train_perplexity=18.659945, train_loss=2.9263792

Batch 69310, train_perplexity=18.659945, train_loss=2.9263792

Batch 69320, train_perplexity=18.659945, train_loss=2.9263792

Batch 69330, train_perplexity=18.659945, train_loss=2.9263792

Batch 69340, train_perplexity=18.659945, train_loss=2.9263792

Batch 69350, train_perplexity=18.659945, train_loss=2.9263792

Batch 69360, train_perplexity=18.659945, train_loss=2.9263792

Batch 69370, train_perplexity=18.659939, train_loss=2.926379

Batch 69380, train_perplexity=18.659945, train_loss=2.9263792

Batch 69390, train_perplexity=18.659939, train_loss=2.926379

Batch 69400, train_perplexity=18.659939, train_loss=2.926379

Batch 69410, train_perplexity=18.659945, train_loss=2.9263792

Batch 69420, train_perplexity=18.659945, train_loss=2.9263792

Batch 69430, train_perplexity=18.659939, train_loss=2.926379

Batch 69440, train_perplexity=18.659935, train_loss=2.9263787

Batch 69450, train_perplexity=18.659939, train_loss=2.926379

Batch 69460, train_perplexity=18.659935, train_loss=2.9263787

Batch 69470, train_perplexity=18.659945, train_loss=2.9263792

Batch 69480, train_perplexity=18.659935, train_loss=2.9263787

Batch 69490, train_perplexity=18.659939, train_loss=2.926379

Batch 69500, train_perplexity=18.659939, train_loss=2.926379

Batch 69510, train_perplexity=18.659935, train_loss=2.9263787

Batch 69520, train_perplexity=18.659935, train_loss=2.9263787

Batch 69530, train_perplexity=18.659935, train_loss=2.9263787

Batch 69540, train_perplexity=18.659935, train_loss=2.9263787

Batch 69550, train_perplexity=18.659935, train_loss=2.9263787

Batch 69560, train_perplexity=18.659935, train_loss=2.9263787

Batch 69570, train_perplexity=18.659931, train_loss=2.9263785

Batch 69580, train_perplexity=18.659935, train_loss=2.9263787

Batch 69590, train_perplexity=18.659935, train_loss=2.9263787

Batch 69600, train_perplexity=18.659935, train_loss=2.9263787

Batch 69610, train_perplexity=18.659931, train_loss=2.9263785

Batch 69620, train_perplexity=18.659931, train_loss=2.9263785

Batch 69630, train_perplexity=18.659931, train_loss=2.9263785

Batch 69640, train_perplexity=18.659931, train_loss=2.9263785

Batch 69650, train_perplexity=18.659931, train_loss=2.9263785

Batch 69660, train_perplexity=18.659931, train_loss=2.9263785

Batch 69670, train_perplexity=18.659931, train_loss=2.9263785

Batch 69680, train_perplexity=18.659931, train_loss=2.9263785

Batch 69690, train_perplexity=18.659931, train_loss=2.9263785

Batch 69700, train_perplexity=18.659931, train_loss=2.9263785

Batch 69710, train_perplexity=18.659935, train_loss=2.9263787

Batch 69720, train_perplexity=18.659925, train_loss=2.9263783

Batch 69730, train_perplexity=18.659931, train_loss=2.9263785

Batch 69740, train_perplexity=18.659925, train_loss=2.9263783

Batch 69750, train_perplexity=18.659925, train_loss=2.9263783

Batch 69760, train_perplexity=18.659925, train_loss=2.9263783

Batch 69770, train_perplexity=18.659925, train_loss=2.9263783

Batch 69780, train_perplexity=18.659925, train_loss=2.9263783

Batch 69790, train_perplexity=18.659925, train_loss=2.9263783

Batch 69800, train_perplexity=18.659922, train_loss=2.926378

Batch 69810, train_perplexity=18.659922, train_loss=2.926378

Batch 69820, train_perplexity=18.659925, train_loss=2.9263783

Batch 69830, train_perplexity=18.659925, train_loss=2.9263783

Batch 69840, train_perplexity=18.659925, train_loss=2.9263783

Batch 69850, train_perplexity=18.659925, train_loss=2.9263783

Batch 69860, train_perplexity=18.659925, train_loss=2.9263783

Batch 69870, train_perplexity=18.659925, train_loss=2.9263783

Batch 69880, train_perplexity=18.659925, train_loss=2.9263783

Batch 69890, train_perplexity=18.659918, train_loss=2.9263778

Batch 69900, train_perplexity=18.659918, train_loss=2.9263778

Batch 69910, train_perplexity=18.659918, train_loss=2.9263778

Batch 69920, train_perplexity=18.659925, train_loss=2.9263783

Batch 69930, train_perplexity=18.659922, train_loss=2.926378

Batch 69940, train_perplexity=18.659918, train_loss=2.9263778

Batch 69950, train_perplexity=18.659918, train_loss=2.9263778

Batch 69960, train_perplexity=18.659922, train_loss=2.926378

Batch 69970, train_perplexity=18.659918, train_loss=2.9263778

Batch 69980, train_perplexity=18.659918, train_loss=2.9263778

Batch 69990, train_perplexity=18.659918, train_loss=2.9263778

Batch 70000, train_perplexity=18.659918, train_loss=2.9263778

Batch 70010, train_perplexity=18.659918, train_loss=2.9263778

Batch 70020, train_perplexity=18.659918, train_loss=2.9263778

Batch 70030, train_perplexity=18.659918, train_loss=2.9263778

Batch 70040, train_perplexity=18.659918, train_loss=2.9263778

Batch 70050, train_perplexity=18.659918, train_loss=2.9263778

Batch 70060, train_perplexity=18.659912, train_loss=2.9263775

Batch 70070, train_perplexity=18.659918, train_loss=2.9263778

Batch 70080, train_perplexity=18.659918, train_loss=2.9263778

Batch 70090, train_perplexity=18.659918, train_loss=2.9263778

Batch 70100, train_perplexity=18.659918, train_loss=2.9263778

Batch 70110, train_perplexity=18.659918, train_loss=2.9263778

Batch 70120, train_perplexity=18.659912, train_loss=2.9263775

Batch 70130, train_perplexity=18.659908, train_loss=2.9263773

Batch 70140, train_perplexity=18.659918, train_loss=2.9263778

Batch 70150, train_perplexity=18.659912, train_loss=2.9263775

Batch 70160, train_perplexity=18.659912, train_loss=2.9263775

Batch 70170, train_perplexity=18.659908, train_loss=2.9263773

Batch 70180, train_perplexity=18.659908, train_loss=2.9263773

Batch 70190, train_perplexity=18.659908, train_loss=2.9263773

Batch 70200, train_perplexity=18.659912, train_loss=2.9263775

Batch 70210, train_perplexity=18.659908, train_loss=2.9263773

Batch 70220, train_perplexity=18.659908, train_loss=2.9263773

Batch 70230, train_perplexity=18.659908, train_loss=2.9263773

Batch 70240, train_perplexity=18.659912, train_loss=2.9263775

Batch 70250, train_perplexity=18.659908, train_loss=2.9263773

Batch 70260, train_perplexity=18.659912, train_loss=2.9263775

Batch 70270, train_perplexity=18.659908, train_loss=2.9263773

Batch 70280, train_perplexity=18.659908, train_loss=2.9263773

Batch 70290, train_perplexity=18.659908, train_loss=2.9263773

Batch 70300, train_perplexity=18.659908, train_loss=2.9263773

Batch 70310, train_perplexity=18.659908, train_loss=2.9263773

Batch 70320, train_perplexity=18.659908, train_loss=2.9263773

Batch 70330, train_perplexity=18.659904, train_loss=2.926377

Batch 70340, train_perplexity=18.659912, train_loss=2.9263775

Batch 70350, train_perplexity=18.659908, train_loss=2.9263773

Batch 70360, train_perplexity=18.659904, train_loss=2.926377
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70370, train_perplexity=18.659908, train_loss=2.9263773

Batch 70380, train_perplexity=18.659904, train_loss=2.926377

Batch 70390, train_perplexity=18.659908, train_loss=2.9263773

Batch 70400, train_perplexity=18.659899, train_loss=2.9263768

Batch 70410, train_perplexity=18.659904, train_loss=2.926377

Batch 70420, train_perplexity=18.659904, train_loss=2.926377

Batch 70430, train_perplexity=18.659904, train_loss=2.926377

Batch 70440, train_perplexity=18.659908, train_loss=2.9263773

Batch 70450, train_perplexity=18.659904, train_loss=2.926377

Batch 70460, train_perplexity=18.659899, train_loss=2.9263768

Batch 70470, train_perplexity=18.659904, train_loss=2.926377

Batch 70480, train_perplexity=18.659908, train_loss=2.9263773

Batch 70490, train_perplexity=18.659904, train_loss=2.926377

Batch 70500, train_perplexity=18.659899, train_loss=2.9263768

Batch 70510, train_perplexity=18.659899, train_loss=2.9263768

Batch 70520, train_perplexity=18.659904, train_loss=2.926377

Batch 70530, train_perplexity=18.659899, train_loss=2.9263768

Batch 70540, train_perplexity=18.659899, train_loss=2.9263768

Batch 70550, train_perplexity=18.659899, train_loss=2.9263768

Batch 70560, train_perplexity=18.659899, train_loss=2.9263768

Batch 70570, train_perplexity=18.659899, train_loss=2.9263768

Batch 70580, train_perplexity=18.659899, train_loss=2.9263768

Batch 70590, train_perplexity=18.659904, train_loss=2.926377

Batch 70600, train_perplexity=18.659899, train_loss=2.9263768

Batch 70610, train_perplexity=18.659899, train_loss=2.9263768

Batch 70620, train_perplexity=18.659895, train_loss=2.9263766

Batch 70630, train_perplexity=18.659899, train_loss=2.9263768

Batch 70640, train_perplexity=18.659899, train_loss=2.9263768

Batch 70650, train_perplexity=18.659895, train_loss=2.9263766

Batch 70660, train_perplexity=18.659899, train_loss=2.9263768

Batch 70670, train_perplexity=18.659899, train_loss=2.9263768

Batch 70680, train_perplexity=18.659895, train_loss=2.9263766

Batch 70690, train_perplexity=18.659891, train_loss=2.9263763

Batch 70700, train_perplexity=18.659891, train_loss=2.9263763

Batch 70710, train_perplexity=18.659891, train_loss=2.9263763

Batch 70720, train_perplexity=18.659891, train_loss=2.9263763

Batch 70730, train_perplexity=18.659895, train_loss=2.9263766

Batch 70740, train_perplexity=18.659891, train_loss=2.9263763

Batch 70750, train_perplexity=18.659899, train_loss=2.9263768

Batch 70760, train_perplexity=18.659895, train_loss=2.9263766

Batch 70770, train_perplexity=18.659895, train_loss=2.9263766

Batch 70780, train_perplexity=18.659891, train_loss=2.9263763

Batch 70790, train_perplexity=18.659891, train_loss=2.9263763

Batch 70800, train_perplexity=18.659891, train_loss=2.9263763

Batch 70810, train_perplexity=18.659891, train_loss=2.9263763

Batch 70820, train_perplexity=18.659891, train_loss=2.9263763

Batch 70830, train_perplexity=18.659899, train_loss=2.9263768

Batch 70840, train_perplexity=18.659891, train_loss=2.9263763

Batch 70850, train_perplexity=18.659891, train_loss=2.9263763

Batch 70860, train_perplexity=18.659891, train_loss=2.9263763

Batch 70870, train_perplexity=18.659891, train_loss=2.9263763

Batch 70880, train_perplexity=18.659891, train_loss=2.9263763

Batch 70890, train_perplexity=18.659891, train_loss=2.9263763

Batch 70900, train_perplexity=18.659891, train_loss=2.9263763

Batch 70910, train_perplexity=18.659891, train_loss=2.9263763

Batch 70920, train_perplexity=18.659891, train_loss=2.9263763

Batch 70930, train_perplexity=18.659891, train_loss=2.9263763

Batch 70940, train_perplexity=18.659891, train_loss=2.9263763

Batch 70950, train_perplexity=18.659885, train_loss=2.926376

Batch 70960, train_perplexity=18.659891, train_loss=2.9263763

Batch 70970, train_perplexity=18.659891, train_loss=2.9263763

Batch 70980, train_perplexity=18.659891, train_loss=2.9263763

Batch 70990, train_perplexity=18.659882, train_loss=2.9263759

Batch 71000, train_perplexity=18.659891, train_loss=2.9263763

Batch 71010, train_perplexity=18.659885, train_loss=2.926376

Batch 71020, train_perplexity=18.659891, train_loss=2.9263763

Batch 71030, train_perplexity=18.659885, train_loss=2.926376

Batch 71040, train_perplexity=18.659885, train_loss=2.926376

Batch 71050, train_perplexity=18.659891, train_loss=2.9263763

Batch 71060, train_perplexity=18.659882, train_loss=2.9263759

Batch 71070, train_perplexity=18.659885, train_loss=2.926376

Batch 71080, train_perplexity=18.659885, train_loss=2.926376

Batch 71090, train_perplexity=18.659885, train_loss=2.926376

Batch 71100, train_perplexity=18.659882, train_loss=2.9263759

Batch 71110, train_perplexity=18.659882, train_loss=2.9263759

Batch 71120, train_perplexity=18.659882, train_loss=2.9263759

Batch 71130, train_perplexity=18.659882, train_loss=2.9263759

Batch 71140, train_perplexity=18.659882, train_loss=2.9263759

Batch 71150, train_perplexity=18.659882, train_loss=2.9263759

Batch 71160, train_perplexity=18.659882, train_loss=2.9263759

Batch 71170, train_perplexity=18.659882, train_loss=2.9263759

Batch 71180, train_perplexity=18.659882, train_loss=2.9263759

Batch 71190, train_perplexity=18.659882, train_loss=2.9263759

Batch 71200, train_perplexity=18.659882, train_loss=2.9263759

Batch 71210, train_perplexity=18.659882, train_loss=2.9263759

Batch 71220, train_perplexity=18.659882, train_loss=2.9263759

Batch 71230, train_perplexity=18.659878, train_loss=2.9263756

Batch 71240, train_perplexity=18.659882, train_loss=2.9263759

Batch 71250, train_perplexity=18.659882, train_loss=2.9263759

Batch 71260, train_perplexity=18.659882, train_loss=2.9263759

Batch 71270, train_perplexity=18.659882, train_loss=2.9263759

Batch 71280, train_perplexity=18.659882, train_loss=2.9263759

Batch 71290, train_perplexity=18.659882, train_loss=2.9263759

Batch 71300, train_perplexity=18.659878, train_loss=2.9263756

Batch 71310, train_perplexity=18.659872, train_loss=2.9263754

Batch 71320, train_perplexity=18.659882, train_loss=2.9263759

Batch 71330, train_perplexity=18.659882, train_loss=2.9263759

Batch 71340, train_perplexity=18.659872, train_loss=2.9263754

Batch 71350, train_perplexity=18.659872, train_loss=2.9263754

Batch 71360, train_perplexity=18.659872, train_loss=2.9263754

Batch 71370, train_perplexity=18.659872, train_loss=2.9263754

Batch 71380, train_perplexity=18.659872, train_loss=2.9263754

Batch 71390, train_perplexity=18.659872, train_loss=2.9263754

Batch 71400, train_perplexity=18.659872, train_loss=2.9263754

Batch 71410, train_perplexity=18.659882, train_loss=2.9263759

Batch 71420, train_perplexity=18.659882, train_loss=2.9263759

Batch 71430, train_perplexity=18.659872, train_loss=2.9263754

Batch 71440, train_perplexity=18.659872, train_loss=2.9263754

Batch 71450, train_perplexity=18.659878, train_loss=2.9263756

Batch 71460, train_perplexity=18.659872, train_loss=2.9263754

Batch 71470, train_perplexity=18.659872, train_loss=2.9263754

Batch 71480, train_perplexity=18.659872, train_loss=2.9263754

Batch 71490, train_perplexity=18.659872, train_loss=2.9263754

Batch 71500, train_perplexity=18.659872, train_loss=2.9263754

Batch 71510, train_perplexity=18.659872, train_loss=2.9263754

Batch 71520, train_perplexity=18.659872, train_loss=2.9263754

Batch 71530, train_perplexity=18.659868, train_loss=2.9263752

Batch 71540, train_perplexity=18.659872, train_loss=2.9263754

Batch 71550, train_perplexity=18.659872, train_loss=2.9263754

Batch 71560, train_perplexity=18.659864, train_loss=2.926375

Batch 71570, train_perplexity=18.659868, train_loss=2.9263752

Batch 71580, train_perplexity=18.659864, train_loss=2.926375

Batch 71590, train_perplexity=18.659868, train_loss=2.9263752

Batch 71600, train_perplexity=18.659864, train_loss=2.926375

Batch 71610, train_perplexity=18.659872, train_loss=2.9263754

Batch 71620, train_perplexity=18.659864, train_loss=2.926375

Batch 71630, train_perplexity=18.659868, train_loss=2.9263752

Batch 71640, train_perplexity=18.659872, train_loss=2.9263754

Batch 71650, train_perplexity=18.659864, train_loss=2.926375

Batch 71660, train_perplexity=18.659868, train_loss=2.9263752

Batch 71670, train_perplexity=18.659868, train_loss=2.9263752
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 71680, train_perplexity=18.659868, train_loss=2.9263752

Batch 71690, train_perplexity=18.659872, train_loss=2.9263754

Batch 71700, train_perplexity=18.659868, train_loss=2.9263752

Batch 71710, train_perplexity=18.659868, train_loss=2.9263752

Batch 71720, train_perplexity=18.659864, train_loss=2.926375

Batch 71730, train_perplexity=18.659864, train_loss=2.926375

Batch 71740, train_perplexity=18.659864, train_loss=2.926375

Batch 71750, train_perplexity=18.659864, train_loss=2.926375

Batch 71760, train_perplexity=18.659864, train_loss=2.926375

Batch 71770, train_perplexity=18.659864, train_loss=2.926375

Batch 71780, train_perplexity=18.659864, train_loss=2.926375

Batch 71790, train_perplexity=18.659864, train_loss=2.926375

Batch 71800, train_perplexity=18.659864, train_loss=2.926375

Batch 71810, train_perplexity=18.659864, train_loss=2.926375

Batch 71820, train_perplexity=18.659864, train_loss=2.926375

Batch 71830, train_perplexity=18.659864, train_loss=2.926375

Batch 71840, train_perplexity=18.659859, train_loss=2.9263747

Batch 71850, train_perplexity=18.659868, train_loss=2.9263752

Batch 71860, train_perplexity=18.659864, train_loss=2.926375

Batch 71870, train_perplexity=18.659864, train_loss=2.926375

Batch 71880, train_perplexity=18.659855, train_loss=2.9263744

Batch 71890, train_perplexity=18.659864, train_loss=2.926375

Batch 71900, train_perplexity=18.659864, train_loss=2.926375

Batch 71910, train_perplexity=18.659864, train_loss=2.926375

Batch 71920, train_perplexity=18.659855, train_loss=2.9263744

Batch 71930, train_perplexity=18.659859, train_loss=2.9263747

Batch 71940, train_perplexity=18.659859, train_loss=2.9263747

Batch 71950, train_perplexity=18.659864, train_loss=2.926375

Batch 71960, train_perplexity=18.659859, train_loss=2.9263747

Batch 71970, train_perplexity=18.659859, train_loss=2.9263747

Batch 71980, train_perplexity=18.659855, train_loss=2.9263744

Batch 71990, train_perplexity=18.659855, train_loss=2.9263744

Batch 72000, train_perplexity=18.659855, train_loss=2.9263744

Batch 72010, train_perplexity=18.659855, train_loss=2.9263744

Batch 72020, train_perplexity=18.659855, train_loss=2.9263744

Batch 72030, train_perplexity=18.659855, train_loss=2.9263744

Batch 72040, train_perplexity=18.659855, train_loss=2.9263744

Batch 72050, train_perplexity=18.659855, train_loss=2.9263744

Batch 72060, train_perplexity=18.659855, train_loss=2.9263744

Batch 72070, train_perplexity=18.659855, train_loss=2.9263744

Batch 72080, train_perplexity=18.659855, train_loss=2.9263744

Batch 72090, train_perplexity=18.659855, train_loss=2.9263744

Batch 72100, train_perplexity=18.659851, train_loss=2.9263742

Batch 72110, train_perplexity=18.659855, train_loss=2.9263744

Batch 72120, train_perplexity=18.659855, train_loss=2.9263744

Batch 72130, train_perplexity=18.659855, train_loss=2.9263744

Batch 72140, train_perplexity=18.659845, train_loss=2.926374

Batch 72150, train_perplexity=18.659845, train_loss=2.926374

Batch 72160, train_perplexity=18.659855, train_loss=2.9263744

Batch 72170, train_perplexity=18.659851, train_loss=2.9263742

Batch 72180, train_perplexity=18.659855, train_loss=2.9263744

Batch 72190, train_perplexity=18.659855, train_loss=2.9263744

Batch 72200, train_perplexity=18.659855, train_loss=2.9263744

Batch 72210, train_perplexity=18.659855, train_loss=2.9263744

Batch 72220, train_perplexity=18.659851, train_loss=2.9263742

Batch 72230, train_perplexity=18.659851, train_loss=2.9263742

Batch 72240, train_perplexity=18.659845, train_loss=2.926374

Batch 72250, train_perplexity=18.659851, train_loss=2.9263742

Batch 72260, train_perplexity=18.659845, train_loss=2.926374

Batch 72270, train_perplexity=18.659855, train_loss=2.9263744

Batch 72280, train_perplexity=18.659851, train_loss=2.9263742

Batch 72290, train_perplexity=18.659845, train_loss=2.926374

Batch 72300, train_perplexity=18.659845, train_loss=2.926374

Batch 72310, train_perplexity=18.659845, train_loss=2.926374

Batch 72320, train_perplexity=18.659845, train_loss=2.926374

Batch 72330, train_perplexity=18.659845, train_loss=2.926374

Batch 72340, train_perplexity=18.659845, train_loss=2.926374

Batch 72350, train_perplexity=18.659845, train_loss=2.926374

Batch 72360, train_perplexity=18.659845, train_loss=2.926374

Batch 72370, train_perplexity=18.659845, train_loss=2.926374

Batch 72380, train_perplexity=18.659845, train_loss=2.926374

Batch 72390, train_perplexity=18.659845, train_loss=2.926374

Batch 72400, train_perplexity=18.659845, train_loss=2.926374

Batch 72410, train_perplexity=18.659845, train_loss=2.926374

Batch 72420, train_perplexity=18.659845, train_loss=2.926374

Batch 72430, train_perplexity=18.659845, train_loss=2.926374

Batch 72440, train_perplexity=18.659845, train_loss=2.926374

Batch 72450, train_perplexity=18.659845, train_loss=2.926374

Batch 72460, train_perplexity=18.659842, train_loss=2.9263737

Batch 72470, train_perplexity=18.659845, train_loss=2.926374

Batch 72480, train_perplexity=18.659845, train_loss=2.926374

Batch 72490, train_perplexity=18.659845, train_loss=2.926374

Batch 72500, train_perplexity=18.659845, train_loss=2.926374

Batch 72510, train_perplexity=18.659845, train_loss=2.926374

Batch 72520, train_perplexity=18.659845, train_loss=2.926374

Batch 72530, train_perplexity=18.659842, train_loss=2.9263737

Batch 72540, train_perplexity=18.659838, train_loss=2.9263735

Batch 72550, train_perplexity=18.659838, train_loss=2.9263735

Batch 72560, train_perplexity=18.659845, train_loss=2.926374

Batch 72570, train_perplexity=18.659845, train_loss=2.926374

Batch 72580, train_perplexity=18.659838, train_loss=2.9263735

Batch 72590, train_perplexity=18.659838, train_loss=2.9263735

Batch 72600, train_perplexity=18.659838, train_loss=2.9263735

Batch 72610, train_perplexity=18.659838, train_loss=2.9263735

Batch 72620, train_perplexity=18.659838, train_loss=2.9263735

Batch 72630, train_perplexity=18.659842, train_loss=2.9263737

Batch 72640, train_perplexity=18.659838, train_loss=2.9263735

Batch 72650, train_perplexity=18.659838, train_loss=2.9263735

Batch 72660, train_perplexity=18.659838, train_loss=2.9263735

Batch 72670, train_perplexity=18.659834, train_loss=2.9263732

Batch 72680, train_perplexity=18.659838, train_loss=2.9263735

Batch 72690, train_perplexity=18.659838, train_loss=2.9263735

Batch 72700, train_perplexity=18.659838, train_loss=2.9263735

Batch 72710, train_perplexity=18.659838, train_loss=2.9263735

Batch 72720, train_perplexity=18.659834, train_loss=2.9263732

Batch 72730, train_perplexity=18.659838, train_loss=2.9263735

Batch 72740, train_perplexity=18.659838, train_loss=2.9263735

Batch 72750, train_perplexity=18.659838, train_loss=2.9263735

Batch 72760, train_perplexity=18.659834, train_loss=2.9263732

Batch 72770, train_perplexity=18.659834, train_loss=2.9263732

Batch 72780, train_perplexity=18.659834, train_loss=2.9263732

Batch 72790, train_perplexity=18.659838, train_loss=2.9263735

Batch 72800, train_perplexity=18.659834, train_loss=2.9263732

Batch 72810, train_perplexity=18.659828, train_loss=2.926373

Batch 72820, train_perplexity=18.659828, train_loss=2.926373

Batch 72830, train_perplexity=18.659838, train_loss=2.9263735

Batch 72840, train_perplexity=18.659838, train_loss=2.9263735

Batch 72850, train_perplexity=18.659828, train_loss=2.926373

Batch 72860, train_perplexity=18.659828, train_loss=2.926373

Batch 72870, train_perplexity=18.659828, train_loss=2.926373

Batch 72880, train_perplexity=18.659834, train_loss=2.9263732

Batch 72890, train_perplexity=18.659834, train_loss=2.9263732

Batch 72900, train_perplexity=18.659828, train_loss=2.926373

Batch 72910, train_perplexity=18.659834, train_loss=2.9263732

Batch 72920, train_perplexity=18.659828, train_loss=2.926373

Batch 72930, train_perplexity=18.659828, train_loss=2.926373

Batch 72940, train_perplexity=18.659828, train_loss=2.926373

Batch 72950, train_perplexity=18.659828, train_loss=2.926373

Batch 72960, train_perplexity=18.659828, train_loss=2.926373

Batch 72970, train_perplexity=18.659828, train_loss=2.926373

Batch 72980, train_perplexity=18.659828, train_loss=2.926373
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72990, train_perplexity=18.659828, train_loss=2.926373

Batch 73000, train_perplexity=18.659828, train_loss=2.926373

Batch 73010, train_perplexity=18.659828, train_loss=2.926373

Batch 73020, train_perplexity=18.659828, train_loss=2.926373

Batch 73030, train_perplexity=18.659828, train_loss=2.926373

Batch 73040, train_perplexity=18.659828, train_loss=2.926373

Batch 73050, train_perplexity=18.659828, train_loss=2.926373

Batch 73060, train_perplexity=18.659828, train_loss=2.926373

Batch 73070, train_perplexity=18.659824, train_loss=2.9263728

Batch 73080, train_perplexity=18.659828, train_loss=2.926373

Batch 73090, train_perplexity=18.659828, train_loss=2.926373

Batch 73100, train_perplexity=18.65982, train_loss=2.9263725

Batch 73110, train_perplexity=18.659824, train_loss=2.9263728

Batch 73120, train_perplexity=18.659828, train_loss=2.926373

Batch 73130, train_perplexity=18.659828, train_loss=2.926373

Batch 73140, train_perplexity=18.659828, train_loss=2.926373

Batch 73150, train_perplexity=18.659828, train_loss=2.926373

Batch 73160, train_perplexity=18.65982, train_loss=2.9263725

Batch 73170, train_perplexity=18.65982, train_loss=2.9263725

Batch 73180, train_perplexity=18.659824, train_loss=2.9263728

Batch 73190, train_perplexity=18.65982, train_loss=2.9263725

Batch 73200, train_perplexity=18.659824, train_loss=2.9263728

Batch 73210, train_perplexity=18.65982, train_loss=2.9263725

Batch 73220, train_perplexity=18.65982, train_loss=2.9263725

Batch 73230, train_perplexity=18.65982, train_loss=2.9263725

Batch 73240, train_perplexity=18.65982, train_loss=2.9263725

Batch 73250, train_perplexity=18.65982, train_loss=2.9263725

Batch 73260, train_perplexity=18.65982, train_loss=2.9263725

Batch 73270, train_perplexity=18.65982, train_loss=2.9263725

Batch 73280, train_perplexity=18.65982, train_loss=2.9263725

Batch 73290, train_perplexity=18.65982, train_loss=2.9263725

Batch 73300, train_perplexity=18.65982, train_loss=2.9263725

Batch 73310, train_perplexity=18.65982, train_loss=2.9263725

Batch 73320, train_perplexity=18.65982, train_loss=2.9263725

Batch 73330, train_perplexity=18.65982, train_loss=2.9263725

Batch 73340, train_perplexity=18.65982, train_loss=2.9263725

Batch 73350, train_perplexity=18.65982, train_loss=2.9263725

Batch 73360, train_perplexity=18.65982, train_loss=2.9263725

Batch 73370, train_perplexity=18.659815, train_loss=2.9263723

Batch 73380, train_perplexity=18.659815, train_loss=2.9263723

Batch 73390, train_perplexity=18.65982, train_loss=2.9263725

Batch 73400, train_perplexity=18.659815, train_loss=2.9263723

Batch 73410, train_perplexity=18.65982, train_loss=2.9263725

Batch 73420, train_perplexity=18.65982, train_loss=2.9263725

Batch 73430, train_perplexity=18.65982, train_loss=2.9263725

Batch 73440, train_perplexity=18.659815, train_loss=2.9263723

Batch 73450, train_perplexity=18.659815, train_loss=2.9263723

Batch 73460, train_perplexity=18.659811, train_loss=2.926372

Batch 73470, train_perplexity=18.659815, train_loss=2.9263723

Batch 73480, train_perplexity=18.65982, train_loss=2.9263725

Batch 73490, train_perplexity=18.659815, train_loss=2.9263723

Batch 73500, train_perplexity=18.659811, train_loss=2.926372

Batch 73510, train_perplexity=18.659811, train_loss=2.926372

Batch 73520, train_perplexity=18.659815, train_loss=2.9263723

Batch 73530, train_perplexity=18.659811, train_loss=2.926372

Batch 73540, train_perplexity=18.659811, train_loss=2.926372

Batch 73550, train_perplexity=18.659811, train_loss=2.926372

Batch 73560, train_perplexity=18.659815, train_loss=2.9263723

Batch 73570, train_perplexity=18.659811, train_loss=2.926372

Batch 73580, train_perplexity=18.659811, train_loss=2.926372

Batch 73590, train_perplexity=18.659811, train_loss=2.926372

Batch 73600, train_perplexity=18.659815, train_loss=2.9263723

Batch 73610, train_perplexity=18.659811, train_loss=2.926372

Batch 73620, train_perplexity=18.659811, train_loss=2.926372

Batch 73630, train_perplexity=18.659811, train_loss=2.926372

Batch 73640, train_perplexity=18.659807, train_loss=2.9263718

Batch 73650, train_perplexity=18.659811, train_loss=2.926372

Batch 73660, train_perplexity=18.659811, train_loss=2.926372

Batch 73670, train_perplexity=18.659807, train_loss=2.9263718

Batch 73680, train_perplexity=18.659807, train_loss=2.9263718

Batch 73690, train_perplexity=18.659807, train_loss=2.9263718

Batch 73700, train_perplexity=18.659807, train_loss=2.9263718

Batch 73710, train_perplexity=18.659807, train_loss=2.9263718

Batch 73720, train_perplexity=18.659811, train_loss=2.926372

Batch 73730, train_perplexity=18.659801, train_loss=2.9263716

Batch 73740, train_perplexity=18.659811, train_loss=2.926372

Batch 73750, train_perplexity=18.659807, train_loss=2.9263718

Batch 73760, train_perplexity=18.659811, train_loss=2.926372

Batch 73770, train_perplexity=18.659801, train_loss=2.9263716

Batch 73780, train_perplexity=18.659801, train_loss=2.9263716

Batch 73790, train_perplexity=18.659801, train_loss=2.9263716

Batch 73800, train_perplexity=18.659801, train_loss=2.9263716

Batch 73810, train_perplexity=18.659801, train_loss=2.9263716

Batch 73820, train_perplexity=18.659801, train_loss=2.9263716

Batch 73830, train_perplexity=18.659801, train_loss=2.9263716

Batch 73840, train_perplexity=18.659801, train_loss=2.9263716

Batch 73850, train_perplexity=18.659801, train_loss=2.9263716

Batch 73860, train_perplexity=18.659798, train_loss=2.9263713

Batch 73870, train_perplexity=18.659801, train_loss=2.9263716

Batch 73880, train_perplexity=18.659801, train_loss=2.9263716

Batch 73890, train_perplexity=18.659801, train_loss=2.9263716

Batch 73900, train_perplexity=18.659801, train_loss=2.9263716

Batch 73910, train_perplexity=18.659801, train_loss=2.9263716

Batch 73920, train_perplexity=18.659801, train_loss=2.9263716

Batch 73930, train_perplexity=18.659801, train_loss=2.9263716

Batch 73940, train_perplexity=18.659801, train_loss=2.9263716

Batch 73950, train_perplexity=18.659801, train_loss=2.9263716

Batch 73960, train_perplexity=18.659801, train_loss=2.9263716

Batch 73970, train_perplexity=18.659794, train_loss=2.926371

Batch 73980, train_perplexity=18.659798, train_loss=2.9263713

Batch 73990, train_perplexity=18.659801, train_loss=2.9263716

Batch 74000, train_perplexity=18.659801, train_loss=2.9263716

Batch 74010, train_perplexity=18.659794, train_loss=2.926371

Batch 74020, train_perplexity=18.659801, train_loss=2.9263716

Batch 74030, train_perplexity=18.659798, train_loss=2.9263713

Batch 74040, train_perplexity=18.659794, train_loss=2.926371

Batch 74050, train_perplexity=18.659798, train_loss=2.9263713

Batch 74060, train_perplexity=18.659801, train_loss=2.9263716

Batch 74070, train_perplexity=18.659798, train_loss=2.9263713

Batch 74080, train_perplexity=18.659801, train_loss=2.9263716

Batch 74090, train_perplexity=18.659798, train_loss=2.9263713

Batch 74100, train_perplexity=18.659798, train_loss=2.9263713

Batch 74110, train_perplexity=18.659794, train_loss=2.926371

Batch 74120, train_perplexity=18.659794, train_loss=2.926371

Batch 74130, train_perplexity=18.659794, train_loss=2.926371

Batch 74140, train_perplexity=18.659794, train_loss=2.926371

Batch 74150, train_perplexity=18.659794, train_loss=2.926371

Batch 74160, train_perplexity=18.659794, train_loss=2.926371

Batch 74170, train_perplexity=18.659794, train_loss=2.926371

Batch 74180, train_perplexity=18.659794, train_loss=2.926371

Batch 74190, train_perplexity=18.659794, train_loss=2.926371

Batch 74200, train_perplexity=18.659794, train_loss=2.926371

Batch 74210, train_perplexity=18.659794, train_loss=2.926371

Batch 74220, train_perplexity=18.659794, train_loss=2.926371

Batch 74230, train_perplexity=18.659794, train_loss=2.926371

Batch 74240, train_perplexity=18.659794, train_loss=2.926371

Batch 74250, train_perplexity=18.659794, train_loss=2.926371

Batch 74260, train_perplexity=18.659794, train_loss=2.926371

Batch 74270, train_perplexity=18.659794, train_loss=2.926371

Batch 74280, train_perplexity=18.659794, train_loss=2.926371

Batch 74290, train_perplexity=18.659794, train_loss=2.926371

Batch 74300, train_perplexity=18.659794, train_loss=2.926371
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74310, train_perplexity=18.659794, train_loss=2.926371

Batch 74320, train_perplexity=18.659794, train_loss=2.926371

Batch 74330, train_perplexity=18.659794, train_loss=2.926371

Batch 74340, train_perplexity=18.659794, train_loss=2.926371

Batch 74350, train_perplexity=18.659788, train_loss=2.9263709

Batch 74360, train_perplexity=18.659794, train_loss=2.926371

Batch 74370, train_perplexity=18.659784, train_loss=2.9263706

Batch 74380, train_perplexity=18.659788, train_loss=2.9263709

Batch 74390, train_perplexity=18.659784, train_loss=2.9263706

Batch 74400, train_perplexity=18.659788, train_loss=2.9263709

Batch 74410, train_perplexity=18.659784, train_loss=2.9263706

Batch 74420, train_perplexity=18.659784, train_loss=2.9263706

Batch 74430, train_perplexity=18.659784, train_loss=2.9263706

Batch 74440, train_perplexity=18.659788, train_loss=2.9263709

Batch 74450, train_perplexity=18.659784, train_loss=2.9263706

Batch 74460, train_perplexity=18.659784, train_loss=2.9263706

Batch 74470, train_perplexity=18.659784, train_loss=2.9263706

Batch 74480, train_perplexity=18.659784, train_loss=2.9263706

Batch 74490, train_perplexity=18.659784, train_loss=2.9263706

Batch 74500, train_perplexity=18.659784, train_loss=2.9263706

Batch 74510, train_perplexity=18.659784, train_loss=2.9263706

Batch 74520, train_perplexity=18.659784, train_loss=2.9263706

Batch 74530, train_perplexity=18.659784, train_loss=2.9263706

Batch 74540, train_perplexity=18.65978, train_loss=2.9263704

Batch 74550, train_perplexity=18.659784, train_loss=2.9263706

Batch 74560, train_perplexity=18.659784, train_loss=2.9263706

Batch 74570, train_perplexity=18.659775, train_loss=2.9263701

Batch 74580, train_perplexity=18.65978, train_loss=2.9263704

Batch 74590, train_perplexity=18.659784, train_loss=2.9263706

Batch 74600, train_perplexity=18.659784, train_loss=2.9263706

Batch 74610, train_perplexity=18.659784, train_loss=2.9263706

Batch 74620, train_perplexity=18.659784, train_loss=2.9263706

Batch 74630, train_perplexity=18.659775, train_loss=2.9263701

Batch 74640, train_perplexity=18.659775, train_loss=2.9263701

Batch 74650, train_perplexity=18.65978, train_loss=2.9263704

Batch 74660, train_perplexity=18.659784, train_loss=2.9263706

Batch 74670, train_perplexity=18.65978, train_loss=2.9263704

Batch 74680, train_perplexity=18.659775, train_loss=2.9263701

Batch 74690, train_perplexity=18.659775, train_loss=2.9263701

Batch 74700, train_perplexity=18.65978, train_loss=2.9263704

Batch 74710, train_perplexity=18.65978, train_loss=2.9263704

Batch 74720, train_perplexity=18.659775, train_loss=2.9263701

Batch 74730, train_perplexity=18.659784, train_loss=2.9263706

Batch 74740, train_perplexity=18.65978, train_loss=2.9263704

Batch 74750, train_perplexity=18.659775, train_loss=2.9263701

Batch 74760, train_perplexity=18.65978, train_loss=2.9263704

Batch 74770, train_perplexity=18.659775, train_loss=2.9263701

Batch 74780, train_perplexity=18.659775, train_loss=2.9263701

Batch 74790, train_perplexity=18.659771, train_loss=2.92637

Batch 74800, train_perplexity=18.659775, train_loss=2.9263701

Batch 74810, train_perplexity=18.65978, train_loss=2.9263704

Batch 74820, train_perplexity=18.659771, train_loss=2.92637

Batch 74830, train_perplexity=18.659775, train_loss=2.9263701

Batch 74840, train_perplexity=18.659775, train_loss=2.9263701

Batch 74850, train_perplexity=18.659775, train_loss=2.9263701

Batch 74860, train_perplexity=18.659775, train_loss=2.9263701

Batch 74870, train_perplexity=18.659775, train_loss=2.9263701

Batch 74880, train_perplexity=18.659771, train_loss=2.92637

Batch 74890, train_perplexity=18.659767, train_loss=2.9263697

Batch 74900, train_perplexity=18.659771, train_loss=2.92637

Batch 74910, train_perplexity=18.659775, train_loss=2.9263701

Batch 74920, train_perplexity=18.659771, train_loss=2.92637

Batch 74930, train_perplexity=18.659771, train_loss=2.92637

Batch 74940, train_perplexity=18.659775, train_loss=2.9263701

Batch 74950, train_perplexity=18.659775, train_loss=2.9263701

Batch 74960, train_perplexity=18.659771, train_loss=2.92637

Batch 74970, train_perplexity=18.659771, train_loss=2.92637

Batch 74980, train_perplexity=18.659771, train_loss=2.92637

Batch 74990, train_perplexity=18.659767, train_loss=2.9263697

Batch 75000, train_perplexity=18.659767, train_loss=2.9263697

Batch 75010, train_perplexity=18.659767, train_loss=2.9263697

Batch 75020, train_perplexity=18.659775, train_loss=2.9263701

Batch 75030, train_perplexity=18.659767, train_loss=2.9263697

Batch 75040, train_perplexity=18.659771, train_loss=2.92637

Batch 75050, train_perplexity=18.659771, train_loss=2.92637

Batch 75060, train_perplexity=18.659767, train_loss=2.9263697

Batch 75070, train_perplexity=18.659767, train_loss=2.9263697

Batch 75080, train_perplexity=18.659767, train_loss=2.9263697

Batch 75090, train_perplexity=18.659767, train_loss=2.9263697

Batch 75100, train_perplexity=18.659767, train_loss=2.9263697

Batch 75110, train_perplexity=18.659767, train_loss=2.9263697

Batch 75120, train_perplexity=18.659767, train_loss=2.9263697

Batch 75130, train_perplexity=18.659767, train_loss=2.9263697

Batch 75140, train_perplexity=18.659767, train_loss=2.9263697

Batch 75150, train_perplexity=18.659767, train_loss=2.9263697

Batch 75160, train_perplexity=18.659767, train_loss=2.9263697

Batch 75170, train_perplexity=18.659767, train_loss=2.9263697

Batch 75180, train_perplexity=18.659761, train_loss=2.9263694

Batch 75190, train_perplexity=18.659767, train_loss=2.9263697

Batch 75200, train_perplexity=18.659761, train_loss=2.9263694

Batch 75210, train_perplexity=18.659767, train_loss=2.9263697

Batch 75220, train_perplexity=18.659761, train_loss=2.9263694

Batch 75230, train_perplexity=18.659761, train_loss=2.9263694

Batch 75240, train_perplexity=18.659767, train_loss=2.9263697

Batch 75250, train_perplexity=18.659761, train_loss=2.9263694

Batch 75260, train_perplexity=18.659767, train_loss=2.9263697

Batch 75270, train_perplexity=18.659758, train_loss=2.9263692

Batch 75280, train_perplexity=18.659758, train_loss=2.9263692

Batch 75290, train_perplexity=18.659761, train_loss=2.9263694

Batch 75300, train_perplexity=18.659758, train_loss=2.9263692

Batch 75310, train_perplexity=18.659761, train_loss=2.9263694

Batch 75320, train_perplexity=18.659758, train_loss=2.9263692

Batch 75330, train_perplexity=18.659761, train_loss=2.9263694

Batch 75340, train_perplexity=18.659758, train_loss=2.9263692

Batch 75350, train_perplexity=18.659758, train_loss=2.9263692

Batch 75360, train_perplexity=18.659758, train_loss=2.9263692

Batch 75370, train_perplexity=18.659758, train_loss=2.9263692

Batch 75380, train_perplexity=18.659758, train_loss=2.9263692

Batch 75390, train_perplexity=18.659758, train_loss=2.9263692

Batch 75400, train_perplexity=18.659758, train_loss=2.9263692

Batch 75410, train_perplexity=18.659758, train_loss=2.9263692

Batch 75420, train_perplexity=18.659758, train_loss=2.9263692

Batch 75430, train_perplexity=18.659758, train_loss=2.9263692

Batch 75440, train_perplexity=18.659754, train_loss=2.926369

Batch 75450, train_perplexity=18.659754, train_loss=2.926369

Batch 75460, train_perplexity=18.659748, train_loss=2.9263687

Batch 75470, train_perplexity=18.659758, train_loss=2.9263692

Batch 75480, train_perplexity=18.659758, train_loss=2.9263692

Batch 75490, train_perplexity=18.659758, train_loss=2.9263692

Batch 75500, train_perplexity=18.659758, train_loss=2.9263692

Batch 75510, train_perplexity=18.659758, train_loss=2.9263692

Batch 75520, train_perplexity=18.659758, train_loss=2.9263692

Batch 75530, train_perplexity=18.659758, train_loss=2.9263692

Batch 75540, train_perplexity=18.659754, train_loss=2.926369

Batch 75550, train_perplexity=18.659758, train_loss=2.9263692

Batch 75560, train_perplexity=18.659758, train_loss=2.9263692

Batch 75570, train_perplexity=18.659748, train_loss=2.9263687

Batch 75580, train_perplexity=18.659754, train_loss=2.926369

Batch 75590, train_perplexity=18.659748, train_loss=2.9263687

Batch 75600, train_perplexity=18.659754, train_loss=2.926369

Batch 75610, train_perplexity=18.659748, train_loss=2.9263687
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 75620, train_perplexity=18.659748, train_loss=2.9263687

Batch 75630, train_perplexity=18.659754, train_loss=2.926369

Batch 75640, train_perplexity=18.659754, train_loss=2.926369

Batch 75650, train_perplexity=18.659748, train_loss=2.9263687

Batch 75660, train_perplexity=18.659748, train_loss=2.9263687

Batch 75670, train_perplexity=18.659754, train_loss=2.926369

Batch 75680, train_perplexity=18.659748, train_loss=2.9263687

Batch 75690, train_perplexity=18.659754, train_loss=2.926369

Batch 75700, train_perplexity=18.659754, train_loss=2.926369

Batch 75710, train_perplexity=18.659748, train_loss=2.9263687

Batch 75720, train_perplexity=18.659748, train_loss=2.9263687

Batch 75730, train_perplexity=18.659744, train_loss=2.9263685

Batch 75740, train_perplexity=18.659748, train_loss=2.9263687

Batch 75750, train_perplexity=18.659748, train_loss=2.9263687

Batch 75760, train_perplexity=18.659748, train_loss=2.9263687

Batch 75770, train_perplexity=18.659748, train_loss=2.9263687

Batch 75780, train_perplexity=18.659748, train_loss=2.9263687

Batch 75790, train_perplexity=18.659744, train_loss=2.9263685

Batch 75800, train_perplexity=18.659748, train_loss=2.9263687

Batch 75810, train_perplexity=18.65974, train_loss=2.9263682

Batch 75820, train_perplexity=18.65974, train_loss=2.9263682

Batch 75830, train_perplexity=18.659748, train_loss=2.9263687

Batch 75840, train_perplexity=18.65974, train_loss=2.9263682

Batch 75850, train_perplexity=18.659748, train_loss=2.9263687

Batch 75860, train_perplexity=18.659748, train_loss=2.9263687

Batch 75870, train_perplexity=18.659748, train_loss=2.9263687

Batch 75880, train_perplexity=18.659744, train_loss=2.9263685

Batch 75890, train_perplexity=18.65974, train_loss=2.9263682

Batch 75900, train_perplexity=18.65974, train_loss=2.9263682

Batch 75910, train_perplexity=18.65974, train_loss=2.9263682

Batch 75920, train_perplexity=18.65974, train_loss=2.9263682

Batch 75930, train_perplexity=18.659744, train_loss=2.9263685

Batch 75940, train_perplexity=18.659744, train_loss=2.9263685

Batch 75950, train_perplexity=18.65974, train_loss=2.9263682

Batch 75960, train_perplexity=18.65974, train_loss=2.9263682

Batch 75970, train_perplexity=18.65974, train_loss=2.9263682

Batch 75980, train_perplexity=18.659744, train_loss=2.9263685

Batch 75990, train_perplexity=18.65974, train_loss=2.9263682

Batch 76000, train_perplexity=18.65974, train_loss=2.9263682

Batch 76010, train_perplexity=18.65974, train_loss=2.9263682

Batch 76020, train_perplexity=18.65974, train_loss=2.9263682

Batch 76030, train_perplexity=18.65974, train_loss=2.9263682

Batch 76040, train_perplexity=18.65974, train_loss=2.9263682

Batch 76050, train_perplexity=18.65974, train_loss=2.9263682

Batch 76060, train_perplexity=18.65974, train_loss=2.9263682

Batch 76070, train_perplexity=18.659735, train_loss=2.926368

Batch 76080, train_perplexity=18.65973, train_loss=2.9263678

Batch 76090, train_perplexity=18.659735, train_loss=2.926368

Batch 76100, train_perplexity=18.65973, train_loss=2.9263678

Batch 76110, train_perplexity=18.65974, train_loss=2.9263682

Batch 76120, train_perplexity=18.659735, train_loss=2.926368

Batch 76130, train_perplexity=18.659735, train_loss=2.926368

Batch 76140, train_perplexity=18.65973, train_loss=2.9263678

Batch 76150, train_perplexity=18.65973, train_loss=2.9263678

Batch 76160, train_perplexity=18.659735, train_loss=2.926368

Batch 76170, train_perplexity=18.65973, train_loss=2.9263678

Batch 76180, train_perplexity=18.65973, train_loss=2.9263678

Batch 76190, train_perplexity=18.65973, train_loss=2.9263678

Batch 76200, train_perplexity=18.659735, train_loss=2.926368

Batch 76210, train_perplexity=18.65974, train_loss=2.9263682

Batch 76220, train_perplexity=18.659735, train_loss=2.926368

Batch 76230, train_perplexity=18.659735, train_loss=2.926368

Batch 76240, train_perplexity=18.65973, train_loss=2.9263678

Batch 76250, train_perplexity=18.65973, train_loss=2.9263678

Batch 76260, train_perplexity=18.65973, train_loss=2.9263678

Batch 76270, train_perplexity=18.65973, train_loss=2.9263678

Batch 76280, train_perplexity=18.65973, train_loss=2.9263678

Batch 76290, train_perplexity=18.65973, train_loss=2.9263678

Batch 76300, train_perplexity=18.65973, train_loss=2.9263678

Batch 76310, train_perplexity=18.65973, train_loss=2.9263678

Batch 76320, train_perplexity=18.65973, train_loss=2.9263678

Batch 76330, train_perplexity=18.65973, train_loss=2.9263678

Batch 76340, train_perplexity=18.65973, train_loss=2.9263678

Batch 76350, train_perplexity=18.65973, train_loss=2.9263678

Batch 76360, train_perplexity=18.65973, train_loss=2.9263678

Batch 76370, train_perplexity=18.65973, train_loss=2.9263678

Batch 76380, train_perplexity=18.659721, train_loss=2.9263673

Batch 76390, train_perplexity=18.65973, train_loss=2.9263678

Batch 76400, train_perplexity=18.65973, train_loss=2.9263678

Batch 76410, train_perplexity=18.659727, train_loss=2.9263675

Batch 76420, train_perplexity=18.659727, train_loss=2.9263675

Batch 76430, train_perplexity=18.659721, train_loss=2.9263673

Batch 76440, train_perplexity=18.65973, train_loss=2.9263678

Batch 76450, train_perplexity=18.65973, train_loss=2.9263678

Batch 76460, train_perplexity=18.659721, train_loss=2.9263673

Batch 76470, train_perplexity=18.659721, train_loss=2.9263673

Batch 76480, train_perplexity=18.659721, train_loss=2.9263673

Batch 76490, train_perplexity=18.659721, train_loss=2.9263673

Batch 76500, train_perplexity=18.659721, train_loss=2.9263673

Batch 76510, train_perplexity=18.659721, train_loss=2.9263673

Batch 76520, train_perplexity=18.659721, train_loss=2.9263673

Batch 76530, train_perplexity=18.659714, train_loss=2.9263668

Batch 76540, train_perplexity=18.659727, train_loss=2.9263675

Batch 76550, train_perplexity=18.659718, train_loss=2.926367

Batch 76560, train_perplexity=18.659721, train_loss=2.9263673

Batch 76570, train_perplexity=18.659718, train_loss=2.926367

Batch 76580, train_perplexity=18.659721, train_loss=2.9263673

Batch 76590, train_perplexity=18.659721, train_loss=2.9263673

Batch 76600, train_perplexity=18.659721, train_loss=2.9263673

Batch 76610, train_perplexity=18.659721, train_loss=2.9263673

Batch 76620, train_perplexity=18.659718, train_loss=2.926367

Batch 76630, train_perplexity=18.659714, train_loss=2.9263668

Batch 76640, train_perplexity=18.659721, train_loss=2.9263673

Batch 76650, train_perplexity=18.659718, train_loss=2.926367

Batch 76660, train_perplexity=18.659718, train_loss=2.926367

Batch 76670, train_perplexity=18.659714, train_loss=2.9263668

Batch 76680, train_perplexity=18.659721, train_loss=2.9263673

Batch 76690, train_perplexity=18.659721, train_loss=2.9263673

Batch 76700, train_perplexity=18.659714, train_loss=2.9263668

Batch 76710, train_perplexity=18.659714, train_loss=2.9263668

Batch 76720, train_perplexity=18.659718, train_loss=2.926367

Batch 76730, train_perplexity=18.659721, train_loss=2.9263673

Batch 76740, train_perplexity=18.659714, train_loss=2.9263668

Batch 76750, train_perplexity=18.659718, train_loss=2.926367

Batch 76760, train_perplexity=18.659714, train_loss=2.9263668

Batch 76770, train_perplexity=18.659714, train_loss=2.9263668

Batch 76780, train_perplexity=18.659718, train_loss=2.926367

Batch 76790, train_perplexity=18.659714, train_loss=2.9263668

Batch 76800, train_perplexity=18.659714, train_loss=2.9263668

Batch 76810, train_perplexity=18.659718, train_loss=2.926367

Batch 76820, train_perplexity=18.659718, train_loss=2.926367

Batch 76830, train_perplexity=18.659714, train_loss=2.9263668

Batch 76840, train_perplexity=18.659708, train_loss=2.9263666

Batch 76850, train_perplexity=18.659714, train_loss=2.9263668

Batch 76860, train_perplexity=18.659714, train_loss=2.9263668

Batch 76870, train_perplexity=18.659714, train_loss=2.9263668

Batch 76880, train_perplexity=18.659714, train_loss=2.9263668

Batch 76890, train_perplexity=18.659714, train_loss=2.9263668

Batch 76900, train_perplexity=18.659714, train_loss=2.9263668

Batch 76910, train_perplexity=18.659714, train_loss=2.9263668

Batch 76920, train_perplexity=18.659714, train_loss=2.9263668

Batch 76930, train_perplexity=18.659714, train_loss=2.9263668
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76940, train_perplexity=18.659714, train_loss=2.9263668

Batch 76950, train_perplexity=18.659708, train_loss=2.9263666

Batch 76960, train_perplexity=18.659708, train_loss=2.9263666

Batch 76970, train_perplexity=18.659704, train_loss=2.9263663

Batch 76980, train_perplexity=18.659714, train_loss=2.9263668

Batch 76990, train_perplexity=18.659714, train_loss=2.9263668

Batch 77000, train_perplexity=18.659708, train_loss=2.9263666

Batch 77010, train_perplexity=18.659714, train_loss=2.9263668

Batch 77020, train_perplexity=18.659714, train_loss=2.9263668

Batch 77030, train_perplexity=18.659704, train_loss=2.9263663

Batch 77040, train_perplexity=18.659704, train_loss=2.9263663

Batch 77050, train_perplexity=18.659704, train_loss=2.9263663

Batch 77060, train_perplexity=18.659714, train_loss=2.9263668

Batch 77070, train_perplexity=18.659704, train_loss=2.9263663

Batch 77080, train_perplexity=18.659704, train_loss=2.9263663

Batch 77090, train_perplexity=18.659704, train_loss=2.9263663

Batch 77100, train_perplexity=18.659704, train_loss=2.9263663

Batch 77110, train_perplexity=18.659704, train_loss=2.9263663

Batch 77120, train_perplexity=18.659704, train_loss=2.9263663

Batch 77130, train_perplexity=18.659704, train_loss=2.9263663

Batch 77140, train_perplexity=18.659704, train_loss=2.9263663

Batch 77150, train_perplexity=18.659704, train_loss=2.9263663

Batch 77160, train_perplexity=18.659704, train_loss=2.9263663

Batch 77170, train_perplexity=18.659695, train_loss=2.9263659

Batch 77180, train_perplexity=18.659695, train_loss=2.9263659

Batch 77190, train_perplexity=18.659695, train_loss=2.9263659

Batch 77200, train_perplexity=18.659704, train_loss=2.9263663

Batch 77210, train_perplexity=18.659704, train_loss=2.9263663

Batch 77220, train_perplexity=18.659695, train_loss=2.9263659

Batch 77230, train_perplexity=18.659695, train_loss=2.9263659

Batch 77240, train_perplexity=18.659704, train_loss=2.9263663

Batch 77250, train_perplexity=18.659704, train_loss=2.9263663

Batch 77260, train_perplexity=18.6597, train_loss=2.926366

Batch 77270, train_perplexity=18.6597, train_loss=2.926366

Batch 77280, train_perplexity=18.659695, train_loss=2.9263659

Batch 77290, train_perplexity=18.659704, train_loss=2.9263663

Batch 77300, train_perplexity=18.659695, train_loss=2.9263659

Batch 77310, train_perplexity=18.659695, train_loss=2.9263659

Batch 77320, train_perplexity=18.659695, train_loss=2.9263659

Batch 77330, train_perplexity=18.6597, train_loss=2.926366

Batch 77340, train_perplexity=18.6597, train_loss=2.926366

Batch 77350, train_perplexity=18.659695, train_loss=2.9263659

Batch 77360, train_perplexity=18.659695, train_loss=2.9263659

Batch 77370, train_perplexity=18.659695, train_loss=2.9263659

Batch 77380, train_perplexity=18.659695, train_loss=2.9263659

Batch 77390, train_perplexity=18.659695, train_loss=2.9263659

Batch 77400, train_perplexity=18.6597, train_loss=2.926366

Batch 77410, train_perplexity=18.659695, train_loss=2.9263659

Batch 77420, train_perplexity=18.659695, train_loss=2.9263659

Batch 77430, train_perplexity=18.659695, train_loss=2.9263659

Batch 77440, train_perplexity=18.659695, train_loss=2.9263659

Batch 77450, train_perplexity=18.659695, train_loss=2.9263659

Batch 77460, train_perplexity=18.65969, train_loss=2.9263656

Batch 77470, train_perplexity=18.659687, train_loss=2.9263654

Batch 77480, train_perplexity=18.659695, train_loss=2.9263659

Batch 77490, train_perplexity=18.65969, train_loss=2.9263656

Batch 77500, train_perplexity=18.65969, train_loss=2.9263656

Batch 77510, train_perplexity=18.65969, train_loss=2.9263656

Batch 77520, train_perplexity=18.659695, train_loss=2.9263659

Batch 77530, train_perplexity=18.659695, train_loss=2.9263659

Batch 77540, train_perplexity=18.65969, train_loss=2.9263656

Batch 77550, train_perplexity=18.659695, train_loss=2.9263659

Batch 77560, train_perplexity=18.65969, train_loss=2.9263656

Batch 77570, train_perplexity=18.659687, train_loss=2.9263654

Batch 77580, train_perplexity=18.65969, train_loss=2.9263656

Batch 77590, train_perplexity=18.659695, train_loss=2.9263659

Batch 77600, train_perplexity=18.65969, train_loss=2.9263656

Batch 77610, train_perplexity=18.659687, train_loss=2.9263654

Batch 77620, train_perplexity=18.65969, train_loss=2.9263656

Batch 77630, train_perplexity=18.65969, train_loss=2.9263656

Batch 77640, train_perplexity=18.659687, train_loss=2.9263654

Batch 77650, train_perplexity=18.65969, train_loss=2.9263656

Batch 77660, train_perplexity=18.659687, train_loss=2.9263654

Batch 77670, train_perplexity=18.659687, train_loss=2.9263654

Batch 77680, train_perplexity=18.659687, train_loss=2.9263654

Batch 77690, train_perplexity=18.659687, train_loss=2.9263654

Batch 77700, train_perplexity=18.659687, train_loss=2.9263654

Batch 77710, train_perplexity=18.659687, train_loss=2.9263654

Batch 77720, train_perplexity=18.65969, train_loss=2.9263656

Batch 77730, train_perplexity=18.659687, train_loss=2.9263654

Batch 77740, train_perplexity=18.659687, train_loss=2.9263654

Batch 77750, train_perplexity=18.659687, train_loss=2.9263654

Batch 77760, train_perplexity=18.659687, train_loss=2.9263654

Batch 77770, train_perplexity=18.659687, train_loss=2.9263654

Batch 77780, train_perplexity=18.659687, train_loss=2.9263654

Batch 77790, train_perplexity=18.659681, train_loss=2.9263651

Batch 77800, train_perplexity=18.659687, train_loss=2.9263654

Batch 77810, train_perplexity=18.659681, train_loss=2.9263651

Batch 77820, train_perplexity=18.659687, train_loss=2.9263654

Batch 77830, train_perplexity=18.659681, train_loss=2.9263651

Batch 77840, train_perplexity=18.659678, train_loss=2.926365

Batch 77850, train_perplexity=18.659678, train_loss=2.926365

Batch 77860, train_perplexity=18.659681, train_loss=2.9263651

Batch 77870, train_perplexity=18.659681, train_loss=2.9263651

Batch 77880, train_perplexity=18.659678, train_loss=2.926365

Batch 77890, train_perplexity=18.659678, train_loss=2.926365

Batch 77900, train_perplexity=18.659678, train_loss=2.926365

Batch 77910, train_perplexity=18.659678, train_loss=2.926365

Batch 77920, train_perplexity=18.659678, train_loss=2.926365

Batch 77930, train_perplexity=18.659674, train_loss=2.9263647

Batch 77940, train_perplexity=18.659678, train_loss=2.926365

Batch 77950, train_perplexity=18.659678, train_loss=2.926365

Batch 77960, train_perplexity=18.659681, train_loss=2.9263651

Batch 77970, train_perplexity=18.659678, train_loss=2.926365

Batch 77980, train_perplexity=18.659678, train_loss=2.926365

Batch 77990, train_perplexity=18.659678, train_loss=2.926365

Batch 78000, train_perplexity=18.659678, train_loss=2.926365

Batch 78010, train_perplexity=18.659678, train_loss=2.926365

Batch 78020, train_perplexity=18.659678, train_loss=2.926365

Batch 78030, train_perplexity=18.659678, train_loss=2.926365

Batch 78040, train_perplexity=18.659678, train_loss=2.926365

Batch 78050, train_perplexity=18.659674, train_loss=2.9263647

Batch 78060, train_perplexity=18.659678, train_loss=2.926365

Batch 78070, train_perplexity=18.659674, train_loss=2.9263647

Batch 78080, train_perplexity=18.659668, train_loss=2.9263644

Batch 78090, train_perplexity=18.659668, train_loss=2.9263644

Batch 78100, train_perplexity=18.659674, train_loss=2.9263647

Batch 78110, train_perplexity=18.659674, train_loss=2.9263647

Batch 78120, train_perplexity=18.659674, train_loss=2.9263647

Batch 78130, train_perplexity=18.659674, train_loss=2.9263647

Batch 78140, train_perplexity=18.659678, train_loss=2.926365

Batch 78150, train_perplexity=18.659674, train_loss=2.9263647

Batch 78160, train_perplexity=18.659668, train_loss=2.9263644

Batch 78170, train_perplexity=18.659674, train_loss=2.9263647

Batch 78180, train_perplexity=18.659668, train_loss=2.9263644

Batch 78190, train_perplexity=18.659668, train_loss=2.9263644

Batch 78200, train_perplexity=18.659674, train_loss=2.9263647

Batch 78210, train_perplexity=18.659668, train_loss=2.9263644

Batch 78220, train_perplexity=18.659668, train_loss=2.9263644

Batch 78230, train_perplexity=18.659668, train_loss=2.9263644

Batch 78240, train_perplexity=18.659668, train_loss=2.9263644
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78250, train_perplexity=18.659668, train_loss=2.9263644

Batch 78260, train_perplexity=18.659668, train_loss=2.9263644

Batch 78270, train_perplexity=18.659674, train_loss=2.9263647

Batch 78280, train_perplexity=18.659678, train_loss=2.926365

Batch 78290, train_perplexity=18.65966, train_loss=2.926364

Batch 78300, train_perplexity=18.659668, train_loss=2.9263644

Batch 78310, train_perplexity=18.659668, train_loss=2.9263644

Batch 78320, train_perplexity=18.659668, train_loss=2.9263644

Batch 78330, train_perplexity=18.659668, train_loss=2.9263644

Batch 78340, train_perplexity=18.659664, train_loss=2.9263642

Batch 78350, train_perplexity=18.659668, train_loss=2.9263644

Batch 78360, train_perplexity=18.659668, train_loss=2.9263644

Batch 78370, train_perplexity=18.659668, train_loss=2.9263644

Batch 78380, train_perplexity=18.659668, train_loss=2.9263644

Batch 78390, train_perplexity=18.659668, train_loss=2.9263644

Batch 78400, train_perplexity=18.659664, train_loss=2.9263642

Batch 78410, train_perplexity=18.659668, train_loss=2.9263644

Batch 78420, train_perplexity=18.659668, train_loss=2.9263644

Batch 78430, train_perplexity=18.65966, train_loss=2.926364

Batch 78440, train_perplexity=18.65966, train_loss=2.926364

Batch 78450, train_perplexity=18.65966, train_loss=2.926364

Batch 78460, train_perplexity=18.659668, train_loss=2.9263644

Batch 78470, train_perplexity=18.659664, train_loss=2.9263642

Batch 78480, train_perplexity=18.659664, train_loss=2.9263642

Batch 78490, train_perplexity=18.65966, train_loss=2.926364

Batch 78500, train_perplexity=18.65966, train_loss=2.926364

Batch 78510, train_perplexity=18.65966, train_loss=2.926364

Batch 78520, train_perplexity=18.65966, train_loss=2.926364

Batch 78530, train_perplexity=18.65966, train_loss=2.926364

Batch 78540, train_perplexity=18.659664, train_loss=2.9263642

Batch 78550, train_perplexity=18.659668, train_loss=2.9263644

Batch 78560, train_perplexity=18.65966, train_loss=2.926364

Batch 78570, train_perplexity=18.65966, train_loss=2.926364

Batch 78580, train_perplexity=18.65966, train_loss=2.926364

Batch 78590, train_perplexity=18.65966, train_loss=2.926364

Batch 78600, train_perplexity=18.65966, train_loss=2.926364

Batch 78610, train_perplexity=18.659655, train_loss=2.9263637

Batch 78620, train_perplexity=18.659655, train_loss=2.9263637

Batch 78630, train_perplexity=18.65966, train_loss=2.926364

Batch 78640, train_perplexity=18.65966, train_loss=2.926364

Batch 78650, train_perplexity=18.65966, train_loss=2.926364

Batch 78660, train_perplexity=18.659664, train_loss=2.9263642

Batch 78670, train_perplexity=18.65966, train_loss=2.926364

Batch 78680, train_perplexity=18.65966, train_loss=2.926364

Batch 78690, train_perplexity=18.65966, train_loss=2.926364

Batch 78700, train_perplexity=18.659655, train_loss=2.9263637

Batch 78710, train_perplexity=18.65966, train_loss=2.926364

Batch 78720, train_perplexity=18.65965, train_loss=2.9263635

Batch 78730, train_perplexity=18.65965, train_loss=2.9263635

Batch 78740, train_perplexity=18.65965, train_loss=2.9263635

Batch 78750, train_perplexity=18.659655, train_loss=2.9263637

Batch 78760, train_perplexity=18.659655, train_loss=2.9263637

Batch 78770, train_perplexity=18.65965, train_loss=2.9263635

Batch 78780, train_perplexity=18.659655, train_loss=2.9263637

Batch 78790, train_perplexity=18.65965, train_loss=2.9263635

Batch 78800, train_perplexity=18.65965, train_loss=2.9263635

Batch 78810, train_perplexity=18.65965, train_loss=2.9263635

Batch 78820, train_perplexity=18.65965, train_loss=2.9263635

Batch 78830, train_perplexity=18.65965, train_loss=2.9263635

Batch 78840, train_perplexity=18.65965, train_loss=2.9263635

Batch 78850, train_perplexity=18.65965, train_loss=2.9263635

Batch 78860, train_perplexity=18.65965, train_loss=2.9263635

Batch 78870, train_perplexity=18.65965, train_loss=2.9263635

Batch 78880, train_perplexity=18.65965, train_loss=2.9263635

Batch 78890, train_perplexity=18.659655, train_loss=2.9263637

Batch 78900, train_perplexity=18.65965, train_loss=2.9263635

Batch 78910, train_perplexity=18.65965, train_loss=2.9263635

Batch 78920, train_perplexity=18.65965, train_loss=2.9263635

Batch 78930, train_perplexity=18.659647, train_loss=2.9263632

Batch 78940, train_perplexity=18.65965, train_loss=2.9263635

Batch 78950, train_perplexity=18.65965, train_loss=2.9263635

Batch 78960, train_perplexity=18.65965, train_loss=2.9263635

Batch 78970, train_perplexity=18.65965, train_loss=2.9263635

Batch 78980, train_perplexity=18.65965, train_loss=2.9263635

Batch 78990, train_perplexity=18.659641, train_loss=2.926363

Batch 79000, train_perplexity=18.65965, train_loss=2.9263635

Batch 79010, train_perplexity=18.65965, train_loss=2.9263635

Batch 79020, train_perplexity=18.65965, train_loss=2.9263635

Batch 79030, train_perplexity=18.659641, train_loss=2.926363

Batch 79040, train_perplexity=18.65965, train_loss=2.9263635

Batch 79050, train_perplexity=18.659647, train_loss=2.9263632

Batch 79060, train_perplexity=18.659641, train_loss=2.926363

Batch 79070, train_perplexity=18.659641, train_loss=2.926363

Batch 79080, train_perplexity=18.659641, train_loss=2.926363

Batch 79090, train_perplexity=18.659641, train_loss=2.926363

Batch 79100, train_perplexity=18.659641, train_loss=2.926363

Batch 79110, train_perplexity=18.659641, train_loss=2.926363

Batch 79120, train_perplexity=18.659641, train_loss=2.926363

Batch 79130, train_perplexity=18.659641, train_loss=2.926363

Batch 79140, train_perplexity=18.659641, train_loss=2.926363

Batch 79150, train_perplexity=18.659641, train_loss=2.926363

Batch 79160, train_perplexity=18.659641, train_loss=2.926363

Batch 79170, train_perplexity=18.659641, train_loss=2.926363

Batch 79180, train_perplexity=18.659641, train_loss=2.926363

Batch 79190, train_perplexity=18.659641, train_loss=2.926363

Batch 79200, train_perplexity=18.659641, train_loss=2.926363

Batch 79210, train_perplexity=18.659641, train_loss=2.926363

Batch 79220, train_perplexity=18.659641, train_loss=2.926363

Batch 79230, train_perplexity=18.659641, train_loss=2.926363

Batch 79240, train_perplexity=18.659641, train_loss=2.926363

Batch 79250, train_perplexity=18.659641, train_loss=2.926363

Batch 79260, train_perplexity=18.659647, train_loss=2.9263632

Batch 79270, train_perplexity=18.659641, train_loss=2.926363

Batch 79280, train_perplexity=18.659641, train_loss=2.926363

Batch 79290, train_perplexity=18.659641, train_loss=2.926363

Batch 79300, train_perplexity=18.659637, train_loss=2.9263628

Batch 79310, train_perplexity=18.659641, train_loss=2.926363

Batch 79320, train_perplexity=18.659641, train_loss=2.926363

Batch 79330, train_perplexity=18.659641, train_loss=2.926363

Batch 79340, train_perplexity=18.659637, train_loss=2.9263628

Batch 79350, train_perplexity=18.659634, train_loss=2.9263625

Batch 79360, train_perplexity=18.659634, train_loss=2.9263625

Batch 79370, train_perplexity=18.659637, train_loss=2.9263628

Batch 79380, train_perplexity=18.659634, train_loss=2.9263625

Batch 79390, train_perplexity=18.659634, train_loss=2.9263625

Batch 79400, train_perplexity=18.659637, train_loss=2.9263628

Batch 79410, train_perplexity=18.659634, train_loss=2.9263625

Batch 79420, train_perplexity=18.659641, train_loss=2.926363

Batch 79430, train_perplexity=18.659634, train_loss=2.9263625

Batch 79440, train_perplexity=18.659634, train_loss=2.9263625

Batch 79450, train_perplexity=18.659634, train_loss=2.9263625

Batch 79460, train_perplexity=18.659634, train_loss=2.9263625

Batch 79470, train_perplexity=18.659634, train_loss=2.9263625

Batch 79480, train_perplexity=18.659637, train_loss=2.9263628

Batch 79490, train_perplexity=18.659634, train_loss=2.9263625

Batch 79500, train_perplexity=18.659634, train_loss=2.9263625

Batch 79510, train_perplexity=18.659634, train_loss=2.9263625

Batch 79520, train_perplexity=18.659634, train_loss=2.9263625

Batch 79530, train_perplexity=18.659634, train_loss=2.9263625

Batch 79540, train_perplexity=18.659634, train_loss=2.9263625

Batch 79550, train_perplexity=18.659634, train_loss=2.9263625

Batch 79560, train_perplexity=18.659634, train_loss=2.9263625
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 79570, train_perplexity=18.659628, train_loss=2.9263623

Batch 79580, train_perplexity=18.659624, train_loss=2.926362

Batch 79590, train_perplexity=18.659628, train_loss=2.9263623

Batch 79600, train_perplexity=18.659634, train_loss=2.9263625

Batch 79610, train_perplexity=18.659634, train_loss=2.9263625

Batch 79620, train_perplexity=18.659624, train_loss=2.926362

Batch 79630, train_perplexity=18.659624, train_loss=2.926362

Batch 79640, train_perplexity=18.659628, train_loss=2.9263623

Batch 79650, train_perplexity=18.659624, train_loss=2.926362

Batch 79660, train_perplexity=18.659634, train_loss=2.9263625

Batch 79670, train_perplexity=18.659624, train_loss=2.926362

Batch 79680, train_perplexity=18.659624, train_loss=2.926362

Batch 79690, train_perplexity=18.659628, train_loss=2.9263623

Batch 79700, train_perplexity=18.659624, train_loss=2.926362

Batch 79710, train_perplexity=18.659624, train_loss=2.926362

Batch 79720, train_perplexity=18.659624, train_loss=2.926362

Batch 79730, train_perplexity=18.659624, train_loss=2.926362

Batch 79740, train_perplexity=18.659628, train_loss=2.9263623

Batch 79750, train_perplexity=18.659624, train_loss=2.926362

Batch 79760, train_perplexity=18.659624, train_loss=2.926362

Batch 79770, train_perplexity=18.659624, train_loss=2.926362

Batch 79780, train_perplexity=18.659624, train_loss=2.926362

Batch 79790, train_perplexity=18.659624, train_loss=2.926362

Batch 79800, train_perplexity=18.659624, train_loss=2.926362

Batch 79810, train_perplexity=18.659615, train_loss=2.9263616

Batch 79820, train_perplexity=18.659624, train_loss=2.926362

Batch 79830, train_perplexity=18.65962, train_loss=2.9263618

Batch 79840, train_perplexity=18.659624, train_loss=2.926362

Batch 79850, train_perplexity=18.659624, train_loss=2.926362

Batch 79860, train_perplexity=18.659624, train_loss=2.926362

Batch 79870, train_perplexity=18.65962, train_loss=2.9263618

Batch 79880, train_perplexity=18.659624, train_loss=2.926362

Batch 79890, train_perplexity=18.659624, train_loss=2.926362

Batch 79900, train_perplexity=18.659615, train_loss=2.9263616

Batch 79910, train_perplexity=18.659624, train_loss=2.926362

Batch 79920, train_perplexity=18.65962, train_loss=2.9263618

Batch 79930, train_perplexity=18.659624, train_loss=2.926362

Batch 79940, train_perplexity=18.659615, train_loss=2.9263616

Batch 79950, train_perplexity=18.65962, train_loss=2.9263618

Batch 79960, train_perplexity=18.65962, train_loss=2.9263618

Batch 79970, train_perplexity=18.659615, train_loss=2.9263616

Batch 79980, train_perplexity=18.659615, train_loss=2.9263616

Batch 79990, train_perplexity=18.65962, train_loss=2.9263618

Batch 80000, train_perplexity=18.659615, train_loss=2.9263616

Batch 80010, train_perplexity=18.659615, train_loss=2.9263616

Batch 80020, train_perplexity=18.659615, train_loss=2.9263616

Batch 80030, train_perplexity=18.659615, train_loss=2.9263616

Batch 80040, train_perplexity=18.659615, train_loss=2.9263616

Batch 80050, train_perplexity=18.659615, train_loss=2.9263616

Batch 80060, train_perplexity=18.65962, train_loss=2.9263618

Batch 80070, train_perplexity=18.659615, train_loss=2.9263616

Batch 80080, train_perplexity=18.659615, train_loss=2.9263616

Batch 80090, train_perplexity=18.659615, train_loss=2.9263616

Batch 80100, train_perplexity=18.659615, train_loss=2.9263616

Batch 80110, train_perplexity=18.659615, train_loss=2.9263616

Batch 80120, train_perplexity=18.65961, train_loss=2.9263613

Batch 80130, train_perplexity=18.65961, train_loss=2.9263613

Batch 80140, train_perplexity=18.659615, train_loss=2.9263616

Batch 80150, train_perplexity=18.659615, train_loss=2.9263616

Batch 80160, train_perplexity=18.659615, train_loss=2.9263616

Batch 80170, train_perplexity=18.659615, train_loss=2.9263616

Batch 80180, train_perplexity=18.659615, train_loss=2.9263616

Batch 80190, train_perplexity=18.659615, train_loss=2.9263616

Batch 80200, train_perplexity=18.659615, train_loss=2.9263616

Batch 80210, train_perplexity=18.65961, train_loss=2.9263613

Batch 80220, train_perplexity=18.659615, train_loss=2.9263616

Batch 80230, train_perplexity=18.659615, train_loss=2.9263616

Batch 80240, train_perplexity=18.659615, train_loss=2.9263616

Batch 80250, train_perplexity=18.659615, train_loss=2.9263616

Batch 80260, train_perplexity=18.65961, train_loss=2.9263613

Batch 80270, train_perplexity=18.65961, train_loss=2.9263613

Batch 80280, train_perplexity=18.659607, train_loss=2.926361

Batch 80290, train_perplexity=18.659607, train_loss=2.926361

Batch 80300, train_perplexity=18.659607, train_loss=2.926361

Batch 80310, train_perplexity=18.659607, train_loss=2.926361

Batch 80320, train_perplexity=18.659607, train_loss=2.926361

Batch 80330, train_perplexity=18.659607, train_loss=2.926361

Batch 80340, train_perplexity=18.659607, train_loss=2.926361

Batch 80350, train_perplexity=18.659607, train_loss=2.926361

Batch 80360, train_perplexity=18.659607, train_loss=2.926361

Batch 80370, train_perplexity=18.659607, train_loss=2.926361

Batch 80380, train_perplexity=18.659607, train_loss=2.926361

Batch 80390, train_perplexity=18.659607, train_loss=2.926361

Batch 80400, train_perplexity=18.659607, train_loss=2.926361

Batch 80410, train_perplexity=18.659607, train_loss=2.926361

Batch 80420, train_perplexity=18.659601, train_loss=2.9263608

Batch 80430, train_perplexity=18.659607, train_loss=2.926361

Batch 80440, train_perplexity=18.659607, train_loss=2.926361

Batch 80450, train_perplexity=18.659607, train_loss=2.926361

Batch 80460, train_perplexity=18.659601, train_loss=2.9263608

Batch 80470, train_perplexity=18.659607, train_loss=2.926361

Batch 80480, train_perplexity=18.659601, train_loss=2.9263608

Batch 80490, train_perplexity=18.659607, train_loss=2.926361

Batch 80500, train_perplexity=18.659601, train_loss=2.9263608

Batch 80510, train_perplexity=18.659601, train_loss=2.9263608

Batch 80520, train_perplexity=18.659597, train_loss=2.9263606

Batch 80530, train_perplexity=18.659607, train_loss=2.926361

Batch 80540, train_perplexity=18.659597, train_loss=2.9263606

Batch 80550, train_perplexity=18.659607, train_loss=2.926361

Batch 80560, train_perplexity=18.659601, train_loss=2.9263608

Batch 80570, train_perplexity=18.659601, train_loss=2.9263608

Batch 80580, train_perplexity=18.659597, train_loss=2.9263606

Batch 80590, train_perplexity=18.659597, train_loss=2.9263606

Batch 80600, train_perplexity=18.659601, train_loss=2.9263608

Batch 80610, train_perplexity=18.659597, train_loss=2.9263606

Batch 80620, train_perplexity=18.659597, train_loss=2.9263606

Batch 80630, train_perplexity=18.659597, train_loss=2.9263606

Batch 80640, train_perplexity=18.659597, train_loss=2.9263606

Batch 80650, train_perplexity=18.659597, train_loss=2.9263606

Batch 80660, train_perplexity=18.659597, train_loss=2.9263606

Batch 80670, train_perplexity=18.659601, train_loss=2.9263608

Batch 80680, train_perplexity=18.659597, train_loss=2.9263606

Batch 80690, train_perplexity=18.659597, train_loss=2.9263606

Batch 80700, train_perplexity=18.659597, train_loss=2.9263606

Batch 80710, train_perplexity=18.659597, train_loss=2.9263606

Batch 80720, train_perplexity=18.659597, train_loss=2.9263606

Batch 80730, train_perplexity=18.659597, train_loss=2.9263606

Batch 80740, train_perplexity=18.659594, train_loss=2.9263604

Batch 80750, train_perplexity=18.659597, train_loss=2.9263606

Batch 80760, train_perplexity=18.659597, train_loss=2.9263606

Batch 80770, train_perplexity=18.659594, train_loss=2.9263604

Batch 80780, train_perplexity=18.659594, train_loss=2.9263604

Batch 80790, train_perplexity=18.659597, train_loss=2.9263606

Batch 80800, train_perplexity=18.659597, train_loss=2.9263606

Batch 80810, train_perplexity=18.659597, train_loss=2.9263606

Batch 80820, train_perplexity=18.659594, train_loss=2.9263604

Batch 80830, train_perplexity=18.659588, train_loss=2.9263601

Batch 80840, train_perplexity=18.659597, train_loss=2.9263606

Batch 80850, train_perplexity=18.659588, train_loss=2.9263601

Batch 80860, train_perplexity=18.659588, train_loss=2.9263601

Batch 80870, train_perplexity=18.659594, train_loss=2.9263604
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 80880, train_perplexity=18.659594, train_loss=2.9263604

Batch 80890, train_perplexity=18.659588, train_loss=2.9263601

Batch 80900, train_perplexity=18.659588, train_loss=2.9263601

Batch 80910, train_perplexity=18.659588, train_loss=2.9263601

Batch 80920, train_perplexity=18.659588, train_loss=2.9263601

Batch 80930, train_perplexity=18.659594, train_loss=2.9263604

Batch 80940, train_perplexity=18.659588, train_loss=2.9263601

Batch 80950, train_perplexity=18.659588, train_loss=2.9263601

Batch 80960, train_perplexity=18.659588, train_loss=2.9263601

Batch 80970, train_perplexity=18.659588, train_loss=2.9263601

Batch 80980, train_perplexity=18.659588, train_loss=2.9263601

Batch 80990, train_perplexity=18.659588, train_loss=2.9263601

Batch 81000, train_perplexity=18.659588, train_loss=2.9263601

Batch 81010, train_perplexity=18.659588, train_loss=2.9263601

Batch 81020, train_perplexity=18.659588, train_loss=2.9263601

Batch 81030, train_perplexity=18.659584, train_loss=2.92636

Batch 81040, train_perplexity=18.659588, train_loss=2.9263601

Batch 81050, train_perplexity=18.659588, train_loss=2.9263601

Batch 81060, train_perplexity=18.659588, train_loss=2.9263601

Batch 81070, train_perplexity=18.659588, train_loss=2.9263601

Batch 81080, train_perplexity=18.659588, train_loss=2.9263601

Batch 81090, train_perplexity=18.659584, train_loss=2.92636

Batch 81100, train_perplexity=18.659588, train_loss=2.9263601

Batch 81110, train_perplexity=18.659588, train_loss=2.9263601

Batch 81120, train_perplexity=18.659588, train_loss=2.9263601

Batch 81130, train_perplexity=18.65958, train_loss=2.9263597

Batch 81140, train_perplexity=18.659584, train_loss=2.92636

Batch 81150, train_perplexity=18.65958, train_loss=2.9263597

Batch 81160, train_perplexity=18.65958, train_loss=2.9263597

Batch 81170, train_perplexity=18.659584, train_loss=2.92636

Batch 81180, train_perplexity=18.65958, train_loss=2.9263597

Batch 81190, train_perplexity=18.65958, train_loss=2.9263597

Batch 81200, train_perplexity=18.65958, train_loss=2.9263597

Batch 81210, train_perplexity=18.65958, train_loss=2.9263597

Batch 81220, train_perplexity=18.659584, train_loss=2.92636

Batch 81230, train_perplexity=18.65958, train_loss=2.9263597

Batch 81240, train_perplexity=18.65958, train_loss=2.9263597

Batch 81250, train_perplexity=18.65958, train_loss=2.9263597

Batch 81260, train_perplexity=18.65958, train_loss=2.9263597

Batch 81270, train_perplexity=18.65958, train_loss=2.9263597

Batch 81280, train_perplexity=18.65958, train_loss=2.9263597

Batch 81290, train_perplexity=18.65958, train_loss=2.9263597

Batch 81300, train_perplexity=18.65958, train_loss=2.9263597

Batch 81310, train_perplexity=18.65958, train_loss=2.9263597

Batch 81320, train_perplexity=18.65958, train_loss=2.9263597

Batch 81330, train_perplexity=18.65958, train_loss=2.9263597

Batch 81340, train_perplexity=18.65958, train_loss=2.9263597

Batch 81350, train_perplexity=18.65958, train_loss=2.9263597

Batch 81360, train_perplexity=18.65958, train_loss=2.9263597

Batch 81370, train_perplexity=18.659575, train_loss=2.9263594

Batch 81380, train_perplexity=18.65958, train_loss=2.9263597

Batch 81390, train_perplexity=18.659575, train_loss=2.9263594

Batch 81400, train_perplexity=18.65958, train_loss=2.9263597

Batch 81410, train_perplexity=18.65957, train_loss=2.9263592

Batch 81420, train_perplexity=18.65958, train_loss=2.9263597

Batch 81430, train_perplexity=18.65958, train_loss=2.9263597

Batch 81440, train_perplexity=18.65957, train_loss=2.9263592

Batch 81450, train_perplexity=18.659575, train_loss=2.9263594

Batch 81460, train_perplexity=18.65957, train_loss=2.9263592

Batch 81470, train_perplexity=18.65958, train_loss=2.9263597

Batch 81480, train_perplexity=18.659575, train_loss=2.9263594

Batch 81490, train_perplexity=18.659575, train_loss=2.9263594

Batch 81500, train_perplexity=18.65957, train_loss=2.9263592

Batch 81510, train_perplexity=18.65957, train_loss=2.9263592

Batch 81520, train_perplexity=18.65957, train_loss=2.9263592

Batch 81530, train_perplexity=18.65957, train_loss=2.9263592

Batch 81540, train_perplexity=18.65957, train_loss=2.9263592

Batch 81550, train_perplexity=18.65957, train_loss=2.9263592

Batch 81560, train_perplexity=18.65957, train_loss=2.9263592

Batch 81570, train_perplexity=18.65957, train_loss=2.9263592

Batch 81580, train_perplexity=18.65957, train_loss=2.9263592

Batch 81590, train_perplexity=18.65957, train_loss=2.9263592

Batch 81600, train_perplexity=18.65957, train_loss=2.9263592

Batch 81610, train_perplexity=18.65957, train_loss=2.9263592

Batch 81620, train_perplexity=18.65957, train_loss=2.9263592

Batch 81630, train_perplexity=18.65957, train_loss=2.9263592

Batch 81640, train_perplexity=18.659567, train_loss=2.926359

Batch 81650, train_perplexity=18.659561, train_loss=2.9263587

Batch 81660, train_perplexity=18.659567, train_loss=2.926359

Batch 81670, train_perplexity=18.65957, train_loss=2.9263592

Batch 81680, train_perplexity=18.65957, train_loss=2.9263592

Batch 81690, train_perplexity=18.65957, train_loss=2.9263592

Batch 81700, train_perplexity=18.659561, train_loss=2.9263587

Batch 81710, train_perplexity=18.659561, train_loss=2.9263587

Batch 81720, train_perplexity=18.659567, train_loss=2.926359

Batch 81730, train_perplexity=18.659567, train_loss=2.926359

Batch 81740, train_perplexity=18.659561, train_loss=2.9263587

Batch 81750, train_perplexity=18.65957, train_loss=2.9263592

Batch 81760, train_perplexity=18.659567, train_loss=2.926359

Batch 81770, train_perplexity=18.659561, train_loss=2.9263587

Batch 81780, train_perplexity=18.65957, train_loss=2.9263592

Batch 81790, train_perplexity=18.659561, train_loss=2.9263587

Batch 81800, train_perplexity=18.659567, train_loss=2.926359

Batch 81810, train_perplexity=18.659561, train_loss=2.9263587

Batch 81820, train_perplexity=18.659561, train_loss=2.9263587

Batch 81830, train_perplexity=18.659561, train_loss=2.9263587

Batch 81840, train_perplexity=18.659561, train_loss=2.9263587

Batch 81850, train_perplexity=18.659561, train_loss=2.9263587

Batch 81860, train_perplexity=18.659561, train_loss=2.9263587

Batch 81870, train_perplexity=18.659561, train_loss=2.9263587

Batch 81880, train_perplexity=18.659561, train_loss=2.9263587

Batch 81890, train_perplexity=18.659561, train_loss=2.9263587

Batch 81900, train_perplexity=18.659561, train_loss=2.9263587

Batch 81910, train_perplexity=18.659561, train_loss=2.9263587

Batch 81920, train_perplexity=18.659561, train_loss=2.9263587

Batch 81930, train_perplexity=18.659561, train_loss=2.9263587

Batch 81940, train_perplexity=18.659554, train_loss=2.9263582

Batch 81950, train_perplexity=18.659561, train_loss=2.9263587

Batch 81960, train_perplexity=18.659557, train_loss=2.9263585

Batch 81970, train_perplexity=18.659561, train_loss=2.9263587

Batch 81980, train_perplexity=18.659561, train_loss=2.9263587

Batch 81990, train_perplexity=18.659557, train_loss=2.9263585

Batch 82000, train_perplexity=18.659561, train_loss=2.9263587

Batch 82010, train_perplexity=18.659561, train_loss=2.9263587

Batch 82020, train_perplexity=18.659557, train_loss=2.9263585

Batch 82030, train_perplexity=18.659554, train_loss=2.9263582

Batch 82040, train_perplexity=18.659554, train_loss=2.9263582

Batch 82050, train_perplexity=18.659561, train_loss=2.9263587

Batch 82060, train_perplexity=18.659554, train_loss=2.9263582

Batch 82070, train_perplexity=18.659557, train_loss=2.9263585

Batch 82080, train_perplexity=18.659554, train_loss=2.9263582

Batch 82090, train_perplexity=18.659557, train_loss=2.9263585

Batch 82100, train_perplexity=18.659554, train_loss=2.9263582

Batch 82110, train_perplexity=18.659554, train_loss=2.9263582

Batch 82120, train_perplexity=18.659544, train_loss=2.9263577

Batch 82130, train_perplexity=18.659554, train_loss=2.9263582

Batch 82140, train_perplexity=18.659554, train_loss=2.9263582

Batch 82150, train_perplexity=18.659554, train_loss=2.9263582

Batch 82160, train_perplexity=18.659554, train_loss=2.9263582

Batch 82170, train_perplexity=18.659554, train_loss=2.9263582

Batch 82180, train_perplexity=18.659561, train_loss=2.9263587

Batch 82190, train_perplexity=18.659554, train_loss=2.9263582
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82200, train_perplexity=18.659554, train_loss=2.9263582

Batch 82210, train_perplexity=18.659554, train_loss=2.9263582

Batch 82220, train_perplexity=18.659548, train_loss=2.926358

Batch 82230, train_perplexity=18.659554, train_loss=2.9263582

Batch 82240, train_perplexity=18.659544, train_loss=2.9263577

Batch 82250, train_perplexity=18.659554, train_loss=2.9263582

Batch 82260, train_perplexity=18.659554, train_loss=2.9263582

Batch 82270, train_perplexity=18.659554, train_loss=2.9263582

Batch 82280, train_perplexity=18.659554, train_loss=2.9263582

Batch 82290, train_perplexity=18.659554, train_loss=2.9263582

Batch 82300, train_perplexity=18.659554, train_loss=2.9263582

Batch 82310, train_perplexity=18.659544, train_loss=2.9263577

Batch 82320, train_perplexity=18.659554, train_loss=2.9263582

Batch 82330, train_perplexity=18.659544, train_loss=2.9263577

Batch 82340, train_perplexity=18.659544, train_loss=2.9263577

Batch 82350, train_perplexity=18.659554, train_loss=2.9263582

Batch 82360, train_perplexity=18.659544, train_loss=2.9263577

Batch 82370, train_perplexity=18.659544, train_loss=2.9263577

Batch 82380, train_perplexity=18.659548, train_loss=2.926358

Batch 82390, train_perplexity=18.659544, train_loss=2.9263577

Batch 82400, train_perplexity=18.659548, train_loss=2.926358

Batch 82410, train_perplexity=18.659544, train_loss=2.9263577

Batch 82420, train_perplexity=18.659544, train_loss=2.9263577

Batch 82430, train_perplexity=18.659548, train_loss=2.926358

Batch 82440, train_perplexity=18.659544, train_loss=2.9263577

Batch 82450, train_perplexity=18.659544, train_loss=2.9263577

Batch 82460, train_perplexity=18.659544, train_loss=2.9263577

Batch 82470, train_perplexity=18.659544, train_loss=2.9263577

Batch 82480, train_perplexity=18.659544, train_loss=2.9263577

Batch 82490, train_perplexity=18.659544, train_loss=2.9263577

Batch 82500, train_perplexity=18.65954, train_loss=2.9263575

Batch 82510, train_perplexity=18.659544, train_loss=2.9263577

Batch 82520, train_perplexity=18.659544, train_loss=2.9263577

Batch 82530, train_perplexity=18.659544, train_loss=2.9263577

Batch 82540, train_perplexity=18.659544, train_loss=2.9263577

Batch 82550, train_perplexity=18.659534, train_loss=2.9263573

Batch 82560, train_perplexity=18.659544, train_loss=2.9263577

Batch 82570, train_perplexity=18.659544, train_loss=2.9263577

Batch 82580, train_perplexity=18.659544, train_loss=2.9263577

Batch 82590, train_perplexity=18.659544, train_loss=2.9263577

Batch 82600, train_perplexity=18.65954, train_loss=2.9263575

Batch 82610, train_perplexity=18.65954, train_loss=2.9263575

Batch 82620, train_perplexity=18.659544, train_loss=2.9263577

Batch 82630, train_perplexity=18.659534, train_loss=2.9263573

Batch 82640, train_perplexity=18.65954, train_loss=2.9263575

Batch 82650, train_perplexity=18.659534, train_loss=2.9263573

Batch 82660, train_perplexity=18.65954, train_loss=2.9263575

Batch 82670, train_perplexity=18.659534, train_loss=2.9263573

Batch 82680, train_perplexity=18.659544, train_loss=2.9263577

Batch 82690, train_perplexity=18.659534, train_loss=2.9263573

Batch 82700, train_perplexity=18.659534, train_loss=2.9263573

Batch 82710, train_perplexity=18.659534, train_loss=2.9263573

Batch 82720, train_perplexity=18.659534, train_loss=2.9263573

Batch 82730, train_perplexity=18.659534, train_loss=2.9263573

Batch 82740, train_perplexity=18.659534, train_loss=2.9263573

Batch 82750, train_perplexity=18.659534, train_loss=2.9263573

Batch 82760, train_perplexity=18.659534, train_loss=2.9263573

Batch 82770, train_perplexity=18.659534, train_loss=2.9263573

Batch 82780, train_perplexity=18.659534, train_loss=2.9263573

Batch 82790, train_perplexity=18.659534, train_loss=2.9263573

Batch 82800, train_perplexity=18.659534, train_loss=2.9263573

Batch 82810, train_perplexity=18.659534, train_loss=2.9263573

Batch 82820, train_perplexity=18.65953, train_loss=2.926357

Batch 82830, train_perplexity=18.659534, train_loss=2.9263573

Batch 82840, train_perplexity=18.659534, train_loss=2.9263573

Batch 82850, train_perplexity=18.659534, train_loss=2.9263573

Batch 82860, train_perplexity=18.65953, train_loss=2.926357

Batch 82870, train_perplexity=18.659534, train_loss=2.9263573

Batch 82880, train_perplexity=18.659534, train_loss=2.9263573

Batch 82890, train_perplexity=18.659534, train_loss=2.9263573

Batch 82900, train_perplexity=18.659527, train_loss=2.9263568

Batch 82910, train_perplexity=18.659527, train_loss=2.9263568

Batch 82920, train_perplexity=18.65953, train_loss=2.926357

Batch 82930, train_perplexity=18.659534, train_loss=2.9263573

Batch 82940, train_perplexity=18.659527, train_loss=2.9263568

Batch 82950, train_perplexity=18.65953, train_loss=2.926357

Batch 82960, train_perplexity=18.65953, train_loss=2.926357

Batch 82970, train_perplexity=18.659527, train_loss=2.9263568

Batch 82980, train_perplexity=18.659527, train_loss=2.9263568

Batch 82990, train_perplexity=18.659527, train_loss=2.9263568

Batch 83000, train_perplexity=18.65953, train_loss=2.926357

Batch 83010, train_perplexity=18.659527, train_loss=2.9263568

Batch 83020, train_perplexity=18.659527, train_loss=2.9263568

Batch 83030, train_perplexity=18.659527, train_loss=2.9263568

Batch 83040, train_perplexity=18.659527, train_loss=2.9263568

Batch 83050, train_perplexity=18.659527, train_loss=2.9263568

Batch 83060, train_perplexity=18.659527, train_loss=2.9263568

Batch 83070, train_perplexity=18.659527, train_loss=2.9263568

Batch 83080, train_perplexity=18.659527, train_loss=2.9263568

Batch 83090, train_perplexity=18.659521, train_loss=2.9263566

Batch 83100, train_perplexity=18.659527, train_loss=2.9263568

Batch 83110, train_perplexity=18.659527, train_loss=2.9263568

Batch 83120, train_perplexity=18.659527, train_loss=2.9263568

Batch 83130, train_perplexity=18.659521, train_loss=2.9263566

Batch 83140, train_perplexity=18.659521, train_loss=2.9263566

Batch 83150, train_perplexity=18.659527, train_loss=2.9263568

Batch 83160, train_perplexity=18.659521, train_loss=2.9263566

Batch 83170, train_perplexity=18.659527, train_loss=2.9263568

Batch 83180, train_perplexity=18.659521, train_loss=2.9263566

Batch 83190, train_perplexity=18.659527, train_loss=2.9263568

Batch 83200, train_perplexity=18.659521, train_loss=2.9263566

Batch 83210, train_perplexity=18.659517, train_loss=2.9263563

Batch 83220, train_perplexity=18.659527, train_loss=2.9263568

Batch 83230, train_perplexity=18.659517, train_loss=2.9263563

Batch 83240, train_perplexity=18.659521, train_loss=2.9263566

Batch 83250, train_perplexity=18.659517, train_loss=2.9263563

Batch 83260, train_perplexity=18.659517, train_loss=2.9263563

Batch 83270, train_perplexity=18.659517, train_loss=2.9263563

Batch 83280, train_perplexity=18.659517, train_loss=2.9263563

Batch 83290, train_perplexity=18.659517, train_loss=2.9263563

Batch 83300, train_perplexity=18.659517, train_loss=2.9263563

Batch 83310, train_perplexity=18.659517, train_loss=2.9263563

Batch 83320, train_perplexity=18.659517, train_loss=2.9263563

Batch 83330, train_perplexity=18.659517, train_loss=2.9263563

Batch 83340, train_perplexity=18.659517, train_loss=2.9263563

Batch 83350, train_perplexity=18.659517, train_loss=2.9263563

Batch 83360, train_perplexity=18.659517, train_loss=2.9263563

Batch 83370, train_perplexity=18.659517, train_loss=2.9263563

Batch 83380, train_perplexity=18.659517, train_loss=2.9263563

Batch 83390, train_perplexity=18.659517, train_loss=2.9263563

Batch 83400, train_perplexity=18.659517, train_loss=2.9263563

Batch 83410, train_perplexity=18.659508, train_loss=2.9263558

Batch 83420, train_perplexity=18.659517, train_loss=2.9263563

Batch 83430, train_perplexity=18.659513, train_loss=2.926356

Batch 83440, train_perplexity=18.659517, train_loss=2.9263563

Batch 83450, train_perplexity=18.659517, train_loss=2.9263563

Batch 83460, train_perplexity=18.659517, train_loss=2.9263563

Batch 83470, train_perplexity=18.659508, train_loss=2.9263558

Batch 83480, train_perplexity=18.659517, train_loss=2.9263563

Batch 83490, train_perplexity=18.659517, train_loss=2.9263563

Batch 83500, train_perplexity=18.659517, train_loss=2.9263563
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 83510, train_perplexity=18.659508, train_loss=2.9263558

Batch 83520, train_perplexity=18.659508, train_loss=2.9263558

Batch 83530, train_perplexity=18.659513, train_loss=2.926356

Batch 83540, train_perplexity=18.659513, train_loss=2.926356

Batch 83550, train_perplexity=18.659517, train_loss=2.9263563

Batch 83560, train_perplexity=18.659513, train_loss=2.926356

Batch 83570, train_perplexity=18.659508, train_loss=2.9263558

Batch 83580, train_perplexity=18.659508, train_loss=2.9263558

Batch 83590, train_perplexity=18.659508, train_loss=2.9263558

Batch 83600, train_perplexity=18.659513, train_loss=2.926356

Batch 83610, train_perplexity=18.659508, train_loss=2.9263558

Batch 83620, train_perplexity=18.659508, train_loss=2.9263558

Batch 83630, train_perplexity=18.659508, train_loss=2.9263558

Batch 83640, train_perplexity=18.659508, train_loss=2.9263558

Batch 83650, train_perplexity=18.659508, train_loss=2.9263558

Batch 83660, train_perplexity=18.659508, train_loss=2.9263558

Batch 83670, train_perplexity=18.659508, train_loss=2.9263558

Batch 83680, train_perplexity=18.659508, train_loss=2.9263558

Batch 83690, train_perplexity=18.659504, train_loss=2.9263556

Batch 83700, train_perplexity=18.659504, train_loss=2.9263556

Batch 83710, train_perplexity=18.659513, train_loss=2.926356

Batch 83720, train_perplexity=18.659504, train_loss=2.9263556

Batch 83730, train_perplexity=18.659508, train_loss=2.9263558

Batch 83740, train_perplexity=18.659508, train_loss=2.9263558

Batch 83750, train_perplexity=18.659508, train_loss=2.9263558

Batch 83760, train_perplexity=18.6595, train_loss=2.9263554

Batch 83770, train_perplexity=18.659508, train_loss=2.9263558

Batch 83780, train_perplexity=18.659508, train_loss=2.9263558

Batch 83790, train_perplexity=18.6595, train_loss=2.9263554

Batch 83800, train_perplexity=18.659504, train_loss=2.9263556

Batch 83810, train_perplexity=18.659504, train_loss=2.9263556

Batch 83820, train_perplexity=18.659508, train_loss=2.9263558

Batch 83830, train_perplexity=18.659508, train_loss=2.9263558

Batch 83840, train_perplexity=18.659504, train_loss=2.9263556

Batch 83850, train_perplexity=18.6595, train_loss=2.9263554

Batch 83860, train_perplexity=18.6595, train_loss=2.9263554

Batch 83870, train_perplexity=18.6595, train_loss=2.9263554

Batch 83880, train_perplexity=18.6595, train_loss=2.9263554

Batch 83890, train_perplexity=18.659504, train_loss=2.9263556

Batch 83900, train_perplexity=18.6595, train_loss=2.9263554

Batch 83910, train_perplexity=18.6595, train_loss=2.9263554

Batch 83920, train_perplexity=18.659504, train_loss=2.9263556

Batch 83930, train_perplexity=18.6595, train_loss=2.9263554

Batch 83940, train_perplexity=18.6595, train_loss=2.9263554

Batch 83950, train_perplexity=18.659504, train_loss=2.9263556

Batch 83960, train_perplexity=18.6595, train_loss=2.9263554

Batch 83970, train_perplexity=18.6595, train_loss=2.9263554

Batch 83980, train_perplexity=18.6595, train_loss=2.9263554

Batch 83990, train_perplexity=18.6595, train_loss=2.9263554

Batch 84000, train_perplexity=18.6595, train_loss=2.9263554

Batch 84010, train_perplexity=18.659494, train_loss=2.9263551

Batch 84020, train_perplexity=18.6595, train_loss=2.9263554

Batch 84030, train_perplexity=18.6595, train_loss=2.9263554

Batch 84040, train_perplexity=18.6595, train_loss=2.9263554

Batch 84050, train_perplexity=18.6595, train_loss=2.9263554

Batch 84060, train_perplexity=18.6595, train_loss=2.9263554

Batch 84070, train_perplexity=18.6595, train_loss=2.9263554

Batch 84080, train_perplexity=18.659494, train_loss=2.9263551

Batch 84090, train_perplexity=18.6595, train_loss=2.9263554

Batch 84100, train_perplexity=18.6595, train_loss=2.9263554

Batch 84110, train_perplexity=18.659494, train_loss=2.9263551

Batch 84120, train_perplexity=18.6595, train_loss=2.9263554

Batch 84130, train_perplexity=18.65949, train_loss=2.926355

Batch 84140, train_perplexity=18.659494, train_loss=2.9263551

Batch 84150, train_perplexity=18.6595, train_loss=2.9263554

Batch 84160, train_perplexity=18.65949, train_loss=2.926355

Batch 84170, train_perplexity=18.6595, train_loss=2.9263554

Batch 84180, train_perplexity=18.65949, train_loss=2.926355

Batch 84190, train_perplexity=18.659494, train_loss=2.9263551

Batch 84200, train_perplexity=18.65949, train_loss=2.926355

Batch 84210, train_perplexity=18.6595, train_loss=2.9263554

Batch 84220, train_perplexity=18.65949, train_loss=2.926355

Batch 84230, train_perplexity=18.65949, train_loss=2.926355

Batch 84240, train_perplexity=18.65949, train_loss=2.926355

Batch 84250, train_perplexity=18.65949, train_loss=2.926355

Batch 84260, train_perplexity=18.65949, train_loss=2.926355

Batch 84270, train_perplexity=18.65949, train_loss=2.926355

Batch 84280, train_perplexity=18.65949, train_loss=2.926355

Batch 84290, train_perplexity=18.65949, train_loss=2.926355

Batch 84300, train_perplexity=18.65949, train_loss=2.926355

Batch 84310, train_perplexity=18.65949, train_loss=2.926355

Batch 84320, train_perplexity=18.65949, train_loss=2.926355

Batch 84330, train_perplexity=18.65949, train_loss=2.926355

Batch 84340, train_perplexity=18.659487, train_loss=2.9263546

Batch 84350, train_perplexity=18.659487, train_loss=2.9263546

Batch 84360, train_perplexity=18.65949, train_loss=2.926355

Batch 84370, train_perplexity=18.65949, train_loss=2.926355

Batch 84380, train_perplexity=18.65949, train_loss=2.926355

Batch 84390, train_perplexity=18.659487, train_loss=2.9263546

Batch 84400, train_perplexity=18.659481, train_loss=2.9263544

Batch 84410, train_perplexity=18.659487, train_loss=2.9263546

Batch 84420, train_perplexity=18.659481, train_loss=2.9263544

Batch 84430, train_perplexity=18.65949, train_loss=2.926355

Batch 84440, train_perplexity=18.659487, train_loss=2.9263546

Batch 84450, train_perplexity=18.65949, train_loss=2.926355

Batch 84460, train_perplexity=18.659481, train_loss=2.9263544

Batch 84470, train_perplexity=18.65949, train_loss=2.926355

Batch 84480, train_perplexity=18.659481, train_loss=2.9263544

Batch 84490, train_perplexity=18.659487, train_loss=2.9263546

Batch 84500, train_perplexity=18.659481, train_loss=2.9263544

Batch 84510, train_perplexity=18.659481, train_loss=2.9263544

Batch 84520, train_perplexity=18.659481, train_loss=2.9263544

Batch 84530, train_perplexity=18.659481, train_loss=2.9263544

Batch 84540, train_perplexity=18.659481, train_loss=2.9263544

Batch 84550, train_perplexity=18.659481, train_loss=2.9263544

Batch 84560, train_perplexity=18.659481, train_loss=2.9263544

Batch 84570, train_perplexity=18.659481, train_loss=2.9263544

Batch 84580, train_perplexity=18.659481, train_loss=2.9263544

Batch 84590, train_perplexity=18.659481, train_loss=2.9263544

Batch 84600, train_perplexity=18.659481, train_loss=2.9263544

Batch 84610, train_perplexity=18.659481, train_loss=2.9263544

Batch 84620, train_perplexity=18.659481, train_loss=2.9263544

Batch 84630, train_perplexity=18.659481, train_loss=2.9263544

Batch 84640, train_perplexity=18.659481, train_loss=2.9263544

Batch 84650, train_perplexity=18.659477, train_loss=2.9263542

Batch 84660, train_perplexity=18.659481, train_loss=2.9263544

Batch 84670, train_perplexity=18.659481, train_loss=2.9263544

Batch 84680, train_perplexity=18.659481, train_loss=2.9263544

Batch 84690, train_perplexity=18.659481, train_loss=2.9263544

Batch 84700, train_perplexity=18.659481, train_loss=2.9263544

Batch 84710, train_perplexity=18.659481, train_loss=2.9263544

Batch 84720, train_perplexity=18.659473, train_loss=2.926354

Batch 84730, train_perplexity=18.659477, train_loss=2.9263542

Batch 84740, train_perplexity=18.659473, train_loss=2.926354

Batch 84750, train_perplexity=18.659473, train_loss=2.926354

Batch 84760, train_perplexity=18.659477, train_loss=2.9263542

Batch 84770, train_perplexity=18.659473, train_loss=2.926354

Batch 84780, train_perplexity=18.659473, train_loss=2.926354

Batch 84790, train_perplexity=18.659473, train_loss=2.926354

Batch 84800, train_perplexity=18.659473, train_loss=2.926354

Batch 84810, train_perplexity=18.659473, train_loss=2.926354

Batch 84820, train_perplexity=18.659473, train_loss=2.926354
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 84830, train_perplexity=18.659473, train_loss=2.926354

Batch 84840, train_perplexity=18.659473, train_loss=2.926354

Batch 84850, train_perplexity=18.659473, train_loss=2.926354

Batch 84860, train_perplexity=18.659473, train_loss=2.926354

Batch 84870, train_perplexity=18.659473, train_loss=2.926354

Batch 84880, train_perplexity=18.659468, train_loss=2.9263537

Batch 84890, train_perplexity=18.659473, train_loss=2.926354

Batch 84900, train_perplexity=18.659473, train_loss=2.926354

Batch 84910, train_perplexity=18.659473, train_loss=2.926354

Batch 84920, train_perplexity=18.659468, train_loss=2.9263537

Batch 84930, train_perplexity=18.659473, train_loss=2.926354

Batch 84940, train_perplexity=18.659473, train_loss=2.926354

Batch 84950, train_perplexity=18.659473, train_loss=2.926354

Batch 84960, train_perplexity=18.659468, train_loss=2.9263537

Batch 84970, train_perplexity=18.659473, train_loss=2.926354

Batch 84980, train_perplexity=18.659473, train_loss=2.926354

Batch 84990, train_perplexity=18.659473, train_loss=2.926354

Batch 85000, train_perplexity=18.659468, train_loss=2.9263537

Batch 85010, train_perplexity=18.659468, train_loss=2.9263537

Batch 85020, train_perplexity=18.659468, train_loss=2.9263537

Batch 85030, train_perplexity=18.659468, train_loss=2.9263537

Batch 85040, train_perplexity=18.659473, train_loss=2.926354

Batch 85050, train_perplexity=18.659468, train_loss=2.9263537

Batch 85060, train_perplexity=18.659468, train_loss=2.9263537

Batch 85070, train_perplexity=18.659464, train_loss=2.9263535

Batch 85080, train_perplexity=18.659464, train_loss=2.9263535

Batch 85090, train_perplexity=18.659464, train_loss=2.9263535

Batch 85100, train_perplexity=18.659468, train_loss=2.9263537

Batch 85110, train_perplexity=18.659468, train_loss=2.9263537

Batch 85120, train_perplexity=18.659464, train_loss=2.9263535

Batch 85130, train_perplexity=18.659464, train_loss=2.9263535

Batch 85140, train_perplexity=18.659464, train_loss=2.9263535

Batch 85150, train_perplexity=18.659464, train_loss=2.9263535

Batch 85160, train_perplexity=18.659468, train_loss=2.9263537

Batch 85170, train_perplexity=18.659464, train_loss=2.9263535

Batch 85180, train_perplexity=18.659464, train_loss=2.9263535

Batch 85190, train_perplexity=18.659464, train_loss=2.9263535

Batch 85200, train_perplexity=18.659464, train_loss=2.9263535

Batch 85210, train_perplexity=18.659464, train_loss=2.9263535

Batch 85220, train_perplexity=18.659464, train_loss=2.9263535

Batch 85230, train_perplexity=18.659464, train_loss=2.9263535

Batch 85240, train_perplexity=18.659464, train_loss=2.9263535

Batch 85250, train_perplexity=18.659464, train_loss=2.9263535

Batch 85260, train_perplexity=18.659464, train_loss=2.9263535

Batch 85270, train_perplexity=18.659464, train_loss=2.9263535

Batch 85280, train_perplexity=18.65946, train_loss=2.9263532

Batch 85290, train_perplexity=18.659454, train_loss=2.926353

Batch 85300, train_perplexity=18.65946, train_loss=2.9263532

Batch 85310, train_perplexity=18.659464, train_loss=2.9263535

Batch 85320, train_perplexity=18.65946, train_loss=2.9263532

Batch 85330, train_perplexity=18.659454, train_loss=2.926353

Batch 85340, train_perplexity=18.659454, train_loss=2.926353

Batch 85350, train_perplexity=18.65946, train_loss=2.9263532

Batch 85360, train_perplexity=18.659464, train_loss=2.9263535

Batch 85370, train_perplexity=18.659454, train_loss=2.926353

Batch 85380, train_perplexity=18.659454, train_loss=2.926353

Batch 85390, train_perplexity=18.65946, train_loss=2.9263532

Batch 85400, train_perplexity=18.659454, train_loss=2.926353

Batch 85410, train_perplexity=18.659454, train_loss=2.926353

Batch 85420, train_perplexity=18.659454, train_loss=2.926353

Batch 85430, train_perplexity=18.659454, train_loss=2.926353

Batch 85440, train_perplexity=18.659454, train_loss=2.926353

Batch 85450, train_perplexity=18.659454, train_loss=2.926353

Batch 85460, train_perplexity=18.659454, train_loss=2.926353

Batch 85470, train_perplexity=18.659454, train_loss=2.926353

Batch 85480, train_perplexity=18.659454, train_loss=2.926353

Batch 85490, train_perplexity=18.659454, train_loss=2.926353

Batch 85500, train_perplexity=18.659454, train_loss=2.926353

Batch 85510, train_perplexity=18.659454, train_loss=2.926353

Batch 85520, train_perplexity=18.659454, train_loss=2.926353

Batch 85530, train_perplexity=18.659454, train_loss=2.926353

Batch 85540, train_perplexity=18.659454, train_loss=2.926353

Batch 85550, train_perplexity=18.659454, train_loss=2.926353

Batch 85560, train_perplexity=18.659454, train_loss=2.926353

Batch 85570, train_perplexity=18.659454, train_loss=2.926353

Batch 85580, train_perplexity=18.659454, train_loss=2.926353

Batch 85590, train_perplexity=18.659454, train_loss=2.926353

Batch 85600, train_perplexity=18.65945, train_loss=2.9263527

Batch 85610, train_perplexity=18.65945, train_loss=2.9263527

Batch 85620, train_perplexity=18.65945, train_loss=2.9263527

Batch 85630, train_perplexity=18.659454, train_loss=2.926353

Batch 85640, train_perplexity=18.65945, train_loss=2.9263527

Batch 85650, train_perplexity=18.65945, train_loss=2.9263527

Batch 85660, train_perplexity=18.65945, train_loss=2.9263527

Batch 85670, train_perplexity=18.65945, train_loss=2.9263527

Batch 85680, train_perplexity=18.65945, train_loss=2.9263527

Batch 85690, train_perplexity=18.659447, train_loss=2.9263525

Batch 85700, train_perplexity=18.659454, train_loss=2.926353

Batch 85710, train_perplexity=18.659441, train_loss=2.9263523

Batch 85720, train_perplexity=18.659447, train_loss=2.9263525

Batch 85730, train_perplexity=18.659447, train_loss=2.9263525

Batch 85740, train_perplexity=18.659447, train_loss=2.9263525

Batch 85750, train_perplexity=18.65945, train_loss=2.9263527

Batch 85760, train_perplexity=18.659447, train_loss=2.9263525

Batch 85770, train_perplexity=18.659447, train_loss=2.9263525

Batch 85780, train_perplexity=18.659447, train_loss=2.9263525

Batch 85790, train_perplexity=18.659447, train_loss=2.9263525

Batch 85800, train_perplexity=18.659447, train_loss=2.9263525

Batch 85810, train_perplexity=18.659447, train_loss=2.9263525

Batch 85820, train_perplexity=18.659447, train_loss=2.9263525

Batch 85830, train_perplexity=18.659447, train_loss=2.9263525

Batch 85840, train_perplexity=18.659447, train_loss=2.9263525

Batch 85850, train_perplexity=18.659447, train_loss=2.9263525

Batch 85860, train_perplexity=18.659447, train_loss=2.9263525

Batch 85870, train_perplexity=18.659447, train_loss=2.9263525

Batch 85880, train_perplexity=18.659441, train_loss=2.9263523

Batch 85890, train_perplexity=18.659447, train_loss=2.9263525

Batch 85900, train_perplexity=18.659447, train_loss=2.9263525

Batch 85910, train_perplexity=18.659441, train_loss=2.9263523

Batch 85920, train_perplexity=18.659441, train_loss=2.9263523

Batch 85930, train_perplexity=18.659447, train_loss=2.9263525

Batch 85940, train_perplexity=18.659447, train_loss=2.9263525

Batch 85950, train_perplexity=18.659447, train_loss=2.9263525

Batch 85960, train_perplexity=18.659447, train_loss=2.9263525

Batch 85970, train_perplexity=18.659447, train_loss=2.9263525

Batch 85980, train_perplexity=18.659437, train_loss=2.926352

Batch 85990, train_perplexity=18.659437, train_loss=2.926352

Batch 86000, train_perplexity=18.659447, train_loss=2.9263525

Batch 86010, train_perplexity=18.659447, train_loss=2.9263525

Batch 86020, train_perplexity=18.659437, train_loss=2.926352

Batch 86030, train_perplexity=18.659437, train_loss=2.926352

Batch 86040, train_perplexity=18.659437, train_loss=2.926352

Batch 86050, train_perplexity=18.659437, train_loss=2.926352

Batch 86060, train_perplexity=18.659437, train_loss=2.926352

Batch 86070, train_perplexity=18.659437, train_loss=2.926352

Batch 86080, train_perplexity=18.659437, train_loss=2.926352

Batch 86090, train_perplexity=18.659437, train_loss=2.926352

Batch 86100, train_perplexity=18.659437, train_loss=2.926352

Batch 86110, train_perplexity=18.659437, train_loss=2.926352

Batch 86120, train_perplexity=18.659437, train_loss=2.926352

Batch 86130, train_perplexity=18.659437, train_loss=2.926352

Batch 86140, train_perplexity=18.659437, train_loss=2.926352
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86150, train_perplexity=18.659437, train_loss=2.926352

Batch 86160, train_perplexity=18.659437, train_loss=2.926352

Batch 86170, train_perplexity=18.659437, train_loss=2.926352

Batch 86180, train_perplexity=18.659433, train_loss=2.9263518

Batch 86190, train_perplexity=18.659437, train_loss=2.926352

Batch 86200, train_perplexity=18.659433, train_loss=2.9263518

Batch 86210, train_perplexity=18.659433, train_loss=2.9263518

Batch 86220, train_perplexity=18.659433, train_loss=2.9263518

Batch 86230, train_perplexity=18.659433, train_loss=2.9263518

Batch 86240, train_perplexity=18.659433, train_loss=2.9263518

Batch 86250, train_perplexity=18.659437, train_loss=2.926352

Batch 86260, train_perplexity=18.659433, train_loss=2.9263518

Batch 86270, train_perplexity=18.659433, train_loss=2.9263518

Batch 86280, train_perplexity=18.659437, train_loss=2.926352

Batch 86290, train_perplexity=18.659437, train_loss=2.926352

Batch 86300, train_perplexity=18.659428, train_loss=2.9263515

Batch 86310, train_perplexity=18.659433, train_loss=2.9263518

Batch 86320, train_perplexity=18.659428, train_loss=2.9263515

Batch 86330, train_perplexity=18.659428, train_loss=2.9263515

Batch 86340, train_perplexity=18.659428, train_loss=2.9263515

Batch 86350, train_perplexity=18.659428, train_loss=2.9263515

Batch 86360, train_perplexity=18.659428, train_loss=2.9263515

Batch 86370, train_perplexity=18.659428, train_loss=2.9263515

Batch 86380, train_perplexity=18.659428, train_loss=2.9263515

Batch 86390, train_perplexity=18.659428, train_loss=2.9263515

Batch 86400, train_perplexity=18.659428, train_loss=2.9263515

Batch 86410, train_perplexity=18.659428, train_loss=2.9263515

Batch 86420, train_perplexity=18.659428, train_loss=2.9263515

Batch 86430, train_perplexity=18.659428, train_loss=2.9263515

Batch 86440, train_perplexity=18.659428, train_loss=2.9263515

Batch 86450, train_perplexity=18.659428, train_loss=2.9263515

Batch 86460, train_perplexity=18.659428, train_loss=2.9263515

Batch 86470, train_perplexity=18.659428, train_loss=2.9263515

Batch 86480, train_perplexity=18.659428, train_loss=2.9263515

Batch 86490, train_perplexity=18.659428, train_loss=2.9263515

Batch 86500, train_perplexity=18.659424, train_loss=2.9263513

Batch 86510, train_perplexity=18.659428, train_loss=2.9263515

Batch 86520, train_perplexity=18.659428, train_loss=2.9263515

Batch 86530, train_perplexity=18.659428, train_loss=2.9263515

Batch 86540, train_perplexity=18.659428, train_loss=2.9263515

Batch 86550, train_perplexity=18.65942, train_loss=2.926351

Batch 86560, train_perplexity=18.659424, train_loss=2.9263513

Batch 86570, train_perplexity=18.65942, train_loss=2.926351

Batch 86580, train_perplexity=18.659428, train_loss=2.9263515

Batch 86590, train_perplexity=18.65942, train_loss=2.926351

Batch 86600, train_perplexity=18.659424, train_loss=2.9263513

Batch 86610, train_perplexity=18.65942, train_loss=2.926351

Batch 86620, train_perplexity=18.659428, train_loss=2.9263515

Batch 86630, train_perplexity=18.65942, train_loss=2.926351

Batch 86640, train_perplexity=18.65942, train_loss=2.926351

Batch 86650, train_perplexity=18.65942, train_loss=2.926351

Batch 86660, train_perplexity=18.65942, train_loss=2.926351

Batch 86670, train_perplexity=18.65942, train_loss=2.926351

Batch 86680, train_perplexity=18.65942, train_loss=2.926351

Batch 86690, train_perplexity=18.65942, train_loss=2.926351

Batch 86700, train_perplexity=18.65942, train_loss=2.926351

Batch 86710, train_perplexity=18.65942, train_loss=2.926351

Batch 86720, train_perplexity=18.65942, train_loss=2.926351

Batch 86730, train_perplexity=18.65942, train_loss=2.926351

Batch 86740, train_perplexity=18.65942, train_loss=2.926351

Batch 86750, train_perplexity=18.65942, train_loss=2.926351

Batch 86760, train_perplexity=18.659414, train_loss=2.9263508

Batch 86770, train_perplexity=18.65942, train_loss=2.926351

Batch 86780, train_perplexity=18.659414, train_loss=2.9263508

Batch 86790, train_perplexity=18.65942, train_loss=2.926351

Batch 86800, train_perplexity=18.65942, train_loss=2.926351

Batch 86810, train_perplexity=18.65941, train_loss=2.9263506

Batch 86820, train_perplexity=18.65942, train_loss=2.926351

Batch 86830, train_perplexity=18.659414, train_loss=2.9263508

Batch 86840, train_perplexity=18.65942, train_loss=2.926351

Batch 86850, train_perplexity=18.65942, train_loss=2.926351

Batch 86860, train_perplexity=18.659414, train_loss=2.9263508

Batch 86870, train_perplexity=18.659414, train_loss=2.9263508

Batch 86880, train_perplexity=18.65941, train_loss=2.9263506

Batch 86890, train_perplexity=18.659414, train_loss=2.9263508

Batch 86900, train_perplexity=18.65941, train_loss=2.9263506

Batch 86910, train_perplexity=18.65941, train_loss=2.9263506

Batch 86920, train_perplexity=18.65941, train_loss=2.9263506

Batch 86930, train_perplexity=18.65941, train_loss=2.9263506

Batch 86940, train_perplexity=18.65941, train_loss=2.9263506

Batch 86950, train_perplexity=18.659414, train_loss=2.9263508

Batch 86960, train_perplexity=18.65941, train_loss=2.9263506

Batch 86970, train_perplexity=18.659414, train_loss=2.9263508

Batch 86980, train_perplexity=18.65941, train_loss=2.9263506

Batch 86990, train_perplexity=18.659414, train_loss=2.9263508

Batch 87000, train_perplexity=18.65941, train_loss=2.9263506

Batch 87010, train_perplexity=18.65941, train_loss=2.9263506

Batch 87020, train_perplexity=18.65941, train_loss=2.9263506

Batch 87030, train_perplexity=18.65941, train_loss=2.9263506

Batch 87040, train_perplexity=18.65941, train_loss=2.9263506

Batch 87050, train_perplexity=18.65941, train_loss=2.9263506

Batch 87060, train_perplexity=18.659407, train_loss=2.9263504

Batch 87070, train_perplexity=18.6594, train_loss=2.92635

Batch 87080, train_perplexity=18.65941, train_loss=2.9263506

Batch 87090, train_perplexity=18.65941, train_loss=2.9263506

Batch 87100, train_perplexity=18.65941, train_loss=2.9263506

Batch 87110, train_perplexity=18.65941, train_loss=2.9263506

Batch 87120, train_perplexity=18.65941, train_loss=2.9263506

Batch 87130, train_perplexity=18.659407, train_loss=2.9263504

Batch 87140, train_perplexity=18.659407, train_loss=2.9263504

Batch 87150, train_perplexity=18.6594, train_loss=2.92635

Batch 87160, train_perplexity=18.659407, train_loss=2.9263504

Batch 87170, train_perplexity=18.65941, train_loss=2.9263506

Batch 87180, train_perplexity=18.6594, train_loss=2.92635

Batch 87190, train_perplexity=18.65941, train_loss=2.9263506

Batch 87200, train_perplexity=18.65941, train_loss=2.9263506

Batch 87210, train_perplexity=18.65941, train_loss=2.9263506

Batch 87220, train_perplexity=18.65941, train_loss=2.9263506

Batch 87230, train_perplexity=18.659407, train_loss=2.9263504

Batch 87240, train_perplexity=18.6594, train_loss=2.92635

Batch 87250, train_perplexity=18.6594, train_loss=2.92635

Batch 87260, train_perplexity=18.6594, train_loss=2.92635

Batch 87270, train_perplexity=18.6594, train_loss=2.92635

Batch 87280, train_perplexity=18.6594, train_loss=2.92635

Batch 87290, train_perplexity=18.6594, train_loss=2.92635

Batch 87300, train_perplexity=18.6594, train_loss=2.92635

Batch 87310, train_perplexity=18.659397, train_loss=2.9263499

Batch 87320, train_perplexity=18.659407, train_loss=2.9263504

Batch 87330, train_perplexity=18.6594, train_loss=2.92635

Batch 87340, train_perplexity=18.659397, train_loss=2.9263499

Batch 87350, train_perplexity=18.6594, train_loss=2.92635

Batch 87360, train_perplexity=18.6594, train_loss=2.92635

Batch 87370, train_perplexity=18.6594, train_loss=2.92635

Batch 87380, train_perplexity=18.6594, train_loss=2.92635

Batch 87390, train_perplexity=18.6594, train_loss=2.92635

Batch 87400, train_perplexity=18.6594, train_loss=2.92635

Batch 87410, train_perplexity=18.659397, train_loss=2.9263499

Batch 87420, train_perplexity=18.6594, train_loss=2.92635

Batch 87430, train_perplexity=18.659313, train_loss=2.9263453

Batch 87440, train_perplexity=18.659597, train_loss=2.9263606

Batch 87450, train_perplexity=18.6595, train_loss=2.9263554

Batch 87460, train_perplexity=18.65945, train_loss=2.9263527

Batch 87470, train_perplexity=18.659428, train_loss=2.9263515
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 87480, train_perplexity=18.65942, train_loss=2.926351

Batch 87490, train_perplexity=18.65941, train_loss=2.9263506

Batch 87500, train_perplexity=18.6594, train_loss=2.92635

Batch 87510, train_perplexity=18.6594, train_loss=2.92635

Batch 87520, train_perplexity=18.6594, train_loss=2.92635

Batch 87530, train_perplexity=18.659397, train_loss=2.9263499

Batch 87540, train_perplexity=18.659393, train_loss=2.9263496

Batch 87550, train_perplexity=18.659393, train_loss=2.9263496

Batch 87560, train_perplexity=18.659393, train_loss=2.9263496

Batch 87570, train_perplexity=18.659393, train_loss=2.9263496

Batch 87580, train_perplexity=18.659393, train_loss=2.9263496

Batch 87590, train_perplexity=18.659393, train_loss=2.9263496

Batch 87600, train_perplexity=18.659388, train_loss=2.9263494

Batch 87610, train_perplexity=18.659384, train_loss=2.9263492

Batch 87620, train_perplexity=18.659393, train_loss=2.9263496

Batch 87630, train_perplexity=18.659393, train_loss=2.9263496

Batch 87640, train_perplexity=18.659388, train_loss=2.9263494

Batch 87650, train_perplexity=18.659388, train_loss=2.9263494

Batch 87660, train_perplexity=18.659384, train_loss=2.9263492

Batch 87670, train_perplexity=18.659393, train_loss=2.9263496

Batch 87680, train_perplexity=18.659384, train_loss=2.9263492

Batch 87690, train_perplexity=18.659384, train_loss=2.9263492

Batch 87700, train_perplexity=18.659393, train_loss=2.9263496

Batch 87710, train_perplexity=18.659384, train_loss=2.9263492

Batch 87720, train_perplexity=18.659384, train_loss=2.9263492

Batch 87730, train_perplexity=18.659388, train_loss=2.9263494

Batch 87740, train_perplexity=18.659384, train_loss=2.9263492

Batch 87750, train_perplexity=18.659384, train_loss=2.9263492

Batch 87760, train_perplexity=18.659388, train_loss=2.9263494

Batch 87770, train_perplexity=18.659384, train_loss=2.9263492

Batch 87780, train_perplexity=18.659384, train_loss=2.9263492

Batch 87790, train_perplexity=18.659384, train_loss=2.9263492

Batch 87800, train_perplexity=18.659384, train_loss=2.9263492

Batch 87810, train_perplexity=18.659393, train_loss=2.9263496

Batch 87820, train_perplexity=18.659384, train_loss=2.9263492

Batch 87830, train_perplexity=18.659384, train_loss=2.9263492

Batch 87840, train_perplexity=18.659384, train_loss=2.9263492

Batch 87850, train_perplexity=18.659393, train_loss=2.9263496

Batch 87860, train_perplexity=18.659393, train_loss=2.9263496

Batch 87870, train_perplexity=18.659384, train_loss=2.9263492

Batch 87880, train_perplexity=18.659384, train_loss=2.9263492

Batch 87890, train_perplexity=18.659384, train_loss=2.9263492

Batch 87900, train_perplexity=18.659384, train_loss=2.9263492

Batch 87910, train_perplexity=18.65938, train_loss=2.926349

Batch 87920, train_perplexity=18.659384, train_loss=2.9263492

Batch 87930, train_perplexity=18.65938, train_loss=2.926349

Batch 87940, train_perplexity=18.65938, train_loss=2.926349

Batch 87950, train_perplexity=18.659384, train_loss=2.9263492

Batch 87960, train_perplexity=18.659384, train_loss=2.9263492

Batch 87970, train_perplexity=18.659384, train_loss=2.9263492

Batch 87980, train_perplexity=18.659384, train_loss=2.9263492

Batch 87990, train_perplexity=18.659374, train_loss=2.9263487

Batch 88000, train_perplexity=18.65938, train_loss=2.926349

Batch 88010, train_perplexity=18.65938, train_loss=2.926349

Batch 88020, train_perplexity=18.65938, train_loss=2.926349

Batch 88030, train_perplexity=18.659374, train_loss=2.9263487

Batch 88040, train_perplexity=18.65938, train_loss=2.926349

Batch 88050, train_perplexity=18.659384, train_loss=2.9263492

Batch 88060, train_perplexity=18.659374, train_loss=2.9263487

Batch 88070, train_perplexity=18.659374, train_loss=2.9263487

Batch 88080, train_perplexity=18.659374, train_loss=2.9263487

Batch 88090, train_perplexity=18.659374, train_loss=2.9263487

Batch 88100, train_perplexity=18.659374, train_loss=2.9263487

Batch 88110, train_perplexity=18.659374, train_loss=2.9263487

Batch 88120, train_perplexity=18.65938, train_loss=2.926349

Batch 88130, train_perplexity=18.659374, train_loss=2.9263487

Batch 88140, train_perplexity=18.659374, train_loss=2.9263487

Batch 88150, train_perplexity=18.65937, train_loss=2.9263484

Batch 88160, train_perplexity=18.659374, train_loss=2.9263487

Batch 88170, train_perplexity=18.65937, train_loss=2.9263484

Batch 88180, train_perplexity=18.65937, train_loss=2.9263484

Batch 88190, train_perplexity=18.659374, train_loss=2.9263487

Batch 88200, train_perplexity=18.65937, train_loss=2.9263484

Batch 88210, train_perplexity=18.65937, train_loss=2.9263484

Batch 88220, train_perplexity=18.659374, train_loss=2.9263487

Batch 88230, train_perplexity=18.65937, train_loss=2.9263484

Batch 88240, train_perplexity=18.65938, train_loss=2.926349

Batch 88250, train_perplexity=18.659374, train_loss=2.9263487

Batch 88260, train_perplexity=18.659374, train_loss=2.9263487

Batch 88270, train_perplexity=18.65937, train_loss=2.9263484

Batch 88280, train_perplexity=18.65937, train_loss=2.9263484

Batch 88290, train_perplexity=18.659374, train_loss=2.9263487

Batch 88300, train_perplexity=18.659374, train_loss=2.9263487

Batch 88310, train_perplexity=18.65937, train_loss=2.9263484

Batch 88320, train_perplexity=18.659367, train_loss=2.9263482

Batch 88330, train_perplexity=18.659367, train_loss=2.9263482

Batch 88340, train_perplexity=18.659367, train_loss=2.9263482

Batch 88350, train_perplexity=18.659367, train_loss=2.9263482

Batch 88360, train_perplexity=18.659367, train_loss=2.9263482

Batch 88370, train_perplexity=18.65937, train_loss=2.9263484

Batch 88380, train_perplexity=18.65937, train_loss=2.9263484

Batch 88390, train_perplexity=18.659374, train_loss=2.9263487

Batch 88400, train_perplexity=18.65937, train_loss=2.9263484

Batch 88410, train_perplexity=18.659367, train_loss=2.9263482

Batch 88420, train_perplexity=18.659367, train_loss=2.9263482

Batch 88430, train_perplexity=18.659367, train_loss=2.9263482

Batch 88440, train_perplexity=18.65936, train_loss=2.926348

Batch 88450, train_perplexity=18.659367, train_loss=2.9263482

Batch 88460, train_perplexity=18.659367, train_loss=2.9263482

Batch 88470, train_perplexity=18.659367, train_loss=2.9263482

Batch 88480, train_perplexity=18.659367, train_loss=2.9263482

Batch 88490, train_perplexity=18.659367, train_loss=2.9263482

Batch 88500, train_perplexity=18.659367, train_loss=2.9263482

Batch 88510, train_perplexity=18.659367, train_loss=2.9263482

Batch 88520, train_perplexity=18.65936, train_loss=2.926348

Batch 88530, train_perplexity=18.659367, train_loss=2.9263482

Batch 88540, train_perplexity=18.65936, train_loss=2.926348

Batch 88550, train_perplexity=18.659367, train_loss=2.9263482

Batch 88560, train_perplexity=18.65936, train_loss=2.926348

Batch 88570, train_perplexity=18.65936, train_loss=2.926348

Batch 88580, train_perplexity=18.659367, train_loss=2.9263482

Batch 88590, train_perplexity=18.659357, train_loss=2.9263477

Batch 88600, train_perplexity=18.65936, train_loss=2.926348

Batch 88610, train_perplexity=18.65936, train_loss=2.926348

Batch 88620, train_perplexity=18.659367, train_loss=2.9263482

Batch 88630, train_perplexity=18.659357, train_loss=2.9263477

Batch 88640, train_perplexity=18.659357, train_loss=2.9263477

Batch 88650, train_perplexity=18.659357, train_loss=2.9263477

Batch 88660, train_perplexity=18.659357, train_loss=2.9263477

Batch 88670, train_perplexity=18.659357, train_loss=2.9263477

Batch 88680, train_perplexity=18.659357, train_loss=2.9263477

Batch 88690, train_perplexity=18.659357, train_loss=2.9263477

Batch 88700, train_perplexity=18.659357, train_loss=2.9263477

Batch 88710, train_perplexity=18.659357, train_loss=2.9263477

Batch 88720, train_perplexity=18.659357, train_loss=2.9263477

Batch 88730, train_perplexity=18.659357, train_loss=2.9263477

Batch 88740, train_perplexity=18.659357, train_loss=2.9263477

Batch 88750, train_perplexity=18.659357, train_loss=2.9263477

Batch 88760, train_perplexity=18.659357, train_loss=2.9263477

Batch 88770, train_perplexity=18.659357, train_loss=2.9263477

Batch 88780, train_perplexity=18.659357, train_loss=2.9263477
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88790, train_perplexity=18.659357, train_loss=2.9263477

Batch 88800, train_perplexity=18.659357, train_loss=2.9263477

Batch 88810, train_perplexity=18.659357, train_loss=2.9263477

Batch 88820, train_perplexity=18.659353, train_loss=2.9263475

Batch 88830, train_perplexity=18.659348, train_loss=2.9263473

Batch 88840, train_perplexity=18.659353, train_loss=2.9263475

Batch 88850, train_perplexity=18.659357, train_loss=2.9263477

Batch 88860, train_perplexity=18.659348, train_loss=2.9263473

Batch 88870, train_perplexity=18.659353, train_loss=2.9263475

Batch 88880, train_perplexity=18.659348, train_loss=2.9263473

Batch 88890, train_perplexity=18.659357, train_loss=2.9263477

Batch 88900, train_perplexity=18.659357, train_loss=2.9263477

Batch 88910, train_perplexity=18.659353, train_loss=2.9263475

Batch 88920, train_perplexity=18.659348, train_loss=2.9263473

Batch 88930, train_perplexity=18.659348, train_loss=2.9263473

Batch 88940, train_perplexity=18.659353, train_loss=2.9263475

Batch 88950, train_perplexity=18.659353, train_loss=2.9263475

Batch 88960, train_perplexity=18.659348, train_loss=2.9263473

Batch 88970, train_perplexity=18.659348, train_loss=2.9263473

Batch 88980, train_perplexity=18.659344, train_loss=2.926347

Batch 88990, train_perplexity=18.659348, train_loss=2.9263473

Batch 89000, train_perplexity=18.659348, train_loss=2.9263473

Batch 89010, train_perplexity=18.659348, train_loss=2.9263473

Batch 89020, train_perplexity=18.659348, train_loss=2.9263473

Batch 89030, train_perplexity=18.659348, train_loss=2.9263473

Batch 89040, train_perplexity=18.659348, train_loss=2.9263473

Batch 89050, train_perplexity=18.659348, train_loss=2.9263473

Batch 89060, train_perplexity=18.659353, train_loss=2.9263475

Batch 89070, train_perplexity=18.659348, train_loss=2.9263473

Batch 89080, train_perplexity=18.659348, train_loss=2.9263473

Batch 89090, train_perplexity=18.659353, train_loss=2.9263475

Batch 89100, train_perplexity=18.659348, train_loss=2.9263473

Batch 89110, train_perplexity=18.659344, train_loss=2.926347

Batch 89120, train_perplexity=18.659353, train_loss=2.9263475

Batch 89130, train_perplexity=18.659344, train_loss=2.926347

Batch 89140, train_perplexity=18.659344, train_loss=2.926347

Batch 89150, train_perplexity=18.659344, train_loss=2.926347

Batch 89160, train_perplexity=18.659348, train_loss=2.9263473

Batch 89170, train_perplexity=18.659348, train_loss=2.9263473

Batch 89180, train_perplexity=18.659344, train_loss=2.926347

Batch 89190, train_perplexity=18.659344, train_loss=2.926347

Batch 89200, train_perplexity=18.65934, train_loss=2.9263468

Batch 89210, train_perplexity=18.659344, train_loss=2.926347

Batch 89220, train_perplexity=18.659348, train_loss=2.9263473

Batch 89230, train_perplexity=18.659348, train_loss=2.9263473

Batch 89240, train_perplexity=18.659344, train_loss=2.926347

Batch 89250, train_perplexity=18.65934, train_loss=2.9263468

Batch 89260, train_perplexity=18.659344, train_loss=2.926347

Batch 89270, train_perplexity=18.65934, train_loss=2.9263468

Batch 89280, train_perplexity=18.65934, train_loss=2.9263468

Batch 89290, train_perplexity=18.659348, train_loss=2.9263473

Batch 89300, train_perplexity=18.65934, train_loss=2.9263468

Batch 89310, train_perplexity=18.65934, train_loss=2.9263468

Batch 89320, train_perplexity=18.65934, train_loss=2.9263468

Batch 89330, train_perplexity=18.65934, train_loss=2.9263468

Batch 89340, train_perplexity=18.65934, train_loss=2.9263468

Batch 89350, train_perplexity=18.65934, train_loss=2.9263468

Batch 89360, train_perplexity=18.65934, train_loss=2.9263468

Batch 89370, train_perplexity=18.65934, train_loss=2.9263468

Batch 89380, train_perplexity=18.65934, train_loss=2.9263468

Batch 89390, train_perplexity=18.65934, train_loss=2.9263468

Batch 89400, train_perplexity=18.659334, train_loss=2.9263465

Batch 89410, train_perplexity=18.659334, train_loss=2.9263465

Batch 89420, train_perplexity=18.659334, train_loss=2.9263465

Batch 89430, train_perplexity=18.659334, train_loss=2.9263465

Batch 89440, train_perplexity=18.659334, train_loss=2.9263465

Batch 89450, train_perplexity=18.659334, train_loss=2.9263465

Batch 89460, train_perplexity=18.65934, train_loss=2.9263468

Batch 89470, train_perplexity=18.659334, train_loss=2.9263465

Batch 89480, train_perplexity=18.65933, train_loss=2.9263463

Batch 89490, train_perplexity=18.65934, train_loss=2.9263468

Batch 89500, train_perplexity=18.65933, train_loss=2.9263463

Batch 89510, train_perplexity=18.65933, train_loss=2.9263463

Batch 89520, train_perplexity=18.659334, train_loss=2.9263465

Batch 89530, train_perplexity=18.65933, train_loss=2.9263463

Batch 89540, train_perplexity=18.65933, train_loss=2.9263463

Batch 89550, train_perplexity=18.65933, train_loss=2.9263463

Batch 89560, train_perplexity=18.65933, train_loss=2.9263463

Batch 89570, train_perplexity=18.65933, train_loss=2.9263463

Batch 89580, train_perplexity=18.65933, train_loss=2.9263463

Batch 89590, train_perplexity=18.65933, train_loss=2.9263463

Batch 89600, train_perplexity=18.65933, train_loss=2.9263463

Batch 89610, train_perplexity=18.65933, train_loss=2.9263463

Batch 89620, train_perplexity=18.65933, train_loss=2.9263463

Batch 89630, train_perplexity=18.65933, train_loss=2.9263463

Batch 89640, train_perplexity=18.65933, train_loss=2.9263463

Batch 89650, train_perplexity=18.65933, train_loss=2.9263463

Batch 89660, train_perplexity=18.659334, train_loss=2.9263465

Batch 89670, train_perplexity=18.65933, train_loss=2.9263463

Batch 89680, train_perplexity=18.65933, train_loss=2.9263463

Batch 89690, train_perplexity=18.659334, train_loss=2.9263465

Batch 89700, train_perplexity=18.65933, train_loss=2.9263463

Batch 89710, train_perplexity=18.659327, train_loss=2.926346

Batch 89720, train_perplexity=18.659327, train_loss=2.926346

Batch 89730, train_perplexity=18.659334, train_loss=2.9263465

Batch 89740, train_perplexity=18.65933, train_loss=2.9263463

Batch 89750, train_perplexity=18.65933, train_loss=2.9263463

Batch 89760, train_perplexity=18.65933, train_loss=2.9263463

Batch 89770, train_perplexity=18.65933, train_loss=2.9263463

Batch 89780, train_perplexity=18.659327, train_loss=2.926346

Batch 89790, train_perplexity=18.659327, train_loss=2.926346

Batch 89800, train_perplexity=18.659327, train_loss=2.926346

Batch 89810, train_perplexity=18.659327, train_loss=2.926346

Batch 89820, train_perplexity=18.659327, train_loss=2.926346

Batch 89830, train_perplexity=18.659327, train_loss=2.926346

Batch 89840, train_perplexity=18.65932, train_loss=2.9263458

Batch 89850, train_perplexity=18.659327, train_loss=2.926346

Batch 89860, train_perplexity=18.659327, train_loss=2.926346

Batch 89870, train_perplexity=18.65932, train_loss=2.9263458

Batch 89880, train_perplexity=18.65932, train_loss=2.9263458

Batch 89890, train_perplexity=18.65932, train_loss=2.9263458

Batch 89900, train_perplexity=18.65932, train_loss=2.9263458

Batch 89910, train_perplexity=18.65932, train_loss=2.9263458

Batch 89920, train_perplexity=18.65932, train_loss=2.9263458

Batch 89930, train_perplexity=18.65932, train_loss=2.9263458

Batch 89940, train_perplexity=18.659327, train_loss=2.926346

Batch 89950, train_perplexity=18.65932, train_loss=2.9263458

Batch 89960, train_perplexity=18.65932, train_loss=2.9263458

Batch 89970, train_perplexity=18.65932, train_loss=2.9263458

Batch 89980, train_perplexity=18.65932, train_loss=2.9263458

Batch 89990, train_perplexity=18.65932, train_loss=2.9263458

Batch 90000, train_perplexity=18.65932, train_loss=2.9263458

Batch 90010, train_perplexity=18.659317, train_loss=2.9263456

Batch 90020, train_perplexity=18.65932, train_loss=2.9263458

Batch 90030, train_perplexity=18.65932, train_loss=2.9263458

Batch 90040, train_perplexity=18.659317, train_loss=2.9263456

Batch 90050, train_perplexity=18.65932, train_loss=2.9263458

Batch 90060, train_perplexity=18.65932, train_loss=2.9263458

Batch 90070, train_perplexity=18.659317, train_loss=2.9263456

Batch 90080, train_perplexity=18.659313, train_loss=2.9263453

Batch 90090, train_perplexity=18.659317, train_loss=2.9263456

Batch 90100, train_perplexity=18.659317, train_loss=2.9263456
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 90110, train_perplexity=18.659313, train_loss=2.9263453

Batch 90120, train_perplexity=18.659317, train_loss=2.9263456

Batch 90130, train_perplexity=18.65932, train_loss=2.9263458

Batch 90140, train_perplexity=18.659317, train_loss=2.9263456

Batch 90150, train_perplexity=18.659317, train_loss=2.9263456

Batch 90160, train_perplexity=18.659313, train_loss=2.9263453

Batch 90170, train_perplexity=18.659317, train_loss=2.9263456

Batch 90180, train_perplexity=18.659313, train_loss=2.9263453

Batch 90190, train_perplexity=18.659313, train_loss=2.9263453

Batch 90200, train_perplexity=18.659313, train_loss=2.9263453

Batch 90210, train_perplexity=18.659307, train_loss=2.926345

Batch 90220, train_perplexity=18.659313, train_loss=2.9263453

Batch 90230, train_perplexity=18.659317, train_loss=2.9263456

Batch 90240, train_perplexity=18.659317, train_loss=2.9263456

Batch 90250, train_perplexity=18.659317, train_loss=2.9263456

Batch 90260, train_perplexity=18.659313, train_loss=2.9263453

Batch 90270, train_perplexity=18.659313, train_loss=2.9263453

Batch 90280, train_perplexity=18.659313, train_loss=2.9263453

Batch 90290, train_perplexity=18.659307, train_loss=2.926345

Batch 90300, train_perplexity=18.659307, train_loss=2.926345

Batch 90310, train_perplexity=18.659313, train_loss=2.9263453

Batch 90320, train_perplexity=18.659313, train_loss=2.9263453

Batch 90330, train_perplexity=18.659313, train_loss=2.9263453

Batch 90340, train_perplexity=18.659313, train_loss=2.9263453

Batch 90350, train_perplexity=18.659313, train_loss=2.9263453

Batch 90360, train_perplexity=18.659307, train_loss=2.926345

Batch 90370, train_perplexity=18.659313, train_loss=2.9263453

Batch 90380, train_perplexity=18.659307, train_loss=2.926345

Batch 90390, train_perplexity=18.659307, train_loss=2.926345

Batch 90400, train_perplexity=18.659307, train_loss=2.926345

Batch 90410, train_perplexity=18.659307, train_loss=2.926345

Batch 90420, train_perplexity=18.659307, train_loss=2.926345

Batch 90430, train_perplexity=18.659304, train_loss=2.9263449

Batch 90440, train_perplexity=18.659304, train_loss=2.9263449

Batch 90450, train_perplexity=18.659304, train_loss=2.9263449

Batch 90460, train_perplexity=18.659304, train_loss=2.9263449

Batch 90470, train_perplexity=18.659304, train_loss=2.9263449

Batch 90480, train_perplexity=18.659307, train_loss=2.926345

Batch 90490, train_perplexity=18.659304, train_loss=2.9263449

Batch 90500, train_perplexity=18.659304, train_loss=2.9263449

Batch 90510, train_perplexity=18.659304, train_loss=2.9263449

Batch 90520, train_perplexity=18.659304, train_loss=2.9263449

Batch 90530, train_perplexity=18.659304, train_loss=2.9263449

Batch 90540, train_perplexity=18.659304, train_loss=2.9263449

Batch 90550, train_perplexity=18.6593, train_loss=2.9263446

Batch 90560, train_perplexity=18.659294, train_loss=2.9263444

Batch 90570, train_perplexity=18.659304, train_loss=2.9263449

Batch 90580, train_perplexity=18.659304, train_loss=2.9263449

Batch 90590, train_perplexity=18.659304, train_loss=2.9263449

Batch 90600, train_perplexity=18.659304, train_loss=2.9263449

Batch 90610, train_perplexity=18.6593, train_loss=2.9263446

Batch 90620, train_perplexity=18.659304, train_loss=2.9263449

Batch 90630, train_perplexity=18.659304, train_loss=2.9263449

Batch 90640, train_perplexity=18.659304, train_loss=2.9263449

Batch 90650, train_perplexity=18.6593, train_loss=2.9263446

Batch 90660, train_perplexity=18.659304, train_loss=2.9263449

Batch 90670, train_perplexity=18.659304, train_loss=2.9263449

Batch 90680, train_perplexity=18.659304, train_loss=2.9263449

Batch 90690, train_perplexity=18.659294, train_loss=2.9263444

Batch 90700, train_perplexity=18.6593, train_loss=2.9263446

Batch 90710, train_perplexity=18.659294, train_loss=2.9263444

Batch 90720, train_perplexity=18.659294, train_loss=2.9263444

Batch 90730, train_perplexity=18.6593, train_loss=2.9263446

Batch 90740, train_perplexity=18.659304, train_loss=2.9263449

Batch 90750, train_perplexity=18.659294, train_loss=2.9263444

Batch 90760, train_perplexity=18.659294, train_loss=2.9263444

Batch 90770, train_perplexity=18.6593, train_loss=2.9263446

Batch 90780, train_perplexity=18.65929, train_loss=2.9263442

Batch 90790, train_perplexity=18.6593, train_loss=2.9263446

Batch 90800, train_perplexity=18.6593, train_loss=2.9263446

Batch 90810, train_perplexity=18.659294, train_loss=2.9263444

Batch 90820, train_perplexity=18.659294, train_loss=2.9263444

Batch 90830, train_perplexity=18.659294, train_loss=2.9263444

Batch 90840, train_perplexity=18.6593, train_loss=2.9263446

Batch 90850, train_perplexity=18.659294, train_loss=2.9263444

Batch 90860, train_perplexity=18.659294, train_loss=2.9263444

Batch 90870, train_perplexity=18.659294, train_loss=2.9263444

Batch 90880, train_perplexity=18.65929, train_loss=2.9263442

Batch 90890, train_perplexity=18.659294, train_loss=2.9263444

Batch 90900, train_perplexity=18.659294, train_loss=2.9263444

Batch 90910, train_perplexity=18.659294, train_loss=2.9263444

Batch 90920, train_perplexity=18.65929, train_loss=2.9263442

Batch 90930, train_perplexity=18.659294, train_loss=2.9263444

Batch 90940, train_perplexity=18.6593, train_loss=2.9263446

Batch 90950, train_perplexity=18.659294, train_loss=2.9263444

Batch 90960, train_perplexity=18.65929, train_loss=2.9263442

Batch 90970, train_perplexity=18.65929, train_loss=2.9263442

Batch 90980, train_perplexity=18.65929, train_loss=2.9263442

Batch 90990, train_perplexity=18.65929, train_loss=2.9263442

Batch 91000, train_perplexity=18.659294, train_loss=2.9263444

Batch 91010, train_perplexity=18.65929, train_loss=2.9263442

Batch 91020, train_perplexity=18.659286, train_loss=2.926344

Batch 91030, train_perplexity=18.65929, train_loss=2.9263442

Batch 91040, train_perplexity=18.659286, train_loss=2.926344

Batch 91050, train_perplexity=18.659286, train_loss=2.926344

Batch 91060, train_perplexity=18.659294, train_loss=2.9263444

Batch 91070, train_perplexity=18.65929, train_loss=2.9263442

Batch 91080, train_perplexity=18.659294, train_loss=2.9263444

Batch 91090, train_perplexity=18.659286, train_loss=2.926344

Batch 91100, train_perplexity=18.65929, train_loss=2.9263442

Batch 91110, train_perplexity=18.65929, train_loss=2.9263442

Batch 91120, train_perplexity=18.65929, train_loss=2.9263442

Batch 91130, train_perplexity=18.65929, train_loss=2.9263442

Batch 91140, train_perplexity=18.659286, train_loss=2.926344

Batch 91150, train_perplexity=18.65929, train_loss=2.9263442

Batch 91160, train_perplexity=18.65928, train_loss=2.9263437

Batch 91170, train_perplexity=18.659286, train_loss=2.926344

Batch 91180, train_perplexity=18.659286, train_loss=2.926344

Batch 91190, train_perplexity=18.659286, train_loss=2.926344

Batch 91200, train_perplexity=18.65928, train_loss=2.9263437

Batch 91210, train_perplexity=18.659286, train_loss=2.926344

Batch 91220, train_perplexity=18.659286, train_loss=2.926344

Batch 91230, train_perplexity=18.659286, train_loss=2.926344

Batch 91240, train_perplexity=18.659286, train_loss=2.926344

Batch 91250, train_perplexity=18.659277, train_loss=2.9263434

Batch 91260, train_perplexity=18.65928, train_loss=2.9263437

Batch 91270, train_perplexity=18.65928, train_loss=2.9263437

Batch 91280, train_perplexity=18.659286, train_loss=2.926344

Batch 91290, train_perplexity=18.65928, train_loss=2.9263437

Batch 91300, train_perplexity=18.65928, train_loss=2.9263437

Batch 91310, train_perplexity=18.65928, train_loss=2.9263437

Batch 91320, train_perplexity=18.659277, train_loss=2.9263434

Batch 91330, train_perplexity=18.65928, train_loss=2.9263437

Batch 91340, train_perplexity=18.659277, train_loss=2.9263434

Batch 91350, train_perplexity=18.659277, train_loss=2.9263434

Batch 91360, train_perplexity=18.659277, train_loss=2.9263434

Batch 91370, train_perplexity=18.65928, train_loss=2.9263437

Batch 91380, train_perplexity=18.659277, train_loss=2.9263434

Batch 91390, train_perplexity=18.659277, train_loss=2.9263434

Batch 91400, train_perplexity=18.65928, train_loss=2.9263437

Batch 91410, train_perplexity=18.659277, train_loss=2.9263434

Batch 91420, train_perplexity=18.659277, train_loss=2.9263434
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 91430, train_perplexity=18.659277, train_loss=2.9263434

Batch 91440, train_perplexity=18.65928, train_loss=2.9263437

Batch 91450, train_perplexity=18.659277, train_loss=2.9263434

Batch 91460, train_perplexity=18.659277, train_loss=2.9263434

Batch 91470, train_perplexity=18.659277, train_loss=2.9263434

Batch 91480, train_perplexity=18.659277, train_loss=2.9263434

Batch 91490, train_perplexity=18.659273, train_loss=2.9263432

Batch 91500, train_perplexity=18.659273, train_loss=2.9263432

Batch 91510, train_perplexity=18.659267, train_loss=2.926343

Batch 91520, train_perplexity=18.659277, train_loss=2.9263434

Batch 91530, train_perplexity=18.659267, train_loss=2.926343

Batch 91540, train_perplexity=18.659277, train_loss=2.9263434

Batch 91550, train_perplexity=18.659267, train_loss=2.926343

Batch 91560, train_perplexity=18.659273, train_loss=2.9263432

Batch 91570, train_perplexity=18.659277, train_loss=2.9263434

Batch 91580, train_perplexity=18.659277, train_loss=2.9263434

Batch 91590, train_perplexity=18.659277, train_loss=2.9263434

Batch 91600, train_perplexity=18.659273, train_loss=2.9263432

Batch 91610, train_perplexity=18.659277, train_loss=2.9263434

Batch 91620, train_perplexity=18.659267, train_loss=2.926343

Batch 91630, train_perplexity=18.659277, train_loss=2.9263434

Batch 91640, train_perplexity=18.659267, train_loss=2.926343

Batch 91650, train_perplexity=18.659267, train_loss=2.926343

Batch 91660, train_perplexity=18.659273, train_loss=2.9263432

Batch 91670, train_perplexity=18.659267, train_loss=2.926343

Batch 91680, train_perplexity=18.659267, train_loss=2.926343

Batch 91690, train_perplexity=18.659267, train_loss=2.926343

Batch 91700, train_perplexity=18.659267, train_loss=2.926343

Batch 91710, train_perplexity=18.659267, train_loss=2.926343

Batch 91720, train_perplexity=18.659267, train_loss=2.926343

Batch 91730, train_perplexity=18.659267, train_loss=2.926343

Batch 91740, train_perplexity=18.659267, train_loss=2.926343

Batch 91750, train_perplexity=18.659267, train_loss=2.926343

Batch 91760, train_perplexity=18.659267, train_loss=2.926343

Batch 91770, train_perplexity=18.659273, train_loss=2.9263432

Batch 91780, train_perplexity=18.659267, train_loss=2.926343

Batch 91790, train_perplexity=18.659267, train_loss=2.926343

Batch 91800, train_perplexity=18.659264, train_loss=2.9263427

Batch 91810, train_perplexity=18.65926, train_loss=2.9263425

Batch 91820, train_perplexity=18.65926, train_loss=2.9263425

Batch 91830, train_perplexity=18.65926, train_loss=2.9263425

Batch 91840, train_perplexity=18.65926, train_loss=2.9263425

Batch 91850, train_perplexity=18.65926, train_loss=2.9263425

Batch 91860, train_perplexity=18.659264, train_loss=2.9263427

Batch 91870, train_perplexity=18.65926, train_loss=2.9263425

Batch 91880, train_perplexity=18.65926, train_loss=2.9263425

Batch 91890, train_perplexity=18.659267, train_loss=2.926343

Batch 91900, train_perplexity=18.65926, train_loss=2.9263425

Batch 91910, train_perplexity=18.65926, train_loss=2.9263425

Batch 91920, train_perplexity=18.659264, train_loss=2.9263427

Batch 91930, train_perplexity=18.65926, train_loss=2.9263425

Batch 91940, train_perplexity=18.65926, train_loss=2.9263425

Batch 91950, train_perplexity=18.65926, train_loss=2.9263425

Batch 91960, train_perplexity=18.65926, train_loss=2.9263425

Batch 91970, train_perplexity=18.65926, train_loss=2.9263425

Batch 91980, train_perplexity=18.65926, train_loss=2.9263425

Batch 91990, train_perplexity=18.65926, train_loss=2.9263425

Batch 92000, train_perplexity=18.65926, train_loss=2.9263425

Batch 92010, train_perplexity=18.65926, train_loss=2.9263425

Batch 92020, train_perplexity=18.65926, train_loss=2.9263425

Batch 92030, train_perplexity=18.65926, train_loss=2.9263425

Batch 92040, train_perplexity=18.659267, train_loss=2.926343

Batch 92050, train_perplexity=18.65926, train_loss=2.9263425

Batch 92060, train_perplexity=18.659254, train_loss=2.9263422

Batch 92070, train_perplexity=18.65926, train_loss=2.9263425

Batch 92080, train_perplexity=18.659254, train_loss=2.9263422

Batch 92090, train_perplexity=18.65926, train_loss=2.9263425

Batch 92100, train_perplexity=18.65926, train_loss=2.9263425

Batch 92110, train_perplexity=18.65926, train_loss=2.9263425

Batch 92120, train_perplexity=18.65925, train_loss=2.926342

Batch 92130, train_perplexity=18.65925, train_loss=2.926342

Batch 92140, train_perplexity=18.65925, train_loss=2.926342

Batch 92150, train_perplexity=18.65925, train_loss=2.926342

Batch 92160, train_perplexity=18.65925, train_loss=2.926342

Batch 92170, train_perplexity=18.659254, train_loss=2.9263422

Batch 92180, train_perplexity=18.659254, train_loss=2.9263422

Batch 92190, train_perplexity=18.65925, train_loss=2.926342

Batch 92200, train_perplexity=18.65925, train_loss=2.926342

Batch 92210, train_perplexity=18.65925, train_loss=2.926342

Batch 92220, train_perplexity=18.65925, train_loss=2.926342

Batch 92230, train_perplexity=18.65925, train_loss=2.926342

Batch 92240, train_perplexity=18.65925, train_loss=2.926342

Batch 92250, train_perplexity=18.65925, train_loss=2.926342

Batch 92260, train_perplexity=18.65925, train_loss=2.926342

Batch 92270, train_perplexity=18.65925, train_loss=2.926342

Batch 92280, train_perplexity=18.65925, train_loss=2.926342

Batch 92290, train_perplexity=18.659246, train_loss=2.9263418

Batch 92300, train_perplexity=18.65925, train_loss=2.926342

Batch 92310, train_perplexity=18.65925, train_loss=2.926342

Batch 92320, train_perplexity=18.65925, train_loss=2.926342

Batch 92330, train_perplexity=18.65925, train_loss=2.926342

Batch 92340, train_perplexity=18.659254, train_loss=2.9263422

Batch 92350, train_perplexity=18.659246, train_loss=2.9263418

Batch 92360, train_perplexity=18.65925, train_loss=2.926342

Batch 92370, train_perplexity=18.659246, train_loss=2.9263418

Batch 92380, train_perplexity=18.659246, train_loss=2.9263418

Batch 92390, train_perplexity=18.659246, train_loss=2.9263418

Batch 92400, train_perplexity=18.659246, train_loss=2.9263418

Batch 92410, train_perplexity=18.659246, train_loss=2.9263418

Batch 92420, train_perplexity=18.65925, train_loss=2.926342

Batch 92430, train_perplexity=18.65925, train_loss=2.926342

Batch 92440, train_perplexity=18.65925, train_loss=2.926342

Batch 92450, train_perplexity=18.659246, train_loss=2.9263418

Batch 92460, train_perplexity=18.659246, train_loss=2.9263418

Batch 92470, train_perplexity=18.65924, train_loss=2.9263415

Batch 92480, train_perplexity=18.65924, train_loss=2.9263415

Batch 92490, train_perplexity=18.659246, train_loss=2.9263418

Batch 92500, train_perplexity=18.659246, train_loss=2.9263418

Batch 92510, train_perplexity=18.659246, train_loss=2.9263418

Batch 92520, train_perplexity=18.65924, train_loss=2.9263415

Batch 92530, train_perplexity=18.65924, train_loss=2.9263415

Batch 92540, train_perplexity=18.65924, train_loss=2.9263415

Batch 92550, train_perplexity=18.65924, train_loss=2.9263415

Batch 92560, train_perplexity=18.65924, train_loss=2.9263415

Batch 92570, train_perplexity=18.65924, train_loss=2.9263415

Batch 92580, train_perplexity=18.65924, train_loss=2.9263415

Batch 92590, train_perplexity=18.65924, train_loss=2.9263415

Batch 92600, train_perplexity=18.65924, train_loss=2.9263415

Batch 92610, train_perplexity=18.65924, train_loss=2.9263415

Batch 92620, train_perplexity=18.65924, train_loss=2.9263415

Batch 92630, train_perplexity=18.65924, train_loss=2.9263415

Batch 92640, train_perplexity=18.65924, train_loss=2.9263415

Batch 92650, train_perplexity=18.65924, train_loss=2.9263415

Batch 92660, train_perplexity=18.65924, train_loss=2.9263415

Batch 92670, train_perplexity=18.65924, train_loss=2.9263415

Batch 92680, train_perplexity=18.65924, train_loss=2.9263415

Batch 92690, train_perplexity=18.659237, train_loss=2.9263413

Batch 92700, train_perplexity=18.65924, train_loss=2.9263415

Batch 92710, train_perplexity=18.659237, train_loss=2.9263413

Batch 92720, train_perplexity=18.65924, train_loss=2.9263415

Batch 92730, train_perplexity=18.659233, train_loss=2.926341

Batch 92740, train_perplexity=18.659237, train_loss=2.9263413
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 92750, train_perplexity=18.659237, train_loss=2.9263413

Batch 92760, train_perplexity=18.659233, train_loss=2.926341

Batch 92770, train_perplexity=18.659233, train_loss=2.926341

Batch 92780, train_perplexity=18.659237, train_loss=2.9263413

Batch 92790, train_perplexity=18.65924, train_loss=2.9263415

Batch 92800, train_perplexity=18.659237, train_loss=2.9263413

Batch 92810, train_perplexity=18.65924, train_loss=2.9263415

Batch 92820, train_perplexity=18.659233, train_loss=2.926341

Batch 92830, train_perplexity=18.659233, train_loss=2.926341

Batch 92840, train_perplexity=18.659233, train_loss=2.926341

Batch 92850, train_perplexity=18.659233, train_loss=2.926341

Batch 92860, train_perplexity=18.659233, train_loss=2.926341

Batch 92870, train_perplexity=18.659233, train_loss=2.926341

Batch 92880, train_perplexity=18.659233, train_loss=2.926341

Batch 92890, train_perplexity=18.659237, train_loss=2.9263413

Batch 92900, train_perplexity=18.659233, train_loss=2.926341

Batch 92910, train_perplexity=18.659233, train_loss=2.926341

Batch 92920, train_perplexity=18.659233, train_loss=2.926341

Batch 92930, train_perplexity=18.659227, train_loss=2.9263408

Batch 92940, train_perplexity=18.659227, train_loss=2.9263408

Batch 92950, train_perplexity=18.659233, train_loss=2.926341

Batch 92960, train_perplexity=18.659233, train_loss=2.926341

Batch 92970, train_perplexity=18.659233, train_loss=2.926341

Batch 92980, train_perplexity=18.659233, train_loss=2.926341

Batch 92990, train_perplexity=18.659227, train_loss=2.9263408

Batch 93000, train_perplexity=18.659233, train_loss=2.926341

Batch 93010, train_perplexity=18.659227, train_loss=2.9263408

Batch 93020, train_perplexity=18.659233, train_loss=2.926341

Batch 93030, train_perplexity=18.659233, train_loss=2.926341

Batch 93040, train_perplexity=18.659224, train_loss=2.9263406

Batch 93050, train_perplexity=18.659227, train_loss=2.9263408

Batch 93060, train_perplexity=18.659224, train_loss=2.9263406

Batch 93070, train_perplexity=18.659227, train_loss=2.9263408

Batch 93080, train_perplexity=18.659227, train_loss=2.9263408

Batch 93090, train_perplexity=18.659227, train_loss=2.9263408

Batch 93100, train_perplexity=18.659224, train_loss=2.9263406

Batch 93110, train_perplexity=18.659224, train_loss=2.9263406

Batch 93120, train_perplexity=18.659227, train_loss=2.9263408

Batch 93130, train_perplexity=18.659224, train_loss=2.9263406

Batch 93140, train_perplexity=18.659227, train_loss=2.9263408

Batch 93150, train_perplexity=18.659224, train_loss=2.9263406

Batch 93160, train_perplexity=18.659224, train_loss=2.9263406

Batch 93170, train_perplexity=18.659227, train_loss=2.9263408

Batch 93180, train_perplexity=18.659224, train_loss=2.9263406

Batch 93190, train_perplexity=18.659224, train_loss=2.9263406

Batch 93200, train_perplexity=18.659224, train_loss=2.9263406

Batch 93210, train_perplexity=18.659224, train_loss=2.9263406

Batch 93220, train_perplexity=18.659224, train_loss=2.9263406

Batch 93230, train_perplexity=18.659224, train_loss=2.9263406

Batch 93240, train_perplexity=18.659224, train_loss=2.9263406

Batch 93250, train_perplexity=18.659224, train_loss=2.9263406

Batch 93260, train_perplexity=18.659224, train_loss=2.9263406

Batch 93270, train_perplexity=18.659224, train_loss=2.9263406

Batch 93280, train_perplexity=18.659224, train_loss=2.9263406

Batch 93290, train_perplexity=18.65922, train_loss=2.9263403

Batch 93300, train_perplexity=18.659224, train_loss=2.9263406

Batch 93310, train_perplexity=18.659214, train_loss=2.92634

Batch 93320, train_perplexity=18.659224, train_loss=2.9263406

Batch 93330, train_perplexity=18.659214, train_loss=2.92634

Batch 93340, train_perplexity=18.659224, train_loss=2.9263406

Batch 93350, train_perplexity=18.659224, train_loss=2.9263406

Batch 93360, train_perplexity=18.65922, train_loss=2.9263403

Batch 93370, train_perplexity=18.65922, train_loss=2.9263403

Batch 93380, train_perplexity=18.659214, train_loss=2.92634

Batch 93390, train_perplexity=18.659214, train_loss=2.92634

Batch 93400, train_perplexity=18.659214, train_loss=2.92634

Batch 93410, train_perplexity=18.65922, train_loss=2.9263403

Batch 93420, train_perplexity=18.659214, train_loss=2.92634

Batch 93430, train_perplexity=18.659214, train_loss=2.92634

Batch 93440, train_perplexity=18.659214, train_loss=2.92634

Batch 93450, train_perplexity=18.659214, train_loss=2.92634

Batch 93460, train_perplexity=18.659214, train_loss=2.92634

Batch 93470, train_perplexity=18.659224, train_loss=2.9263406

Batch 93480, train_perplexity=18.659214, train_loss=2.92634

Batch 93490, train_perplexity=18.659214, train_loss=2.92634

Batch 93500, train_perplexity=18.659214, train_loss=2.92634

Batch 93510, train_perplexity=18.659214, train_loss=2.92634

Batch 93520, train_perplexity=18.659214, train_loss=2.92634

Batch 93530, train_perplexity=18.65921, train_loss=2.9263399

Batch 93540, train_perplexity=18.659214, train_loss=2.92634

Batch 93550, train_perplexity=18.65921, train_loss=2.9263399

Batch 93560, train_perplexity=18.65921, train_loss=2.9263399

Batch 93570, train_perplexity=18.659214, train_loss=2.92634

Batch 93580, train_perplexity=18.659214, train_loss=2.92634

Batch 93590, train_perplexity=18.659214, train_loss=2.92634

Batch 93600, train_perplexity=18.659206, train_loss=2.9263396

Batch 93610, train_perplexity=18.65921, train_loss=2.9263399

Batch 93620, train_perplexity=18.65921, train_loss=2.9263399

Batch 93630, train_perplexity=18.659214, train_loss=2.92634

Batch 93640, train_perplexity=18.65921, train_loss=2.9263399

Batch 93650, train_perplexity=18.659206, train_loss=2.9263396

Batch 93660, train_perplexity=18.65921, train_loss=2.9263399

Batch 93670, train_perplexity=18.65921, train_loss=2.9263399

Batch 93680, train_perplexity=18.659214, train_loss=2.92634

Batch 93690, train_perplexity=18.659206, train_loss=2.9263396

Batch 93700, train_perplexity=18.65921, train_loss=2.9263399

Batch 93710, train_perplexity=18.659214, train_loss=2.92634

Batch 93720, train_perplexity=18.659206, train_loss=2.9263396

Batch 93730, train_perplexity=18.659206, train_loss=2.9263396

Batch 93740, train_perplexity=18.659206, train_loss=2.9263396

Batch 93750, train_perplexity=18.659206, train_loss=2.9263396

Batch 93760, train_perplexity=18.659206, train_loss=2.9263396

Batch 93770, train_perplexity=18.659206, train_loss=2.9263396

Batch 93780, train_perplexity=18.659206, train_loss=2.9263396

Batch 93790, train_perplexity=18.659206, train_loss=2.9263396

Batch 93800, train_perplexity=18.659206, train_loss=2.9263396

Batch 93810, train_perplexity=18.659206, train_loss=2.9263396

Batch 93820, train_perplexity=18.659206, train_loss=2.9263396

Batch 93830, train_perplexity=18.6592, train_loss=2.9263394

Batch 93840, train_perplexity=18.659206, train_loss=2.9263396

Batch 93850, train_perplexity=18.659206, train_loss=2.9263396

Batch 93860, train_perplexity=18.659197, train_loss=2.9263391

Batch 93870, train_perplexity=18.659206, train_loss=2.9263396

Batch 93880, train_perplexity=18.659206, train_loss=2.9263396

Batch 93890, train_perplexity=18.659206, train_loss=2.9263396

Batch 93900, train_perplexity=18.659197, train_loss=2.9263391

Batch 93910, train_perplexity=18.659197, train_loss=2.9263391

Batch 93920, train_perplexity=18.659197, train_loss=2.9263391

Batch 93930, train_perplexity=18.659206, train_loss=2.9263396

Batch 93940, train_perplexity=18.659206, train_loss=2.9263396

Batch 93950, train_perplexity=18.659206, train_loss=2.9263396

Batch 93960, train_perplexity=18.6592, train_loss=2.9263394

Batch 93970, train_perplexity=18.659197, train_loss=2.9263391

Batch 93980, train_perplexity=18.659206, train_loss=2.9263396

Batch 93990, train_perplexity=18.659206, train_loss=2.9263396

Batch 94000, train_perplexity=18.659197, train_loss=2.9263391

Batch 94010, train_perplexity=18.659206, train_loss=2.9263396

Batch 94020, train_perplexity=18.659197, train_loss=2.9263391

Batch 94030, train_perplexity=18.6592, train_loss=2.9263394

Batch 94040, train_perplexity=18.659197, train_loss=2.9263391

Batch 94050, train_perplexity=18.6592, train_loss=2.9263394

Batch 94060, train_perplexity=18.659197, train_loss=2.9263391
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 94070, train_perplexity=18.659197, train_loss=2.9263391

Batch 94080, train_perplexity=18.659197, train_loss=2.9263391

Batch 94090, train_perplexity=18.659197, train_loss=2.9263391

Batch 94100, train_perplexity=18.659197, train_loss=2.9263391

Batch 94110, train_perplexity=18.659197, train_loss=2.9263391

Batch 94120, train_perplexity=18.659197, train_loss=2.9263391

Batch 94130, train_perplexity=18.659197, train_loss=2.9263391

Batch 94140, train_perplexity=18.659197, train_loss=2.9263391

Batch 94150, train_perplexity=18.659197, train_loss=2.9263391

Batch 94160, train_perplexity=18.659197, train_loss=2.9263391

Batch 94170, train_perplexity=18.659193, train_loss=2.926339

Batch 94180, train_perplexity=18.659193, train_loss=2.926339

Batch 94190, train_perplexity=18.659197, train_loss=2.9263391

Batch 94200, train_perplexity=18.659187, train_loss=2.9263387

Batch 94210, train_perplexity=18.659197, train_loss=2.9263391

Batch 94220, train_perplexity=18.659193, train_loss=2.926339

Batch 94230, train_perplexity=18.659197, train_loss=2.9263391

Batch 94240, train_perplexity=18.659193, train_loss=2.926339

Batch 94250, train_perplexity=18.659193, train_loss=2.926339

Batch 94260, train_perplexity=18.659193, train_loss=2.926339

Batch 94270, train_perplexity=18.659193, train_loss=2.926339

Batch 94280, train_perplexity=18.659193, train_loss=2.926339

Batch 94290, train_perplexity=18.659193, train_loss=2.926339

Batch 94300, train_perplexity=18.659193, train_loss=2.926339

Batch 94310, train_perplexity=18.659187, train_loss=2.9263387

Batch 94320, train_perplexity=18.659187, train_loss=2.9263387

Batch 94330, train_perplexity=18.659187, train_loss=2.9263387

Batch 94340, train_perplexity=18.659187, train_loss=2.9263387

Batch 94350, train_perplexity=18.659197, train_loss=2.9263391

Batch 94360, train_perplexity=18.659187, train_loss=2.9263387

Batch 94370, train_perplexity=18.659187, train_loss=2.9263387

Batch 94380, train_perplexity=18.659187, train_loss=2.9263387

Batch 94390, train_perplexity=18.659187, train_loss=2.9263387

Batch 94400, train_perplexity=18.659187, train_loss=2.9263387

Batch 94410, train_perplexity=18.659187, train_loss=2.9263387

Batch 94420, train_perplexity=18.659187, train_loss=2.9263387

Batch 94430, train_perplexity=18.659187, train_loss=2.9263387

Batch 94440, train_perplexity=18.659187, train_loss=2.9263387

Batch 94450, train_perplexity=18.659187, train_loss=2.9263387

Batch 94460, train_perplexity=18.659187, train_loss=2.9263387

Batch 94470, train_perplexity=18.659187, train_loss=2.9263387

Batch 94480, train_perplexity=18.659187, train_loss=2.9263387

Batch 94490, train_perplexity=18.659187, train_loss=2.9263387

Batch 94500, train_perplexity=18.659187, train_loss=2.9263387

Batch 94510, train_perplexity=18.659187, train_loss=2.9263387

Batch 94520, train_perplexity=18.659187, train_loss=2.9263387

Batch 94530, train_perplexity=18.659187, train_loss=2.9263387

Batch 94540, train_perplexity=18.659184, train_loss=2.9263384

Batch 94550, train_perplexity=18.659184, train_loss=2.9263384

Batch 94560, train_perplexity=18.659187, train_loss=2.9263387

Batch 94570, train_perplexity=18.65918, train_loss=2.9263382

Batch 94580, train_perplexity=18.659184, train_loss=2.9263384

Batch 94590, train_perplexity=18.65918, train_loss=2.9263382

Batch 94600, train_perplexity=18.65918, train_loss=2.9263382

Batch 94610, train_perplexity=18.65918, train_loss=2.9263382

Batch 94620, train_perplexity=18.659184, train_loss=2.9263384

Batch 94630, train_perplexity=18.65918, train_loss=2.9263382

Batch 94640, train_perplexity=18.659184, train_loss=2.9263384

Batch 94650, train_perplexity=18.65918, train_loss=2.9263382

Batch 94660, train_perplexity=18.65918, train_loss=2.9263382

Batch 94670, train_perplexity=18.65918, train_loss=2.9263382

Batch 94680, train_perplexity=18.65918, train_loss=2.9263382

Batch 94690, train_perplexity=18.65918, train_loss=2.9263382

Batch 94700, train_perplexity=18.65918, train_loss=2.9263382

Batch 94710, train_perplexity=18.65918, train_loss=2.9263382

Batch 94720, train_perplexity=18.65918, train_loss=2.9263382

Batch 94730, train_perplexity=18.65918, train_loss=2.9263382

Batch 94740, train_perplexity=18.65918, train_loss=2.9263382

Batch 94750, train_perplexity=18.65918, train_loss=2.9263382

Batch 94760, train_perplexity=18.65918, train_loss=2.9263382

Batch 94770, train_perplexity=18.65918, train_loss=2.9263382

Batch 94780, train_perplexity=18.65918, train_loss=2.9263382

Batch 94790, train_perplexity=18.659174, train_loss=2.926338

Batch 94800, train_perplexity=18.65918, train_loss=2.9263382

Batch 94810, train_perplexity=18.65918, train_loss=2.9263382

Batch 94820, train_perplexity=18.65918, train_loss=2.9263382

Batch 94830, train_perplexity=18.65918, train_loss=2.9263382

Batch 94840, train_perplexity=18.65918, train_loss=2.9263382

Batch 94850, train_perplexity=18.659174, train_loss=2.926338

Batch 94860, train_perplexity=18.659174, train_loss=2.926338

Batch 94870, train_perplexity=18.65917, train_loss=2.9263377

Batch 94880, train_perplexity=18.65918, train_loss=2.9263382

Batch 94890, train_perplexity=18.65917, train_loss=2.9263377

Batch 94900, train_perplexity=18.65917, train_loss=2.9263377

Batch 94910, train_perplexity=18.65917, train_loss=2.9263377

Batch 94920, train_perplexity=18.65917, train_loss=2.9263377

Batch 94930, train_perplexity=18.65917, train_loss=2.9263377

Batch 94940, train_perplexity=18.659174, train_loss=2.926338

Batch 94950, train_perplexity=18.659174, train_loss=2.926338

Batch 94960, train_perplexity=18.65917, train_loss=2.9263377

Batch 94970, train_perplexity=18.65917, train_loss=2.9263377

Batch 94980, train_perplexity=18.659174, train_loss=2.926338

Batch 94990, train_perplexity=18.65917, train_loss=2.9263377

Batch 95000, train_perplexity=18.65917, train_loss=2.9263377

Batch 95010, train_perplexity=18.65917, train_loss=2.9263377

Batch 95020, train_perplexity=18.65917, train_loss=2.9263377

Batch 95030, train_perplexity=18.65917, train_loss=2.9263377

Batch 95040, train_perplexity=18.65917, train_loss=2.9263377

Batch 95050, train_perplexity=18.659166, train_loss=2.9263375

Batch 95060, train_perplexity=18.65917, train_loss=2.9263377

Batch 95070, train_perplexity=18.659166, train_loss=2.9263375

Batch 95080, train_perplexity=18.65916, train_loss=2.9263372

Batch 95090, train_perplexity=18.65917, train_loss=2.9263377

Batch 95100, train_perplexity=18.65916, train_loss=2.9263372

Batch 95110, train_perplexity=18.65917, train_loss=2.9263377

Batch 95120, train_perplexity=18.65917, train_loss=2.9263377

Batch 95130, train_perplexity=18.65917, train_loss=2.9263377

Batch 95140, train_perplexity=18.659166, train_loss=2.9263375

Batch 95150, train_perplexity=18.65916, train_loss=2.9263372

Batch 95160, train_perplexity=18.65917, train_loss=2.9263377

Batch 95170, train_perplexity=18.65916, train_loss=2.9263372

Batch 95180, train_perplexity=18.65916, train_loss=2.9263372

Batch 95190, train_perplexity=18.65916, train_loss=2.9263372

Batch 95200, train_perplexity=18.65916, train_loss=2.9263372

Batch 95210, train_perplexity=18.659166, train_loss=2.9263375

Batch 95220, train_perplexity=18.65917, train_loss=2.9263377

Batch 95230, train_perplexity=18.65916, train_loss=2.9263372

Batch 95240, train_perplexity=18.65916, train_loss=2.9263372

Batch 95250, train_perplexity=18.65916, train_loss=2.9263372

Batch 95260, train_perplexity=18.65916, train_loss=2.9263372

Batch 95270, train_perplexity=18.65916, train_loss=2.9263372

Batch 95280, train_perplexity=18.65916, train_loss=2.9263372

Batch 95290, train_perplexity=18.65916, train_loss=2.9263372

Batch 95300, train_perplexity=18.65916, train_loss=2.9263372

Batch 95310, train_perplexity=18.65916, train_loss=2.9263372

Batch 95320, train_perplexity=18.659157, train_loss=2.926337

Batch 95330, train_perplexity=18.659157, train_loss=2.926337

Batch 95340, train_perplexity=18.659153, train_loss=2.9263368

Batch 95350, train_perplexity=18.65916, train_loss=2.9263372

Batch 95360, train_perplexity=18.65916, train_loss=2.9263372

Batch 95370, train_perplexity=18.659157, train_loss=2.926337

Batch 95380, train_perplexity=18.659153, train_loss=2.9263368
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 95390, train_perplexity=18.65916, train_loss=2.9263372

Batch 95400, train_perplexity=18.659157, train_loss=2.926337

Batch 95410, train_perplexity=18.659153, train_loss=2.9263368

Batch 95420, train_perplexity=18.659157, train_loss=2.926337

Batch 95430, train_perplexity=18.659157, train_loss=2.926337

Batch 95440, train_perplexity=18.659157, train_loss=2.926337

Batch 95450, train_perplexity=18.659153, train_loss=2.9263368

Batch 95460, train_perplexity=18.659153, train_loss=2.9263368

Batch 95470, train_perplexity=18.659153, train_loss=2.9263368

Batch 95480, train_perplexity=18.659153, train_loss=2.9263368

Batch 95490, train_perplexity=18.659153, train_loss=2.9263368

Batch 95500, train_perplexity=18.659153, train_loss=2.9263368

Batch 95510, train_perplexity=18.659153, train_loss=2.9263368

Batch 95520, train_perplexity=18.659157, train_loss=2.926337

Batch 95530, train_perplexity=18.659153, train_loss=2.9263368

Batch 95540, train_perplexity=18.659153, train_loss=2.9263368

Batch 95550, train_perplexity=18.659153, train_loss=2.9263368

Batch 95560, train_perplexity=18.659153, train_loss=2.9263368

Batch 95570, train_perplexity=18.659153, train_loss=2.9263368

Batch 95580, train_perplexity=18.659153, train_loss=2.9263368

Batch 95590, train_perplexity=18.659153, train_loss=2.9263368

Batch 95600, train_perplexity=18.659153, train_loss=2.9263368

Batch 95610, train_perplexity=18.659147, train_loss=2.9263365

Batch 95620, train_perplexity=18.659147, train_loss=2.9263365

Batch 95630, train_perplexity=18.659143, train_loss=2.9263363

Batch 95640, train_perplexity=18.659147, train_loss=2.9263365

Batch 95650, train_perplexity=18.659153, train_loss=2.9263368

Batch 95660, train_perplexity=18.659153, train_loss=2.9263368

Batch 95670, train_perplexity=18.659153, train_loss=2.9263368

Batch 95680, train_perplexity=18.659153, train_loss=2.9263368

Batch 95690, train_perplexity=18.659153, train_loss=2.9263368

Batch 95700, train_perplexity=18.659143, train_loss=2.9263363

Batch 95710, train_perplexity=18.659143, train_loss=2.9263363

Batch 95720, train_perplexity=18.659153, train_loss=2.9263368

Batch 95730, train_perplexity=18.659153, train_loss=2.9263368

Batch 95740, train_perplexity=18.659143, train_loss=2.9263363

Batch 95750, train_perplexity=18.659143, train_loss=2.9263363

Batch 95760, train_perplexity=18.659143, train_loss=2.9263363

Batch 95770, train_perplexity=18.659143, train_loss=2.9263363

Batch 95780, train_perplexity=18.659143, train_loss=2.9263363

Batch 95790, train_perplexity=18.659143, train_loss=2.9263363

Batch 95800, train_perplexity=18.659143, train_loss=2.9263363

Batch 95810, train_perplexity=18.659143, train_loss=2.9263363

Batch 95820, train_perplexity=18.659143, train_loss=2.9263363

Batch 95830, train_perplexity=18.659143, train_loss=2.9263363

Batch 95840, train_perplexity=18.659143, train_loss=2.9263363

Batch 95850, train_perplexity=18.659143, train_loss=2.9263363

Batch 95860, train_perplexity=18.659143, train_loss=2.9263363

Batch 95870, train_perplexity=18.659143, train_loss=2.9263363

Batch 95880, train_perplexity=18.659134, train_loss=2.9263358

Batch 95890, train_perplexity=18.659143, train_loss=2.9263363

Batch 95900, train_perplexity=18.659143, train_loss=2.9263363

Batch 95910, train_perplexity=18.65914, train_loss=2.926336

Batch 95920, train_perplexity=18.659143, train_loss=2.9263363

Batch 95930, train_perplexity=18.659134, train_loss=2.9263358

Batch 95940, train_perplexity=18.65914, train_loss=2.926336

Batch 95950, train_perplexity=18.659134, train_loss=2.9263358

Batch 95960, train_perplexity=18.659134, train_loss=2.9263358

Batch 95970, train_perplexity=18.659143, train_loss=2.9263363

Batch 95980, train_perplexity=18.65914, train_loss=2.926336

Batch 95990, train_perplexity=18.659134, train_loss=2.9263358

Batch 96000, train_perplexity=18.659134, train_loss=2.9263358

Batch 96010, train_perplexity=18.659134, train_loss=2.9263358

Batch 96020, train_perplexity=18.659134, train_loss=2.9263358

Batch 96030, train_perplexity=18.659134, train_loss=2.9263358

Batch 96040, train_perplexity=18.659134, train_loss=2.9263358

Batch 96050, train_perplexity=18.659134, train_loss=2.9263358

Batch 96060, train_perplexity=18.659134, train_loss=2.9263358

Batch 96070, train_perplexity=18.659134, train_loss=2.9263358

Batch 96080, train_perplexity=18.659134, train_loss=2.9263358

Batch 96090, train_perplexity=18.659134, train_loss=2.9263358

Batch 96100, train_perplexity=18.659134, train_loss=2.9263358

Batch 96110, train_perplexity=18.659134, train_loss=2.9263358

Batch 96120, train_perplexity=18.659134, train_loss=2.9263358

Batch 96130, train_perplexity=18.659134, train_loss=2.9263358

Batch 96140, train_perplexity=18.659134, train_loss=2.9263358

Batch 96150, train_perplexity=18.659134, train_loss=2.9263358

Batch 96160, train_perplexity=18.65913, train_loss=2.9263356

Batch 96170, train_perplexity=18.65913, train_loss=2.9263356

Batch 96180, train_perplexity=18.659134, train_loss=2.9263358

Batch 96190, train_perplexity=18.659134, train_loss=2.9263358

Batch 96200, train_perplexity=18.659126, train_loss=2.9263353

Batch 96210, train_perplexity=18.659126, train_loss=2.9263353

Batch 96220, train_perplexity=18.659134, train_loss=2.9263358

Batch 96230, train_perplexity=18.659134, train_loss=2.9263358

Batch 96240, train_perplexity=18.659126, train_loss=2.9263353

Batch 96250, train_perplexity=18.659126, train_loss=2.9263353

Batch 96260, train_perplexity=18.659134, train_loss=2.9263358

Batch 96270, train_perplexity=18.659134, train_loss=2.9263358

Batch 96280, train_perplexity=18.659126, train_loss=2.9263353

Batch 96290, train_perplexity=18.659126, train_loss=2.9263353

Batch 96300, train_perplexity=18.659126, train_loss=2.9263353

Batch 96310, train_perplexity=18.659126, train_loss=2.9263353

Batch 96320, train_perplexity=18.659126, train_loss=2.9263353

Batch 96330, train_perplexity=18.659126, train_loss=2.9263353

Batch 96340, train_perplexity=18.659126, train_loss=2.9263353

Batch 96350, train_perplexity=18.659126, train_loss=2.9263353

Batch 96360, train_perplexity=18.659126, train_loss=2.9263353

Batch 96370, train_perplexity=18.659126, train_loss=2.9263353

Batch 96380, train_perplexity=18.659126, train_loss=2.9263353

Batch 96390, train_perplexity=18.659126, train_loss=2.9263353

Batch 96400, train_perplexity=18.65912, train_loss=2.926335

Batch 96410, train_perplexity=18.659126, train_loss=2.9263353

Batch 96420, train_perplexity=18.659126, train_loss=2.9263353

Batch 96430, train_perplexity=18.659126, train_loss=2.9263353

Batch 96440, train_perplexity=18.65912, train_loss=2.926335

Batch 96450, train_perplexity=18.659126, train_loss=2.9263353

Batch 96460, train_perplexity=18.659126, train_loss=2.9263353

Batch 96470, train_perplexity=18.65912, train_loss=2.926335

Batch 96480, train_perplexity=18.659126, train_loss=2.9263353

Batch 96490, train_perplexity=18.65912, train_loss=2.926335

Batch 96500, train_perplexity=18.659126, train_loss=2.9263353

Batch 96510, train_perplexity=18.65912, train_loss=2.926335

Batch 96520, train_perplexity=18.659117, train_loss=2.9263349

Batch 96530, train_perplexity=18.659117, train_loss=2.9263349

Batch 96540, train_perplexity=18.659117, train_loss=2.9263349

Batch 96550, train_perplexity=18.659117, train_loss=2.9263349

Batch 96560, train_perplexity=18.659117, train_loss=2.9263349

Batch 96570, train_perplexity=18.659117, train_loss=2.9263349

Batch 96580, train_perplexity=18.65912, train_loss=2.926335

Batch 96590, train_perplexity=18.659117, train_loss=2.9263349

Batch 96600, train_perplexity=18.659117, train_loss=2.9263349

Batch 96610, train_perplexity=18.659117, train_loss=2.9263349

Batch 96620, train_perplexity=18.659117, train_loss=2.9263349

Batch 96630, train_perplexity=18.659117, train_loss=2.9263349

Batch 96640, train_perplexity=18.659117, train_loss=2.9263349

Batch 96650, train_perplexity=18.659117, train_loss=2.9263349

Batch 96660, train_perplexity=18.659117, train_loss=2.9263349

Batch 96670, train_perplexity=18.659117, train_loss=2.9263349

Batch 96680, train_perplexity=18.659117, train_loss=2.9263349

Batch 96690, train_perplexity=18.659117, train_loss=2.9263349
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 96700, train_perplexity=18.659117, train_loss=2.9263349

Batch 96710, train_perplexity=18.659117, train_loss=2.9263349

Batch 96720, train_perplexity=18.659117, train_loss=2.9263349

Batch 96730, train_perplexity=18.659117, train_loss=2.9263349

Batch 96740, train_perplexity=18.659117, train_loss=2.9263349

Batch 96750, train_perplexity=18.659107, train_loss=2.9263344

Batch 96760, train_perplexity=18.659117, train_loss=2.9263349

Batch 96770, train_perplexity=18.659117, train_loss=2.9263349

Batch 96780, train_perplexity=18.659117, train_loss=2.9263349

Batch 96790, train_perplexity=18.659117, train_loss=2.9263349

Batch 96800, train_perplexity=18.659117, train_loss=2.9263349

Batch 96810, train_perplexity=18.659117, train_loss=2.9263349

Batch 96820, train_perplexity=18.659113, train_loss=2.9263346

Batch 96830, train_perplexity=18.659107, train_loss=2.9263344

Batch 96840, train_perplexity=18.659107, train_loss=2.9263344

Batch 96850, train_perplexity=18.659117, train_loss=2.9263349

Batch 96860, train_perplexity=18.659117, train_loss=2.9263349

Batch 96870, train_perplexity=18.659117, train_loss=2.9263349

Batch 96880, train_perplexity=18.659107, train_loss=2.9263344

Batch 96890, train_perplexity=18.659117, train_loss=2.9263349

Batch 96900, train_perplexity=18.659107, train_loss=2.9263344

Batch 96910, train_perplexity=18.659107, train_loss=2.9263344

Batch 96920, train_perplexity=18.659107, train_loss=2.9263344

Batch 96930, train_perplexity=18.659107, train_loss=2.9263344

Batch 96940, train_perplexity=18.659107, train_loss=2.9263344

Batch 96950, train_perplexity=18.659107, train_loss=2.9263344

Batch 96960, train_perplexity=18.659107, train_loss=2.9263344

Batch 96970, train_perplexity=18.659107, train_loss=2.9263344

Batch 96980, train_perplexity=18.659107, train_loss=2.9263344

Batch 96990, train_perplexity=18.659107, train_loss=2.9263344

Batch 97000, train_perplexity=18.659107, train_loss=2.9263344

Batch 97010, train_perplexity=18.659107, train_loss=2.9263344

Batch 97020, train_perplexity=18.659107, train_loss=2.9263344

Batch 97030, train_perplexity=18.659107, train_loss=2.9263344

Batch 97040, train_perplexity=18.659107, train_loss=2.9263344

Batch 97050, train_perplexity=18.659103, train_loss=2.9263341

Batch 97060, train_perplexity=18.659107, train_loss=2.9263344

Batch 97070, train_perplexity=18.659107, train_loss=2.9263344

Batch 97080, train_perplexity=18.659107, train_loss=2.9263344

Batch 97090, train_perplexity=18.659107, train_loss=2.9263344

Batch 97100, train_perplexity=18.659103, train_loss=2.9263341

Batch 97110, train_perplexity=18.6591, train_loss=2.926334

Batch 97120, train_perplexity=18.659103, train_loss=2.9263341

Batch 97130, train_perplexity=18.659103, train_loss=2.9263341

Batch 97140, train_perplexity=18.659103, train_loss=2.9263341

Batch 97150, train_perplexity=18.659103, train_loss=2.9263341

Batch 97160, train_perplexity=18.6591, train_loss=2.926334

Batch 97170, train_perplexity=18.659107, train_loss=2.9263344

Batch 97180, train_perplexity=18.6591, train_loss=2.926334

Batch 97190, train_perplexity=18.6591, train_loss=2.926334

Batch 97200, train_perplexity=18.6591, train_loss=2.926334

Batch 97210, train_perplexity=18.6591, train_loss=2.926334

Batch 97220, train_perplexity=18.6591, train_loss=2.926334

Batch 97230, train_perplexity=18.6591, train_loss=2.926334

Batch 97240, train_perplexity=18.6591, train_loss=2.926334

Batch 97250, train_perplexity=18.6591, train_loss=2.926334

Batch 97260, train_perplexity=18.6591, train_loss=2.926334

Batch 97270, train_perplexity=18.6591, train_loss=2.926334

Batch 97280, train_perplexity=18.6591, train_loss=2.926334

Batch 97290, train_perplexity=18.6591, train_loss=2.926334

Batch 97300, train_perplexity=18.6591, train_loss=2.926334

Batch 97310, train_perplexity=18.6591, train_loss=2.926334

Batch 97320, train_perplexity=18.6591, train_loss=2.926334

Batch 97330, train_perplexity=18.659094, train_loss=2.9263337

Batch 97340, train_perplexity=18.6591, train_loss=2.926334

Batch 97350, train_perplexity=18.6591, train_loss=2.926334

Batch 97360, train_perplexity=18.6591, train_loss=2.926334

Batch 97370, train_perplexity=18.65909, train_loss=2.9263334

Batch 97380, train_perplexity=18.659094, train_loss=2.9263337

Batch 97390, train_perplexity=18.6591, train_loss=2.926334

Batch 97400, train_perplexity=18.65909, train_loss=2.9263334

Batch 97410, train_perplexity=18.65909, train_loss=2.9263334

Batch 97420, train_perplexity=18.659094, train_loss=2.9263337

Batch 97430, train_perplexity=18.659094, train_loss=2.9263337

Batch 97440, train_perplexity=18.659094, train_loss=2.9263337

Batch 97450, train_perplexity=18.65909, train_loss=2.9263334

Batch 97460, train_perplexity=18.659094, train_loss=2.9263337

Batch 97470, train_perplexity=18.659094, train_loss=2.9263337

Batch 97480, train_perplexity=18.65909, train_loss=2.9263334

Batch 97490, train_perplexity=18.65909, train_loss=2.9263334

Batch 97500, train_perplexity=18.65909, train_loss=2.9263334

Batch 97510, train_perplexity=18.65909, train_loss=2.9263334

Batch 97520, train_perplexity=18.65909, train_loss=2.9263334

Batch 97530, train_perplexity=18.65909, train_loss=2.9263334

Batch 97540, train_perplexity=18.65909, train_loss=2.9263334

Batch 97550, train_perplexity=18.65909, train_loss=2.9263334

Batch 97560, train_perplexity=18.659094, train_loss=2.9263337

Batch 97570, train_perplexity=18.65909, train_loss=2.9263334

Batch 97580, train_perplexity=18.65909, train_loss=2.9263334

Batch 97590, train_perplexity=18.65909, train_loss=2.9263334

Batch 97600, train_perplexity=18.65909, train_loss=2.9263334

Batch 97610, train_perplexity=18.65909, train_loss=2.9263334

Batch 97620, train_perplexity=18.65909, train_loss=2.9263334

Batch 97630, train_perplexity=18.65909, train_loss=2.9263334

Batch 97640, train_perplexity=18.65909, train_loss=2.9263334

Batch 97650, train_perplexity=18.65909, train_loss=2.9263334

Batch 97660, train_perplexity=18.65909, train_loss=2.9263334

Batch 97670, train_perplexity=18.659086, train_loss=2.9263332

Batch 97680, train_perplexity=18.65909, train_loss=2.9263334

Batch 97690, train_perplexity=18.65909, train_loss=2.9263334

Batch 97700, train_perplexity=18.65908, train_loss=2.926333

Batch 97710, train_perplexity=18.65909, train_loss=2.9263334

Batch 97720, train_perplexity=18.659086, train_loss=2.9263332

Batch 97730, train_perplexity=18.65908, train_loss=2.926333

Batch 97740, train_perplexity=18.659086, train_loss=2.9263332

Batch 97750, train_perplexity=18.659086, train_loss=2.9263332

Batch 97760, train_perplexity=18.659086, train_loss=2.9263332

Batch 97770, train_perplexity=18.65908, train_loss=2.926333

Batch 97780, train_perplexity=18.65908, train_loss=2.926333

Batch 97790, train_perplexity=18.65908, train_loss=2.926333

Batch 97800, train_perplexity=18.65908, train_loss=2.926333

Batch 97810, train_perplexity=18.65908, train_loss=2.926333

Batch 97820, train_perplexity=18.659086, train_loss=2.9263332

Batch 97830, train_perplexity=18.65908, train_loss=2.926333

Batch 97840, train_perplexity=18.65908, train_loss=2.926333

Batch 97850, train_perplexity=18.65908, train_loss=2.926333

Batch 97860, train_perplexity=18.65908, train_loss=2.926333

Batch 97870, train_perplexity=18.65908, train_loss=2.926333

Batch 97880, train_perplexity=18.65908, train_loss=2.926333

Batch 97890, train_perplexity=18.65908, train_loss=2.926333

Batch 97900, train_perplexity=18.65908, train_loss=2.926333

Batch 97910, train_perplexity=18.65908, train_loss=2.926333

Batch 97920, train_perplexity=18.65908, train_loss=2.926333

Batch 97930, train_perplexity=18.65908, train_loss=2.926333

Batch 97940, train_perplexity=18.65908, train_loss=2.926333

Batch 97950, train_perplexity=18.65908, train_loss=2.926333

Batch 97960, train_perplexity=18.65908, train_loss=2.926333

Batch 97970, train_perplexity=18.65908, train_loss=2.926333

Batch 97980, train_perplexity=18.659077, train_loss=2.9263327

Batch 97990, train_perplexity=18.659073, train_loss=2.9263325

Batch 98000, train_perplexity=18.65908, train_loss=2.926333

Batch 98010, train_perplexity=18.65908, train_loss=2.926333

Batch 98020, train_perplexity=18.65908, train_loss=2.926333
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 98030, train_perplexity=18.659073, train_loss=2.9263325

Batch 98040, train_perplexity=18.65908, train_loss=2.926333

Batch 98050, train_perplexity=18.65908, train_loss=2.926333

Batch 98060, train_perplexity=18.659073, train_loss=2.9263325

Batch 98070, train_perplexity=18.659073, train_loss=2.9263325

Batch 98080, train_perplexity=18.659077, train_loss=2.9263327

Batch 98090, train_perplexity=18.659073, train_loss=2.9263325

Batch 98100, train_perplexity=18.659073, train_loss=2.9263325

Batch 98110, train_perplexity=18.659073, train_loss=2.9263325

Batch 98120, train_perplexity=18.659073, train_loss=2.9263325

Batch 98130, train_perplexity=18.659073, train_loss=2.9263325

Batch 98140, train_perplexity=18.659073, train_loss=2.9263325

Batch 98150, train_perplexity=18.659073, train_loss=2.9263325

Batch 98160, train_perplexity=18.659073, train_loss=2.9263325

Batch 98170, train_perplexity=18.659073, train_loss=2.9263325

Batch 98180, train_perplexity=18.659073, train_loss=2.9263325

Batch 98190, train_perplexity=18.659067, train_loss=2.9263322

Batch 98200, train_perplexity=18.659073, train_loss=2.9263325

Batch 98210, train_perplexity=18.659073, train_loss=2.9263325

Batch 98220, train_perplexity=18.659073, train_loss=2.9263325

Batch 98230, train_perplexity=18.659073, train_loss=2.9263325

Batch 98240, train_perplexity=18.659073, train_loss=2.9263325

Batch 98250, train_perplexity=18.659067, train_loss=2.9263322

Batch 98260, train_perplexity=18.659073, train_loss=2.9263325

Batch 98270, train_perplexity=18.659067, train_loss=2.9263322

Batch 98280, train_perplexity=18.659063, train_loss=2.926332

Batch 98290, train_perplexity=18.659073, train_loss=2.9263325

Batch 98300, train_perplexity=18.659067, train_loss=2.9263322

Batch 98310, train_perplexity=18.659073, train_loss=2.9263325

Batch 98320, train_perplexity=18.659073, train_loss=2.9263325

Batch 98330, train_perplexity=18.659067, train_loss=2.9263322

Batch 98340, train_perplexity=18.659073, train_loss=2.9263325

Batch 98350, train_perplexity=18.659073, train_loss=2.9263325

Batch 98360, train_perplexity=18.659063, train_loss=2.926332

Batch 98370, train_perplexity=18.659067, train_loss=2.9263322

Batch 98380, train_perplexity=18.659067, train_loss=2.9263322

Batch 98390, train_perplexity=18.659063, train_loss=2.926332

Batch 98400, train_perplexity=18.659063, train_loss=2.926332

Batch 98410, train_perplexity=18.659067, train_loss=2.9263322

Batch 98420, train_perplexity=18.659063, train_loss=2.926332

Batch 98430, train_perplexity=18.659063, train_loss=2.926332

Batch 98440, train_perplexity=18.659063, train_loss=2.926332

Batch 98450, train_perplexity=18.659063, train_loss=2.926332

Batch 98460, train_perplexity=18.659063, train_loss=2.926332

Batch 98470, train_perplexity=18.659063, train_loss=2.926332

Batch 98480, train_perplexity=18.659063, train_loss=2.926332

Batch 98490, train_perplexity=18.659063, train_loss=2.926332

Batch 98500, train_perplexity=18.659067, train_loss=2.9263322

Batch 98510, train_perplexity=18.659063, train_loss=2.926332

Batch 98520, train_perplexity=18.659063, train_loss=2.926332

Batch 98530, train_perplexity=18.659063, train_loss=2.926332

Batch 98540, train_perplexity=18.65906, train_loss=2.9263318

Batch 98550, train_perplexity=18.659063, train_loss=2.926332

Batch 98560, train_perplexity=18.659054, train_loss=2.9263315

Batch 98570, train_perplexity=18.65906, train_loss=2.9263318

Batch 98580, train_perplexity=18.65906, train_loss=2.9263318

Batch 98590, train_perplexity=18.659063, train_loss=2.926332

Batch 98600, train_perplexity=18.659054, train_loss=2.9263315

Batch 98610, train_perplexity=18.659054, train_loss=2.9263315

Batch 98620, train_perplexity=18.65906, train_loss=2.9263318

Batch 98630, train_perplexity=18.659063, train_loss=2.926332

Batch 98640, train_perplexity=18.659054, train_loss=2.9263315

Batch 98650, train_perplexity=18.659054, train_loss=2.9263315

Batch 98660, train_perplexity=18.659063, train_loss=2.926332

Batch 98670, train_perplexity=18.65906, train_loss=2.9263318

Batch 98680, train_perplexity=18.659054, train_loss=2.9263315

Batch 98690, train_perplexity=18.659054, train_loss=2.9263315

Batch 98700, train_perplexity=18.65906, train_loss=2.9263318

Batch 98710, train_perplexity=18.659054, train_loss=2.9263315

Batch 98720, train_perplexity=18.659063, train_loss=2.926332

Batch 98730, train_perplexity=18.659054, train_loss=2.9263315

Batch 98740, train_perplexity=18.659054, train_loss=2.9263315

Batch 98750, train_perplexity=18.659054, train_loss=2.9263315

Batch 98760, train_perplexity=18.659054, train_loss=2.9263315

Batch 98770, train_perplexity=18.659054, train_loss=2.9263315

Batch 98780, train_perplexity=18.659054, train_loss=2.9263315

Batch 98790, train_perplexity=18.659054, train_loss=2.9263315

Batch 98800, train_perplexity=18.659054, train_loss=2.9263315

Batch 98810, train_perplexity=18.659054, train_loss=2.9263315

Batch 98820, train_perplexity=18.659054, train_loss=2.9263315

Batch 98830, train_perplexity=18.659046, train_loss=2.926331

Batch 98840, train_perplexity=18.659054, train_loss=2.9263315

Batch 98850, train_perplexity=18.65905, train_loss=2.9263313

Batch 98860, train_perplexity=18.659054, train_loss=2.9263315

Batch 98870, train_perplexity=18.659046, train_loss=2.926331

Batch 98880, train_perplexity=18.659054, train_loss=2.9263315

Batch 98890, train_perplexity=18.659054, train_loss=2.9263315

Batch 98900, train_perplexity=18.659046, train_loss=2.926331

Batch 98910, train_perplexity=18.65905, train_loss=2.9263313

Batch 98920, train_perplexity=18.65905, train_loss=2.9263313

Batch 98930, train_perplexity=18.659046, train_loss=2.926331

Batch 98940, train_perplexity=18.659046, train_loss=2.926331

Batch 98950, train_perplexity=18.659054, train_loss=2.9263315

Batch 98960, train_perplexity=18.659054, train_loss=2.9263315

Batch 98970, train_perplexity=18.659046, train_loss=2.926331

Batch 98980, train_perplexity=18.65905, train_loss=2.9263313

Batch 98990, train_perplexity=18.659046, train_loss=2.926331

Batch 99000, train_perplexity=18.659054, train_loss=2.9263315

Batch 99010, train_perplexity=18.65905, train_loss=2.9263313

Batch 99020, train_perplexity=18.659046, train_loss=2.926331

Batch 99030, train_perplexity=18.659046, train_loss=2.926331

Batch 99040, train_perplexity=18.65904, train_loss=2.9263308

Batch 99050, train_perplexity=18.659046, train_loss=2.926331

Batch 99060, train_perplexity=18.659046, train_loss=2.926331

Batch 99070, train_perplexity=18.659046, train_loss=2.926331

Batch 99080, train_perplexity=18.659046, train_loss=2.926331

Batch 99090, train_perplexity=18.659046, train_loss=2.926331

Batch 99100, train_perplexity=18.65904, train_loss=2.9263308

Batch 99110, train_perplexity=18.659046, train_loss=2.926331

Batch 99120, train_perplexity=18.659046, train_loss=2.926331

Batch 99130, train_perplexity=18.659046, train_loss=2.926331

Batch 99140, train_perplexity=18.659046, train_loss=2.926331

Batch 99150, train_perplexity=18.659046, train_loss=2.926331

Batch 99160, train_perplexity=18.659046, train_loss=2.926331

Batch 99170, train_perplexity=18.659046, train_loss=2.926331

Batch 99180, train_perplexity=18.659046, train_loss=2.926331

Batch 99190, train_perplexity=18.65904, train_loss=2.9263308

Batch 99200, train_perplexity=18.65904, train_loss=2.9263308

Batch 99210, train_perplexity=18.659046, train_loss=2.926331

Batch 99220, train_perplexity=18.659046, train_loss=2.926331

Batch 99230, train_perplexity=18.659046, train_loss=2.926331

Batch 99240, train_perplexity=18.659046, train_loss=2.926331

Batch 99250, train_perplexity=18.65904, train_loss=2.9263308

Batch 99260, train_perplexity=18.659046, train_loss=2.926331

Batch 99270, train_perplexity=18.659037, train_loss=2.9263306

Batch 99280, train_perplexity=18.65904, train_loss=2.9263308

Batch 99290, train_perplexity=18.659037, train_loss=2.9263306

Batch 99300, train_perplexity=18.659037, train_loss=2.9263306

Batch 99310, train_perplexity=18.659037, train_loss=2.9263306

Batch 99320, train_perplexity=18.65904, train_loss=2.9263308

Batch 99330, train_perplexity=18.659037, train_loss=2.9263306

Batch 99340, train_perplexity=18.659037, train_loss=2.9263306
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 99350, train_perplexity=18.659037, train_loss=2.9263306

Batch 99360, train_perplexity=18.659037, train_loss=2.9263306

Batch 99370, train_perplexity=18.659037, train_loss=2.9263306

Batch 99380, train_perplexity=18.659037, train_loss=2.9263306

Batch 99390, train_perplexity=18.659037, train_loss=2.9263306

Batch 99400, train_perplexity=18.659037, train_loss=2.9263306

Batch 99410, train_perplexity=18.659037, train_loss=2.9263306

Batch 99420, train_perplexity=18.65904, train_loss=2.9263308

Batch 99430, train_perplexity=18.659037, train_loss=2.9263306

Batch 99440, train_perplexity=18.659037, train_loss=2.9263306

Batch 99450, train_perplexity=18.659037, train_loss=2.9263306

Batch 99460, train_perplexity=18.659037, train_loss=2.9263306

Batch 99470, train_perplexity=18.659037, train_loss=2.9263306

Batch 99480, train_perplexity=18.659037, train_loss=2.9263306

Batch 99490, train_perplexity=18.659037, train_loss=2.9263306

Batch 99500, train_perplexity=18.659037, train_loss=2.9263306

Batch 99510, train_perplexity=18.659027, train_loss=2.92633

Batch 99520, train_perplexity=18.659027, train_loss=2.92633

Batch 99530, train_perplexity=18.659033, train_loss=2.9263303

Batch 99540, train_perplexity=18.659037, train_loss=2.9263306

Batch 99550, train_perplexity=18.659037, train_loss=2.9263306

Batch 99560, train_perplexity=18.659033, train_loss=2.9263303

Batch 99570, train_perplexity=18.659027, train_loss=2.92633

Batch 99580, train_perplexity=18.659027, train_loss=2.92633

Batch 99590, train_perplexity=18.659027, train_loss=2.92633

Batch 99600, train_perplexity=18.659027, train_loss=2.92633

Batch 99610, train_perplexity=18.659027, train_loss=2.92633

Batch 99620, train_perplexity=18.659027, train_loss=2.92633

Batch 99630, train_perplexity=18.659033, train_loss=2.9263303

Batch 99640, train_perplexity=18.659033, train_loss=2.9263303

Batch 99650, train_perplexity=18.659027, train_loss=2.92633

Batch 99660, train_perplexity=18.659037, train_loss=2.9263306

Batch 99670, train_perplexity=18.659027, train_loss=2.92633

Batch 99680, train_perplexity=18.659027, train_loss=2.92633

Batch 99690, train_perplexity=18.659027, train_loss=2.92633

Batch 99700, train_perplexity=18.659027, train_loss=2.92633

Batch 99710, train_perplexity=18.659027, train_loss=2.92633

Batch 99720, train_perplexity=18.659027, train_loss=2.92633

Batch 99730, train_perplexity=18.659027, train_loss=2.92633

Batch 99740, train_perplexity=18.659027, train_loss=2.92633

Batch 99750, train_perplexity=18.659027, train_loss=2.92633

Batch 99760, train_perplexity=18.659023, train_loss=2.9263299

Batch 99770, train_perplexity=18.659027, train_loss=2.92633

Batch 99780, train_perplexity=18.659027, train_loss=2.92633

Batch 99790, train_perplexity=18.659023, train_loss=2.9263299

Batch 99800, train_perplexity=18.659027, train_loss=2.92633

Batch 99810, train_perplexity=18.659027, train_loss=2.92633

Batch 99820, train_perplexity=18.659023, train_loss=2.9263299

Batch 99830, train_perplexity=18.659027, train_loss=2.92633

Batch 99840, train_perplexity=18.659027, train_loss=2.92633

Batch 99850, train_perplexity=18.659027, train_loss=2.92633

Batch 99860, train_perplexity=18.65902, train_loss=2.9263296

Batch 99870, train_perplexity=18.65902, train_loss=2.9263296

Batch 99880, train_perplexity=18.659023, train_loss=2.9263299

Batch 99890, train_perplexity=18.65902, train_loss=2.9263296

Batch 99900, train_perplexity=18.659014, train_loss=2.9263294

Batch 99910, train_perplexity=18.65902, train_loss=2.9263296

Batch 99920, train_perplexity=18.659023, train_loss=2.9263299

Batch 99930, train_perplexity=18.65902, train_loss=2.9263296

Batch 99940, train_perplexity=18.65902, train_loss=2.9263296

Batch 99950, train_perplexity=18.659023, train_loss=2.9263299

Batch 99960, train_perplexity=18.65902, train_loss=2.9263296

Batch 99970, train_perplexity=18.65902, train_loss=2.9263296

Batch 99980, train_perplexity=18.659023, train_loss=2.9263299

Batch 99990, train_perplexity=18.659027, train_loss=2.92633

Batch 100000, train_perplexity=18.65902, train_loss=2.9263296

Batch 100010, train_perplexity=18.65902, train_loss=2.9263296

Batch 100020, train_perplexity=18.65902, train_loss=2.9263296

Batch 100030, train_perplexity=18.65902, train_loss=2.9263296

Batch 100040, train_perplexity=18.65902, train_loss=2.9263296

Batch 100050, train_perplexity=18.65902, train_loss=2.9263296

Batch 100060, train_perplexity=18.65902, train_loss=2.9263296

Batch 100070, train_perplexity=18.65902, train_loss=2.9263296

Batch 100080, train_perplexity=18.65902, train_loss=2.9263296

Batch 100090, train_perplexity=18.65902, train_loss=2.9263296

Batch 100100, train_perplexity=18.65902, train_loss=2.9263296

Batch 100110, train_perplexity=18.65902, train_loss=2.9263296

Batch 100120, train_perplexity=18.65902, train_loss=2.9263296

Batch 100130, train_perplexity=18.65902, train_loss=2.9263296

Batch 100140, train_perplexity=18.659014, train_loss=2.9263294

Batch 100150, train_perplexity=18.65902, train_loss=2.9263296

Batch 100160, train_perplexity=18.65902, train_loss=2.9263296

Batch 100170, train_perplexity=18.659014, train_loss=2.9263294

Batch 100180, train_perplexity=18.65902, train_loss=2.9263296

Batch 100190, train_perplexity=18.659014, train_loss=2.9263294

Batch 100200, train_perplexity=18.65901, train_loss=2.9263291

Batch 100210, train_perplexity=18.659014, train_loss=2.9263294

Batch 100220, train_perplexity=18.65901, train_loss=2.9263291

Batch 100230, train_perplexity=18.65901, train_loss=2.9263291

Batch 100240, train_perplexity=18.65901, train_loss=2.9263291

Batch 100250, train_perplexity=18.65901, train_loss=2.9263291

Batch 100260, train_perplexity=18.65901, train_loss=2.9263291

Batch 100270, train_perplexity=18.65901, train_loss=2.9263291

Batch 100280, train_perplexity=18.659014, train_loss=2.9263294

Batch 100290, train_perplexity=18.65901, train_loss=2.9263291

Batch 100300, train_perplexity=18.65901, train_loss=2.9263291

Batch 100310, train_perplexity=18.659006, train_loss=2.926329

Batch 100320, train_perplexity=18.65901, train_loss=2.9263291

Batch 100330, train_perplexity=18.65901, train_loss=2.9263291

Batch 100340, train_perplexity=18.65901, train_loss=2.9263291

Batch 100350, train_perplexity=18.65901, train_loss=2.9263291

Batch 100360, train_perplexity=18.65901, train_loss=2.9263291

Batch 100370, train_perplexity=18.65901, train_loss=2.9263291

Batch 100380, train_perplexity=18.65901, train_loss=2.9263291

Batch 100390, train_perplexity=18.659006, train_loss=2.926329

Batch 100400, train_perplexity=18.65901, train_loss=2.9263291

Batch 100410, train_perplexity=18.65901, train_loss=2.9263291

Batch 100420, train_perplexity=18.65901, train_loss=2.9263291

Batch 100430, train_perplexity=18.65901, train_loss=2.9263291

Batch 100440, train_perplexity=18.659006, train_loss=2.926329

Batch 100450, train_perplexity=18.65901, train_loss=2.9263291

Batch 100460, train_perplexity=18.659006, train_loss=2.926329

Batch 100470, train_perplexity=18.659, train_loss=2.9263287

Batch 100480, train_perplexity=18.659006, train_loss=2.926329

Batch 100490, train_perplexity=18.659, train_loss=2.9263287

Batch 100500, train_perplexity=18.659006, train_loss=2.926329

Batch 100510, train_perplexity=18.659, train_loss=2.9263287

Batch 100520, train_perplexity=18.659, train_loss=2.9263287

Batch 100530, train_perplexity=18.659, train_loss=2.9263287

Batch 100540, train_perplexity=18.659006, train_loss=2.926329

Batch 100550, train_perplexity=18.659, train_loss=2.9263287

Batch 100560, train_perplexity=18.659, train_loss=2.9263287

Batch 100570, train_perplexity=18.659, train_loss=2.9263287

Batch 100580, train_perplexity=18.659006, train_loss=2.926329

Batch 100590, train_perplexity=18.658997, train_loss=2.9263284

Batch 100600, train_perplexity=18.659, train_loss=2.9263287

Batch 100610, train_perplexity=18.659, train_loss=2.9263287

Batch 100620, train_perplexity=18.659, train_loss=2.9263287

Batch 100630, train_perplexity=18.659, train_loss=2.9263287

Batch 100640, train_perplexity=18.659, train_loss=2.9263287

Batch 100650, train_perplexity=18.659, train_loss=2.9263287

Batch 100660, train_perplexity=18.659, train_loss=2.9263287
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 100670, train_perplexity=18.659, train_loss=2.9263287

Batch 100680, train_perplexity=18.658993, train_loss=2.9263282

Batch 100690, train_perplexity=18.659, train_loss=2.9263287

Batch 100700, train_perplexity=18.659, train_loss=2.9263287

Batch 100710, train_perplexity=18.658997, train_loss=2.9263284

Batch 100720, train_perplexity=18.659, train_loss=2.9263287

Batch 100730, train_perplexity=18.658997, train_loss=2.9263284

Batch 100740, train_perplexity=18.659, train_loss=2.9263287

Batch 100750, train_perplexity=18.658997, train_loss=2.9263284

Batch 100760, train_perplexity=18.658997, train_loss=2.9263284

Batch 100770, train_perplexity=18.658997, train_loss=2.9263284

Batch 100780, train_perplexity=18.658997, train_loss=2.9263284

Batch 100790, train_perplexity=18.658993, train_loss=2.9263282

Batch 100800, train_perplexity=18.658997, train_loss=2.9263284

Batch 100810, train_perplexity=18.658993, train_loss=2.9263282

Batch 100820, train_perplexity=18.658993, train_loss=2.9263282

Batch 100830, train_perplexity=18.658993, train_loss=2.9263282

Batch 100840, train_perplexity=18.658993, train_loss=2.9263282

Batch 100850, train_perplexity=18.658993, train_loss=2.9263282

Batch 100860, train_perplexity=18.658997, train_loss=2.9263284

Batch 100870, train_perplexity=18.658993, train_loss=2.9263282

Batch 100880, train_perplexity=18.658993, train_loss=2.9263282

Batch 100890, train_perplexity=18.658993, train_loss=2.9263282

Batch 100900, train_perplexity=18.658993, train_loss=2.9263282

Batch 100910, train_perplexity=18.658987, train_loss=2.926328

Batch 100920, train_perplexity=18.658993, train_loss=2.9263282

Batch 100930, train_perplexity=18.658993, train_loss=2.9263282

Batch 100940, train_perplexity=18.658993, train_loss=2.9263282

Batch 100950, train_perplexity=18.658993, train_loss=2.9263282

Batch 100960, train_perplexity=18.658993, train_loss=2.9263282

Batch 100970, train_perplexity=18.658993, train_loss=2.9263282

Batch 100980, train_perplexity=18.658993, train_loss=2.9263282

Batch 100990, train_perplexity=18.658993, train_loss=2.9263282

Batch 101000, train_perplexity=18.658987, train_loss=2.926328

Batch 101010, train_perplexity=18.658993, train_loss=2.9263282

Batch 101020, train_perplexity=18.658993, train_loss=2.9263282

Batch 101030, train_perplexity=18.658993, train_loss=2.9263282

Batch 101040, train_perplexity=18.658993, train_loss=2.9263282

Batch 101050, train_perplexity=18.658993, train_loss=2.9263282

Batch 101060, train_perplexity=18.658993, train_loss=2.9263282

Batch 101070, train_perplexity=18.658993, train_loss=2.9263282

Batch 101080, train_perplexity=18.658987, train_loss=2.926328

Batch 101090, train_perplexity=18.658983, train_loss=2.9263277

Batch 101100, train_perplexity=18.658987, train_loss=2.926328

Batch 101110, train_perplexity=18.658983, train_loss=2.9263277

Batch 101120, train_perplexity=18.658983, train_loss=2.9263277

Batch 101130, train_perplexity=18.658983, train_loss=2.9263277

Batch 101140, train_perplexity=18.658987, train_loss=2.926328

Batch 101150, train_perplexity=18.658983, train_loss=2.9263277

Batch 101160, train_perplexity=18.658987, train_loss=2.926328

Batch 101170, train_perplexity=18.658983, train_loss=2.9263277

Batch 101180, train_perplexity=18.658983, train_loss=2.9263277

Batch 101190, train_perplexity=18.658983, train_loss=2.9263277

Batch 101200, train_perplexity=18.658983, train_loss=2.9263277

Batch 101210, train_perplexity=18.658993, train_loss=2.9263282

Batch 101220, train_perplexity=18.658983, train_loss=2.9263277

Batch 101230, train_perplexity=18.658983, train_loss=2.9263277

Batch 101240, train_perplexity=18.658983, train_loss=2.9263277

Batch 101250, train_perplexity=18.658983, train_loss=2.9263277

Batch 101260, train_perplexity=18.658983, train_loss=2.9263277

Batch 101270, train_perplexity=18.658983, train_loss=2.9263277

Batch 101280, train_perplexity=18.658983, train_loss=2.9263277

Batch 101290, train_perplexity=18.65898, train_loss=2.9263275

Batch 101300, train_perplexity=18.658983, train_loss=2.9263277

Batch 101310, train_perplexity=18.658983, train_loss=2.9263277

Batch 101320, train_perplexity=18.658983, train_loss=2.9263277

Batch 101330, train_perplexity=18.65898, train_loss=2.9263275

Batch 101340, train_perplexity=18.658974, train_loss=2.9263272

Batch 101350, train_perplexity=18.658983, train_loss=2.9263277

Batch 101360, train_perplexity=18.658983, train_loss=2.9263277

Batch 101370, train_perplexity=18.658983, train_loss=2.9263277

Batch 101380, train_perplexity=18.658983, train_loss=2.9263277

Batch 101390, train_perplexity=18.65898, train_loss=2.9263275

Batch 101400, train_perplexity=18.658974, train_loss=2.9263272

Batch 101410, train_perplexity=18.65898, train_loss=2.9263275

Batch 101420, train_perplexity=18.658983, train_loss=2.9263277

Batch 101430, train_perplexity=18.658974, train_loss=2.9263272

Batch 101440, train_perplexity=18.658974, train_loss=2.9263272

Batch 101450, train_perplexity=18.658974, train_loss=2.9263272

Batch 101460, train_perplexity=18.658974, train_loss=2.9263272

Batch 101470, train_perplexity=18.658983, train_loss=2.9263277

Batch 101480, train_perplexity=18.658983, train_loss=2.9263277

Batch 101490, train_perplexity=18.658974, train_loss=2.9263272

Batch 101500, train_perplexity=18.65898, train_loss=2.9263275

Batch 101510, train_perplexity=18.658974, train_loss=2.9263272

Batch 101520, train_perplexity=18.658974, train_loss=2.9263272

Batch 101530, train_perplexity=18.658974, train_loss=2.9263272

Batch 101540, train_perplexity=18.658974, train_loss=2.9263272

Batch 101550, train_perplexity=18.658974, train_loss=2.9263272

Batch 101560, train_perplexity=18.658974, train_loss=2.9263272

Batch 101570, train_perplexity=18.658974, train_loss=2.9263272

Batch 101580, train_perplexity=18.65897, train_loss=2.926327

Batch 101590, train_perplexity=18.658974, train_loss=2.9263272

Batch 101600, train_perplexity=18.658974, train_loss=2.9263272

Batch 101610, train_perplexity=18.65897, train_loss=2.926327

Batch 101620, train_perplexity=18.65897, train_loss=2.926327

Batch 101630, train_perplexity=18.658974, train_loss=2.9263272

Batch 101640, train_perplexity=18.658974, train_loss=2.9263272

Batch 101650, train_perplexity=18.658974, train_loss=2.9263272

Batch 101660, train_perplexity=18.658974, train_loss=2.9263272

Batch 101670, train_perplexity=18.658966, train_loss=2.9263268

Batch 101680, train_perplexity=18.658966, train_loss=2.9263268

Batch 101690, train_perplexity=18.658974, train_loss=2.9263272

Batch 101700, train_perplexity=18.65897, train_loss=2.926327

Batch 101710, train_perplexity=18.658974, train_loss=2.9263272

Batch 101720, train_perplexity=18.658966, train_loss=2.9263268

Batch 101730, train_perplexity=18.658966, train_loss=2.9263268

Batch 101740, train_perplexity=18.65897, train_loss=2.926327

Batch 101750, train_perplexity=18.65897, train_loss=2.926327

Batch 101760, train_perplexity=18.658966, train_loss=2.9263268

Batch 101770, train_perplexity=18.658966, train_loss=2.9263268

Batch 101780, train_perplexity=18.658966, train_loss=2.9263268

Batch 101790, train_perplexity=18.658966, train_loss=2.9263268

Batch 101800, train_perplexity=18.658966, train_loss=2.9263268

Batch 101810, train_perplexity=18.658966, train_loss=2.9263268

Batch 101820, train_perplexity=18.658966, train_loss=2.9263268

Batch 101830, train_perplexity=18.65897, train_loss=2.926327

Batch 101840, train_perplexity=18.658966, train_loss=2.9263268

Batch 101850, train_perplexity=18.658974, train_loss=2.9263272

Batch 101860, train_perplexity=18.65897, train_loss=2.926327

Batch 101870, train_perplexity=18.658966, train_loss=2.9263268

Batch 101880, train_perplexity=18.658966, train_loss=2.9263268

Batch 101890, train_perplexity=18.65896, train_loss=2.9263265

Batch 101900, train_perplexity=18.658966, train_loss=2.9263268

Batch 101910, train_perplexity=18.658966, train_loss=2.9263268

Batch 101920, train_perplexity=18.658966, train_loss=2.9263268

Batch 101930, train_perplexity=18.658966, train_loss=2.9263268

Batch 101940, train_perplexity=18.658966, train_loss=2.9263268

Batch 101950, train_perplexity=18.658966, train_loss=2.9263268
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 101960, train_perplexity=18.658966, train_loss=2.9263268

Batch 101970, train_perplexity=18.658966, train_loss=2.9263268

Batch 101980, train_perplexity=18.658966, train_loss=2.9263268

Batch 101990, train_perplexity=18.65896, train_loss=2.9263265

Batch 102000, train_perplexity=18.65896, train_loss=2.9263265

Batch 102010, train_perplexity=18.658957, train_loss=2.9263263

Batch 102020, train_perplexity=18.65896, train_loss=2.9263265

Batch 102030, train_perplexity=18.65896, train_loss=2.9263265

Batch 102040, train_perplexity=18.658966, train_loss=2.9263268

Batch 102050, train_perplexity=18.65896, train_loss=2.9263265

Batch 102060, train_perplexity=18.658957, train_loss=2.9263263

Batch 102070, train_perplexity=18.658957, train_loss=2.9263263

Batch 102080, train_perplexity=18.658957, train_loss=2.9263263

Batch 102090, train_perplexity=18.658957, train_loss=2.9263263

Batch 102100, train_perplexity=18.658957, train_loss=2.9263263

Batch 102110, train_perplexity=18.658957, train_loss=2.9263263

Batch 102120, train_perplexity=18.658957, train_loss=2.9263263

Batch 102130, train_perplexity=18.658957, train_loss=2.9263263

Batch 102140, train_perplexity=18.658957, train_loss=2.9263263

Batch 102150, train_perplexity=18.658957, train_loss=2.9263263

Batch 102160, train_perplexity=18.658957, train_loss=2.9263263

Batch 102170, train_perplexity=18.658957, train_loss=2.9263263

Batch 102180, train_perplexity=18.658957, train_loss=2.9263263

Batch 102190, train_perplexity=18.658953, train_loss=2.926326

Batch 102200, train_perplexity=18.658957, train_loss=2.9263263

Batch 102210, train_perplexity=18.658953, train_loss=2.926326

Batch 102220, train_perplexity=18.658957, train_loss=2.9263263

Batch 102230, train_perplexity=18.658953, train_loss=2.926326

Batch 102240, train_perplexity=18.658957, train_loss=2.9263263

Batch 102250, train_perplexity=18.658953, train_loss=2.926326

Batch 102260, train_perplexity=18.658953, train_loss=2.926326

Batch 102270, train_perplexity=18.658957, train_loss=2.9263263

Batch 102280, train_perplexity=18.658947, train_loss=2.9263258

Batch 102290, train_perplexity=18.658953, train_loss=2.926326

Batch 102300, train_perplexity=18.658953, train_loss=2.926326

Batch 102310, train_perplexity=18.658957, train_loss=2.9263263

Batch 102320, train_perplexity=18.658947, train_loss=2.9263258

Batch 102330, train_perplexity=18.658947, train_loss=2.9263258

Batch 102340, train_perplexity=18.658947, train_loss=2.9263258

Batch 102350, train_perplexity=18.658957, train_loss=2.9263263

Batch 102360, train_perplexity=18.658957, train_loss=2.9263263

Batch 102370, train_perplexity=18.658957, train_loss=2.9263263

Batch 102380, train_perplexity=18.658947, train_loss=2.9263258

Batch 102390, train_perplexity=18.658947, train_loss=2.9263258

Batch 102400, train_perplexity=18.658947, train_loss=2.9263258

Batch 102410, train_perplexity=18.658947, train_loss=2.9263258

Batch 102420, train_perplexity=18.658947, train_loss=2.9263258

Batch 102430, train_perplexity=18.658947, train_loss=2.9263258

Batch 102440, train_perplexity=18.658947, train_loss=2.9263258

Batch 102450, train_perplexity=18.658947, train_loss=2.9263258

Batch 102460, train_perplexity=18.658947, train_loss=2.9263258

Batch 102470, train_perplexity=18.658947, train_loss=2.9263258

Batch 102480, train_perplexity=18.658947, train_loss=2.9263258

Batch 102490, train_perplexity=18.65894, train_loss=2.9263253

Batch 102500, train_perplexity=18.658947, train_loss=2.9263258

Batch 102510, train_perplexity=18.658947, train_loss=2.9263258

Batch 102520, train_perplexity=18.658947, train_loss=2.9263258

Batch 102530, train_perplexity=18.658947, train_loss=2.9263258

Batch 102540, train_perplexity=18.658947, train_loss=2.9263258

Batch 102550, train_perplexity=18.658947, train_loss=2.9263258

Batch 102560, train_perplexity=18.658947, train_loss=2.9263258

Batch 102570, train_perplexity=18.658947, train_loss=2.9263258

Batch 102580, train_perplexity=18.65894, train_loss=2.9263253

Batch 102590, train_perplexity=18.65894, train_loss=2.9263253

Batch 102600, train_perplexity=18.658947, train_loss=2.9263258

Batch 102610, train_perplexity=18.658947, train_loss=2.9263258

Batch 102620, train_perplexity=18.658947, train_loss=2.9263258

Batch 102630, train_perplexity=18.658943, train_loss=2.9263256

Batch 102640, train_perplexity=18.65894, train_loss=2.9263253

Batch 102650, train_perplexity=18.65894, train_loss=2.9263253

Batch 102660, train_perplexity=18.658943, train_loss=2.9263256

Batch 102670, train_perplexity=18.658947, train_loss=2.9263258

Batch 102680, train_perplexity=18.65894, train_loss=2.9263253

Batch 102690, train_perplexity=18.65894, train_loss=2.9263253

Batch 102700, train_perplexity=18.65894, train_loss=2.9263253

Batch 102710, train_perplexity=18.65894, train_loss=2.9263253

Batch 102720, train_perplexity=18.65894, train_loss=2.9263253

Batch 102730, train_perplexity=18.65894, train_loss=2.9263253

Batch 102740, train_perplexity=18.658934, train_loss=2.926325

Batch 102750, train_perplexity=18.658943, train_loss=2.9263256

Batch 102760, train_perplexity=18.65894, train_loss=2.9263253

Batch 102770, train_perplexity=18.65894, train_loss=2.9263253

Batch 102780, train_perplexity=18.65894, train_loss=2.9263253

Batch 102790, train_perplexity=18.65894, train_loss=2.9263253

Batch 102800, train_perplexity=18.65894, train_loss=2.9263253

Batch 102810, train_perplexity=18.658934, train_loss=2.926325

Batch 102820, train_perplexity=18.65894, train_loss=2.9263253

Batch 102830, train_perplexity=18.658934, train_loss=2.926325

Batch 102840, train_perplexity=18.658934, train_loss=2.926325

Batch 102850, train_perplexity=18.658934, train_loss=2.926325

Batch 102860, train_perplexity=18.65894, train_loss=2.9263253

Batch 102870, train_perplexity=18.65894, train_loss=2.9263253

Batch 102880, train_perplexity=18.65893, train_loss=2.9263248

Batch 102890, train_perplexity=18.65894, train_loss=2.9263253

Batch 102900, train_perplexity=18.65894, train_loss=2.9263253

Batch 102910, train_perplexity=18.65893, train_loss=2.9263248

Batch 102920, train_perplexity=18.658934, train_loss=2.926325

Batch 102930, train_perplexity=18.65894, train_loss=2.9263253

Batch 102940, train_perplexity=18.65894, train_loss=2.9263253

Batch 102950, train_perplexity=18.658934, train_loss=2.926325

Batch 102960, train_perplexity=18.65893, train_loss=2.9263248

Batch 102970, train_perplexity=18.658934, train_loss=2.926325

Batch 102980, train_perplexity=18.65894, train_loss=2.9263253

Batch 102990, train_perplexity=18.65893, train_loss=2.9263248

Batch 103000, train_perplexity=18.658926, train_loss=2.9263246

Batch 103010, train_perplexity=18.65893, train_loss=2.9263248

Batch 103020, train_perplexity=18.658934, train_loss=2.926325

Batch 103030, train_perplexity=18.65893, train_loss=2.9263248

Batch 103040, train_perplexity=18.658926, train_loss=2.9263246

Batch 103050, train_perplexity=18.65893, train_loss=2.9263248

Batch 103060, train_perplexity=18.65893, train_loss=2.9263248

Batch 103070, train_perplexity=18.65893, train_loss=2.9263248

Batch 103080, train_perplexity=18.65893, train_loss=2.9263248

Batch 103090, train_perplexity=18.65893, train_loss=2.9263248

Batch 103100, train_perplexity=18.658926, train_loss=2.9263246

Batch 103110, train_perplexity=18.658934, train_loss=2.926325

Batch 103120, train_perplexity=18.65893, train_loss=2.9263248

Batch 103130, train_perplexity=18.65893, train_loss=2.9263248

Batch 103140, train_perplexity=18.65893, train_loss=2.9263248

Batch 103150, train_perplexity=18.65893, train_loss=2.9263248

Batch 103160, train_perplexity=18.65893, train_loss=2.9263248

Batch 103170, train_perplexity=18.65893, train_loss=2.9263248

Batch 103180, train_perplexity=18.65893, train_loss=2.9263248

Batch 103190, train_perplexity=18.65893, train_loss=2.9263248

Batch 103200, train_perplexity=18.65893, train_loss=2.9263248

Batch 103210, train_perplexity=18.65893, train_loss=2.9263248

Batch 103220, train_perplexity=18.65893, train_loss=2.9263248

Batch 103230, train_perplexity=18.65892, train_loss=2.9263244

Batch 103240, train_perplexity=18.65893, train_loss=2.9263248

Batch 103250, train_perplexity=18.65893, train_loss=2.9263248
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 103260, train_perplexity=18.65892, train_loss=2.9263244

Batch 103270, train_perplexity=18.658926, train_loss=2.9263246

Batch 103280, train_perplexity=18.658926, train_loss=2.9263246

Batch 103290, train_perplexity=18.65893, train_loss=2.9263248

Batch 103300, train_perplexity=18.65892, train_loss=2.9263244

Batch 103310, train_perplexity=18.65892, train_loss=2.9263244

Batch 103320, train_perplexity=18.65892, train_loss=2.9263244

Batch 103330, train_perplexity=18.65892, train_loss=2.9263244

Batch 103340, train_perplexity=18.65892, train_loss=2.9263244

Batch 103350, train_perplexity=18.658926, train_loss=2.9263246

Batch 103360, train_perplexity=18.658916, train_loss=2.9263241

Batch 103370, train_perplexity=18.65892, train_loss=2.9263244

Batch 103380, train_perplexity=18.65892, train_loss=2.9263244

Batch 103390, train_perplexity=18.65892, train_loss=2.9263244

Batch 103400, train_perplexity=18.65892, train_loss=2.9263244

Batch 103410, train_perplexity=18.65892, train_loss=2.9263244

Batch 103420, train_perplexity=18.65892, train_loss=2.9263244

Batch 103430, train_perplexity=18.65892, train_loss=2.9263244

Batch 103440, train_perplexity=18.65892, train_loss=2.9263244

Batch 103450, train_perplexity=18.65892, train_loss=2.9263244

Batch 103460, train_perplexity=18.658913, train_loss=2.926324

Batch 103470, train_perplexity=18.65892, train_loss=2.9263244

Batch 103480, train_perplexity=18.65892, train_loss=2.9263244

Batch 103490, train_perplexity=18.65892, train_loss=2.9263244

Batch 103500, train_perplexity=18.65892, train_loss=2.9263244

Batch 103510, train_perplexity=18.658916, train_loss=2.9263241

Batch 103520, train_perplexity=18.65892, train_loss=2.9263244

Batch 103530, train_perplexity=18.65892, train_loss=2.9263244

Batch 103540, train_perplexity=18.658916, train_loss=2.9263241

Batch 103550, train_perplexity=18.65892, train_loss=2.9263244

Batch 103560, train_perplexity=18.65892, train_loss=2.9263244

Batch 103570, train_perplexity=18.65892, train_loss=2.9263244

Batch 103580, train_perplexity=18.658916, train_loss=2.9263241

Batch 103590, train_perplexity=18.658916, train_loss=2.9263241

Batch 103600, train_perplexity=18.658916, train_loss=2.9263241

Batch 103610, train_perplexity=18.658916, train_loss=2.9263241

Batch 103620, train_perplexity=18.658913, train_loss=2.926324

Batch 103630, train_perplexity=18.658913, train_loss=2.926324

Batch 103640, train_perplexity=18.658916, train_loss=2.9263241

Batch 103650, train_perplexity=18.658913, train_loss=2.926324

Batch 103660, train_perplexity=18.658916, train_loss=2.9263241

Batch 103670, train_perplexity=18.658916, train_loss=2.9263241

Batch 103680, train_perplexity=18.658913, train_loss=2.926324

Batch 103690, train_perplexity=18.658913, train_loss=2.926324

Batch 103700, train_perplexity=18.658913, train_loss=2.926324

Batch 103710, train_perplexity=18.658913, train_loss=2.926324

Batch 103720, train_perplexity=18.658913, train_loss=2.926324

Batch 103730, train_perplexity=18.658913, train_loss=2.926324

Batch 103740, train_perplexity=18.658907, train_loss=2.9263237

Batch 103750, train_perplexity=18.658903, train_loss=2.9263234

Batch 103760, train_perplexity=18.658913, train_loss=2.926324

Batch 103770, train_perplexity=18.658913, train_loss=2.926324

Batch 103780, train_perplexity=18.658913, train_loss=2.926324

Batch 103790, train_perplexity=18.658913, train_loss=2.926324

Batch 103800, train_perplexity=18.658913, train_loss=2.926324

Batch 103810, train_perplexity=18.658913, train_loss=2.926324

Batch 103820, train_perplexity=18.658913, train_loss=2.926324

Batch 103830, train_perplexity=18.658913, train_loss=2.926324

Batch 103840, train_perplexity=18.658907, train_loss=2.9263237

Batch 103850, train_perplexity=18.658903, train_loss=2.9263234

Batch 103860, train_perplexity=18.658913, train_loss=2.926324

Batch 103870, train_perplexity=18.658907, train_loss=2.9263237

Batch 103880, train_perplexity=18.658913, train_loss=2.926324

Batch 103890, train_perplexity=18.658903, train_loss=2.9263234

Batch 103900, train_perplexity=18.658907, train_loss=2.9263237

Batch 103910, train_perplexity=18.658903, train_loss=2.9263234

Batch 103920, train_perplexity=18.658903, train_loss=2.9263234

Batch 103930, train_perplexity=18.658907, train_loss=2.9263237

Batch 103940, train_perplexity=18.658903, train_loss=2.9263234

Batch 103950, train_perplexity=18.658903, train_loss=2.9263234

Batch 103960, train_perplexity=18.658903, train_loss=2.9263234

Batch 103970, train_perplexity=18.658903, train_loss=2.9263234

Batch 103980, train_perplexity=18.658903, train_loss=2.9263234

Batch 103990, train_perplexity=18.658903, train_loss=2.9263234

Batch 104000, train_perplexity=18.658903, train_loss=2.9263234

Batch 104010, train_perplexity=18.658903, train_loss=2.9263234

Batch 104020, train_perplexity=18.658903, train_loss=2.9263234

Batch 104030, train_perplexity=18.658903, train_loss=2.9263234

Batch 104040, train_perplexity=18.6589, train_loss=2.9263232

Batch 104050, train_perplexity=18.658903, train_loss=2.9263234

Batch 104060, train_perplexity=18.658903, train_loss=2.9263234

Batch 104070, train_perplexity=18.6589, train_loss=2.9263232

Batch 104080, train_perplexity=18.6589, train_loss=2.9263232

Batch 104090, train_perplexity=18.658894, train_loss=2.926323

Batch 104100, train_perplexity=18.658903, train_loss=2.9263234

Batch 104110, train_perplexity=18.658903, train_loss=2.9263234

Batch 104120, train_perplexity=18.6589, train_loss=2.9263232

Batch 104130, train_perplexity=18.658903, train_loss=2.9263234

Batch 104140, train_perplexity=18.658903, train_loss=2.9263234

Batch 104150, train_perplexity=18.658903, train_loss=2.9263234

Batch 104160, train_perplexity=18.658903, train_loss=2.9263234

Batch 104170, train_perplexity=18.658894, train_loss=2.926323

Batch 104180, train_perplexity=18.6589, train_loss=2.9263232

Batch 104190, train_perplexity=18.658894, train_loss=2.926323

Batch 104200, train_perplexity=18.658894, train_loss=2.926323

Batch 104210, train_perplexity=18.658903, train_loss=2.9263234

Batch 104220, train_perplexity=18.6589, train_loss=2.9263232

Batch 104230, train_perplexity=18.658894, train_loss=2.926323

Batch 104240, train_perplexity=18.658894, train_loss=2.926323

Batch 104250, train_perplexity=18.658894, train_loss=2.926323

Batch 104260, train_perplexity=18.658894, train_loss=2.926323

Batch 104270, train_perplexity=18.658894, train_loss=2.926323

Batch 104280, train_perplexity=18.6589, train_loss=2.9263232

Batch 104290, train_perplexity=18.65889, train_loss=2.9263227

Batch 104300, train_perplexity=18.658894, train_loss=2.926323

Batch 104310, train_perplexity=18.658894, train_loss=2.926323

Batch 104320, train_perplexity=18.658894, train_loss=2.926323

Batch 104330, train_perplexity=18.658894, train_loss=2.926323

Batch 104340, train_perplexity=18.658894, train_loss=2.926323

Batch 104350, train_perplexity=18.658894, train_loss=2.926323

Batch 104360, train_perplexity=18.658894, train_loss=2.926323

Batch 104370, train_perplexity=18.658894, train_loss=2.926323

Batch 104380, train_perplexity=18.658894, train_loss=2.926323

Batch 104390, train_perplexity=18.658894, train_loss=2.926323

Batch 104400, train_perplexity=18.65889, train_loss=2.9263227

Batch 104410, train_perplexity=18.65889, train_loss=2.9263227

Batch 104420, train_perplexity=18.65889, train_loss=2.9263227

Batch 104430, train_perplexity=18.658894, train_loss=2.926323

Batch 104440, train_perplexity=18.658894, train_loss=2.926323

Batch 104450, train_perplexity=18.658894, train_loss=2.926323

Batch 104460, train_perplexity=18.658886, train_loss=2.9263225

Batch 104470, train_perplexity=18.658886, train_loss=2.9263225

Batch 104480, train_perplexity=18.658886, train_loss=2.9263225

Batch 104490, train_perplexity=18.658886, train_loss=2.9263225

Batch 104500, train_perplexity=18.65889, train_loss=2.9263227

Batch 104510, train_perplexity=18.658886, train_loss=2.9263225

Batch 104520, train_perplexity=18.658886, train_loss=2.9263225

Batch 104530, train_perplexity=18.658886, train_loss=2.9263225

Batch 104540, train_perplexity=18.658886, train_loss=2.9263225

Batch 104550, train_perplexity=18.658886, train_loss=2.9263225
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 104560, train_perplexity=18.658886, train_loss=2.9263225

Batch 104570, train_perplexity=18.658886, train_loss=2.9263225

Batch 104580, train_perplexity=18.658886, train_loss=2.9263225

Batch 104590, train_perplexity=18.65888, train_loss=2.9263222

Batch 104600, train_perplexity=18.658886, train_loss=2.9263225

Batch 104610, train_perplexity=18.658886, train_loss=2.9263225

Batch 104620, train_perplexity=18.658876, train_loss=2.926322

Batch 104630, train_perplexity=18.658886, train_loss=2.9263225

Batch 104640, train_perplexity=18.65888, train_loss=2.9263222

Batch 104650, train_perplexity=18.658886, train_loss=2.9263225

Batch 104660, train_perplexity=18.658886, train_loss=2.9263225

Batch 104670, train_perplexity=18.658876, train_loss=2.926322

Batch 104680, train_perplexity=18.658876, train_loss=2.926322

Batch 104690, train_perplexity=18.658876, train_loss=2.926322

Batch 104700, train_perplexity=18.658876, train_loss=2.926322

Batch 104710, train_perplexity=18.658876, train_loss=2.926322

Batch 104720, train_perplexity=18.658876, train_loss=2.926322

Batch 104730, train_perplexity=18.658876, train_loss=2.926322

Batch 104740, train_perplexity=18.658876, train_loss=2.926322

Batch 104750, train_perplexity=18.658876, train_loss=2.926322

Batch 104760, train_perplexity=18.658886, train_loss=2.9263225

Batch 104770, train_perplexity=18.658873, train_loss=2.9263217

Batch 104780, train_perplexity=18.658873, train_loss=2.9263217

Batch 104790, train_perplexity=18.658876, train_loss=2.926322

Batch 104800, train_perplexity=18.658876, train_loss=2.926322

Batch 104810, train_perplexity=18.658876, train_loss=2.926322

Batch 104820, train_perplexity=18.658876, train_loss=2.926322

Batch 104830, train_perplexity=18.658873, train_loss=2.9263217

Batch 104840, train_perplexity=18.658876, train_loss=2.926322

Batch 104850, train_perplexity=18.658876, train_loss=2.926322

Batch 104860, train_perplexity=18.658876, train_loss=2.926322

Batch 104870, train_perplexity=18.658873, train_loss=2.9263217

Batch 104880, train_perplexity=18.658867, train_loss=2.9263215

Batch 104890, train_perplexity=18.658876, train_loss=2.926322

Batch 104900, train_perplexity=18.658873, train_loss=2.9263217

Batch 104910, train_perplexity=18.658873, train_loss=2.9263217

Batch 104920, train_perplexity=18.658873, train_loss=2.9263217

Batch 104930, train_perplexity=18.658873, train_loss=2.9263217

Batch 104940, train_perplexity=18.658873, train_loss=2.9263217

Batch 104950, train_perplexity=18.658867, train_loss=2.9263215

Batch 104960, train_perplexity=18.658876, train_loss=2.926322

Batch 104970, train_perplexity=18.658873, train_loss=2.9263217

Batch 104980, train_perplexity=18.658867, train_loss=2.9263215

Batch 104990, train_perplexity=18.658867, train_loss=2.9263215

Batch 105000, train_perplexity=18.658876, train_loss=2.926322

Batch 105010, train_perplexity=18.658867, train_loss=2.9263215

Batch 105020, train_perplexity=18.658867, train_loss=2.9263215

Batch 105030, train_perplexity=18.658867, train_loss=2.9263215

Batch 105040, train_perplexity=18.658867, train_loss=2.9263215

Batch 105050, train_perplexity=18.658873, train_loss=2.9263217

Batch 105060, train_perplexity=18.658867, train_loss=2.9263215

Batch 105070, train_perplexity=18.658867, train_loss=2.9263215

Batch 105080, train_perplexity=18.658867, train_loss=2.9263215

Batch 105090, train_perplexity=18.658867, train_loss=2.9263215

Batch 105100, train_perplexity=18.658867, train_loss=2.9263215

Batch 105110, train_perplexity=18.658867, train_loss=2.9263215

Batch 105120, train_perplexity=18.658863, train_loss=2.9263213

Batch 105130, train_perplexity=18.658863, train_loss=2.9263213

Batch 105140, train_perplexity=18.65886, train_loss=2.926321

Batch 105150, train_perplexity=18.658867, train_loss=2.9263215

Batch 105160, train_perplexity=18.658863, train_loss=2.9263213

Batch 105170, train_perplexity=18.658867, train_loss=2.9263215

Batch 105180, train_perplexity=18.658867, train_loss=2.9263215

Batch 105190, train_perplexity=18.658867, train_loss=2.9263215

Batch 105200, train_perplexity=18.65886, train_loss=2.926321

Batch 105210, train_perplexity=18.658863, train_loss=2.9263213

Batch 105220, train_perplexity=18.658863, train_loss=2.9263213

Batch 105230, train_perplexity=18.658867, train_loss=2.9263215

Batch 105240, train_perplexity=18.658863, train_loss=2.9263213

Batch 105250, train_perplexity=18.65886, train_loss=2.926321

Batch 105260, train_perplexity=18.65886, train_loss=2.926321

Batch 105270, train_perplexity=18.65886, train_loss=2.926321

Batch 105280, train_perplexity=18.658867, train_loss=2.9263215

Batch 105290, train_perplexity=18.65886, train_loss=2.926321

Batch 105300, train_perplexity=18.65886, train_loss=2.926321

Batch 105310, train_perplexity=18.65886, train_loss=2.926321

Batch 105320, train_perplexity=18.65886, train_loss=2.926321

Batch 105330, train_perplexity=18.65886, train_loss=2.926321

Batch 105340, train_perplexity=18.65886, train_loss=2.926321

Batch 105350, train_perplexity=18.658863, train_loss=2.9263213

Batch 105360, train_perplexity=18.658854, train_loss=2.9263208

Batch 105370, train_perplexity=18.65886, train_loss=2.926321

Batch 105380, train_perplexity=18.658854, train_loss=2.9263208

Batch 105390, train_perplexity=18.65886, train_loss=2.926321

Batch 105400, train_perplexity=18.65886, train_loss=2.926321

Batch 105410, train_perplexity=18.65886, train_loss=2.926321

Batch 105420, train_perplexity=18.65886, train_loss=2.926321

Batch 105430, train_perplexity=18.65886, train_loss=2.926321

Batch 105440, train_perplexity=18.65886, train_loss=2.926321

Batch 105450, train_perplexity=18.658854, train_loss=2.9263208

Batch 105460, train_perplexity=18.65885, train_loss=2.9263206

Batch 105470, train_perplexity=18.65886, train_loss=2.926321

Batch 105480, train_perplexity=18.65886, train_loss=2.926321

Batch 105490, train_perplexity=18.65885, train_loss=2.9263206

Batch 105500, train_perplexity=18.65886, train_loss=2.926321

Batch 105510, train_perplexity=18.658854, train_loss=2.9263208

Batch 105520, train_perplexity=18.65886, train_loss=2.926321

Batch 105530, train_perplexity=18.658854, train_loss=2.9263208

Batch 105540, train_perplexity=18.658854, train_loss=2.9263208

Batch 105550, train_perplexity=18.658854, train_loss=2.9263208

Batch 105560, train_perplexity=18.65885, train_loss=2.9263206

Batch 105570, train_perplexity=18.65885, train_loss=2.9263206

Batch 105580, train_perplexity=18.65885, train_loss=2.9263206

Batch 105590, train_perplexity=18.65886, train_loss=2.926321

Batch 105600, train_perplexity=18.65885, train_loss=2.9263206

Batch 105610, train_perplexity=18.65885, train_loss=2.9263206

Batch 105620, train_perplexity=18.65885, train_loss=2.9263206

Batch 105630, train_perplexity=18.65885, train_loss=2.9263206

Batch 105640, train_perplexity=18.65885, train_loss=2.9263206

Batch 105650, train_perplexity=18.65885, train_loss=2.9263206

Batch 105660, train_perplexity=18.65885, train_loss=2.9263206

Batch 105670, train_perplexity=18.65885, train_loss=2.9263206

Batch 105680, train_perplexity=18.65885, train_loss=2.9263206

Batch 105690, train_perplexity=18.65885, train_loss=2.9263206

Batch 105700, train_perplexity=18.658846, train_loss=2.9263203

Batch 105710, train_perplexity=18.658846, train_loss=2.9263203

Batch 105720, train_perplexity=18.65885, train_loss=2.9263206

Batch 105730, train_perplexity=18.65885, train_loss=2.9263206

Batch 105740, train_perplexity=18.65885, train_loss=2.9263206

Batch 105750, train_perplexity=18.658846, train_loss=2.9263203

Batch 105760, train_perplexity=18.658846, train_loss=2.9263203

Batch 105770, train_perplexity=18.658846, train_loss=2.9263203

Batch 105780, train_perplexity=18.65885, train_loss=2.9263206

Batch 105790, train_perplexity=18.658846, train_loss=2.9263203

Batch 105800, train_perplexity=18.65885, train_loss=2.9263206

Batch 105810, train_perplexity=18.658846, train_loss=2.9263203

Batch 105820, train_perplexity=18.65885, train_loss=2.9263206

Batch 105830, train_perplexity=18.65884, train_loss=2.92632

Batch 105840, train_perplexity=18.65884, train_loss=2.92632

Batch 105850, train_perplexity=18.65884, train_loss=2.92632
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 105860, train_perplexity=18.65884, train_loss=2.92632

Batch 105870, train_perplexity=18.65884, train_loss=2.92632

Batch 105880, train_perplexity=18.65884, train_loss=2.92632

Batch 105890, train_perplexity=18.65884, train_loss=2.92632

Batch 105900, train_perplexity=18.658836, train_loss=2.9263198

Batch 105910, train_perplexity=18.658846, train_loss=2.9263203

Batch 105920, train_perplexity=18.65884, train_loss=2.92632

Batch 105930, train_perplexity=18.65884, train_loss=2.92632

Batch 105940, train_perplexity=18.658836, train_loss=2.9263198

Batch 105950, train_perplexity=18.65884, train_loss=2.92632

Batch 105960, train_perplexity=18.658846, train_loss=2.9263203

Batch 105970, train_perplexity=18.65884, train_loss=2.92632

Batch 105980, train_perplexity=18.65884, train_loss=2.92632

Batch 105990, train_perplexity=18.65884, train_loss=2.92632

Batch 106000, train_perplexity=18.65884, train_loss=2.92632

Batch 106010, train_perplexity=18.65884, train_loss=2.92632

Batch 106020, train_perplexity=18.658836, train_loss=2.9263198

Batch 106030, train_perplexity=18.65884, train_loss=2.92632

Batch 106040, train_perplexity=18.658836, train_loss=2.9263198

Batch 106050, train_perplexity=18.658836, train_loss=2.9263198

Batch 106060, train_perplexity=18.658836, train_loss=2.9263198

Batch 106070, train_perplexity=18.658833, train_loss=2.9263196

Batch 106080, train_perplexity=18.65884, train_loss=2.92632

Batch 106090, train_perplexity=18.658833, train_loss=2.9263196

Batch 106100, train_perplexity=18.65884, train_loss=2.92632

Batch 106110, train_perplexity=18.658833, train_loss=2.9263196

Batch 106120, train_perplexity=18.658833, train_loss=2.9263196

Batch 106130, train_perplexity=18.658833, train_loss=2.9263196

Batch 106140, train_perplexity=18.658833, train_loss=2.9263196

Batch 106150, train_perplexity=18.658836, train_loss=2.9263198

Batch 106160, train_perplexity=18.658833, train_loss=2.9263196

Batch 106170, train_perplexity=18.658833, train_loss=2.9263196

Batch 106180, train_perplexity=18.658833, train_loss=2.9263196

Batch 106190, train_perplexity=18.658833, train_loss=2.9263196

Batch 106200, train_perplexity=18.658833, train_loss=2.9263196

Batch 106210, train_perplexity=18.658833, train_loss=2.9263196

Batch 106220, train_perplexity=18.658833, train_loss=2.9263196

Batch 106230, train_perplexity=18.658833, train_loss=2.9263196

Batch 106240, train_perplexity=18.658833, train_loss=2.9263196

Batch 106250, train_perplexity=18.658827, train_loss=2.9263194

Batch 106260, train_perplexity=18.658833, train_loss=2.9263196

Batch 106270, train_perplexity=18.658833, train_loss=2.9263196

Batch 106280, train_perplexity=18.658833, train_loss=2.9263196

Batch 106290, train_perplexity=18.658833, train_loss=2.9263196

Batch 106300, train_perplexity=18.658833, train_loss=2.9263196

Batch 106310, train_perplexity=18.658827, train_loss=2.9263194

Batch 106320, train_perplexity=18.658833, train_loss=2.9263196

Batch 106330, train_perplexity=18.658833, train_loss=2.9263196

Batch 106340, train_perplexity=18.658823, train_loss=2.9263191

Batch 106350, train_perplexity=18.658833, train_loss=2.9263196

Batch 106360, train_perplexity=18.658833, train_loss=2.9263196

Batch 106370, train_perplexity=18.658823, train_loss=2.9263191

Batch 106380, train_perplexity=18.658827, train_loss=2.9263194

Batch 106390, train_perplexity=18.658827, train_loss=2.9263194

Batch 106400, train_perplexity=18.658833, train_loss=2.9263196

Batch 106410, train_perplexity=18.658827, train_loss=2.9263194

Batch 106420, train_perplexity=18.658823, train_loss=2.9263191

Batch 106430, train_perplexity=18.658827, train_loss=2.9263194

Batch 106440, train_perplexity=18.658823, train_loss=2.9263191

Batch 106450, train_perplexity=18.658823, train_loss=2.9263191

Batch 106460, train_perplexity=18.658823, train_loss=2.9263191

Batch 106470, train_perplexity=18.658823, train_loss=2.9263191

Batch 106480, train_perplexity=18.658823, train_loss=2.9263191

Batch 106490, train_perplexity=18.658823, train_loss=2.9263191

Batch 106500, train_perplexity=18.658823, train_loss=2.9263191

Batch 106510, train_perplexity=18.658823, train_loss=2.9263191

Batch 106520, train_perplexity=18.658823, train_loss=2.9263191

Batch 106530, train_perplexity=18.658823, train_loss=2.9263191

Batch 106540, train_perplexity=18.658823, train_loss=2.9263191

Batch 106550, train_perplexity=18.65882, train_loss=2.926319

Batch 106560, train_perplexity=18.658823, train_loss=2.9263191

Batch 106570, train_perplexity=18.658823, train_loss=2.9263191

Batch 106580, train_perplexity=18.658823, train_loss=2.9263191

Batch 106590, train_perplexity=18.65882, train_loss=2.926319

Batch 106600, train_perplexity=18.658813, train_loss=2.9263186

Batch 106610, train_perplexity=18.658823, train_loss=2.9263191

Batch 106620, train_perplexity=18.658823, train_loss=2.9263191

Batch 106630, train_perplexity=18.65882, train_loss=2.926319

Batch 106640, train_perplexity=18.65882, train_loss=2.926319

Batch 106650, train_perplexity=18.658823, train_loss=2.9263191

Batch 106660, train_perplexity=18.658813, train_loss=2.9263186

Batch 106670, train_perplexity=18.658823, train_loss=2.9263191

Batch 106680, train_perplexity=18.658813, train_loss=2.9263186

Batch 106690, train_perplexity=18.658813, train_loss=2.9263186

Batch 106700, train_perplexity=18.658823, train_loss=2.9263191

Batch 106710, train_perplexity=18.658823, train_loss=2.9263191

Batch 106720, train_perplexity=18.65882, train_loss=2.926319

Batch 106730, train_perplexity=18.658813, train_loss=2.9263186

Batch 106740, train_perplexity=18.658813, train_loss=2.9263186

Batch 106750, train_perplexity=18.658813, train_loss=2.9263186

Batch 106760, train_perplexity=18.658823, train_loss=2.9263191

Batch 106770, train_perplexity=18.658813, train_loss=2.9263186

Batch 106780, train_perplexity=18.65882, train_loss=2.926319

Batch 106790, train_perplexity=18.658813, train_loss=2.9263186

Batch 106800, train_perplexity=18.65882, train_loss=2.926319

Batch 106810, train_perplexity=18.658813, train_loss=2.9263186

Batch 106820, train_perplexity=18.658813, train_loss=2.9263186

Batch 106830, train_perplexity=18.658813, train_loss=2.9263186

Batch 106840, train_perplexity=18.658813, train_loss=2.9263186

Batch 106850, train_perplexity=18.658813, train_loss=2.9263186

Batch 106860, train_perplexity=18.658813, train_loss=2.9263186

Batch 106870, train_perplexity=18.65881, train_loss=2.9263184

Batch 106880, train_perplexity=18.658813, train_loss=2.9263186

Batch 106890, train_perplexity=18.658813, train_loss=2.9263186

Batch 106900, train_perplexity=18.658806, train_loss=2.9263182

Batch 106910, train_perplexity=18.658813, train_loss=2.9263186

Batch 106920, train_perplexity=18.65881, train_loss=2.9263184

Batch 106930, train_perplexity=18.65881, train_loss=2.9263184

Batch 106940, train_perplexity=18.65881, train_loss=2.9263184

Batch 106950, train_perplexity=18.658813, train_loss=2.9263186

Batch 106960, train_perplexity=18.658806, train_loss=2.9263182

Batch 106970, train_perplexity=18.658813, train_loss=2.9263186

Batch 106980, train_perplexity=18.658813, train_loss=2.9263186

Batch 106990, train_perplexity=18.658806, train_loss=2.9263182

Batch 107000, train_perplexity=18.65881, train_loss=2.9263184

Batch 107010, train_perplexity=18.65881, train_loss=2.9263184

Batch 107020, train_perplexity=18.65881, train_loss=2.9263184

Batch 107030, train_perplexity=18.6588, train_loss=2.926318

Batch 107040, train_perplexity=18.658806, train_loss=2.9263182

Batch 107050, train_perplexity=18.658806, train_loss=2.9263182

Batch 107060, train_perplexity=18.6588, train_loss=2.926318

Batch 107070, train_perplexity=18.65881, train_loss=2.9263184

Batch 107080, train_perplexity=18.658806, train_loss=2.9263182

Batch 107090, train_perplexity=18.658806, train_loss=2.9263182

Batch 107100, train_perplexity=18.65881, train_loss=2.9263184

Batch 107110, train_perplexity=18.658806, train_loss=2.9263182

Batch 107120, train_perplexity=18.658806, train_loss=2.9263182

Batch 107130, train_perplexity=18.658806, train_loss=2.9263182

Batch 107140, train_perplexity=18.658806, train_loss=2.9263182

Batch 107150, train_perplexity=18.658806, train_loss=2.9263182
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 107160, train_perplexity=18.658806, train_loss=2.9263182

Batch 107170, train_perplexity=18.658806, train_loss=2.9263182

Batch 107180, train_perplexity=18.658806, train_loss=2.9263182

Batch 107190, train_perplexity=18.6588, train_loss=2.926318

Batch 107200, train_perplexity=18.6588, train_loss=2.926318

Batch 107210, train_perplexity=18.6588, train_loss=2.926318

Batch 107220, train_perplexity=18.658806, train_loss=2.9263182

Batch 107230, train_perplexity=18.658806, train_loss=2.9263182

Batch 107240, train_perplexity=18.6588, train_loss=2.926318

Batch 107250, train_perplexity=18.658796, train_loss=2.9263177

Batch 107260, train_perplexity=18.658796, train_loss=2.9263177

Batch 107270, train_perplexity=18.658806, train_loss=2.9263182

Batch 107280, train_perplexity=18.658796, train_loss=2.9263177

Batch 107290, train_perplexity=18.6588, train_loss=2.926318

Batch 107300, train_perplexity=18.6588, train_loss=2.926318

Batch 107310, train_perplexity=18.658806, train_loss=2.9263182

Batch 107320, train_perplexity=18.658796, train_loss=2.9263177

Batch 107330, train_perplexity=18.6588, train_loss=2.926318

Batch 107340, train_perplexity=18.6588, train_loss=2.926318

Batch 107350, train_perplexity=18.658796, train_loss=2.9263177

Batch 107360, train_perplexity=18.6588, train_loss=2.926318

Batch 107370, train_perplexity=18.658796, train_loss=2.9263177

Batch 107380, train_perplexity=18.658796, train_loss=2.9263177

Batch 107390, train_perplexity=18.658796, train_loss=2.9263177

Batch 107400, train_perplexity=18.658796, train_loss=2.9263177

Batch 107410, train_perplexity=18.658796, train_loss=2.9263177

Batch 107420, train_perplexity=18.658796, train_loss=2.9263177

Batch 107430, train_perplexity=18.658796, train_loss=2.9263177

Batch 107440, train_perplexity=18.658796, train_loss=2.9263177

Batch 107450, train_perplexity=18.658796, train_loss=2.9263177

Batch 107460, train_perplexity=18.658796, train_loss=2.9263177

Batch 107470, train_perplexity=18.658792, train_loss=2.9263175

Batch 107480, train_perplexity=18.658792, train_loss=2.9263175

Batch 107490, train_perplexity=18.658796, train_loss=2.9263177

Batch 107500, train_perplexity=18.658792, train_loss=2.9263175

Batch 107510, train_perplexity=18.658796, train_loss=2.9263177

Batch 107520, train_perplexity=18.658796, train_loss=2.9263177

Batch 107530, train_perplexity=18.658796, train_loss=2.9263177

Batch 107540, train_perplexity=18.658796, train_loss=2.9263177

Batch 107550, train_perplexity=18.658796, train_loss=2.9263177

Batch 107560, train_perplexity=18.658792, train_loss=2.9263175

Batch 107570, train_perplexity=18.658787, train_loss=2.9263172

Batch 107580, train_perplexity=18.658787, train_loss=2.9263172

Batch 107590, train_perplexity=18.658792, train_loss=2.9263175

Batch 107600, train_perplexity=18.658796, train_loss=2.9263177

Batch 107610, train_perplexity=18.658792, train_loss=2.9263175

Batch 107620, train_perplexity=18.658787, train_loss=2.9263172

Batch 107630, train_perplexity=18.658787, train_loss=2.9263172

Batch 107640, train_perplexity=18.658787, train_loss=2.9263172

Batch 107650, train_perplexity=18.658792, train_loss=2.9263175

Batch 107660, train_perplexity=18.658796, train_loss=2.9263177

Batch 107670, train_perplexity=18.658787, train_loss=2.9263172

Batch 107680, train_perplexity=18.658787, train_loss=2.9263172

Batch 107690, train_perplexity=18.658787, train_loss=2.9263172

Batch 107700, train_perplexity=18.658792, train_loss=2.9263175

Batch 107710, train_perplexity=18.658787, train_loss=2.9263172

Batch 107720, train_perplexity=18.658787, train_loss=2.9263172

Batch 107730, train_perplexity=18.658787, train_loss=2.9263172

Batch 107740, train_perplexity=18.658783, train_loss=2.926317

Batch 107750, train_perplexity=18.658787, train_loss=2.9263172

Batch 107760, train_perplexity=18.658787, train_loss=2.9263172

Batch 107770, train_perplexity=18.658787, train_loss=2.9263172

Batch 107780, train_perplexity=18.658787, train_loss=2.9263172

Batch 107790, train_perplexity=18.658787, train_loss=2.9263172

Batch 107800, train_perplexity=18.658787, train_loss=2.9263172

Batch 107810, train_perplexity=18.658787, train_loss=2.9263172

Batch 107820, train_perplexity=18.658792, train_loss=2.9263175

Batch 107830, train_perplexity=18.658787, train_loss=2.9263172

Batch 107840, train_perplexity=18.658787, train_loss=2.9263172

Batch 107850, train_perplexity=18.658787, train_loss=2.9263172

Batch 107860, train_perplexity=18.658787, train_loss=2.9263172

Batch 107870, train_perplexity=18.658787, train_loss=2.9263172

Batch 107880, train_perplexity=18.658787, train_loss=2.9263172

Batch 107890, train_perplexity=18.658787, train_loss=2.9263172

Batch 107900, train_perplexity=18.658787, train_loss=2.9263172

Batch 107910, train_perplexity=18.658783, train_loss=2.926317

Batch 107920, train_perplexity=18.65878, train_loss=2.9263167

Batch 107930, train_perplexity=18.65878, train_loss=2.9263167

Batch 107940, train_perplexity=18.658783, train_loss=2.926317

Batch 107950, train_perplexity=18.658783, train_loss=2.926317

Batch 107960, train_perplexity=18.658787, train_loss=2.9263172

Batch 107970, train_perplexity=18.65878, train_loss=2.9263167

Batch 107980, train_perplexity=18.65878, train_loss=2.9263167

Batch 107990, train_perplexity=18.65878, train_loss=2.9263167

Batch 108000, train_perplexity=18.65878, train_loss=2.9263167

Batch 108010, train_perplexity=18.658787, train_loss=2.9263172

Batch 108020, train_perplexity=18.658773, train_loss=2.9263165

Batch 108030, train_perplexity=18.65878, train_loss=2.9263167

Batch 108040, train_perplexity=18.65878, train_loss=2.9263167

Batch 108050, train_perplexity=18.658783, train_loss=2.926317

Batch 108060, train_perplexity=18.65878, train_loss=2.9263167

Batch 108070, train_perplexity=18.658773, train_loss=2.9263165

Batch 108080, train_perplexity=18.65878, train_loss=2.9263167

Batch 108090, train_perplexity=18.65878, train_loss=2.9263167

Batch 108100, train_perplexity=18.65878, train_loss=2.9263167

Batch 108110, train_perplexity=18.658783, train_loss=2.926317

Batch 108120, train_perplexity=18.65878, train_loss=2.9263167

Batch 108130, train_perplexity=18.658773, train_loss=2.9263165

Batch 108140, train_perplexity=18.658773, train_loss=2.9263165

Batch 108150, train_perplexity=18.658773, train_loss=2.9263165

Batch 108160, train_perplexity=18.658773, train_loss=2.9263165

Batch 108170, train_perplexity=18.65877, train_loss=2.9263163

Batch 108180, train_perplexity=18.658773, train_loss=2.9263165

Batch 108190, train_perplexity=18.658773, train_loss=2.9263165

Batch 108200, train_perplexity=18.65877, train_loss=2.9263163

Batch 108210, train_perplexity=18.65878, train_loss=2.9263167

Batch 108220, train_perplexity=18.65878, train_loss=2.9263167

Batch 108230, train_perplexity=18.65878, train_loss=2.9263167

Batch 108240, train_perplexity=18.65877, train_loss=2.9263163

Batch 108250, train_perplexity=18.658773, train_loss=2.9263165

Batch 108260, train_perplexity=18.65878, train_loss=2.9263167

Batch 108270, train_perplexity=18.65877, train_loss=2.9263163

Batch 108280, train_perplexity=18.65877, train_loss=2.9263163

Batch 108290, train_perplexity=18.65877, train_loss=2.9263163

Batch 108300, train_perplexity=18.658766, train_loss=2.926316

Batch 108310, train_perplexity=18.658766, train_loss=2.926316

Batch 108320, train_perplexity=18.65877, train_loss=2.9263163

Batch 108330, train_perplexity=18.65877, train_loss=2.9263163

Batch 108340, train_perplexity=18.65877, train_loss=2.9263163

Batch 108350, train_perplexity=18.658773, train_loss=2.9263165

Batch 108360, train_perplexity=18.65877, train_loss=2.9263163

Batch 108370, train_perplexity=18.65877, train_loss=2.9263163

Batch 108380, train_perplexity=18.658766, train_loss=2.926316

Batch 108390, train_perplexity=18.65877, train_loss=2.9263163

Batch 108400, train_perplexity=18.658773, train_loss=2.9263165

Batch 108410, train_perplexity=18.658766, train_loss=2.926316

Batch 108420, train_perplexity=18.658766, train_loss=2.926316

Batch 108430, train_perplexity=18.65877, train_loss=2.9263163

Batch 108440, train_perplexity=18.658766, train_loss=2.926316

Batch 108450, train_perplexity=18.65877, train_loss=2.9263163
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 108460, train_perplexity=18.658766, train_loss=2.926316

Batch 108470, train_perplexity=18.658773, train_loss=2.9263165

Batch 108480, train_perplexity=18.65876, train_loss=2.9263158

Batch 108490, train_perplexity=18.658766, train_loss=2.926316

Batch 108500, train_perplexity=18.65877, train_loss=2.9263163

Batch 108510, train_perplexity=18.658766, train_loss=2.926316

Batch 108520, train_perplexity=18.658766, train_loss=2.926316

Batch 108530, train_perplexity=18.658766, train_loss=2.926316

Batch 108540, train_perplexity=18.65877, train_loss=2.9263163

Batch 108550, train_perplexity=18.65876, train_loss=2.9263158

Batch 108560, train_perplexity=18.65877, train_loss=2.9263163

Batch 108570, train_perplexity=18.65876, train_loss=2.9263158

Batch 108580, train_perplexity=18.65876, train_loss=2.9263158

Batch 108590, train_perplexity=18.65876, train_loss=2.9263158

Batch 108600, train_perplexity=18.65876, train_loss=2.9263158

Batch 108610, train_perplexity=18.658766, train_loss=2.926316

Batch 108620, train_perplexity=18.65876, train_loss=2.9263158

Batch 108630, train_perplexity=18.65876, train_loss=2.9263158

Batch 108640, train_perplexity=18.65876, train_loss=2.9263158

Batch 108650, train_perplexity=18.658756, train_loss=2.9263155

Batch 108660, train_perplexity=18.65876, train_loss=2.9263158

Batch 108670, train_perplexity=18.65876, train_loss=2.9263158

Batch 108680, train_perplexity=18.65876, train_loss=2.9263158

Batch 108690, train_perplexity=18.65876, train_loss=2.9263158

Batch 108700, train_perplexity=18.658756, train_loss=2.9263155

Batch 108710, train_perplexity=18.65876, train_loss=2.9263158

Batch 108720, train_perplexity=18.65876, train_loss=2.9263158

Batch 108730, train_perplexity=18.65876, train_loss=2.9263158

Batch 108740, train_perplexity=18.65876, train_loss=2.9263158

Batch 108750, train_perplexity=18.65876, train_loss=2.9263158

Batch 108760, train_perplexity=18.65876, train_loss=2.9263158

Batch 108770, train_perplexity=18.658756, train_loss=2.9263155

Batch 108780, train_perplexity=18.65876, train_loss=2.9263158

Batch 108790, train_perplexity=18.65876, train_loss=2.9263158

Batch 108800, train_perplexity=18.658756, train_loss=2.9263155

Batch 108810, train_perplexity=18.658756, train_loss=2.9263155

Batch 108820, train_perplexity=18.658756, train_loss=2.9263155

Batch 108830, train_perplexity=18.65876, train_loss=2.9263158

Batch 108840, train_perplexity=18.658756, train_loss=2.9263155

Batch 108850, train_perplexity=18.658752, train_loss=2.9263153

Batch 108860, train_perplexity=18.658752, train_loss=2.9263153

Batch 108870, train_perplexity=18.658756, train_loss=2.9263155

Batch 108880, train_perplexity=18.65876, train_loss=2.9263158

Batch 108890, train_perplexity=18.658752, train_loss=2.9263153

Batch 108900, train_perplexity=18.658752, train_loss=2.9263153

Batch 108910, train_perplexity=18.658752, train_loss=2.9263153

Batch 108920, train_perplexity=18.658752, train_loss=2.9263153

Batch 108930, train_perplexity=18.658752, train_loss=2.9263153

Batch 108940, train_perplexity=18.658747, train_loss=2.926315

Batch 108950, train_perplexity=18.658752, train_loss=2.9263153

Batch 108960, train_perplexity=18.658747, train_loss=2.926315

Batch 108970, train_perplexity=18.658752, train_loss=2.9263153

Batch 108980, train_perplexity=18.658743, train_loss=2.9263148

Batch 108990, train_perplexity=18.658747, train_loss=2.926315

Batch 109000, train_perplexity=18.658743, train_loss=2.9263148

Batch 109010, train_perplexity=18.658752, train_loss=2.9263153

Batch 109020, train_perplexity=18.658747, train_loss=2.926315

Batch 109030, train_perplexity=18.658747, train_loss=2.926315

Batch 109040, train_perplexity=18.658747, train_loss=2.926315

Batch 109050, train_perplexity=18.658747, train_loss=2.926315

Batch 109060, train_perplexity=18.658752, train_loss=2.9263153

Batch 109070, train_perplexity=18.658747, train_loss=2.926315

Batch 109080, train_perplexity=18.658747, train_loss=2.926315

Batch 109090, train_perplexity=18.658747, train_loss=2.926315

Batch 109100, train_perplexity=18.658752, train_loss=2.9263153

Batch 109110, train_perplexity=18.658743, train_loss=2.9263148

Batch 109120, train_perplexity=18.658747, train_loss=2.926315

Batch 109130, train_perplexity=18.658743, train_loss=2.9263148

Batch 109140, train_perplexity=18.658752, train_loss=2.9263153

Batch 109150, train_perplexity=18.658743, train_loss=2.9263148

Batch 109160, train_perplexity=18.658747, train_loss=2.926315

Batch 109170, train_perplexity=18.658752, train_loss=2.9263153

Batch 109180, train_perplexity=18.658743, train_loss=2.9263148

Batch 109190, train_perplexity=18.658752, train_loss=2.9263153

Batch 109200, train_perplexity=18.658743, train_loss=2.9263148

Batch 109210, train_perplexity=18.658743, train_loss=2.9263148

Batch 109220, train_perplexity=18.658743, train_loss=2.9263148

Batch 109230, train_perplexity=18.658743, train_loss=2.9263148

Batch 109240, train_perplexity=18.658743, train_loss=2.9263148

Batch 109250, train_perplexity=18.658743, train_loss=2.9263148

Batch 109260, train_perplexity=18.658743, train_loss=2.9263148

Batch 109270, train_perplexity=18.658743, train_loss=2.9263148

Batch 109280, train_perplexity=18.658743, train_loss=2.9263148

Batch 109290, train_perplexity=18.65874, train_loss=2.9263146

Batch 109300, train_perplexity=18.658743, train_loss=2.9263148

Batch 109310, train_perplexity=18.658743, train_loss=2.9263148

Batch 109320, train_perplexity=18.658743, train_loss=2.9263148

Batch 109330, train_perplexity=18.658743, train_loss=2.9263148

Batch 109340, train_perplexity=18.65874, train_loss=2.9263146

Batch 109350, train_perplexity=18.658743, train_loss=2.9263148

Batch 109360, train_perplexity=18.65874, train_loss=2.9263146

Batch 109370, train_perplexity=18.658743, train_loss=2.9263148

Batch 109380, train_perplexity=18.65874, train_loss=2.9263146

Batch 109390, train_perplexity=18.65874, train_loss=2.9263146

Batch 109400, train_perplexity=18.658743, train_loss=2.9263148

Batch 109410, train_perplexity=18.65874, train_loss=2.9263146

Batch 109420, train_perplexity=18.65874, train_loss=2.9263146

Batch 109430, train_perplexity=18.658743, train_loss=2.9263148

Batch 109440, train_perplexity=18.65874, train_loss=2.9263146

Batch 109450, train_perplexity=18.658743, train_loss=2.9263148

Batch 109460, train_perplexity=18.65874, train_loss=2.9263146

Batch 109470, train_perplexity=18.65874, train_loss=2.9263146

Batch 109480, train_perplexity=18.658733, train_loss=2.9263144

Batch 109490, train_perplexity=18.658733, train_loss=2.9263144

Batch 109500, train_perplexity=18.658733, train_loss=2.9263144

Batch 109510, train_perplexity=18.658733, train_loss=2.9263144

Batch 109520, train_perplexity=18.658733, train_loss=2.9263144

Batch 109530, train_perplexity=18.658733, train_loss=2.9263144

Batch 109540, train_perplexity=18.658733, train_loss=2.9263144

Batch 109550, train_perplexity=18.658733, train_loss=2.9263144

Batch 109560, train_perplexity=18.658726, train_loss=2.9263139

Batch 109570, train_perplexity=18.658733, train_loss=2.9263144

Batch 109580, train_perplexity=18.658733, train_loss=2.9263144

Batch 109590, train_perplexity=18.658733, train_loss=2.9263144

Batch 109600, train_perplexity=18.658733, train_loss=2.9263144

Batch 109610, train_perplexity=18.658733, train_loss=2.9263144

Batch 109620, train_perplexity=18.658733, train_loss=2.9263144

Batch 109630, train_perplexity=18.658733, train_loss=2.9263144

Batch 109640, train_perplexity=18.65873, train_loss=2.926314

Batch 109650, train_perplexity=18.65873, train_loss=2.926314

Batch 109660, train_perplexity=18.658733, train_loss=2.9263144

Batch 109670, train_perplexity=18.658733, train_loss=2.9263144

Batch 109680, train_perplexity=18.658733, train_loss=2.9263144

Batch 109690, train_perplexity=18.658733, train_loss=2.9263144

Batch 109700, train_perplexity=18.658726, train_loss=2.9263139

Batch 109710, train_perplexity=18.65873, train_loss=2.926314

Batch 109720, train_perplexity=18.658726, train_loss=2.9263139

Batch 109730, train_perplexity=18.65873, train_loss=2.926314

Batch 109740, train_perplexity=18.65873, train_loss=2.926314

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 109750, train_perplexity=18.658733, train_loss=2.9263144

Batch 109760, train_perplexity=18.65873, train_loss=2.926314

Batch 109770, train_perplexity=18.658726, train_loss=2.9263139

Batch 109780, train_perplexity=18.658726, train_loss=2.9263139

Batch 109790, train_perplexity=18.658726, train_loss=2.9263139

Batch 109800, train_perplexity=18.658726, train_loss=2.9263139

Batch 109810, train_perplexity=18.65873, train_loss=2.926314

Batch 109820, train_perplexity=18.65873, train_loss=2.926314

Batch 109830, train_perplexity=18.65872, train_loss=2.9263136

Batch 109840, train_perplexity=18.65873, train_loss=2.926314

Batch 109850, train_perplexity=18.658726, train_loss=2.9263139

Batch 109860, train_perplexity=18.658726, train_loss=2.9263139

Batch 109870, train_perplexity=18.65873, train_loss=2.926314

Batch 109880, train_perplexity=18.658733, train_loss=2.9263144

Batch 109890, train_perplexity=18.658726, train_loss=2.9263139

Batch 109900, train_perplexity=18.658726, train_loss=2.9263139

Batch 109910, train_perplexity=18.65872, train_loss=2.9263136

Batch 109920, train_perplexity=18.658726, train_loss=2.9263139

Batch 109930, train_perplexity=18.658726, train_loss=2.9263139

Batch 109940, train_perplexity=18.658726, train_loss=2.9263139

Batch 109950, train_perplexity=18.658726, train_loss=2.9263139

Batch 109960, train_perplexity=18.65872, train_loss=2.9263136

Batch 109970, train_perplexity=18.65872, train_loss=2.9263136

Batch 109980, train_perplexity=18.658716, train_loss=2.9263134

Batch 109990, train_perplexity=18.658726, train_loss=2.9263139

Batch 110000, train_perplexity=18.65872, train_loss=2.9263136

Batch 110010, train_perplexity=18.65872, train_loss=2.9263136

Batch 110020, train_perplexity=18.658726, train_loss=2.9263139

Batch 110030, train_perplexity=18.658716, train_loss=2.9263134

Batch 110040, train_perplexity=18.65872, train_loss=2.9263136

Batch 110050, train_perplexity=18.658726, train_loss=2.9263139

Batch 110060, train_perplexity=18.658716, train_loss=2.9263134

Batch 110070, train_perplexity=18.658726, train_loss=2.9263139

Batch 110080, train_perplexity=18.658716, train_loss=2.9263134

Batch 110090, train_perplexity=18.658716, train_loss=2.9263134

Batch 110100, train_perplexity=18.658716, train_loss=2.9263134

Batch 110110, train_perplexity=18.658716, train_loss=2.9263134

Batch 110120, train_perplexity=18.65872, train_loss=2.9263136

Batch 110130, train_perplexity=18.658716, train_loss=2.9263134

Batch 110140, train_perplexity=18.658716, train_loss=2.9263134

Batch 110150, train_perplexity=18.658716, train_loss=2.9263134

Batch 110160, train_perplexity=18.658716, train_loss=2.9263134

Batch 110170, train_perplexity=18.658712, train_loss=2.9263132

Batch 110180, train_perplexity=18.658712, train_loss=2.9263132

Batch 110190, train_perplexity=18.658716, train_loss=2.9263134

Batch 110200, train_perplexity=18.658716, train_loss=2.9263134

Batch 110210, train_perplexity=18.658716, train_loss=2.9263134

Batch 110220, train_perplexity=18.658716, train_loss=2.9263134

Batch 110230, train_perplexity=18.658716, train_loss=2.9263134

Batch 110240, train_perplexity=18.658712, train_loss=2.9263132

Batch 110250, train_perplexity=18.658712, train_loss=2.9263132

Batch 110260, train_perplexity=18.658712, train_loss=2.9263132

Batch 110270, train_perplexity=18.658716, train_loss=2.9263134

Batch 110280, train_perplexity=18.658716, train_loss=2.9263134

Batch 110290, train_perplexity=18.658712, train_loss=2.9263132

Batch 110300, train_perplexity=18.658712, train_loss=2.9263132

Batch 110310, train_perplexity=18.658712, train_loss=2.9263132

Batch 110320, train_perplexity=18.658712, train_loss=2.9263132

Batch 110330, train_perplexity=18.658707, train_loss=2.926313

Batch 110340, train_perplexity=18.658716, train_loss=2.9263134

Batch 110350, train_perplexity=18.658707, train_loss=2.926313

Batch 110360, train_perplexity=18.658712, train_loss=2.9263132

Batch 110370, train_perplexity=18.658712, train_loss=2.9263132

Batch 110380, train_perplexity=18.658712, train_loss=2.9263132

Batch 110390, train_perplexity=18.658716, train_loss=2.9263134

Batch 110400, train_perplexity=18.658707, train_loss=2.926313

Batch 110410, train_perplexity=18.658707, train_loss=2.926313

Batch 110420, train_perplexity=18.658707, train_loss=2.926313

Batch 110430, train_perplexity=18.658716, train_loss=2.9263134

Batch 110440, train_perplexity=18.658707, train_loss=2.926313

Batch 110450, train_perplexity=18.658707, train_loss=2.926313

Batch 110460, train_perplexity=18.658707, train_loss=2.926313

Batch 110470, train_perplexity=18.658712, train_loss=2.9263132

Batch 110480, train_perplexity=18.658716, train_loss=2.9263134

Batch 110490, train_perplexity=18.658703, train_loss=2.9263127

Batch 110500, train_perplexity=18.658707, train_loss=2.926313

Batch 110510, train_perplexity=18.658707, train_loss=2.926313

Batch 110520, train_perplexity=18.658703, train_loss=2.9263127

Batch 110530, train_perplexity=18.658703, train_loss=2.9263127

Batch 110540, train_perplexity=18.658707, train_loss=2.926313

Batch 110550, train_perplexity=18.658703, train_loss=2.9263127

Batch 110560, train_perplexity=18.658707, train_loss=2.926313

Batch 110570, train_perplexity=18.658707, train_loss=2.926313

Batch 110580, train_perplexity=18.658707, train_loss=2.926313

Batch 110590, train_perplexity=18.658707, train_loss=2.926313

Batch 110600, train_perplexity=18.658703, train_loss=2.9263127

Batch 110610, train_perplexity=18.658703, train_loss=2.9263127

Batch 110620, train_perplexity=18.658703, train_loss=2.9263127

Batch 110630, train_perplexity=18.658703, train_loss=2.9263127

Batch 110640, train_perplexity=18.658699, train_loss=2.9263124

Batch 110650, train_perplexity=18.658703, train_loss=2.9263127

Batch 110660, train_perplexity=18.658703, train_loss=2.9263127

Batch 110670, train_perplexity=18.658707, train_loss=2.926313

Batch 110680, train_perplexity=18.658699, train_loss=2.9263124

Batch 110690, train_perplexity=18.658699, train_loss=2.9263124

Batch 110700, train_perplexity=18.658699, train_loss=2.9263124

Batch 110710, train_perplexity=18.658699, train_loss=2.9263124

Batch 110720, train_perplexity=18.658707, train_loss=2.926313

Batch 110730, train_perplexity=18.658703, train_loss=2.9263127

Batch 110740, train_perplexity=18.658707, train_loss=2.926313

Batch 110750, train_perplexity=18.658699, train_loss=2.9263124

Batch 110760, train_perplexity=18.658703, train_loss=2.9263127

Batch 110770, train_perplexity=18.658693, train_loss=2.9263122

Batch 110780, train_perplexity=18.658703, train_loss=2.9263127

Batch 110790, train_perplexity=18.658699, train_loss=2.9263124

Batch 110800, train_perplexity=18.658699, train_loss=2.9263124

Batch 110810, train_perplexity=18.658699, train_loss=2.9263124

Batch 110820, train_perplexity=18.658699, train_loss=2.9263124

Batch 110830, train_perplexity=18.658699, train_loss=2.9263124

Batch 110840, train_perplexity=18.658693, train_loss=2.9263122

Batch 110850, train_perplexity=18.658699, train_loss=2.9263124

Batch 110860, train_perplexity=18.658699, train_loss=2.9263124

Batch 110870, train_perplexity=18.658699, train_loss=2.9263124

Batch 110880, train_perplexity=18.658693, train_loss=2.9263122

Batch 110890, train_perplexity=18.658699, train_loss=2.9263124

Batch 110900, train_perplexity=18.658693, train_loss=2.9263122

Batch 110910, train_perplexity=18.658693, train_loss=2.9263122

Batch 110920, train_perplexity=18.658693, train_loss=2.9263122

Batch 110930, train_perplexity=18.658693, train_loss=2.9263122

Batch 110940, train_perplexity=18.658693, train_loss=2.9263122

Batch 110950, train_perplexity=18.658693, train_loss=2.9263122

Batch 110960, train_perplexity=18.658693, train_loss=2.9263122

Batch 110970, train_perplexity=18.65869, train_loss=2.926312

Batch 110980, train_perplexity=18.658693, train_loss=2.9263122

Batch 110990, train_perplexity=18.658693, train_loss=2.9263122

Batch 111000, train_perplexity=18.65869, train_loss=2.926312

Batch 111010, train_perplexity=18.658693, train_loss=2.9263122

Batch 111020, train_perplexity=18.658693, train_loss=2.9263122

Batch 111030, train_perplexity=18.658693, train_loss=2.9263122
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 111040, train_perplexity=18.658693, train_loss=2.9263122

Batch 111050, train_perplexity=18.65869, train_loss=2.926312

Batch 111060, train_perplexity=18.65869, train_loss=2.926312

Batch 111070, train_perplexity=18.658686, train_loss=2.9263117

Batch 111080, train_perplexity=18.65869, train_loss=2.926312

Batch 111090, train_perplexity=18.65869, train_loss=2.926312

Batch 111100, train_perplexity=18.65869, train_loss=2.926312

Batch 111110, train_perplexity=18.65869, train_loss=2.926312

Batch 111120, train_perplexity=18.65869, train_loss=2.926312

Batch 111130, train_perplexity=18.658686, train_loss=2.9263117

Batch 111140, train_perplexity=18.658686, train_loss=2.9263117

Batch 111150, train_perplexity=18.658686, train_loss=2.9263117

Batch 111160, train_perplexity=18.65869, train_loss=2.926312

Batch 111170, train_perplexity=18.65869, train_loss=2.926312

Batch 111180, train_perplexity=18.65869, train_loss=2.926312

Batch 111190, train_perplexity=18.658686, train_loss=2.9263117

Batch 111200, train_perplexity=18.658686, train_loss=2.9263117

Batch 111210, train_perplexity=18.65869, train_loss=2.926312

Batch 111220, train_perplexity=18.658686, train_loss=2.9263117

Batch 111230, train_perplexity=18.658686, train_loss=2.9263117

Batch 111240, train_perplexity=18.658686, train_loss=2.9263117

Batch 111250, train_perplexity=18.65868, train_loss=2.9263115

Batch 111260, train_perplexity=18.658686, train_loss=2.9263117

Batch 111270, train_perplexity=18.658686, train_loss=2.9263117

Batch 111280, train_perplexity=18.65869, train_loss=2.926312

Batch 111290, train_perplexity=18.658686, train_loss=2.9263117

Batch 111300, train_perplexity=18.65868, train_loss=2.9263115

Batch 111310, train_perplexity=18.65868, train_loss=2.9263115

Batch 111320, train_perplexity=18.658686, train_loss=2.9263117

Batch 111330, train_perplexity=18.65868, train_loss=2.9263115

Batch 111340, train_perplexity=18.658686, train_loss=2.9263117

Batch 111350, train_perplexity=18.65868, train_loss=2.9263115

Batch 111360, train_perplexity=18.658686, train_loss=2.9263117

Batch 111370, train_perplexity=18.65868, train_loss=2.9263115

Batch 111380, train_perplexity=18.658686, train_loss=2.9263117

Batch 111390, train_perplexity=18.65868, train_loss=2.9263115

Batch 111400, train_perplexity=18.65868, train_loss=2.9263115

Batch 111410, train_perplexity=18.658676, train_loss=2.9263113

Batch 111420, train_perplexity=18.65868, train_loss=2.9263115

Batch 111430, train_perplexity=18.65868, train_loss=2.9263115

Batch 111440, train_perplexity=18.65868, train_loss=2.9263115

Batch 111450, train_perplexity=18.65868, train_loss=2.9263115

Batch 111460, train_perplexity=18.658676, train_loss=2.9263113

Batch 111470, train_perplexity=18.65868, train_loss=2.9263115

Batch 111480, train_perplexity=18.65868, train_loss=2.9263115

Batch 111490, train_perplexity=18.65868, train_loss=2.9263115

Batch 111500, train_perplexity=18.65868, train_loss=2.9263115

Batch 111510, train_perplexity=18.65868, train_loss=2.9263115

Batch 111520, train_perplexity=18.65868, train_loss=2.9263115

Batch 111530, train_perplexity=18.658676, train_loss=2.9263113

Batch 111540, train_perplexity=18.658676, train_loss=2.9263113

Batch 111550, train_perplexity=18.658672, train_loss=2.926311

Batch 111560, train_perplexity=18.658676, train_loss=2.9263113

Batch 111570, train_perplexity=18.658672, train_loss=2.926311

Batch 111580, train_perplexity=18.658672, train_loss=2.926311

Batch 111590, train_perplexity=18.65868, train_loss=2.9263115

Batch 111600, train_perplexity=18.658672, train_loss=2.926311

Batch 111610, train_perplexity=18.658676, train_loss=2.9263113

Batch 111620, train_perplexity=18.65868, train_loss=2.9263115

Batch 111630, train_perplexity=18.658676, train_loss=2.9263113

Batch 111640, train_perplexity=18.658672, train_loss=2.926311

Batch 111650, train_perplexity=18.658672, train_loss=2.926311

Batch 111660, train_perplexity=18.65868, train_loss=2.9263115

Batch 111670, train_perplexity=18.658672, train_loss=2.926311

Batch 111680, train_perplexity=18.658672, train_loss=2.926311

Batch 111690, train_perplexity=18.658667, train_loss=2.9263108

Batch 111700, train_perplexity=18.658672, train_loss=2.926311

Batch 111710, train_perplexity=18.658672, train_loss=2.926311

Batch 111720, train_perplexity=18.658672, train_loss=2.926311

Batch 111730, train_perplexity=18.658672, train_loss=2.926311

Batch 111740, train_perplexity=18.658672, train_loss=2.926311

Batch 111750, train_perplexity=18.658672, train_loss=2.926311

Batch 111760, train_perplexity=18.658672, train_loss=2.926311

Batch 111770, train_perplexity=18.658672, train_loss=2.926311

Batch 111780, train_perplexity=18.658672, train_loss=2.926311

Batch 111790, train_perplexity=18.658672, train_loss=2.926311

Batch 111800, train_perplexity=18.658672, train_loss=2.926311

Batch 111810, train_perplexity=18.658672, train_loss=2.926311

Batch 111820, train_perplexity=18.658672, train_loss=2.926311

Batch 111830, train_perplexity=18.658667, train_loss=2.9263108

Batch 111840, train_perplexity=18.658667, train_loss=2.9263108

Batch 111850, train_perplexity=18.658672, train_loss=2.926311

Batch 111860, train_perplexity=18.658667, train_loss=2.9263108

Batch 111870, train_perplexity=18.658672, train_loss=2.926311

Batch 111880, train_perplexity=18.658667, train_loss=2.9263108

Batch 111890, train_perplexity=18.658672, train_loss=2.926311

Batch 111900, train_perplexity=18.658667, train_loss=2.9263108

Batch 111910, train_perplexity=18.658667, train_loss=2.9263108

Batch 111920, train_perplexity=18.658663, train_loss=2.9263105

Batch 111930, train_perplexity=18.658667, train_loss=2.9263108

Batch 111940, train_perplexity=18.658663, train_loss=2.9263105

Batch 111950, train_perplexity=18.658663, train_loss=2.9263105

Batch 111960, train_perplexity=18.658672, train_loss=2.926311

Batch 111970, train_perplexity=18.658663, train_loss=2.9263105

Batch 111980, train_perplexity=18.658663, train_loss=2.9263105

Batch 111990, train_perplexity=18.658659, train_loss=2.9263103

Batch 112000, train_perplexity=18.658663, train_loss=2.9263105

Batch 112010, train_perplexity=18.658663, train_loss=2.9263105

Batch 112020, train_perplexity=18.658663, train_loss=2.9263105

Batch 112030, train_perplexity=18.658663, train_loss=2.9263105

Batch 112040, train_perplexity=18.658659, train_loss=2.9263103

Batch 112050, train_perplexity=18.658659, train_loss=2.9263103

Batch 112060, train_perplexity=18.658663, train_loss=2.9263105

Batch 112070, train_perplexity=18.658653, train_loss=2.92631

Batch 112080, train_perplexity=18.658663, train_loss=2.9263105

Batch 112090, train_perplexity=18.658663, train_loss=2.9263105

Batch 112100, train_perplexity=18.658663, train_loss=2.9263105

Batch 112110, train_perplexity=18.658659, train_loss=2.9263103

Batch 112120, train_perplexity=18.658663, train_loss=2.9263105

Batch 112130, train_perplexity=18.658659, train_loss=2.9263103

Batch 112140, train_perplexity=18.658663, train_loss=2.9263105

Batch 112150, train_perplexity=18.658663, train_loss=2.9263105

Batch 112160, train_perplexity=18.658663, train_loss=2.9263105

Batch 112170, train_perplexity=18.658659, train_loss=2.9263103

Batch 112180, train_perplexity=18.658659, train_loss=2.9263103

Batch 112190, train_perplexity=18.658659, train_loss=2.9263103

Batch 112200, train_perplexity=18.658653, train_loss=2.92631

Batch 112210, train_perplexity=18.658659, train_loss=2.9263103

Batch 112220, train_perplexity=18.658653, train_loss=2.92631

Batch 112230, train_perplexity=18.658663, train_loss=2.9263105

Batch 112240, train_perplexity=18.658653, train_loss=2.92631

Batch 112250, train_perplexity=18.658653, train_loss=2.92631

Batch 112260, train_perplexity=18.658653, train_loss=2.92631

Batch 112270, train_perplexity=18.658659, train_loss=2.9263103

Batch 112280, train_perplexity=18.658653, train_loss=2.92631

Batch 112290, train_perplexity=18.658653, train_loss=2.92631

Batch 112300, train_perplexity=18.658653, train_loss=2.92631

Batch 112310, train_perplexity=18.658653, train_loss=2.92631

Batch 112320, train_perplexity=18.65865, train_loss=2.9263098

Batch 112330, train_perplexity=18.658653, train_loss=2.92631
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 112340, train_perplexity=18.658653, train_loss=2.92631

Batch 112350, train_perplexity=18.658653, train_loss=2.92631

Batch 112360, train_perplexity=18.658653, train_loss=2.92631

Batch 112370, train_perplexity=18.658653, train_loss=2.92631

Batch 112380, train_perplexity=18.65865, train_loss=2.9263098

Batch 112390, train_perplexity=18.65865, train_loss=2.9263098

Batch 112400, train_perplexity=18.65865, train_loss=2.9263098

Batch 112410, train_perplexity=18.658653, train_loss=2.92631

Batch 112420, train_perplexity=18.65865, train_loss=2.9263098

Batch 112430, train_perplexity=18.658653, train_loss=2.92631

Batch 112440, train_perplexity=18.658653, train_loss=2.92631

Batch 112450, train_perplexity=18.658653, train_loss=2.92631

Batch 112460, train_perplexity=18.65865, train_loss=2.9263098

Batch 112470, train_perplexity=18.658653, train_loss=2.92631

Batch 112480, train_perplexity=18.658646, train_loss=2.9263096

Batch 112490, train_perplexity=18.65865, train_loss=2.9263098

Batch 112500, train_perplexity=18.658646, train_loss=2.9263096

Batch 112510, train_perplexity=18.65865, train_loss=2.9263098

Batch 112520, train_perplexity=18.65865, train_loss=2.9263098

Batch 112530, train_perplexity=18.658653, train_loss=2.92631

Batch 112540, train_perplexity=18.658646, train_loss=2.9263096

Batch 112550, train_perplexity=18.65865, train_loss=2.9263098

Batch 112560, train_perplexity=18.65865, train_loss=2.9263098

Batch 112570, train_perplexity=18.65865, train_loss=2.9263098

Batch 112580, train_perplexity=18.658646, train_loss=2.9263096

Batch 112590, train_perplexity=18.65864, train_loss=2.9263093

Batch 112600, train_perplexity=18.658646, train_loss=2.9263096

Batch 112610, train_perplexity=18.65865, train_loss=2.9263098

Batch 112620, train_perplexity=18.65865, train_loss=2.9263098

Batch 112630, train_perplexity=18.658646, train_loss=2.9263096

Batch 112640, train_perplexity=18.658646, train_loss=2.9263096

Batch 112650, train_perplexity=18.65864, train_loss=2.9263093

Batch 112660, train_perplexity=18.658646, train_loss=2.9263096

Batch 112670, train_perplexity=18.658646, train_loss=2.9263096

Batch 112680, train_perplexity=18.658646, train_loss=2.9263096

Batch 112690, train_perplexity=18.658636, train_loss=2.926309

Batch 112700, train_perplexity=18.658646, train_loss=2.9263096

Batch 112710, train_perplexity=18.658646, train_loss=2.9263096

Batch 112720, train_perplexity=18.65864, train_loss=2.9263093

Batch 112730, train_perplexity=18.658646, train_loss=2.9263096

Batch 112740, train_perplexity=18.65864, train_loss=2.9263093

Batch 112750, train_perplexity=18.65864, train_loss=2.9263093

Batch 112760, train_perplexity=18.65864, train_loss=2.9263093

Batch 112770, train_perplexity=18.658636, train_loss=2.926309

Batch 112780, train_perplexity=18.658646, train_loss=2.9263096

Batch 112790, train_perplexity=18.65864, train_loss=2.9263093

Batch 112800, train_perplexity=18.65864, train_loss=2.9263093

Batch 112810, train_perplexity=18.658646, train_loss=2.9263096

Batch 112820, train_perplexity=18.65864, train_loss=2.9263093

Batch 112830, train_perplexity=18.65864, train_loss=2.9263093

Batch 112840, train_perplexity=18.658636, train_loss=2.926309

Batch 112850, train_perplexity=18.65864, train_loss=2.9263093

Batch 112860, train_perplexity=18.65864, train_loss=2.9263093

Batch 112870, train_perplexity=18.65864, train_loss=2.9263093

Batch 112880, train_perplexity=18.65864, train_loss=2.9263093

Batch 112890, train_perplexity=18.658636, train_loss=2.926309

Batch 112900, train_perplexity=18.658636, train_loss=2.926309

Batch 112910, train_perplexity=18.658636, train_loss=2.926309

Batch 112920, train_perplexity=18.65864, train_loss=2.9263093

Batch 112930, train_perplexity=18.658636, train_loss=2.926309

Batch 112940, train_perplexity=18.658636, train_loss=2.926309

Batch 112950, train_perplexity=18.658636, train_loss=2.926309

Batch 112960, train_perplexity=18.658636, train_loss=2.926309

Batch 112970, train_perplexity=18.658632, train_loss=2.9263089

Batch 112980, train_perplexity=18.658636, train_loss=2.926309

Batch 112990, train_perplexity=18.658636, train_loss=2.926309

Batch 113000, train_perplexity=18.658636, train_loss=2.926309

Batch 113010, train_perplexity=18.658632, train_loss=2.9263089

Batch 113020, train_perplexity=18.658636, train_loss=2.926309

Batch 113030, train_perplexity=18.658636, train_loss=2.926309

Batch 113040, train_perplexity=18.658632, train_loss=2.9263089

Batch 113050, train_perplexity=18.658632, train_loss=2.9263089

Batch 113060, train_perplexity=18.658636, train_loss=2.926309

Batch 113070, train_perplexity=18.658632, train_loss=2.9263089

Batch 113080, train_perplexity=18.658632, train_loss=2.9263089

Batch 113090, train_perplexity=18.658636, train_loss=2.926309

Batch 113100, train_perplexity=18.658636, train_loss=2.926309

Batch 113110, train_perplexity=18.658632, train_loss=2.9263089

Batch 113120, train_perplexity=18.658632, train_loss=2.9263089

Batch 113130, train_perplexity=18.658636, train_loss=2.926309

Batch 113140, train_perplexity=18.658632, train_loss=2.9263089

Batch 113150, train_perplexity=18.658636, train_loss=2.926309

Batch 113160, train_perplexity=18.658627, train_loss=2.9263086

Batch 113170, train_perplexity=18.658632, train_loss=2.9263089

Batch 113180, train_perplexity=18.658632, train_loss=2.9263089

Batch 113190, train_perplexity=18.658636, train_loss=2.926309

Batch 113200, train_perplexity=18.658632, train_loss=2.9263089

Batch 113210, train_perplexity=18.658627, train_loss=2.9263086

Batch 113220, train_perplexity=18.658627, train_loss=2.9263086

Batch 113230, train_perplexity=18.658627, train_loss=2.9263086

Batch 113240, train_perplexity=18.658632, train_loss=2.9263089

Batch 113250, train_perplexity=18.658627, train_loss=2.9263086

Batch 113260, train_perplexity=18.658627, train_loss=2.9263086

Batch 113270, train_perplexity=18.658632, train_loss=2.9263089

Batch 113280, train_perplexity=18.658627, train_loss=2.9263086

Batch 113290, train_perplexity=18.658627, train_loss=2.9263086

Batch 113300, train_perplexity=18.658623, train_loss=2.9263084

Batch 113310, train_perplexity=18.658627, train_loss=2.9263086

Batch 113320, train_perplexity=18.658623, train_loss=2.9263084

Batch 113330, train_perplexity=18.658623, train_loss=2.9263084

Batch 113340, train_perplexity=18.658627, train_loss=2.9263086

Batch 113350, train_perplexity=18.658627, train_loss=2.9263086

Batch 113360, train_perplexity=18.658627, train_loss=2.9263086

Batch 113370, train_perplexity=18.658619, train_loss=2.9263082

Batch 113380, train_perplexity=18.658623, train_loss=2.9263084

Batch 113390, train_perplexity=18.658627, train_loss=2.9263086

Batch 113400, train_perplexity=18.658619, train_loss=2.9263082

Batch 113410, train_perplexity=18.658619, train_loss=2.9263082

Batch 113420, train_perplexity=18.658623, train_loss=2.9263084

Batch 113430, train_perplexity=18.658619, train_loss=2.9263082

Batch 113440, train_perplexity=18.658627, train_loss=2.9263086

Batch 113450, train_perplexity=18.658623, train_loss=2.9263084

Batch 113460, train_perplexity=18.658627, train_loss=2.9263086

Batch 113470, train_perplexity=18.658623, train_loss=2.9263084

Batch 113480, train_perplexity=18.658619, train_loss=2.9263082

Batch 113490, train_perplexity=18.658627, train_loss=2.9263086

Batch 113500, train_perplexity=18.658619, train_loss=2.9263082

Batch 113510, train_perplexity=18.658619, train_loss=2.9263082

Batch 113520, train_perplexity=18.658619, train_loss=2.9263082

Batch 113530, train_perplexity=18.658619, train_loss=2.9263082

Batch 113540, train_perplexity=18.658619, train_loss=2.9263082

Batch 113550, train_perplexity=18.658619, train_loss=2.9263082

Batch 113560, train_perplexity=18.658623, train_loss=2.9263084

Batch 113570, train_perplexity=18.658613, train_loss=2.926308

Batch 113580, train_perplexity=18.658623, train_loss=2.9263084

Batch 113590, train_perplexity=18.658619, train_loss=2.9263082

Batch 113600, train_perplexity=18.658619, train_loss=2.9263082

Batch 113610, train_perplexity=18.658619, train_loss=2.9263082

Batch 113620, train_perplexity=18.658613, train_loss=2.926308

Batch 113630, train_perplexity=18.658619, train_loss=2.9263082
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 113640, train_perplexity=18.658619, train_loss=2.9263082

Batch 113650, train_perplexity=18.658619, train_loss=2.9263082

Batch 113660, train_perplexity=18.658619, train_loss=2.9263082

Batch 113670, train_perplexity=18.658619, train_loss=2.9263082

Batch 113680, train_perplexity=18.658619, train_loss=2.9263082

Batch 113690, train_perplexity=18.65861, train_loss=2.9263077

Batch 113700, train_perplexity=18.658613, train_loss=2.926308

Batch 113710, train_perplexity=18.658619, train_loss=2.9263082

Batch 113720, train_perplexity=18.658613, train_loss=2.926308

Batch 113730, train_perplexity=18.658619, train_loss=2.9263082

Batch 113740, train_perplexity=18.658619, train_loss=2.9263082

Batch 113750, train_perplexity=18.658619, train_loss=2.9263082

Batch 113760, train_perplexity=18.65861, train_loss=2.9263077

Batch 113770, train_perplexity=18.65861, train_loss=2.9263077

Batch 113780, train_perplexity=18.658613, train_loss=2.926308

Batch 113790, train_perplexity=18.658613, train_loss=2.926308

Batch 113800, train_perplexity=18.658613, train_loss=2.926308

Batch 113810, train_perplexity=18.65861, train_loss=2.9263077

Batch 113820, train_perplexity=18.65861, train_loss=2.9263077

Batch 113830, train_perplexity=18.65861, train_loss=2.9263077

Batch 113840, train_perplexity=18.65861, train_loss=2.9263077

Batch 113850, train_perplexity=18.65861, train_loss=2.9263077

Batch 113860, train_perplexity=18.65861, train_loss=2.9263077

Batch 113870, train_perplexity=18.65861, train_loss=2.9263077

Batch 113880, train_perplexity=18.65861, train_loss=2.9263077

Batch 113890, train_perplexity=18.65861, train_loss=2.9263077

Batch 113900, train_perplexity=18.65861, train_loss=2.9263077

Batch 113910, train_perplexity=18.65861, train_loss=2.9263077

Batch 113920, train_perplexity=18.65861, train_loss=2.9263077

Batch 113930, train_perplexity=18.658606, train_loss=2.9263074

Batch 113940, train_perplexity=18.65861, train_loss=2.9263077

Batch 113950, train_perplexity=18.65861, train_loss=2.9263077

Batch 113960, train_perplexity=18.65861, train_loss=2.9263077

Batch 113970, train_perplexity=18.65861, train_loss=2.9263077

Batch 113980, train_perplexity=18.65861, train_loss=2.9263077

Batch 113990, train_perplexity=18.658606, train_loss=2.9263074

Batch 114000, train_perplexity=18.65861, train_loss=2.9263077

Batch 114010, train_perplexity=18.658606, train_loss=2.9263074

Batch 114020, train_perplexity=18.65861, train_loss=2.9263077

Batch 114030, train_perplexity=18.658606, train_loss=2.9263074

Batch 114040, train_perplexity=18.65861, train_loss=2.9263077

Batch 114050, train_perplexity=18.6586, train_loss=2.9263072

Batch 114060, train_perplexity=18.6586, train_loss=2.9263072

Batch 114070, train_perplexity=18.658606, train_loss=2.9263074

Batch 114080, train_perplexity=18.6586, train_loss=2.9263072

Batch 114090, train_perplexity=18.658606, train_loss=2.9263074

Batch 114100, train_perplexity=18.6586, train_loss=2.9263072

Batch 114110, train_perplexity=18.6586, train_loss=2.9263072

Batch 114120, train_perplexity=18.6586, train_loss=2.9263072

Batch 114130, train_perplexity=18.6586, train_loss=2.9263072

Batch 114140, train_perplexity=18.6586, train_loss=2.9263072

Batch 114150, train_perplexity=18.6586, train_loss=2.9263072

Batch 114160, train_perplexity=18.6586, train_loss=2.9263072

Batch 114170, train_perplexity=18.6586, train_loss=2.9263072

Batch 114180, train_perplexity=18.6586, train_loss=2.9263072

Batch 114190, train_perplexity=18.6586, train_loss=2.9263072

Batch 114200, train_perplexity=18.6586, train_loss=2.9263072

Batch 114210, train_perplexity=18.6586, train_loss=2.9263072

Batch 114220, train_perplexity=18.658596, train_loss=2.926307

Batch 114230, train_perplexity=18.6586, train_loss=2.9263072

Batch 114240, train_perplexity=18.6586, train_loss=2.9263072

Batch 114250, train_perplexity=18.658596, train_loss=2.926307

Batch 114260, train_perplexity=18.6586, train_loss=2.9263072

Batch 114270, train_perplexity=18.658596, train_loss=2.926307

Batch 114280, train_perplexity=18.6586, train_loss=2.9263072

Batch 114290, train_perplexity=18.6586, train_loss=2.9263072

Batch 114300, train_perplexity=18.658592, train_loss=2.9263067

Batch 114310, train_perplexity=18.658596, train_loss=2.926307

Batch 114320, train_perplexity=18.6586, train_loss=2.9263072

Batch 114330, train_perplexity=18.658592, train_loss=2.9263067

Batch 114340, train_perplexity=18.6586, train_loss=2.9263072

Batch 114350, train_perplexity=18.6586, train_loss=2.9263072

Batch 114360, train_perplexity=18.658596, train_loss=2.926307

Batch 114370, train_perplexity=18.6586, train_loss=2.9263072

Batch 114380, train_perplexity=18.658596, train_loss=2.926307

Batch 114390, train_perplexity=18.658596, train_loss=2.926307

Batch 114400, train_perplexity=18.658596, train_loss=2.926307

Batch 114410, train_perplexity=18.658596, train_loss=2.926307

Batch 114420, train_perplexity=18.658596, train_loss=2.926307

Batch 114430, train_perplexity=18.658592, train_loss=2.9263067

Batch 114440, train_perplexity=18.658592, train_loss=2.9263067

Batch 114450, train_perplexity=18.6586, train_loss=2.9263072

Batch 114460, train_perplexity=18.658592, train_loss=2.9263067

Batch 114470, train_perplexity=18.658592, train_loss=2.9263067

Batch 114480, train_perplexity=18.658592, train_loss=2.9263067

Batch 114490, train_perplexity=18.658592, train_loss=2.9263067

Batch 114500, train_perplexity=18.658592, train_loss=2.9263067

Batch 114510, train_perplexity=18.658592, train_loss=2.9263067

Batch 114520, train_perplexity=18.658592, train_loss=2.9263067

Batch 114530, train_perplexity=18.658592, train_loss=2.9263067

Batch 114540, train_perplexity=18.658592, train_loss=2.9263067

Batch 114550, train_perplexity=18.658587, train_loss=2.9263065

Batch 114560, train_perplexity=18.658592, train_loss=2.9263067

Batch 114570, train_perplexity=18.658592, train_loss=2.9263067

Batch 114580, train_perplexity=18.658583, train_loss=2.9263062

Batch 114590, train_perplexity=18.658583, train_loss=2.9263062

Batch 114600, train_perplexity=18.658592, train_loss=2.9263067

Batch 114610, train_perplexity=18.658587, train_loss=2.9263065

Batch 114620, train_perplexity=18.658583, train_loss=2.9263062

Batch 114630, train_perplexity=18.658592, train_loss=2.9263067

Batch 114640, train_perplexity=18.658583, train_loss=2.9263062

Batch 114650, train_perplexity=18.658583, train_loss=2.9263062

Batch 114660, train_perplexity=18.658587, train_loss=2.9263065

Batch 114670, train_perplexity=18.658583, train_loss=2.9263062

Batch 114680, train_perplexity=18.658583, train_loss=2.9263062

Batch 114690, train_perplexity=18.658587, train_loss=2.9263065

Batch 114700, train_perplexity=18.658587, train_loss=2.9263065

Batch 114710, train_perplexity=18.658583, train_loss=2.9263062

Batch 114720, train_perplexity=18.658583, train_loss=2.9263062

Batch 114730, train_perplexity=18.658583, train_loss=2.9263062

Batch 114740, train_perplexity=18.658583, train_loss=2.9263062

Batch 114750, train_perplexity=18.658573, train_loss=2.9263058

Batch 114760, train_perplexity=18.658579, train_loss=2.926306

Batch 114770, train_perplexity=18.658583, train_loss=2.9263062

Batch 114780, train_perplexity=18.658583, train_loss=2.9263062

Batch 114790, train_perplexity=18.658583, train_loss=2.9263062

Batch 114800, train_perplexity=18.658583, train_loss=2.9263062

Batch 114810, train_perplexity=18.658583, train_loss=2.9263062

Batch 114820, train_perplexity=18.658583, train_loss=2.9263062

Batch 114830, train_perplexity=18.658583, train_loss=2.9263062

Batch 114840, train_perplexity=18.658579, train_loss=2.926306

Batch 114850, train_perplexity=18.658583, train_loss=2.9263062

Batch 114860, train_perplexity=18.658583, train_loss=2.9263062

Batch 114870, train_perplexity=18.658579, train_loss=2.926306

Batch 114880, train_perplexity=18.658583, train_loss=2.9263062

Batch 114890, train_perplexity=18.658579, train_loss=2.926306

Batch 114900, train_perplexity=18.658579, train_loss=2.926306

Batch 114910, train_perplexity=18.658583, train_loss=2.9263062

Batch 114920, train_perplexity=18.658579, train_loss=2.926306

Batch 114930, train_perplexity=18.658583, train_loss=2.9263062
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 114940, train_perplexity=18.658579, train_loss=2.926306

Batch 114950, train_perplexity=18.658573, train_loss=2.9263058

Batch 114960, train_perplexity=18.658573, train_loss=2.9263058

Batch 114970, train_perplexity=18.658579, train_loss=2.926306

Batch 114980, train_perplexity=18.658573, train_loss=2.9263058

Batch 114990, train_perplexity=18.65857, train_loss=2.9263055

Batch 115000, train_perplexity=18.658579, train_loss=2.926306

Batch 115010, train_perplexity=18.658579, train_loss=2.926306

Batch 115020, train_perplexity=18.658573, train_loss=2.9263058

Batch 115030, train_perplexity=18.658573, train_loss=2.9263058

Batch 115040, train_perplexity=18.658573, train_loss=2.9263058

Batch 115050, train_perplexity=18.658573, train_loss=2.9263058

Batch 115060, train_perplexity=18.658573, train_loss=2.9263058

Batch 115070, train_perplexity=18.658573, train_loss=2.9263058

Batch 115080, train_perplexity=18.658579, train_loss=2.926306

Batch 115090, train_perplexity=18.658573, train_loss=2.9263058

Batch 115100, train_perplexity=18.658573, train_loss=2.9263058

Batch 115110, train_perplexity=18.658573, train_loss=2.9263058

Batch 115120, train_perplexity=18.658573, train_loss=2.9263058

Batch 115130, train_perplexity=18.658573, train_loss=2.9263058

Batch 115140, train_perplexity=18.658573, train_loss=2.9263058

Batch 115150, train_perplexity=18.658573, train_loss=2.9263058

Batch 115160, train_perplexity=18.658573, train_loss=2.9263058

Batch 115170, train_perplexity=18.65857, train_loss=2.9263055

Batch 115180, train_perplexity=18.658573, train_loss=2.9263058

Batch 115190, train_perplexity=18.658573, train_loss=2.9263058

Batch 115200, train_perplexity=18.65857, train_loss=2.9263055

Batch 115210, train_perplexity=18.658573, train_loss=2.9263058

Batch 115220, train_perplexity=18.65857, train_loss=2.9263055

Batch 115230, train_perplexity=18.65857, train_loss=2.9263055

Batch 115240, train_perplexity=18.658573, train_loss=2.9263058

Batch 115250, train_perplexity=18.65857, train_loss=2.9263055

Batch 115260, train_perplexity=18.65857, train_loss=2.9263055

Batch 115270, train_perplexity=18.658566, train_loss=2.9263053

Batch 115280, train_perplexity=18.658566, train_loss=2.9263053

Batch 115290, train_perplexity=18.65857, train_loss=2.9263055

Batch 115300, train_perplexity=18.658566, train_loss=2.9263053

Batch 115310, train_perplexity=18.658566, train_loss=2.9263053

Batch 115320, train_perplexity=18.658566, train_loss=2.9263053

Batch 115330, train_perplexity=18.65857, train_loss=2.9263055

Batch 115340, train_perplexity=18.658566, train_loss=2.9263053

Batch 115350, train_perplexity=18.65857, train_loss=2.9263055

Batch 115360, train_perplexity=18.658566, train_loss=2.9263053

Batch 115370, train_perplexity=18.658566, train_loss=2.9263053

Batch 115380, train_perplexity=18.658566, train_loss=2.9263053

Batch 115390, train_perplexity=18.658566, train_loss=2.9263053

Batch 115400, train_perplexity=18.65856, train_loss=2.926305

Batch 115410, train_perplexity=18.658566, train_loss=2.9263053

Batch 115420, train_perplexity=18.658566, train_loss=2.9263053

Batch 115430, train_perplexity=18.658566, train_loss=2.9263053

Batch 115440, train_perplexity=18.65856, train_loss=2.926305

Batch 115450, train_perplexity=18.658566, train_loss=2.9263053

Batch 115460, train_perplexity=18.658566, train_loss=2.9263053

Batch 115470, train_perplexity=18.65856, train_loss=2.926305

Batch 115480, train_perplexity=18.658566, train_loss=2.9263053

Batch 115490, train_perplexity=18.658566, train_loss=2.9263053

Batch 115500, train_perplexity=18.658566, train_loss=2.9263053

Batch 115510, train_perplexity=18.65856, train_loss=2.926305

Batch 115520, train_perplexity=18.65856, train_loss=2.926305

Batch 115530, train_perplexity=18.65856, train_loss=2.926305

Batch 115540, train_perplexity=18.658556, train_loss=2.9263048

Batch 115550, train_perplexity=18.65856, train_loss=2.926305

Batch 115560, train_perplexity=18.65856, train_loss=2.926305

Batch 115570, train_perplexity=18.65856, train_loss=2.926305

Batch 115580, train_perplexity=18.65856, train_loss=2.926305

Batch 115590, train_perplexity=18.65856, train_loss=2.926305

Batch 115600, train_perplexity=18.658556, train_loss=2.9263048

Batch 115610, train_perplexity=18.658556, train_loss=2.9263048

Batch 115620, train_perplexity=18.658556, train_loss=2.9263048

Batch 115630, train_perplexity=18.65856, train_loss=2.926305

Batch 115640, train_perplexity=18.658556, train_loss=2.9263048

Batch 115650, train_perplexity=18.658556, train_loss=2.9263048

Batch 115660, train_perplexity=18.658556, train_loss=2.9263048

Batch 115670, train_perplexity=18.658556, train_loss=2.9263048

Batch 115680, train_perplexity=18.658556, train_loss=2.9263048

Batch 115690, train_perplexity=18.658556, train_loss=2.9263048

Batch 115700, train_perplexity=18.658556, train_loss=2.9263048

Batch 115710, train_perplexity=18.658556, train_loss=2.9263048

Batch 115720, train_perplexity=18.658552, train_loss=2.9263046

Batch 115730, train_perplexity=18.658552, train_loss=2.9263046

Batch 115740, train_perplexity=18.658556, train_loss=2.9263048

Batch 115750, train_perplexity=18.658556, train_loss=2.9263048

Batch 115760, train_perplexity=18.658552, train_loss=2.9263046

Batch 115770, train_perplexity=18.658552, train_loss=2.9263046

Batch 115780, train_perplexity=18.658556, train_loss=2.9263048

Batch 115790, train_perplexity=18.658546, train_loss=2.9263043

Batch 115800, train_perplexity=18.658552, train_loss=2.9263046

Batch 115810, train_perplexity=18.658552, train_loss=2.9263046

Batch 115820, train_perplexity=18.658556, train_loss=2.9263048

Batch 115830, train_perplexity=18.658556, train_loss=2.9263048

Batch 115840, train_perplexity=18.658552, train_loss=2.9263046

Batch 115850, train_perplexity=18.658552, train_loss=2.9263046

Batch 115860, train_perplexity=18.658552, train_loss=2.9263046

Batch 115870, train_perplexity=18.658546, train_loss=2.9263043

Batch 115880, train_perplexity=18.658552, train_loss=2.9263046

Batch 115890, train_perplexity=18.658552, train_loss=2.9263046

Batch 115900, train_perplexity=18.658546, train_loss=2.9263043

Batch 115910, train_perplexity=18.658546, train_loss=2.9263043

Batch 115920, train_perplexity=18.658546, train_loss=2.9263043

Batch 115930, train_perplexity=18.658546, train_loss=2.9263043

Batch 115940, train_perplexity=18.658546, train_loss=2.9263043

Batch 115950, train_perplexity=18.658546, train_loss=2.9263043

Batch 115960, train_perplexity=18.658546, train_loss=2.9263043

Batch 115970, train_perplexity=18.658546, train_loss=2.9263043

Batch 115980, train_perplexity=18.658546, train_loss=2.9263043

Batch 115990, train_perplexity=18.658546, train_loss=2.9263043

Batch 116000, train_perplexity=18.658543, train_loss=2.926304

Batch 116010, train_perplexity=18.658546, train_loss=2.9263043

Batch 116020, train_perplexity=18.658543, train_loss=2.926304

Batch 116030, train_perplexity=18.658546, train_loss=2.9263043

Batch 116040, train_perplexity=18.658546, train_loss=2.9263043

Batch 116050, train_perplexity=18.658546, train_loss=2.9263043

Batch 116060, train_perplexity=18.658546, train_loss=2.9263043

Batch 116070, train_perplexity=18.658543, train_loss=2.926304

Batch 116080, train_perplexity=18.658546, train_loss=2.9263043

Batch 116090, train_perplexity=18.658543, train_loss=2.926304

Batch 116100, train_perplexity=18.658546, train_loss=2.9263043

Batch 116110, train_perplexity=18.658543, train_loss=2.926304

Batch 116120, train_perplexity=18.658543, train_loss=2.926304

Batch 116130, train_perplexity=18.658546, train_loss=2.9263043

Batch 116140, train_perplexity=18.658543, train_loss=2.926304

Batch 116150, train_perplexity=18.658546, train_loss=2.9263043

Batch 116160, train_perplexity=18.658543, train_loss=2.926304

Batch 116170, train_perplexity=18.658543, train_loss=2.926304

Batch 116180, train_perplexity=18.658539, train_loss=2.9263039

Batch 116190, train_perplexity=18.658543, train_loss=2.926304

Batch 116200, train_perplexity=18.658539, train_loss=2.9263039

Batch 116210, train_perplexity=18.658543, train_loss=2.926304

Batch 116220, train_perplexity=18.658543, train_loss=2.926304
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 116230, train_perplexity=18.658543, train_loss=2.926304

Batch 116240, train_perplexity=18.658543, train_loss=2.926304

Batch 116250, train_perplexity=18.658543, train_loss=2.926304

Batch 116260, train_perplexity=18.658539, train_loss=2.9263039

Batch 116270, train_perplexity=18.658539, train_loss=2.9263039

Batch 116280, train_perplexity=18.658539, train_loss=2.9263039

Batch 116290, train_perplexity=18.658539, train_loss=2.9263039

Batch 116300, train_perplexity=18.658539, train_loss=2.9263039

Batch 116310, train_perplexity=18.658539, train_loss=2.9263039

Batch 116320, train_perplexity=18.65853, train_loss=2.9263034

Batch 116330, train_perplexity=18.658539, train_loss=2.9263039

Batch 116340, train_perplexity=18.658533, train_loss=2.9263036

Batch 116350, train_perplexity=18.658539, train_loss=2.9263039

Batch 116360, train_perplexity=18.65853, train_loss=2.9263034

Batch 116370, train_perplexity=18.658539, train_loss=2.9263039

Batch 116380, train_perplexity=18.658539, train_loss=2.9263039

Batch 116390, train_perplexity=18.65853, train_loss=2.9263034

Batch 116400, train_perplexity=18.658539, train_loss=2.9263039

Batch 116410, train_perplexity=18.65853, train_loss=2.9263034

Batch 116420, train_perplexity=18.65853, train_loss=2.9263034

Batch 116430, train_perplexity=18.658533, train_loss=2.9263036

Batch 116440, train_perplexity=18.65853, train_loss=2.9263034

Batch 116450, train_perplexity=18.658539, train_loss=2.9263039

Batch 116460, train_perplexity=18.65853, train_loss=2.9263034

Batch 116470, train_perplexity=18.658533, train_loss=2.9263036

Batch 116480, train_perplexity=18.658533, train_loss=2.9263036

Batch 116490, train_perplexity=18.658533, train_loss=2.9263036

Batch 116500, train_perplexity=18.65853, train_loss=2.9263034

Batch 116510, train_perplexity=18.65853, train_loss=2.9263034

Batch 116520, train_perplexity=18.65853, train_loss=2.9263034

Batch 116530, train_perplexity=18.65853, train_loss=2.9263034

Batch 116540, train_perplexity=18.65853, train_loss=2.9263034

Batch 116550, train_perplexity=18.65853, train_loss=2.9263034

Batch 116560, train_perplexity=18.65853, train_loss=2.9263034

Batch 116570, train_perplexity=18.65853, train_loss=2.9263034

Batch 116580, train_perplexity=18.658525, train_loss=2.9263031

Batch 116590, train_perplexity=18.65853, train_loss=2.9263034

Batch 116600, train_perplexity=18.658525, train_loss=2.9263031

Batch 116610, train_perplexity=18.65853, train_loss=2.9263034

Batch 116620, train_perplexity=18.65853, train_loss=2.9263034

Batch 116630, train_perplexity=18.65853, train_loss=2.9263034

Batch 116640, train_perplexity=18.65853, train_loss=2.9263034

Batch 116650, train_perplexity=18.65852, train_loss=2.926303

Batch 116660, train_perplexity=18.65853, train_loss=2.9263034

Batch 116670, train_perplexity=18.65853, train_loss=2.9263034

Batch 116680, train_perplexity=18.65853, train_loss=2.9263034

Batch 116690, train_perplexity=18.658525, train_loss=2.9263031

Batch 116700, train_perplexity=18.658525, train_loss=2.9263031

Batch 116710, train_perplexity=18.65853, train_loss=2.9263034

Batch 116720, train_perplexity=18.65853, train_loss=2.9263034

Batch 116730, train_perplexity=18.65852, train_loss=2.926303

Batch 116740, train_perplexity=18.65853, train_loss=2.9263034

Batch 116750, train_perplexity=18.658525, train_loss=2.9263031

Batch 116760, train_perplexity=18.658525, train_loss=2.9263031

Batch 116770, train_perplexity=18.658525, train_loss=2.9263031

Batch 116780, train_perplexity=18.65852, train_loss=2.926303

Batch 116790, train_perplexity=18.658525, train_loss=2.9263031

Batch 116800, train_perplexity=18.658525, train_loss=2.9263031

Batch 116810, train_perplexity=18.658525, train_loss=2.9263031

Batch 116820, train_perplexity=18.65852, train_loss=2.926303

Batch 116830, train_perplexity=18.658525, train_loss=2.9263031

Batch 116840, train_perplexity=18.65852, train_loss=2.926303

Batch 116850, train_perplexity=18.65852, train_loss=2.926303

Batch 116860, train_perplexity=18.65852, train_loss=2.926303

Batch 116870, train_perplexity=18.65852, train_loss=2.926303

Batch 116880, train_perplexity=18.65852, train_loss=2.926303

Batch 116890, train_perplexity=18.65852, train_loss=2.926303

Batch 116900, train_perplexity=18.65852, train_loss=2.926303

Batch 116910, train_perplexity=18.658525, train_loss=2.9263031

Batch 116920, train_perplexity=18.658525, train_loss=2.9263031

Batch 116930, train_perplexity=18.65852, train_loss=2.926303

Batch 116940, train_perplexity=18.65852, train_loss=2.926303

Batch 116950, train_perplexity=18.65852, train_loss=2.926303

Batch 116960, train_perplexity=18.65852, train_loss=2.926303

Batch 116970, train_perplexity=18.65852, train_loss=2.926303

Batch 116980, train_perplexity=18.658516, train_loss=2.9263027

Batch 116990, train_perplexity=18.65852, train_loss=2.926303

Batch 117000, train_perplexity=18.658512, train_loss=2.9263024

Batch 117010, train_perplexity=18.658512, train_loss=2.9263024

Batch 117020, train_perplexity=18.658516, train_loss=2.9263027

Batch 117030, train_perplexity=18.658516, train_loss=2.9263027

Batch 117040, train_perplexity=18.658512, train_loss=2.9263024

Batch 117050, train_perplexity=18.658512, train_loss=2.9263024

Batch 117060, train_perplexity=18.658512, train_loss=2.9263024

Batch 117070, train_perplexity=18.658512, train_loss=2.9263024

Batch 117080, train_perplexity=18.658516, train_loss=2.9263027

Batch 117090, train_perplexity=18.658516, train_loss=2.9263027

Batch 117100, train_perplexity=18.658512, train_loss=2.9263024

Batch 117110, train_perplexity=18.658512, train_loss=2.9263024

Batch 117120, train_perplexity=18.658516, train_loss=2.9263027

Batch 117130, train_perplexity=18.658512, train_loss=2.9263024

Batch 117140, train_perplexity=18.658512, train_loss=2.9263024

Batch 117150, train_perplexity=18.658512, train_loss=2.9263024

Batch 117160, train_perplexity=18.658516, train_loss=2.9263027

Batch 117170, train_perplexity=18.658516, train_loss=2.9263027

Batch 117180, train_perplexity=18.658503, train_loss=2.926302

Batch 117190, train_perplexity=18.658512, train_loss=2.9263024

Batch 117200, train_perplexity=18.658512, train_loss=2.9263024

Batch 117210, train_perplexity=18.658512, train_loss=2.9263024

Batch 117220, train_perplexity=18.658512, train_loss=2.9263024

Batch 117230, train_perplexity=18.658512, train_loss=2.9263024

Batch 117240, train_perplexity=18.658512, train_loss=2.9263024

Batch 117250, train_perplexity=18.658512, train_loss=2.9263024

Batch 117260, train_perplexity=18.658512, train_loss=2.9263024

Batch 117270, train_perplexity=18.658512, train_loss=2.9263024

Batch 117280, train_perplexity=18.658512, train_loss=2.9263024

Batch 117290, train_perplexity=18.658512, train_loss=2.9263024

Batch 117300, train_perplexity=18.658506, train_loss=2.9263022

Batch 117310, train_perplexity=18.658506, train_loss=2.9263022

Batch 117320, train_perplexity=18.658506, train_loss=2.9263022

Batch 117330, train_perplexity=18.658506, train_loss=2.9263022

Batch 117340, train_perplexity=18.658512, train_loss=2.9263024

Batch 117350, train_perplexity=18.658506, train_loss=2.9263022

Batch 117360, train_perplexity=18.658506, train_loss=2.9263022

Batch 117370, train_perplexity=18.658506, train_loss=2.9263022

Batch 117380, train_perplexity=18.658506, train_loss=2.9263022

Batch 117390, train_perplexity=18.658506, train_loss=2.9263022

Batch 117400, train_perplexity=18.658503, train_loss=2.926302

Batch 117410, train_perplexity=18.658503, train_loss=2.926302

Batch 117420, train_perplexity=18.658503, train_loss=2.926302

Batch 117430, train_perplexity=18.658503, train_loss=2.926302

Batch 117440, train_perplexity=18.658506, train_loss=2.9263022

Batch 117450, train_perplexity=18.658503, train_loss=2.926302

Batch 117460, train_perplexity=18.658506, train_loss=2.9263022

Batch 117470, train_perplexity=18.658503, train_loss=2.926302

Batch 117480, train_perplexity=18.658503, train_loss=2.926302

Batch 117490, train_perplexity=18.658503, train_loss=2.926302

Batch 117500, train_perplexity=18.658503, train_loss=2.926302

Batch 117510, train_perplexity=18.658503, train_loss=2.926302

Batch 117520, train_perplexity=18.658503, train_loss=2.926302
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 117530, train_perplexity=18.658503, train_loss=2.926302

Batch 117540, train_perplexity=18.658503, train_loss=2.926302

Batch 117550, train_perplexity=18.658499, train_loss=2.9263017

Batch 117560, train_perplexity=18.658503, train_loss=2.926302

Batch 117570, train_perplexity=18.658503, train_loss=2.926302

Batch 117580, train_perplexity=18.658503, train_loss=2.926302

Batch 117590, train_perplexity=18.658493, train_loss=2.9263015

Batch 117600, train_perplexity=18.658503, train_loss=2.926302

Batch 117610, train_perplexity=18.658503, train_loss=2.926302

Batch 117620, train_perplexity=18.658493, train_loss=2.9263015

Batch 117630, train_perplexity=18.658503, train_loss=2.926302

Batch 117640, train_perplexity=18.658493, train_loss=2.9263015

Batch 117650, train_perplexity=18.658499, train_loss=2.9263017

Batch 117660, train_perplexity=18.658503, train_loss=2.926302

Batch 117670, train_perplexity=18.658503, train_loss=2.926302

Batch 117680, train_perplexity=18.658503, train_loss=2.926302

Batch 117690, train_perplexity=18.658499, train_loss=2.9263017

Batch 117700, train_perplexity=18.658499, train_loss=2.9263017

Batch 117710, train_perplexity=18.658499, train_loss=2.9263017

Batch 117720, train_perplexity=18.658499, train_loss=2.9263017

Batch 117730, train_perplexity=18.658499, train_loss=2.9263017

Batch 117740, train_perplexity=18.658493, train_loss=2.9263015

Batch 117750, train_perplexity=18.658493, train_loss=2.9263015

Batch 117760, train_perplexity=18.658499, train_loss=2.9263017

Batch 117770, train_perplexity=18.658493, train_loss=2.9263015

Batch 117780, train_perplexity=18.658493, train_loss=2.9263015

Batch 117790, train_perplexity=18.658499, train_loss=2.9263017

Batch 117800, train_perplexity=18.658493, train_loss=2.9263015

Batch 117810, train_perplexity=18.658493, train_loss=2.9263015

Batch 117820, train_perplexity=18.658493, train_loss=2.9263015

Batch 117830, train_perplexity=18.658493, train_loss=2.9263015

Batch 117840, train_perplexity=18.65849, train_loss=2.9263012

Batch 117850, train_perplexity=18.658493, train_loss=2.9263015

Batch 117860, train_perplexity=18.658493, train_loss=2.9263015

Batch 117870, train_perplexity=18.658493, train_loss=2.9263015

Batch 117880, train_perplexity=18.658493, train_loss=2.9263015

Batch 117890, train_perplexity=18.65849, train_loss=2.9263012

Batch 117900, train_perplexity=18.65849, train_loss=2.9263012

Batch 117910, train_perplexity=18.658485, train_loss=2.926301

Batch 117920, train_perplexity=18.658493, train_loss=2.9263015

Batch 117930, train_perplexity=18.65849, train_loss=2.9263012

Batch 117940, train_perplexity=18.658493, train_loss=2.9263015

Batch 117950, train_perplexity=18.65849, train_loss=2.9263012

Batch 117960, train_perplexity=18.658485, train_loss=2.926301

Batch 117970, train_perplexity=18.658493, train_loss=2.9263015

Batch 117980, train_perplexity=18.658485, train_loss=2.926301

Batch 117990, train_perplexity=18.658485, train_loss=2.926301

Batch 118000, train_perplexity=18.65849, train_loss=2.9263012

Batch 118010, train_perplexity=18.658485, train_loss=2.926301

Batch 118020, train_perplexity=18.658485, train_loss=2.926301

Batch 118030, train_perplexity=18.658485, train_loss=2.926301

Batch 118040, train_perplexity=18.65849, train_loss=2.9263012

Batch 118050, train_perplexity=18.658485, train_loss=2.926301

Batch 118060, train_perplexity=18.658485, train_loss=2.926301

Batch 118070, train_perplexity=18.658485, train_loss=2.926301

Batch 118080, train_perplexity=18.658485, train_loss=2.926301

Batch 118090, train_perplexity=18.658485, train_loss=2.926301

Batch 118100, train_perplexity=18.658485, train_loss=2.926301

Batch 118110, train_perplexity=18.658485, train_loss=2.926301

Batch 118120, train_perplexity=18.658485, train_loss=2.926301

Batch 118130, train_perplexity=18.658485, train_loss=2.926301

Batch 118140, train_perplexity=18.658485, train_loss=2.926301

Batch 118150, train_perplexity=18.658485, train_loss=2.926301

Batch 118160, train_perplexity=18.658485, train_loss=2.926301

Batch 118170, train_perplexity=18.658485, train_loss=2.926301

Batch 118180, train_perplexity=18.658476, train_loss=2.9263005

Batch 118190, train_perplexity=18.658476, train_loss=2.9263005

Batch 118200, train_perplexity=18.658485, train_loss=2.926301

Batch 118210, train_perplexity=18.658485, train_loss=2.926301

Batch 118220, train_perplexity=18.658476, train_loss=2.9263005

Batch 118230, train_perplexity=18.658476, train_loss=2.9263005

Batch 118240, train_perplexity=18.658485, train_loss=2.926301

Batch 118250, train_perplexity=18.658476, train_loss=2.9263005

Batch 118260, train_perplexity=18.658485, train_loss=2.926301

Batch 118270, train_perplexity=18.658476, train_loss=2.9263005

Batch 118280, train_perplexity=18.658476, train_loss=2.9263005

Batch 118290, train_perplexity=18.65848, train_loss=2.9263008

Batch 118300, train_perplexity=18.658485, train_loss=2.926301

Batch 118310, train_perplexity=18.658476, train_loss=2.9263005

Batch 118320, train_perplexity=18.658476, train_loss=2.9263005

Batch 118330, train_perplexity=18.658476, train_loss=2.9263005

Batch 118340, train_perplexity=18.65848, train_loss=2.9263008

Batch 118350, train_perplexity=18.658476, train_loss=2.9263005

Batch 118360, train_perplexity=18.658476, train_loss=2.9263005

Batch 118370, train_perplexity=18.65848, train_loss=2.9263008

Batch 118380, train_perplexity=18.658476, train_loss=2.9263005

Batch 118390, train_perplexity=18.658476, train_loss=2.9263005

Batch 118400, train_perplexity=18.658476, train_loss=2.9263005

Batch 118410, train_perplexity=18.658476, train_loss=2.9263005

Batch 118420, train_perplexity=18.658476, train_loss=2.9263005

Batch 118430, train_perplexity=18.65848, train_loss=2.9263008

Batch 118440, train_perplexity=18.658476, train_loss=2.9263005

Batch 118450, train_perplexity=18.658476, train_loss=2.9263005

Batch 118460, train_perplexity=18.658476, train_loss=2.9263005

Batch 118470, train_perplexity=18.658476, train_loss=2.9263005

Batch 118480, train_perplexity=18.658476, train_loss=2.9263005

Batch 118490, train_perplexity=18.658476, train_loss=2.9263005

Batch 118500, train_perplexity=18.658472, train_loss=2.9263003

Batch 118510, train_perplexity=18.658476, train_loss=2.9263005

Batch 118520, train_perplexity=18.658472, train_loss=2.9263003

Batch 118530, train_perplexity=18.658476, train_loss=2.9263005

Batch 118540, train_perplexity=18.658476, train_loss=2.9263005

Batch 118550, train_perplexity=18.658472, train_loss=2.9263003

Batch 118560, train_perplexity=18.658466, train_loss=2.9263

Batch 118570, train_perplexity=18.658472, train_loss=2.9263003

Batch 118580, train_perplexity=18.658476, train_loss=2.9263005

Batch 118590, train_perplexity=18.658466, train_loss=2.9263

Batch 118600, train_perplexity=18.658466, train_loss=2.9263

Batch 118610, train_perplexity=18.658472, train_loss=2.9263003

Batch 118620, train_perplexity=18.658466, train_loss=2.9263

Batch 118630, train_perplexity=18.658466, train_loss=2.9263

Batch 118640, train_perplexity=18.658472, train_loss=2.9263003

Batch 118650, train_perplexity=18.658472, train_loss=2.9263003

Batch 118660, train_perplexity=18.658466, train_loss=2.9263

Batch 118670, train_perplexity=18.658472, train_loss=2.9263003

Batch 118680, train_perplexity=18.658472, train_loss=2.9263003

Batch 118690, train_perplexity=18.658466, train_loss=2.9263

Batch 118700, train_perplexity=18.658463, train_loss=2.9262998

Batch 118710, train_perplexity=18.658463, train_loss=2.9262998

Batch 118720, train_perplexity=18.658466, train_loss=2.9263

Batch 118730, train_perplexity=18.658463, train_loss=2.9262998

Batch 118740, train_perplexity=18.658466, train_loss=2.9263

Batch 118750, train_perplexity=18.658472, train_loss=2.9263003

Batch 118760, train_perplexity=18.658466, train_loss=2.9263

Batch 118770, train_perplexity=18.658466, train_loss=2.9263

Batch 118780, train_perplexity=18.658466, train_loss=2.9263

Batch 118790, train_perplexity=18.658466, train_loss=2.9263

Batch 118800, train_perplexity=18.658466, train_loss=2.9263

Batch 118810, train_perplexity=18.658466, train_loss=2.9263

Batch 118820, train_perplexity=18.658466, train_loss=2.9263
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 118830, train_perplexity=18.658459, train_loss=2.9262996

Batch 118840, train_perplexity=18.658466, train_loss=2.9263

Batch 118850, train_perplexity=18.658459, train_loss=2.9262996

Batch 118860, train_perplexity=18.658466, train_loss=2.9263

Batch 118870, train_perplexity=18.658459, train_loss=2.9262996

Batch 118880, train_perplexity=18.658463, train_loss=2.9262998

Batch 118890, train_perplexity=18.658459, train_loss=2.9262996

Batch 118900, train_perplexity=18.658459, train_loss=2.9262996

Batch 118910, train_perplexity=18.658459, train_loss=2.9262996

Batch 118920, train_perplexity=18.658459, train_loss=2.9262996

Batch 118930, train_perplexity=18.658459, train_loss=2.9262996

Batch 118940, train_perplexity=18.658459, train_loss=2.9262996

Batch 118950, train_perplexity=18.658463, train_loss=2.9262998

Batch 118960, train_perplexity=18.658459, train_loss=2.9262996

Batch 118970, train_perplexity=18.658459, train_loss=2.9262996

Batch 118980, train_perplexity=18.658459, train_loss=2.9262996

Batch 118990, train_perplexity=18.658459, train_loss=2.9262996

Batch 119000, train_perplexity=18.658459, train_loss=2.9262996

Batch 119010, train_perplexity=18.658463, train_loss=2.9262998

Batch 119020, train_perplexity=18.658463, train_loss=2.9262998

Batch 119030, train_perplexity=18.658459, train_loss=2.9262996

Batch 119040, train_perplexity=18.658459, train_loss=2.9262996

Batch 119050, train_perplexity=18.658459, train_loss=2.9262996

Batch 119060, train_perplexity=18.658459, train_loss=2.9262996

Batch 119070, train_perplexity=18.658453, train_loss=2.9262993

Batch 119080, train_perplexity=18.658459, train_loss=2.9262996

Batch 119090, train_perplexity=18.658459, train_loss=2.9262996

Batch 119100, train_perplexity=18.658459, train_loss=2.9262996

Batch 119110, train_perplexity=18.658453, train_loss=2.9262993

Batch 119120, train_perplexity=18.65845, train_loss=2.926299

Batch 119130, train_perplexity=18.658459, train_loss=2.9262996

Batch 119140, train_perplexity=18.658453, train_loss=2.9262993

Batch 119150, train_perplexity=18.65845, train_loss=2.926299

Batch 119160, train_perplexity=18.658459, train_loss=2.9262996

Batch 119170, train_perplexity=18.658459, train_loss=2.9262996

Batch 119180, train_perplexity=18.658453, train_loss=2.9262993

Batch 119190, train_perplexity=18.658453, train_loss=2.9262993

Batch 119200, train_perplexity=18.658459, train_loss=2.9262996

Batch 119210, train_perplexity=18.658459, train_loss=2.9262996

Batch 119220, train_perplexity=18.658453, train_loss=2.9262993

Batch 119230, train_perplexity=18.658459, train_loss=2.9262996

Batch 119240, train_perplexity=18.65845, train_loss=2.926299

Batch 119250, train_perplexity=18.65845, train_loss=2.926299

Batch 119260, train_perplexity=18.658453, train_loss=2.9262993

Batch 119270, train_perplexity=18.65845, train_loss=2.926299

Batch 119280, train_perplexity=18.65845, train_loss=2.926299

Batch 119290, train_perplexity=18.65845, train_loss=2.926299

Batch 119300, train_perplexity=18.65845, train_loss=2.926299

Batch 119310, train_perplexity=18.65845, train_loss=2.926299

Batch 119320, train_perplexity=18.65845, train_loss=2.926299

Batch 119330, train_perplexity=18.65845, train_loss=2.926299

Batch 119340, train_perplexity=18.65845, train_loss=2.926299

Batch 119350, train_perplexity=18.65845, train_loss=2.926299

Batch 119360, train_perplexity=18.65845, train_loss=2.926299

Batch 119370, train_perplexity=18.65845, train_loss=2.926299

Batch 119380, train_perplexity=18.65845, train_loss=2.926299

Batch 119390, train_perplexity=18.65845, train_loss=2.926299

Batch 119400, train_perplexity=18.65845, train_loss=2.926299

Batch 119410, train_perplexity=18.65845, train_loss=2.926299

Batch 119420, train_perplexity=18.65844, train_loss=2.9262986

Batch 119430, train_perplexity=18.65844, train_loss=2.9262986

Batch 119440, train_perplexity=18.658445, train_loss=2.9262989

Batch 119450, train_perplexity=18.65845, train_loss=2.926299

Batch 119460, train_perplexity=18.65845, train_loss=2.926299

Batch 119470, train_perplexity=18.658445, train_loss=2.9262989

Batch 119480, train_perplexity=18.65844, train_loss=2.9262986

Batch 119490, train_perplexity=18.658445, train_loss=2.9262989

Batch 119500, train_perplexity=18.65844, train_loss=2.9262986

Batch 119510, train_perplexity=18.65844, train_loss=2.9262986

Batch 119520, train_perplexity=18.65844, train_loss=2.9262986

Batch 119530, train_perplexity=18.65844, train_loss=2.9262986

Batch 119540, train_perplexity=18.65844, train_loss=2.9262986

Batch 119550, train_perplexity=18.65845, train_loss=2.926299

Batch 119560, train_perplexity=18.65844, train_loss=2.9262986

Batch 119570, train_perplexity=18.658445, train_loss=2.9262989

Batch 119580, train_perplexity=18.65844, train_loss=2.9262986

Batch 119590, train_perplexity=18.65844, train_loss=2.9262986

Batch 119600, train_perplexity=18.65844, train_loss=2.9262986

Batch 119610, train_perplexity=18.65845, train_loss=2.926299

Batch 119620, train_perplexity=18.65844, train_loss=2.9262986

Batch 119630, train_perplexity=18.658445, train_loss=2.9262989

Batch 119640, train_perplexity=18.65844, train_loss=2.9262986

Batch 119650, train_perplexity=18.658436, train_loss=2.9262984

Batch 119660, train_perplexity=18.658445, train_loss=2.9262989

Batch 119670, train_perplexity=18.65844, train_loss=2.9262986

Batch 119680, train_perplexity=18.658445, train_loss=2.9262989

Batch 119690, train_perplexity=18.658436, train_loss=2.9262984

Batch 119700, train_perplexity=18.65844, train_loss=2.9262986

Batch 119710, train_perplexity=18.658436, train_loss=2.9262984

Batch 119720, train_perplexity=18.658436, train_loss=2.9262984

Batch 119730, train_perplexity=18.65844, train_loss=2.9262986

Batch 119740, train_perplexity=18.65844, train_loss=2.9262986

Batch 119750, train_perplexity=18.658436, train_loss=2.9262984

Batch 119760, train_perplexity=18.65844, train_loss=2.9262986

Batch 119770, train_perplexity=18.65844, train_loss=2.9262986

Batch 119780, train_perplexity=18.65844, train_loss=2.9262986

Batch 119790, train_perplexity=18.658432, train_loss=2.9262981

Batch 119800, train_perplexity=18.658436, train_loss=2.9262984

Batch 119810, train_perplexity=18.65844, train_loss=2.9262986

Batch 119820, train_perplexity=18.658432, train_loss=2.9262981

Batch 119830, train_perplexity=18.65844, train_loss=2.9262986

Batch 119840, train_perplexity=18.658436, train_loss=2.9262984

Batch 119850, train_perplexity=18.658436, train_loss=2.9262984

Batch 119860, train_perplexity=18.658432, train_loss=2.9262981

Batch 119870, train_perplexity=18.658436, train_loss=2.9262984

Batch 119880, train_perplexity=18.658436, train_loss=2.9262984

Batch 119890, train_perplexity=18.658432, train_loss=2.9262981

Batch 119900, train_perplexity=18.658432, train_loss=2.9262981

Batch 119910, train_perplexity=18.658436, train_loss=2.9262984

Batch 119920, train_perplexity=18.658436, train_loss=2.9262984

Batch 119930, train_perplexity=18.658432, train_loss=2.9262981

Batch 119940, train_perplexity=18.658432, train_loss=2.9262981

Batch 119950, train_perplexity=18.658432, train_loss=2.9262981

Batch 119960, train_perplexity=18.658432, train_loss=2.9262981

Batch 119970, train_perplexity=18.658432, train_loss=2.9262981

Batch 119980, train_perplexity=18.658432, train_loss=2.9262981

Batch 119990, train_perplexity=18.658432, train_loss=2.9262981

Batch 120000, train_perplexity=18.658426, train_loss=2.926298

Batch 120010, train_perplexity=18.658432, train_loss=2.9262981

Batch 120020, train_perplexity=18.658432, train_loss=2.9262981

Batch 120030, train_perplexity=18.658426, train_loss=2.926298

Batch 120040, train_perplexity=18.658432, train_loss=2.9262981

Batch 120050, train_perplexity=18.658426, train_loss=2.926298

Batch 120060, train_perplexity=18.658432, train_loss=2.9262981

Batch 120070, train_perplexity=18.658422, train_loss=2.9262977

Batch 120080, train_perplexity=18.658422, train_loss=2.9262977

Batch 120090, train_perplexity=18.658432, train_loss=2.9262981

Batch 120100, train_perplexity=18.658432, train_loss=2.9262981

Batch 120110, train_perplexity=18.658422, train_loss=2.9262977

Batch 120120, train_perplexity=18.658432, train_loss=2.9262981
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 120130, train_perplexity=18.658426, train_loss=2.926298

Batch 120140, train_perplexity=18.658422, train_loss=2.9262977

Batch 120150, train_perplexity=18.658422, train_loss=2.9262977

Batch 120160, train_perplexity=18.658426, train_loss=2.926298

Batch 120170, train_perplexity=18.658422, train_loss=2.9262977

Batch 120180, train_perplexity=18.658426, train_loss=2.926298

Batch 120190, train_perplexity=18.658422, train_loss=2.9262977

Batch 120200, train_perplexity=18.658422, train_loss=2.9262977

Batch 120210, train_perplexity=18.658426, train_loss=2.926298

Batch 120220, train_perplexity=18.658422, train_loss=2.9262977

Batch 120230, train_perplexity=18.658422, train_loss=2.9262977

Batch 120240, train_perplexity=18.658419, train_loss=2.9262974

Batch 120250, train_perplexity=18.658422, train_loss=2.9262977

Batch 120260, train_perplexity=18.658422, train_loss=2.9262977

Batch 120270, train_perplexity=18.658422, train_loss=2.9262977

Batch 120280, train_perplexity=18.658422, train_loss=2.9262977

Batch 120290, train_perplexity=18.658422, train_loss=2.9262977

Batch 120300, train_perplexity=18.658422, train_loss=2.9262977

Batch 120310, train_perplexity=18.658419, train_loss=2.9262974

Batch 120320, train_perplexity=18.658419, train_loss=2.9262974

Batch 120330, train_perplexity=18.658422, train_loss=2.9262977

Batch 120340, train_perplexity=18.658419, train_loss=2.9262974

Batch 120350, train_perplexity=18.658422, train_loss=2.9262977

Batch 120360, train_perplexity=18.658422, train_loss=2.9262977

Batch 120370, train_perplexity=18.658419, train_loss=2.9262974

Batch 120380, train_perplexity=18.658422, train_loss=2.9262977

Batch 120390, train_perplexity=18.658422, train_loss=2.9262977

Batch 120400, train_perplexity=18.658419, train_loss=2.9262974

Batch 120410, train_perplexity=18.658419, train_loss=2.9262974

Batch 120420, train_perplexity=18.658419, train_loss=2.9262974

Batch 120430, train_perplexity=18.658419, train_loss=2.9262974

Batch 120440, train_perplexity=18.658422, train_loss=2.9262977

Batch 120450, train_perplexity=18.658419, train_loss=2.9262974

Batch 120460, train_perplexity=18.658413, train_loss=2.9262972

Batch 120470, train_perplexity=18.658413, train_loss=2.9262972

Batch 120480, train_perplexity=18.658413, train_loss=2.9262972

Batch 120490, train_perplexity=18.658413, train_loss=2.9262972

Batch 120500, train_perplexity=18.658413, train_loss=2.9262972

Batch 120510, train_perplexity=18.658413, train_loss=2.9262972

Batch 120520, train_perplexity=18.658419, train_loss=2.9262974

Batch 120530, train_perplexity=18.658419, train_loss=2.9262974

Batch 120540, train_perplexity=18.658413, train_loss=2.9262972

Batch 120550, train_perplexity=18.658419, train_loss=2.9262974

Batch 120560, train_perplexity=18.658413, train_loss=2.9262972

Batch 120570, train_perplexity=18.658413, train_loss=2.9262972

Batch 120580, train_perplexity=18.658413, train_loss=2.9262972

Batch 120590, train_perplexity=18.658413, train_loss=2.9262972

Batch 120600, train_perplexity=18.65841, train_loss=2.926297

Batch 120610, train_perplexity=18.65841, train_loss=2.926297

Batch 120620, train_perplexity=18.658413, train_loss=2.9262972

Batch 120630, train_perplexity=18.65841, train_loss=2.926297

Batch 120640, train_perplexity=18.65841, train_loss=2.926297

Batch 120650, train_perplexity=18.658413, train_loss=2.9262972

Batch 120660, train_perplexity=18.658413, train_loss=2.9262972

Batch 120670, train_perplexity=18.658419, train_loss=2.9262974

Batch 120680, train_perplexity=18.658413, train_loss=2.9262972

Batch 120690, train_perplexity=18.65841, train_loss=2.926297

Batch 120700, train_perplexity=18.65841, train_loss=2.926297

Batch 120710, train_perplexity=18.658413, train_loss=2.9262972

Batch 120720, train_perplexity=18.65841, train_loss=2.926297

Batch 120730, train_perplexity=18.65841, train_loss=2.926297

Batch 120740, train_perplexity=18.65841, train_loss=2.926297

Batch 120750, train_perplexity=18.658405, train_loss=2.9262967

Batch 120760, train_perplexity=18.65841, train_loss=2.926297

Batch 120770, train_perplexity=18.658405, train_loss=2.9262967

Batch 120780, train_perplexity=18.65841, train_loss=2.926297

Batch 120790, train_perplexity=18.65841, train_loss=2.926297

Batch 120800, train_perplexity=18.658405, train_loss=2.9262967

Batch 120810, train_perplexity=18.65841, train_loss=2.926297

Batch 120820, train_perplexity=18.658405, train_loss=2.9262967

Batch 120830, train_perplexity=18.658405, train_loss=2.9262967

Batch 120840, train_perplexity=18.65841, train_loss=2.926297

Batch 120850, train_perplexity=18.658405, train_loss=2.9262967

Batch 120860, train_perplexity=18.658405, train_loss=2.9262967

Batch 120870, train_perplexity=18.658405, train_loss=2.9262967

Batch 120880, train_perplexity=18.658405, train_loss=2.9262967

Batch 120890, train_perplexity=18.65841, train_loss=2.926297

Batch 120900, train_perplexity=18.65841, train_loss=2.926297

Batch 120910, train_perplexity=18.658405, train_loss=2.9262967

Batch 120920, train_perplexity=18.658405, train_loss=2.9262967

Batch 120930, train_perplexity=18.658405, train_loss=2.9262967

Batch 120940, train_perplexity=18.658405, train_loss=2.9262967

Batch 120950, train_perplexity=18.658405, train_loss=2.9262967

Batch 120960, train_perplexity=18.658405, train_loss=2.9262967

Batch 120970, train_perplexity=18.658405, train_loss=2.9262967

Batch 120980, train_perplexity=18.658405, train_loss=2.9262967

Batch 120990, train_perplexity=18.658396, train_loss=2.9262962

Batch 121000, train_perplexity=18.6584, train_loss=2.9262965

Batch 121010, train_perplexity=18.6584, train_loss=2.9262965

Batch 121020, train_perplexity=18.6584, train_loss=2.9262965

Batch 121030, train_perplexity=18.658405, train_loss=2.9262967

Batch 121040, train_perplexity=18.6584, train_loss=2.9262965

Batch 121050, train_perplexity=18.658396, train_loss=2.9262962

Batch 121060, train_perplexity=18.6584, train_loss=2.9262965

Batch 121070, train_perplexity=18.658405, train_loss=2.9262967

Batch 121080, train_perplexity=18.6584, train_loss=2.9262965

Batch 121090, train_perplexity=18.6584, train_loss=2.9262965

Batch 121100, train_perplexity=18.658396, train_loss=2.9262962

Batch 121110, train_perplexity=18.658396, train_loss=2.9262962

Batch 121120, train_perplexity=18.658396, train_loss=2.9262962

Batch 121130, train_perplexity=18.6584, train_loss=2.9262965

Batch 121140, train_perplexity=18.6584, train_loss=2.9262965

Batch 121150, train_perplexity=18.658396, train_loss=2.9262962

Batch 121160, train_perplexity=18.658396, train_loss=2.9262962

Batch 121170, train_perplexity=18.6584, train_loss=2.9262965

Batch 121180, train_perplexity=18.658392, train_loss=2.926296

Batch 121190, train_perplexity=18.658392, train_loss=2.926296

Batch 121200, train_perplexity=18.6584, train_loss=2.9262965

Batch 121210, train_perplexity=18.658392, train_loss=2.926296

Batch 121220, train_perplexity=18.658386, train_loss=2.9262958

Batch 121230, train_perplexity=18.658396, train_loss=2.9262962

Batch 121240, train_perplexity=18.658396, train_loss=2.9262962

Batch 121250, train_perplexity=18.658386, train_loss=2.9262958

Batch 121260, train_perplexity=18.658396, train_loss=2.9262962

Batch 121270, train_perplexity=18.658392, train_loss=2.926296

Batch 121280, train_perplexity=18.658386, train_loss=2.9262958

Batch 121290, train_perplexity=18.658396, train_loss=2.9262962

Batch 121300, train_perplexity=18.658392, train_loss=2.926296

Batch 121310, train_perplexity=18.658396, train_loss=2.9262962

Batch 121320, train_perplexity=18.658392, train_loss=2.926296

Batch 121330, train_perplexity=18.658396, train_loss=2.9262962

Batch 121340, train_perplexity=18.658392, train_loss=2.926296

Batch 121350, train_perplexity=18.658392, train_loss=2.926296

Batch 121360, train_perplexity=18.658396, train_loss=2.9262962

Batch 121370, train_perplexity=18.658396, train_loss=2.9262962

Batch 121380, train_perplexity=18.658386, train_loss=2.9262958

Batch 121390, train_perplexity=18.658392, train_loss=2.926296

Batch 121400, train_perplexity=18.658386, train_loss=2.9262958

Batch 121410, train_perplexity=18.658392, train_loss=2.926296

Batch 121420, train_perplexity=18.658386, train_loss=2.9262958
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 121430, train_perplexity=18.658386, train_loss=2.9262958

Batch 121440, train_perplexity=18.658386, train_loss=2.9262958

Batch 121450, train_perplexity=18.658392, train_loss=2.926296

Batch 121460, train_perplexity=18.658386, train_loss=2.9262958

Batch 121470, train_perplexity=18.658386, train_loss=2.9262958

Batch 121480, train_perplexity=18.658386, train_loss=2.9262958

Batch 121490, train_perplexity=18.658392, train_loss=2.926296

Batch 121500, train_perplexity=18.658386, train_loss=2.9262958

Batch 121510, train_perplexity=18.658386, train_loss=2.9262958

Batch 121520, train_perplexity=18.658386, train_loss=2.9262958

Batch 121530, train_perplexity=18.658386, train_loss=2.9262958

Batch 121540, train_perplexity=18.658386, train_loss=2.9262958

Batch 121550, train_perplexity=18.658386, train_loss=2.9262958

Batch 121560, train_perplexity=18.658386, train_loss=2.9262958

Batch 121570, train_perplexity=18.658386, train_loss=2.9262958

Batch 121580, train_perplexity=18.658386, train_loss=2.9262958

Batch 121590, train_perplexity=18.658386, train_loss=2.9262958

Batch 121600, train_perplexity=18.658382, train_loss=2.9262955

Batch 121610, train_perplexity=18.658386, train_loss=2.9262958

Batch 121620, train_perplexity=18.658386, train_loss=2.9262958

Batch 121630, train_perplexity=18.658382, train_loss=2.9262955

Batch 121640, train_perplexity=18.658379, train_loss=2.9262953

Batch 121650, train_perplexity=18.658382, train_loss=2.9262955

Batch 121660, train_perplexity=18.658379, train_loss=2.9262953

Batch 121670, train_perplexity=18.658382, train_loss=2.9262955

Batch 121680, train_perplexity=18.658382, train_loss=2.9262955

Batch 121690, train_perplexity=18.658386, train_loss=2.9262958

Batch 121700, train_perplexity=18.658382, train_loss=2.9262955

Batch 121710, train_perplexity=18.658386, train_loss=2.9262958

Batch 121720, train_perplexity=18.658379, train_loss=2.9262953

Batch 121730, train_perplexity=18.658379, train_loss=2.9262953

Batch 121740, train_perplexity=18.658379, train_loss=2.9262953

Batch 121750, train_perplexity=18.658379, train_loss=2.9262953

Batch 121760, train_perplexity=18.658382, train_loss=2.9262955

Batch 121770, train_perplexity=18.658379, train_loss=2.9262953

Batch 121780, train_perplexity=18.658379, train_loss=2.9262953

Batch 121790, train_perplexity=18.658379, train_loss=2.9262953

Batch 121800, train_perplexity=18.658382, train_loss=2.9262955

Batch 121810, train_perplexity=18.658379, train_loss=2.9262953

Batch 121820, train_perplexity=18.658382, train_loss=2.9262955

Batch 121830, train_perplexity=18.658379, train_loss=2.9262953

Batch 121840, train_perplexity=18.658379, train_loss=2.9262953

Batch 121850, train_perplexity=18.658379, train_loss=2.9262953

Batch 121860, train_perplexity=18.658379, train_loss=2.9262953

Batch 121870, train_perplexity=18.658379, train_loss=2.9262953

Batch 121880, train_perplexity=18.658379, train_loss=2.9262953

Batch 121890, train_perplexity=18.65837, train_loss=2.9262948

Batch 121900, train_perplexity=18.65837, train_loss=2.9262948

Batch 121910, train_perplexity=18.658373, train_loss=2.926295

Batch 121920, train_perplexity=18.658373, train_loss=2.926295

Batch 121930, train_perplexity=18.658379, train_loss=2.9262953

Batch 121940, train_perplexity=18.65837, train_loss=2.9262948

Batch 121950, train_perplexity=18.658379, train_loss=2.9262953

Batch 121960, train_perplexity=18.658379, train_loss=2.9262953

Batch 121970, train_perplexity=18.658379, train_loss=2.9262953

Batch 121980, train_perplexity=18.658373, train_loss=2.926295

Batch 121990, train_perplexity=18.658373, train_loss=2.926295

Batch 122000, train_perplexity=18.65837, train_loss=2.9262948

Batch 122010, train_perplexity=18.658373, train_loss=2.926295

Batch 122020, train_perplexity=18.65837, train_loss=2.9262948

Batch 122030, train_perplexity=18.658373, train_loss=2.926295

Batch 122040, train_perplexity=18.65837, train_loss=2.9262948

Batch 122050, train_perplexity=18.65837, train_loss=2.9262948

Batch 122060, train_perplexity=18.65837, train_loss=2.9262948

Batch 122070, train_perplexity=18.65837, train_loss=2.9262948

Batch 122080, train_perplexity=18.658373, train_loss=2.926295

Batch 122090, train_perplexity=18.65837, train_loss=2.9262948

Batch 122100, train_perplexity=18.65837, train_loss=2.9262948

Batch 122110, train_perplexity=18.65837, train_loss=2.9262948

Batch 122120, train_perplexity=18.658365, train_loss=2.9262946

Batch 122130, train_perplexity=18.65837, train_loss=2.9262948

Batch 122140, train_perplexity=18.65837, train_loss=2.9262948

Batch 122150, train_perplexity=18.65837, train_loss=2.9262948

Batch 122160, train_perplexity=18.65837, train_loss=2.9262948

Batch 122170, train_perplexity=18.65836, train_loss=2.9262943

Batch 122180, train_perplexity=18.65836, train_loss=2.9262943

Batch 122190, train_perplexity=18.65837, train_loss=2.9262948

Batch 122200, train_perplexity=18.65836, train_loss=2.9262943

Batch 122210, train_perplexity=18.658365, train_loss=2.9262946

Batch 122220, train_perplexity=18.65836, train_loss=2.9262943

Batch 122230, train_perplexity=18.65837, train_loss=2.9262948

Batch 122240, train_perplexity=18.65837, train_loss=2.9262948

Batch 122250, train_perplexity=18.658365, train_loss=2.9262946

Batch 122260, train_perplexity=18.658373, train_loss=2.926295

Batch 122270, train_perplexity=18.65836, train_loss=2.9262943

Batch 122280, train_perplexity=18.65836, train_loss=2.9262943

Batch 122290, train_perplexity=18.658373, train_loss=2.926295

Batch 122300, train_perplexity=18.65837, train_loss=2.9262948

Batch 122310, train_perplexity=18.658365, train_loss=2.9262946

Batch 122320, train_perplexity=18.65836, train_loss=2.9262943

Batch 122330, train_perplexity=18.65837, train_loss=2.9262948

Batch 122340, train_perplexity=18.65836, train_loss=2.9262943

Batch 122350, train_perplexity=18.65836, train_loss=2.9262943

Batch 122360, train_perplexity=18.658365, train_loss=2.9262946

Batch 122370, train_perplexity=18.658365, train_loss=2.9262946

Batch 122380, train_perplexity=18.658365, train_loss=2.9262946

Batch 122390, train_perplexity=18.65836, train_loss=2.9262943

Batch 122400, train_perplexity=18.65836, train_loss=2.9262943

Batch 122410, train_perplexity=18.658365, train_loss=2.9262946

Batch 122420, train_perplexity=18.65836, train_loss=2.9262943

Batch 122430, train_perplexity=18.65836, train_loss=2.9262943

Batch 122440, train_perplexity=18.658356, train_loss=2.926294

Batch 122450, train_perplexity=18.65836, train_loss=2.9262943

Batch 122460, train_perplexity=18.65836, train_loss=2.9262943

Batch 122470, train_perplexity=18.65836, train_loss=2.9262943

Batch 122480, train_perplexity=18.658356, train_loss=2.926294

Batch 122490, train_perplexity=18.65836, train_loss=2.9262943

Batch 122500, train_perplexity=18.658356, train_loss=2.926294

Batch 122510, train_perplexity=18.658356, train_loss=2.926294

Batch 122520, train_perplexity=18.658365, train_loss=2.9262946

Batch 122530, train_perplexity=18.65836, train_loss=2.9262943

Batch 122540, train_perplexity=18.65836, train_loss=2.9262943

Batch 122550, train_perplexity=18.65836, train_loss=2.9262943

Batch 122560, train_perplexity=18.65836, train_loss=2.9262943

Batch 122570, train_perplexity=18.658356, train_loss=2.926294

Batch 122580, train_perplexity=18.658352, train_loss=2.9262938

Batch 122590, train_perplexity=18.658352, train_loss=2.9262938

Batch 122600, train_perplexity=18.658356, train_loss=2.926294

Batch 122610, train_perplexity=18.658352, train_loss=2.9262938

Batch 122620, train_perplexity=18.658352, train_loss=2.9262938

Batch 122630, train_perplexity=18.658352, train_loss=2.9262938

Batch 122640, train_perplexity=18.658352, train_loss=2.9262938

Batch 122650, train_perplexity=18.658352, train_loss=2.9262938

Batch 122660, train_perplexity=18.658352, train_loss=2.9262938

Batch 122670, train_perplexity=18.658352, train_loss=2.9262938

Batch 122680, train_perplexity=18.658352, train_loss=2.9262938

Batch 122690, train_perplexity=18.658352, train_loss=2.9262938

Batch 122700, train_perplexity=18.658356, train_loss=2.926294

Batch 122710, train_perplexity=18.65836, train_loss=2.9262943
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 122720, train_perplexity=18.658352, train_loss=2.9262938

Batch 122730, train_perplexity=18.658352, train_loss=2.9262938

Batch 122740, train_perplexity=18.658352, train_loss=2.9262938

Batch 122750, train_perplexity=18.658352, train_loss=2.9262938

Batch 122760, train_perplexity=18.658352, train_loss=2.9262938

Batch 122770, train_perplexity=18.658352, train_loss=2.9262938

Batch 122780, train_perplexity=18.658352, train_loss=2.9262938

Batch 122790, train_perplexity=18.658352, train_loss=2.9262938

Batch 122800, train_perplexity=18.658342, train_loss=2.9262934

Batch 122810, train_perplexity=18.658346, train_loss=2.9262936

Batch 122820, train_perplexity=18.658342, train_loss=2.9262934

Batch 122830, train_perplexity=18.658346, train_loss=2.9262936

Batch 122840, train_perplexity=18.658352, train_loss=2.9262938

Batch 122850, train_perplexity=18.658352, train_loss=2.9262938

Batch 122860, train_perplexity=18.658346, train_loss=2.9262936

Batch 122870, train_perplexity=18.658352, train_loss=2.9262938

Batch 122880, train_perplexity=18.658346, train_loss=2.9262936

Batch 122890, train_perplexity=18.658342, train_loss=2.9262934

Batch 122900, train_perplexity=18.658342, train_loss=2.9262934

Batch 122910, train_perplexity=18.658352, train_loss=2.9262938

Batch 122920, train_perplexity=18.658342, train_loss=2.9262934

Batch 122930, train_perplexity=18.658346, train_loss=2.9262936

Batch 122940, train_perplexity=18.658346, train_loss=2.9262936

Batch 122950, train_perplexity=18.658342, train_loss=2.9262934

Batch 122960, train_perplexity=18.658342, train_loss=2.9262934

Batch 122970, train_perplexity=18.658342, train_loss=2.9262934

Batch 122980, train_perplexity=18.658346, train_loss=2.9262936

Batch 122990, train_perplexity=18.658346, train_loss=2.9262936

Batch 123000, train_perplexity=18.658342, train_loss=2.9262934

Batch 123010, train_perplexity=18.658346, train_loss=2.9262936

Batch 123020, train_perplexity=18.658342, train_loss=2.9262934

Batch 123030, train_perplexity=18.658342, train_loss=2.9262934

Batch 123040, train_perplexity=18.658342, train_loss=2.9262934

Batch 123050, train_perplexity=18.658339, train_loss=2.9262931

Batch 123060, train_perplexity=18.658342, train_loss=2.9262934

Batch 123070, train_perplexity=18.658339, train_loss=2.9262931

Batch 123080, train_perplexity=18.658342, train_loss=2.9262934

Batch 123090, train_perplexity=18.658342, train_loss=2.9262934

Batch 123100, train_perplexity=18.658342, train_loss=2.9262934

Batch 123110, train_perplexity=18.658342, train_loss=2.9262934

Batch 123120, train_perplexity=18.658342, train_loss=2.9262934

Batch 123130, train_perplexity=18.658342, train_loss=2.9262934

Batch 123140, train_perplexity=18.658342, train_loss=2.9262934

Batch 123150, train_perplexity=18.658342, train_loss=2.9262934

Batch 123160, train_perplexity=18.658342, train_loss=2.9262934

Batch 123170, train_perplexity=18.658333, train_loss=2.926293

Batch 123180, train_perplexity=18.658342, train_loss=2.9262934

Batch 123190, train_perplexity=18.658333, train_loss=2.926293

Batch 123200, train_perplexity=18.658342, train_loss=2.9262934

Batch 123210, train_perplexity=18.658333, train_loss=2.926293

Batch 123220, train_perplexity=18.658342, train_loss=2.9262934

Batch 123230, train_perplexity=18.658339, train_loss=2.9262931

Batch 123240, train_perplexity=18.658333, train_loss=2.926293

Batch 123250, train_perplexity=18.658342, train_loss=2.9262934

Batch 123260, train_perplexity=18.658333, train_loss=2.926293

Batch 123270, train_perplexity=18.658333, train_loss=2.926293

Batch 123280, train_perplexity=18.658333, train_loss=2.926293

Batch 123290, train_perplexity=18.658339, train_loss=2.9262931

Batch 123300, train_perplexity=18.658339, train_loss=2.9262931

Batch 123310, train_perplexity=18.658333, train_loss=2.926293

Batch 123320, train_perplexity=18.658333, train_loss=2.926293

Batch 123330, train_perplexity=18.658333, train_loss=2.926293

Batch 123340, train_perplexity=18.658333, train_loss=2.926293

Batch 123350, train_perplexity=18.658333, train_loss=2.926293

Batch 123360, train_perplexity=18.658333, train_loss=2.926293

Batch 123370, train_perplexity=18.658333, train_loss=2.926293

Batch 123380, train_perplexity=18.658333, train_loss=2.926293

Batch 123390, train_perplexity=18.658333, train_loss=2.926293

Batch 123400, train_perplexity=18.658333, train_loss=2.926293

Batch 123410, train_perplexity=18.658333, train_loss=2.926293

Batch 123420, train_perplexity=18.658333, train_loss=2.926293

Batch 123430, train_perplexity=18.658333, train_loss=2.926293

Batch 123440, train_perplexity=18.658329, train_loss=2.9262927

Batch 123450, train_perplexity=18.658333, train_loss=2.926293

Batch 123460, train_perplexity=18.658333, train_loss=2.926293

Batch 123470, train_perplexity=18.658325, train_loss=2.9262924

Batch 123480, train_perplexity=18.658329, train_loss=2.9262927

Batch 123490, train_perplexity=18.658329, train_loss=2.9262927

Batch 123500, train_perplexity=18.658325, train_loss=2.9262924

Batch 123510, train_perplexity=18.658325, train_loss=2.9262924

Batch 123520, train_perplexity=18.658325, train_loss=2.9262924

Batch 123530, train_perplexity=18.658329, train_loss=2.9262927

Batch 123540, train_perplexity=18.658329, train_loss=2.9262927

Batch 123550, train_perplexity=18.658325, train_loss=2.9262924

Batch 123560, train_perplexity=18.658329, train_loss=2.9262927

Batch 123570, train_perplexity=18.658325, train_loss=2.9262924

Batch 123580, train_perplexity=18.658325, train_loss=2.9262924

Batch 123590, train_perplexity=18.658325, train_loss=2.9262924

Batch 123600, train_perplexity=18.658325, train_loss=2.9262924

Batch 123610, train_perplexity=18.658329, train_loss=2.9262927

Batch 123620, train_perplexity=18.658329, train_loss=2.9262927

Batch 123630, train_perplexity=18.658325, train_loss=2.9262924

Batch 123640, train_perplexity=18.658329, train_loss=2.9262927

Batch 123650, train_perplexity=18.658325, train_loss=2.9262924

Batch 123660, train_perplexity=18.658325, train_loss=2.9262924

Batch 123670, train_perplexity=18.658325, train_loss=2.9262924

Batch 123680, train_perplexity=18.658325, train_loss=2.9262924

Batch 123690, train_perplexity=18.65832, train_loss=2.9262922

Batch 123700, train_perplexity=18.658325, train_loss=2.9262924

Batch 123710, train_perplexity=18.658325, train_loss=2.9262924

Batch 123720, train_perplexity=18.65832, train_loss=2.9262922

Batch 123730, train_perplexity=18.65832, train_loss=2.9262922

Batch 123740, train_perplexity=18.65832, train_loss=2.9262922

Batch 123750, train_perplexity=18.658325, train_loss=2.9262924

Batch 123760, train_perplexity=18.658325, train_loss=2.9262924

Batch 123770, train_perplexity=18.658325, train_loss=2.9262924

Batch 123780, train_perplexity=18.658316, train_loss=2.926292

Batch 123790, train_perplexity=18.658325, train_loss=2.9262924

Batch 123800, train_perplexity=18.65832, train_loss=2.9262922

Batch 123810, train_perplexity=18.65832, train_loss=2.9262922

Batch 123820, train_perplexity=18.658316, train_loss=2.926292

Batch 123830, train_perplexity=18.658316, train_loss=2.926292

Batch 123840, train_perplexity=18.658316, train_loss=2.926292

Batch 123850, train_perplexity=18.658316, train_loss=2.926292

Batch 123860, train_perplexity=18.65832, train_loss=2.9262922

Batch 123870, train_perplexity=18.658316, train_loss=2.926292

Batch 123880, train_perplexity=18.65832, train_loss=2.9262922

Batch 123890, train_perplexity=18.658316, train_loss=2.926292

Batch 123900, train_perplexity=18.658316, train_loss=2.926292

Batch 123910, train_perplexity=18.658316, train_loss=2.926292

Batch 123920, train_perplexity=18.65832, train_loss=2.9262922

Batch 123930, train_perplexity=18.658316, train_loss=2.926292

Batch 123940, train_perplexity=18.658316, train_loss=2.926292

Batch 123950, train_perplexity=18.658316, train_loss=2.926292

Batch 123960, train_perplexity=18.658316, train_loss=2.926292

Batch 123970, train_perplexity=18.658316, train_loss=2.926292

Batch 123980, train_perplexity=18.658316, train_loss=2.926292

Batch 123990, train_perplexity=18.658316, train_loss=2.926292

Batch 124000, train_perplexity=18.658316, train_loss=2.926292
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 124010, train_perplexity=18.658316, train_loss=2.926292

Batch 124020, train_perplexity=18.658316, train_loss=2.926292

Batch 124030, train_perplexity=18.658316, train_loss=2.926292

Batch 124040, train_perplexity=18.658316, train_loss=2.926292

Batch 124050, train_perplexity=18.658306, train_loss=2.9262915

Batch 124060, train_perplexity=18.658306, train_loss=2.9262915

Batch 124070, train_perplexity=18.658312, train_loss=2.9262917

Batch 124080, train_perplexity=18.658316, train_loss=2.926292

Batch 124090, train_perplexity=18.658316, train_loss=2.926292

Batch 124100, train_perplexity=18.658316, train_loss=2.926292

Batch 124110, train_perplexity=18.658306, train_loss=2.9262915

Batch 124120, train_perplexity=18.658306, train_loss=2.9262915

Batch 124130, train_perplexity=18.658312, train_loss=2.9262917

Batch 124140, train_perplexity=18.658316, train_loss=2.926292

Batch 124150, train_perplexity=18.658306, train_loss=2.9262915

Batch 124160, train_perplexity=18.658306, train_loss=2.9262915

Batch 124170, train_perplexity=18.658306, train_loss=2.9262915

Batch 124180, train_perplexity=18.658306, train_loss=2.9262915

Batch 124190, train_perplexity=18.658312, train_loss=2.9262917

Batch 124200, train_perplexity=18.658302, train_loss=2.9262912

Batch 124210, train_perplexity=18.658306, train_loss=2.9262915

Batch 124220, train_perplexity=18.658306, train_loss=2.9262915

Batch 124230, train_perplexity=18.658306, train_loss=2.9262915

Batch 124240, train_perplexity=18.658306, train_loss=2.9262915

Batch 124250, train_perplexity=18.658306, train_loss=2.9262915

Batch 124260, train_perplexity=18.658306, train_loss=2.9262915

Batch 124270, train_perplexity=18.658306, train_loss=2.9262915

Batch 124280, train_perplexity=18.658302, train_loss=2.9262912

Batch 124290, train_perplexity=18.658306, train_loss=2.9262915

Batch 124300, train_perplexity=18.658306, train_loss=2.9262915

Batch 124310, train_perplexity=18.658306, train_loss=2.9262915

Batch 124320, train_perplexity=18.658306, train_loss=2.9262915

Batch 124330, train_perplexity=18.658306, train_loss=2.9262915

Batch 124340, train_perplexity=18.658302, train_loss=2.9262912

Batch 124350, train_perplexity=18.658306, train_loss=2.9262915

Batch 124360, train_perplexity=18.658306, train_loss=2.9262915

Batch 124370, train_perplexity=18.658306, train_loss=2.9262915

Batch 124380, train_perplexity=18.658302, train_loss=2.9262912

Batch 124390, train_perplexity=18.658306, train_loss=2.9262915

Batch 124400, train_perplexity=18.658298, train_loss=2.926291

Batch 124410, train_perplexity=18.658302, train_loss=2.9262912

Batch 124420, train_perplexity=18.658306, train_loss=2.9262915

Batch 124430, train_perplexity=18.658298, train_loss=2.926291

Batch 124440, train_perplexity=18.658298, train_loss=2.926291

Batch 124450, train_perplexity=18.658298, train_loss=2.926291

Batch 124460, train_perplexity=18.658302, train_loss=2.9262912

Batch 124470, train_perplexity=18.658302, train_loss=2.9262912

Batch 124480, train_perplexity=18.658298, train_loss=2.926291

Batch 124490, train_perplexity=18.658302, train_loss=2.9262912

Batch 124500, train_perplexity=18.658298, train_loss=2.926291

Batch 124510, train_perplexity=18.658298, train_loss=2.926291

Batch 124520, train_perplexity=18.658302, train_loss=2.9262912

Batch 124530, train_perplexity=18.658289, train_loss=2.9262905

Batch 124540, train_perplexity=18.658298, train_loss=2.926291

Batch 124550, train_perplexity=18.658298, train_loss=2.926291

Batch 124560, train_perplexity=18.658298, train_loss=2.926291

Batch 124570, train_perplexity=18.658298, train_loss=2.926291

Batch 124580, train_perplexity=18.658298, train_loss=2.926291

Batch 124590, train_perplexity=18.658293, train_loss=2.9262908

Batch 124600, train_perplexity=18.658298, train_loss=2.926291

Batch 124610, train_perplexity=18.658293, train_loss=2.9262908

Batch 124620, train_perplexity=18.658298, train_loss=2.926291

Batch 124630, train_perplexity=18.658298, train_loss=2.926291

Batch 124640, train_perplexity=18.658289, train_loss=2.9262905

Batch 124650, train_perplexity=18.658298, train_loss=2.926291

Batch 124660, train_perplexity=18.658289, train_loss=2.9262905

Batch 124670, train_perplexity=18.658298, train_loss=2.926291

Batch 124680, train_perplexity=18.658289, train_loss=2.9262905

Batch 124690, train_perplexity=18.658293, train_loss=2.9262908

Batch 124700, train_perplexity=18.658298, train_loss=2.926291

Batch 124710, train_perplexity=18.658293, train_loss=2.9262908

Batch 124720, train_perplexity=18.658293, train_loss=2.9262908

Batch 124730, train_perplexity=18.658293, train_loss=2.9262908

Batch 124740, train_perplexity=18.658293, train_loss=2.9262908

Batch 124750, train_perplexity=18.658293, train_loss=2.9262908

Batch 124760, train_perplexity=18.658289, train_loss=2.9262905

Batch 124770, train_perplexity=18.658289, train_loss=2.9262905

Batch 124780, train_perplexity=18.658293, train_loss=2.9262908

Batch 124790, train_perplexity=18.658289, train_loss=2.9262905

Batch 124800, train_perplexity=18.658289, train_loss=2.9262905

Batch 124810, train_perplexity=18.658289, train_loss=2.9262905

Batch 124820, train_perplexity=18.658289, train_loss=2.9262905

Batch 124830, train_perplexity=18.658289, train_loss=2.9262905

Batch 124840, train_perplexity=18.658289, train_loss=2.9262905

Batch 124850, train_perplexity=18.658289, train_loss=2.9262905

Batch 124860, train_perplexity=18.658289, train_loss=2.9262905

Batch 124870, train_perplexity=18.658289, train_loss=2.9262905

Batch 124880, train_perplexity=18.658289, train_loss=2.9262905

Batch 124890, train_perplexity=18.658289, train_loss=2.9262905

Batch 124900, train_perplexity=18.658289, train_loss=2.9262905

Batch 124910, train_perplexity=18.658289, train_loss=2.9262905

Batch 124920, train_perplexity=18.658289, train_loss=2.9262905

Batch 124930, train_perplexity=18.658289, train_loss=2.9262905

Batch 124940, train_perplexity=18.658289, train_loss=2.9262905

Batch 124950, train_perplexity=18.658285, train_loss=2.9262903

Batch 124960, train_perplexity=18.658289, train_loss=2.9262905

Batch 124970, train_perplexity=18.658289, train_loss=2.9262905

Batch 124980, train_perplexity=18.658289, train_loss=2.9262905

Batch 124990, train_perplexity=18.658285, train_loss=2.9262903

Batch 125000, train_perplexity=18.658289, train_loss=2.9262905

Batch 125010, train_perplexity=18.658285, train_loss=2.9262903

Batch 125020, train_perplexity=18.658285, train_loss=2.9262903

Batch 125030, train_perplexity=18.658289, train_loss=2.9262905

Batch 125040, train_perplexity=18.658285, train_loss=2.9262903

Batch 125050, train_perplexity=18.658289, train_loss=2.9262905

Batch 125060, train_perplexity=18.65828, train_loss=2.92629

Batch 125070, train_perplexity=18.65828, train_loss=2.92629

Batch 125080, train_perplexity=18.65828, train_loss=2.92629

Batch 125090, train_perplexity=18.65828, train_loss=2.92629

Batch 125100, train_perplexity=18.65828, train_loss=2.92629

Batch 125110, train_perplexity=18.658285, train_loss=2.9262903

Batch 125120, train_perplexity=18.65828, train_loss=2.92629

Batch 125130, train_perplexity=18.65828, train_loss=2.92629

Batch 125140, train_perplexity=18.65828, train_loss=2.92629

Batch 125150, train_perplexity=18.65828, train_loss=2.92629

Batch 125160, train_perplexity=18.65828, train_loss=2.92629

Batch 125170, train_perplexity=18.65828, train_loss=2.92629

Batch 125180, train_perplexity=18.65828, train_loss=2.92629

Batch 125190, train_perplexity=18.65828, train_loss=2.92629

Batch 125200, train_perplexity=18.65828, train_loss=2.92629

Batch 125210, train_perplexity=18.65828, train_loss=2.92629

Batch 125220, train_perplexity=18.65828, train_loss=2.92629

Batch 125230, train_perplexity=18.65828, train_loss=2.92629

Batch 125240, train_perplexity=18.65828, train_loss=2.92629

Batch 125250, train_perplexity=18.65828, train_loss=2.92629

Batch 125260, train_perplexity=18.658272, train_loss=2.9262896

Batch 125270, train_perplexity=18.658272, train_loss=2.9262896

Batch 125280, train_perplexity=18.65828, train_loss=2.92629

Batch 125290, train_perplexity=18.65828, train_loss=2.92629

Batch 125300, train_perplexity=18.658272, train_loss=2.9262896
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 125310, train_perplexity=18.65828, train_loss=2.92629

Batch 125320, train_perplexity=18.65828, train_loss=2.92629

Batch 125330, train_perplexity=18.658272, train_loss=2.9262896

Batch 125340, train_perplexity=18.65828, train_loss=2.92629

Batch 125350, train_perplexity=18.658276, train_loss=2.9262898

Batch 125360, train_perplexity=18.658276, train_loss=2.9262898

Batch 125370, train_perplexity=18.65828, train_loss=2.92629

Batch 125380, train_perplexity=18.658272, train_loss=2.9262896

Batch 125390, train_perplexity=18.658272, train_loss=2.9262896

Batch 125400, train_perplexity=18.658272, train_loss=2.9262896

Batch 125410, train_perplexity=18.658272, train_loss=2.9262896

Batch 125420, train_perplexity=18.658272, train_loss=2.9262896

Batch 125430, train_perplexity=18.658272, train_loss=2.9262896

Batch 125440, train_perplexity=18.658272, train_loss=2.9262896

Batch 125450, train_perplexity=18.658272, train_loss=2.9262896

Batch 125460, train_perplexity=18.658272, train_loss=2.9262896

Batch 125470, train_perplexity=18.658272, train_loss=2.9262896

Batch 125480, train_perplexity=18.658272, train_loss=2.9262896

Batch 125490, train_perplexity=18.658272, train_loss=2.9262896

Batch 125500, train_perplexity=18.658272, train_loss=2.9262896

Batch 125510, train_perplexity=18.658272, train_loss=2.9262896

Batch 125520, train_perplexity=18.658272, train_loss=2.9262896

Batch 125530, train_perplexity=18.658262, train_loss=2.926289

Batch 125540, train_perplexity=18.658272, train_loss=2.9262896

Batch 125550, train_perplexity=18.658272, train_loss=2.9262896

Batch 125560, train_perplexity=18.658272, train_loss=2.9262896

Batch 125570, train_perplexity=18.658268, train_loss=2.9262893

Batch 125580, train_perplexity=18.658272, train_loss=2.9262896

Batch 125590, train_perplexity=18.658272, train_loss=2.9262896

Batch 125600, train_perplexity=18.658268, train_loss=2.9262893

Batch 125610, train_perplexity=18.658268, train_loss=2.9262893

Batch 125620, train_perplexity=18.658272, train_loss=2.9262896

Batch 125630, train_perplexity=18.658268, train_loss=2.9262893

Batch 125640, train_perplexity=18.658268, train_loss=2.9262893

Batch 125650, train_perplexity=18.658268, train_loss=2.9262893

Batch 125660, train_perplexity=18.658268, train_loss=2.9262893

Batch 125670, train_perplexity=18.658268, train_loss=2.9262893

Batch 125680, train_perplexity=18.658262, train_loss=2.926289

Batch 125690, train_perplexity=18.658272, train_loss=2.9262896

Batch 125700, train_perplexity=18.658262, train_loss=2.926289

Batch 125710, train_perplexity=18.658262, train_loss=2.926289

Batch 125720, train_perplexity=18.658268, train_loss=2.9262893

Batch 125730, train_perplexity=18.658262, train_loss=2.926289

Batch 125740, train_perplexity=18.658262, train_loss=2.926289

Batch 125750, train_perplexity=18.658262, train_loss=2.926289

Batch 125760, train_perplexity=18.658262, train_loss=2.926289

Batch 125770, train_perplexity=18.658262, train_loss=2.926289

Batch 125780, train_perplexity=18.658262, train_loss=2.926289

Batch 125790, train_perplexity=18.658262, train_loss=2.926289

Batch 125800, train_perplexity=18.658262, train_loss=2.926289

Batch 125810, train_perplexity=18.658262, train_loss=2.926289

Batch 125820, train_perplexity=18.658262, train_loss=2.926289

Batch 125830, train_perplexity=18.658262, train_loss=2.926289

Batch 125840, train_perplexity=18.658262, train_loss=2.926289

Batch 125850, train_perplexity=18.658258, train_loss=2.9262888

Batch 125860, train_perplexity=18.658262, train_loss=2.926289

Batch 125870, train_perplexity=18.658262, train_loss=2.926289

Batch 125880, train_perplexity=18.658262, train_loss=2.926289

Batch 125890, train_perplexity=18.658262, train_loss=2.926289

Batch 125900, train_perplexity=18.658258, train_loss=2.9262888

Batch 125910, train_perplexity=18.658262, train_loss=2.926289

Batch 125920, train_perplexity=18.658262, train_loss=2.926289

Batch 125930, train_perplexity=18.658262, train_loss=2.926289

Batch 125940, train_perplexity=18.658255, train_loss=2.9262886

Batch 125950, train_perplexity=18.658262, train_loss=2.926289

Batch 125960, train_perplexity=18.658255, train_loss=2.9262886

Batch 125970, train_perplexity=18.658262, train_loss=2.926289

Batch 125980, train_perplexity=18.658255, train_loss=2.9262886

Batch 125990, train_perplexity=18.658258, train_loss=2.9262888

Batch 126000, train_perplexity=18.658255, train_loss=2.9262886

Batch 126010, train_perplexity=18.658255, train_loss=2.9262886

Batch 126020, train_perplexity=18.658255, train_loss=2.9262886

Batch 126030, train_perplexity=18.658262, train_loss=2.926289

Batch 126040, train_perplexity=18.658255, train_loss=2.9262886

Batch 126050, train_perplexity=18.658255, train_loss=2.9262886

Batch 126060, train_perplexity=18.658255, train_loss=2.9262886

Batch 126070, train_perplexity=18.658255, train_loss=2.9262886

Batch 126080, train_perplexity=18.658255, train_loss=2.9262886

Batch 126090, train_perplexity=18.658255, train_loss=2.9262886

Batch 126100, train_perplexity=18.658255, train_loss=2.9262886

Batch 126110, train_perplexity=18.658255, train_loss=2.9262886

Batch 126120, train_perplexity=18.658255, train_loss=2.9262886

Batch 126130, train_perplexity=18.658255, train_loss=2.9262886

Batch 126140, train_perplexity=18.658245, train_loss=2.9262881

Batch 126150, train_perplexity=18.658249, train_loss=2.9262884

Batch 126160, train_perplexity=18.658249, train_loss=2.9262884

Batch 126170, train_perplexity=18.658255, train_loss=2.9262886

Batch 126180, train_perplexity=18.658255, train_loss=2.9262886

Batch 126190, train_perplexity=18.658255, train_loss=2.9262886

Batch 126200, train_perplexity=18.658255, train_loss=2.9262886

Batch 126210, train_perplexity=18.658249, train_loss=2.9262884

Batch 126220, train_perplexity=18.658249, train_loss=2.9262884

Batch 126230, train_perplexity=18.658255, train_loss=2.9262886

Batch 126240, train_perplexity=18.658249, train_loss=2.9262884

Batch 126250, train_perplexity=18.658249, train_loss=2.9262884

Batch 126260, train_perplexity=18.658245, train_loss=2.9262881

Batch 126270, train_perplexity=18.658245, train_loss=2.9262881

Batch 126280, train_perplexity=18.658245, train_loss=2.9262881

Batch 126290, train_perplexity=18.658249, train_loss=2.9262884

Batch 126300, train_perplexity=18.658245, train_loss=2.9262881

Batch 126310, train_perplexity=18.658245, train_loss=2.9262881

Batch 126320, train_perplexity=18.658245, train_loss=2.9262881

Batch 126330, train_perplexity=18.658245, train_loss=2.9262881

Batch 126340, train_perplexity=18.658245, train_loss=2.9262881

Batch 126350, train_perplexity=18.658245, train_loss=2.9262881

Batch 126360, train_perplexity=18.658245, train_loss=2.9262881

Batch 126370, train_perplexity=18.658245, train_loss=2.9262881

Batch 126380, train_perplexity=18.658245, train_loss=2.9262881

Batch 126390, train_perplexity=18.658245, train_loss=2.9262881

Batch 126400, train_perplexity=18.658245, train_loss=2.9262881

Batch 126410, train_perplexity=18.658245, train_loss=2.9262881

Batch 126420, train_perplexity=18.658245, train_loss=2.9262881

Batch 126430, train_perplexity=18.658245, train_loss=2.9262881

Batch 126440, train_perplexity=18.658245, train_loss=2.9262881

Batch 126450, train_perplexity=18.658241, train_loss=2.926288

Batch 126460, train_perplexity=18.658241, train_loss=2.926288

Batch 126470, train_perplexity=18.658245, train_loss=2.9262881

Batch 126480, train_perplexity=18.658245, train_loss=2.9262881

Batch 126490, train_perplexity=18.658241, train_loss=2.926288

Batch 126500, train_perplexity=18.658241, train_loss=2.926288

Batch 126510, train_perplexity=18.658241, train_loss=2.926288

Batch 126520, train_perplexity=18.658236, train_loss=2.9262877

Batch 126530, train_perplexity=18.658236, train_loss=2.9262877

Batch 126540, train_perplexity=18.658245, train_loss=2.9262881

Batch 126550, train_perplexity=18.658241, train_loss=2.926288

Batch 126560, train_perplexity=18.658236, train_loss=2.9262877

Batch 126570, train_perplexity=18.658236, train_loss=2.9262877

Batch 126580, train_perplexity=18.658245, train_loss=2.9262881

Batch 126590, train_perplexity=18.658236, train_loss=2.9262877
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 126600, train_perplexity=18.658236, train_loss=2.9262877

Batch 126610, train_perplexity=18.658236, train_loss=2.9262877

Batch 126620, train_perplexity=18.658236, train_loss=2.9262877

Batch 126630, train_perplexity=18.658241, train_loss=2.926288

Batch 126640, train_perplexity=18.658236, train_loss=2.9262877

Batch 126650, train_perplexity=18.658241, train_loss=2.926288

Batch 126660, train_perplexity=18.658236, train_loss=2.9262877

Batch 126670, train_perplexity=18.658236, train_loss=2.9262877

Batch 126680, train_perplexity=18.658236, train_loss=2.9262877

Batch 126690, train_perplexity=18.658236, train_loss=2.9262877

Batch 126700, train_perplexity=18.658236, train_loss=2.9262877

Batch 126710, train_perplexity=18.658236, train_loss=2.9262877

Batch 126720, train_perplexity=18.658236, train_loss=2.9262877

Batch 126730, train_perplexity=18.658236, train_loss=2.9262877

Batch 126740, train_perplexity=18.658232, train_loss=2.9262874

Batch 126750, train_perplexity=18.658236, train_loss=2.9262877

Batch 126760, train_perplexity=18.658236, train_loss=2.9262877

Batch 126770, train_perplexity=18.658236, train_loss=2.9262877

Batch 126780, train_perplexity=18.658236, train_loss=2.9262877

Batch 126790, train_perplexity=18.658236, train_loss=2.9262877

Batch 126800, train_perplexity=18.658236, train_loss=2.9262877

Batch 126810, train_perplexity=18.658236, train_loss=2.9262877

Batch 126820, train_perplexity=18.658232, train_loss=2.9262874

Batch 126830, train_perplexity=18.658232, train_loss=2.9262874

Batch 126840, train_perplexity=18.658236, train_loss=2.9262877

Batch 126850, train_perplexity=18.658232, train_loss=2.9262874

Batch 126860, train_perplexity=18.658232, train_loss=2.9262874

Batch 126870, train_perplexity=18.658228, train_loss=2.9262872

Batch 126880, train_perplexity=18.658228, train_loss=2.9262872

Batch 126890, train_perplexity=18.658222, train_loss=2.926287

Batch 126900, train_perplexity=18.658228, train_loss=2.9262872

Batch 126910, train_perplexity=18.658228, train_loss=2.9262872

Batch 126920, train_perplexity=18.658228, train_loss=2.9262872

Batch 126930, train_perplexity=18.658232, train_loss=2.9262874

Batch 126940, train_perplexity=18.658228, train_loss=2.9262872

Batch 126950, train_perplexity=18.658228, train_loss=2.9262872

Batch 126960, train_perplexity=18.658228, train_loss=2.9262872

Batch 126970, train_perplexity=18.658228, train_loss=2.9262872

Batch 126980, train_perplexity=18.658228, train_loss=2.9262872

Batch 126990, train_perplexity=18.658228, train_loss=2.9262872

Batch 127000, train_perplexity=18.658228, train_loss=2.9262872

Batch 127010, train_perplexity=18.658228, train_loss=2.9262872

Batch 127020, train_perplexity=18.658228, train_loss=2.9262872

Batch 127030, train_perplexity=18.658228, train_loss=2.9262872

Batch 127040, train_perplexity=18.658222, train_loss=2.926287

Batch 127050, train_perplexity=18.658228, train_loss=2.9262872

Batch 127060, train_perplexity=18.658228, train_loss=2.9262872

Batch 127070, train_perplexity=18.658218, train_loss=2.9262867

Batch 127080, train_perplexity=18.658228, train_loss=2.9262872

Batch 127090, train_perplexity=18.658228, train_loss=2.9262872

Batch 127100, train_perplexity=18.658222, train_loss=2.926287

Batch 127110, train_perplexity=18.658228, train_loss=2.9262872

Batch 127120, train_perplexity=18.658222, train_loss=2.926287

Batch 127130, train_perplexity=18.658228, train_loss=2.9262872

Batch 127140, train_perplexity=18.658228, train_loss=2.9262872

Batch 127150, train_perplexity=18.658222, train_loss=2.926287

Batch 127160, train_perplexity=18.658228, train_loss=2.9262872

Batch 127170, train_perplexity=18.658222, train_loss=2.926287

Batch 127180, train_perplexity=18.658222, train_loss=2.926287

Batch 127190, train_perplexity=18.658222, train_loss=2.926287

Batch 127200, train_perplexity=18.658218, train_loss=2.9262867

Batch 127210, train_perplexity=18.658218, train_loss=2.9262867

Batch 127220, train_perplexity=18.714108, train_loss=2.9292777

Batch 127230, train_perplexity=18.67636, train_loss=2.9272585

Batch 127240, train_perplexity=18.666998, train_loss=2.926757

Batch 127250, train_perplexity=18.66318, train_loss=2.9265525

Batch 127260, train_perplexity=18.66127, train_loss=2.9264503

Batch 127270, train_perplexity=18.660192, train_loss=2.9263926

Batch 127280, train_perplexity=18.659554, train_loss=2.9263582

Batch 127290, train_perplexity=18.659143, train_loss=2.9263363

Batch 127300, train_perplexity=18.658873, train_loss=2.9263217

Batch 127310, train_perplexity=18.65869, train_loss=2.926312

Batch 127320, train_perplexity=18.65857, train_loss=2.9263055

Batch 127330, train_perplexity=18.658485, train_loss=2.926301

Batch 127340, train_perplexity=18.658422, train_loss=2.9262977

Batch 127350, train_perplexity=18.658379, train_loss=2.9262953

Batch 127360, train_perplexity=18.658342, train_loss=2.9262934

Batch 127370, train_perplexity=18.658316, train_loss=2.926292

Batch 127380, train_perplexity=18.658298, train_loss=2.926291

Batch 127390, train_perplexity=18.658289, train_loss=2.9262905

Batch 127400, train_perplexity=18.658276, train_loss=2.9262898

Batch 127410, train_perplexity=18.658262, train_loss=2.926289

Batch 127420, train_perplexity=18.658268, train_loss=2.9262893

Batch 127430, train_perplexity=18.658258, train_loss=2.9262888

Batch 127440, train_perplexity=18.658255, train_loss=2.9262886

Batch 127450, train_perplexity=18.658255, train_loss=2.9262886

Batch 127460, train_perplexity=18.658255, train_loss=2.9262886

Batch 127470, train_perplexity=18.658255, train_loss=2.9262886

Batch 127480, train_perplexity=18.658255, train_loss=2.9262886

Batch 127490, train_perplexity=18.658245, train_loss=2.9262881

Batch 127500, train_perplexity=18.658245, train_loss=2.9262881

Batch 127510, train_perplexity=18.658245, train_loss=2.9262881

Batch 127520, train_perplexity=18.658245, train_loss=2.9262881

Batch 127530, train_perplexity=18.658255, train_loss=2.9262886

Batch 127540, train_perplexity=18.658245, train_loss=2.9262881

Batch 127550, train_perplexity=18.658245, train_loss=2.9262881

Batch 127560, train_perplexity=18.658241, train_loss=2.926288

Batch 127570, train_perplexity=18.658245, train_loss=2.9262881

Batch 127580, train_perplexity=18.658245, train_loss=2.9262881

Batch 127590, train_perplexity=18.658245, train_loss=2.9262881

Batch 127600, train_perplexity=18.658241, train_loss=2.926288

Batch 127610, train_perplexity=18.658245, train_loss=2.9262881

Batch 127620, train_perplexity=18.658245, train_loss=2.9262881

Batch 127630, train_perplexity=18.658245, train_loss=2.9262881

Batch 127640, train_perplexity=18.658241, train_loss=2.926288

Batch 127650, train_perplexity=18.658236, train_loss=2.9262877

Batch 127660, train_perplexity=18.658245, train_loss=2.9262881

Batch 127670, train_perplexity=18.658241, train_loss=2.926288

Batch 127680, train_perplexity=18.658236, train_loss=2.9262877

Batch 127690, train_perplexity=18.658236, train_loss=2.9262877

Batch 127700, train_perplexity=18.658236, train_loss=2.9262877

Batch 127710, train_perplexity=18.658245, train_loss=2.9262881

Batch 127720, train_perplexity=18.658245, train_loss=2.9262881

Batch 127730, train_perplexity=18.658241, train_loss=2.926288

Batch 127740, train_perplexity=18.658236, train_loss=2.9262877

Batch 127750, train_perplexity=18.658241, train_loss=2.926288

Batch 127760, train_perplexity=18.658241, train_loss=2.926288

Batch 127770, train_perplexity=18.658236, train_loss=2.9262877

Batch 127780, train_perplexity=18.658236, train_loss=2.9262877

Batch 127790, train_perplexity=18.658236, train_loss=2.9262877

Batch 127800, train_perplexity=18.658232, train_loss=2.9262874

Batch 127810, train_perplexity=18.658236, train_loss=2.9262877

Batch 127820, train_perplexity=18.658241, train_loss=2.926288

Batch 127830, train_perplexity=18.658236, train_loss=2.9262877

Batch 127840, train_perplexity=18.658236, train_loss=2.9262877

Batch 127850, train_perplexity=18.658236, train_loss=2.9262877

Batch 127860, train_perplexity=18.658232, train_loss=2.9262874

Batch 127870, train_perplexity=18.658236, train_loss=2.9262877

Batch 127880, train_perplexity=18.658236, train_loss=2.9262877
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 127890, train_perplexity=18.658232, train_loss=2.9262874

Batch 127900, train_perplexity=18.658236, train_loss=2.9262877

Batch 127910, train_perplexity=18.658236, train_loss=2.9262877

Batch 127920, train_perplexity=18.658236, train_loss=2.9262877

Batch 127930, train_perplexity=18.658232, train_loss=2.9262874

Batch 127940, train_perplexity=18.658236, train_loss=2.9262877

Batch 127950, train_perplexity=18.658232, train_loss=2.9262874

Batch 127960, train_perplexity=18.658236, train_loss=2.9262877

Batch 127970, train_perplexity=18.658236, train_loss=2.9262877

Batch 127980, train_perplexity=18.658232, train_loss=2.9262874

Batch 127990, train_perplexity=18.658236, train_loss=2.9262877

Batch 128000, train_perplexity=18.658236, train_loss=2.9262877

Batch 128010, train_perplexity=18.658232, train_loss=2.9262874

Batch 128020, train_perplexity=18.658228, train_loss=2.9262872

Batch 128030, train_perplexity=18.658232, train_loss=2.9262874

Batch 128040, train_perplexity=18.658222, train_loss=2.926287

Batch 128050, train_perplexity=18.658228, train_loss=2.9262872

Batch 128060, train_perplexity=18.658228, train_loss=2.9262872

Batch 128070, train_perplexity=18.658228, train_loss=2.9262872

Batch 128080, train_perplexity=18.658232, train_loss=2.9262874

Batch 128090, train_perplexity=18.658236, train_loss=2.9262877

Batch 128100, train_perplexity=18.658228, train_loss=2.9262872

Batch 128110, train_perplexity=18.658222, train_loss=2.926287

Batch 128120, train_perplexity=18.658228, train_loss=2.9262872

Batch 128130, train_perplexity=18.658228, train_loss=2.9262872

Batch 128140, train_perplexity=18.658222, train_loss=2.926287

Batch 128150, train_perplexity=18.658228, train_loss=2.9262872

Batch 128160, train_perplexity=18.658228, train_loss=2.9262872

Batch 128170, train_perplexity=18.658228, train_loss=2.9262872

Batch 128180, train_perplexity=18.658228, train_loss=2.9262872

Batch 128190, train_perplexity=18.658228, train_loss=2.9262872

Batch 128200, train_perplexity=18.658222, train_loss=2.926287

Batch 128210, train_perplexity=18.658228, train_loss=2.9262872

Batch 128220, train_perplexity=18.658228, train_loss=2.9262872

Batch 128230, train_perplexity=18.658228, train_loss=2.9262872

Batch 128240, train_perplexity=18.658228, train_loss=2.9262872

Batch 128250, train_perplexity=18.658228, train_loss=2.9262872

Batch 128260, train_perplexity=18.658218, train_loss=2.9262867

Batch 128270, train_perplexity=18.658218, train_loss=2.9262867

Batch 128280, train_perplexity=18.658222, train_loss=2.926287

Batch 128290, train_perplexity=18.658218, train_loss=2.9262867

Batch 128300, train_perplexity=18.658218, train_loss=2.9262867

Batch 128310, train_perplexity=18.658228, train_loss=2.9262872

Batch 128320, train_perplexity=18.658228, train_loss=2.9262872

Batch 128330, train_perplexity=18.658228, train_loss=2.9262872

Batch 128340, train_perplexity=18.658218, train_loss=2.9262867

Batch 128350, train_perplexity=18.658218, train_loss=2.9262867

Batch 128360, train_perplexity=18.658218, train_loss=2.9262867

Batch 128370, train_perplexity=18.658218, train_loss=2.9262867

Batch 128380, train_perplexity=18.658218, train_loss=2.9262867

Batch 128390, train_perplexity=18.658222, train_loss=2.926287

Batch 128400, train_perplexity=18.658222, train_loss=2.926287

Batch 128410, train_perplexity=18.658218, train_loss=2.9262867

Batch 128420, train_perplexity=18.658218, train_loss=2.9262867

Batch 128430, train_perplexity=18.658222, train_loss=2.926287

Batch 128440, train_perplexity=18.658218, train_loss=2.9262867

Batch 128450, train_perplexity=18.658218, train_loss=2.9262867

Batch 128460, train_perplexity=18.658218, train_loss=2.9262867

Batch 128470, train_perplexity=18.658215, train_loss=2.9262865

Batch 128480, train_perplexity=18.658218, train_loss=2.9262867

Batch 128490, train_perplexity=18.658215, train_loss=2.9262865

Batch 128500, train_perplexity=18.658218, train_loss=2.9262867

Batch 128510, train_perplexity=18.658218, train_loss=2.9262867

Batch 128520, train_perplexity=18.658218, train_loss=2.9262867

Batch 128530, train_perplexity=18.658218, train_loss=2.9262867

Batch 128540, train_perplexity=18.658218, train_loss=2.9262867

Batch 128550, train_perplexity=18.658218, train_loss=2.9262867

Batch 128560, train_perplexity=18.658218, train_loss=2.9262867

Batch 128570, train_perplexity=18.658218, train_loss=2.9262867

Batch 128580, train_perplexity=18.658215, train_loss=2.9262865

Batch 128590, train_perplexity=18.658218, train_loss=2.9262867

Batch 128600, train_perplexity=18.658218, train_loss=2.9262867

Batch 128610, train_perplexity=18.658209, train_loss=2.9262862

Batch 128620, train_perplexity=18.658215, train_loss=2.9262865

Batch 128630, train_perplexity=18.658209, train_loss=2.9262862

Batch 128640, train_perplexity=18.658215, train_loss=2.9262865

Batch 128650, train_perplexity=18.658209, train_loss=2.9262862

Batch 128660, train_perplexity=18.658209, train_loss=2.9262862

Batch 128670, train_perplexity=18.658218, train_loss=2.9262867

Batch 128680, train_perplexity=18.658209, train_loss=2.9262862

Batch 128690, train_perplexity=18.658209, train_loss=2.9262862

Batch 128700, train_perplexity=18.658209, train_loss=2.9262862

Batch 128710, train_perplexity=18.658209, train_loss=2.9262862

Batch 128720, train_perplexity=18.658209, train_loss=2.9262862

Batch 128730, train_perplexity=18.658209, train_loss=2.9262862

Batch 128740, train_perplexity=18.658209, train_loss=2.9262862

Batch 128750, train_perplexity=18.658209, train_loss=2.9262862

Batch 128760, train_perplexity=18.658209, train_loss=2.9262862

Batch 128770, train_perplexity=18.658209, train_loss=2.9262862

Batch 128780, train_perplexity=18.658209, train_loss=2.9262862

Batch 128790, train_perplexity=18.658205, train_loss=2.926286

Batch 128800, train_perplexity=18.658205, train_loss=2.926286

Batch 128810, train_perplexity=18.658209, train_loss=2.9262862

Batch 128820, train_perplexity=18.658209, train_loss=2.9262862

Batch 128830, train_perplexity=18.658209, train_loss=2.9262862

Batch 128840, train_perplexity=18.658209, train_loss=2.9262862

Batch 128850, train_perplexity=18.658205, train_loss=2.926286

Batch 128860, train_perplexity=18.658205, train_loss=2.926286

Batch 128870, train_perplexity=18.658201, train_loss=2.9262857

Batch 128880, train_perplexity=18.658205, train_loss=2.926286

Batch 128890, train_perplexity=18.658201, train_loss=2.9262857

Batch 128900, train_perplexity=18.658209, train_loss=2.9262862

Batch 128910, train_perplexity=18.658209, train_loss=2.9262862

Batch 128920, train_perplexity=18.658201, train_loss=2.9262857

Batch 128930, train_perplexity=18.658209, train_loss=2.9262862

Batch 128940, train_perplexity=18.658205, train_loss=2.926286

Batch 128950, train_perplexity=18.658205, train_loss=2.926286

Batch 128960, train_perplexity=18.658201, train_loss=2.9262857

Batch 128970, train_perplexity=18.658201, train_loss=2.9262857

Batch 128980, train_perplexity=18.658201, train_loss=2.9262857

Batch 128990, train_perplexity=18.658201, train_loss=2.9262857

Batch 129000, train_perplexity=18.658201, train_loss=2.9262857

Batch 129010, train_perplexity=18.658201, train_loss=2.9262857

Batch 129020, train_perplexity=18.658201, train_loss=2.9262857

Batch 129030, train_perplexity=18.658201, train_loss=2.9262857

Batch 129040, train_perplexity=18.658205, train_loss=2.926286

Batch 129050, train_perplexity=18.658195, train_loss=2.9262855

Batch 129060, train_perplexity=18.658201, train_loss=2.9262857

Batch 129070, train_perplexity=18.658201, train_loss=2.9262857

Batch 129080, train_perplexity=18.658201, train_loss=2.9262857

Batch 129090, train_perplexity=18.658195, train_loss=2.9262855

Batch 129100, train_perplexity=18.658201, train_loss=2.9262857

Batch 129110, train_perplexity=18.658195, train_loss=2.9262855

Batch 129120, train_perplexity=18.658201, train_loss=2.9262857

Batch 129130, train_perplexity=18.658201, train_loss=2.9262857

Batch 129140, train_perplexity=18.658201, train_loss=2.9262857

Batch 129150, train_perplexity=18.658195, train_loss=2.9262855

Batch 129160, train_perplexity=18.658192, train_loss=2.9262853

Batch 129170, train_perplexity=18.658195, train_loss=2.9262855
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 129180, train_perplexity=18.658195, train_loss=2.9262855

Batch 129190, train_perplexity=18.658201, train_loss=2.9262857

Batch 129200, train_perplexity=18.658192, train_loss=2.9262853

Batch 129210, train_perplexity=18.658192, train_loss=2.9262853

Batch 129220, train_perplexity=18.658195, train_loss=2.9262855

Batch 129230, train_perplexity=18.658192, train_loss=2.9262853

Batch 129240, train_perplexity=18.658195, train_loss=2.9262855

Batch 129250, train_perplexity=18.658195, train_loss=2.9262855

Batch 129260, train_perplexity=18.658192, train_loss=2.9262853

Batch 129270, train_perplexity=18.658192, train_loss=2.9262853

Batch 129280, train_perplexity=18.658192, train_loss=2.9262853

Batch 129290, train_perplexity=18.658192, train_loss=2.9262853

Batch 129300, train_perplexity=18.658192, train_loss=2.9262853

Batch 129310, train_perplexity=18.658192, train_loss=2.9262853

Batch 129320, train_perplexity=18.658192, train_loss=2.9262853

Batch 129330, train_perplexity=18.658192, train_loss=2.9262853

Batch 129340, train_perplexity=18.658192, train_loss=2.9262853

Batch 129350, train_perplexity=18.658192, train_loss=2.9262853

Batch 129360, train_perplexity=18.658192, train_loss=2.9262853

Batch 129370, train_perplexity=18.658192, train_loss=2.9262853

Batch 129380, train_perplexity=18.658192, train_loss=2.9262853

Batch 129390, train_perplexity=18.658188, train_loss=2.926285

Batch 129400, train_perplexity=18.658192, train_loss=2.9262853

Batch 129410, train_perplexity=18.658192, train_loss=2.9262853

Batch 129420, train_perplexity=18.658192, train_loss=2.9262853

Batch 129430, train_perplexity=18.658192, train_loss=2.9262853

Batch 129440, train_perplexity=18.658192, train_loss=2.9262853

Batch 129450, train_perplexity=18.658192, train_loss=2.9262853

Batch 129460, train_perplexity=18.658182, train_loss=2.9262848

Batch 129470, train_perplexity=18.658188, train_loss=2.926285

Batch 129480, train_perplexity=18.658188, train_loss=2.926285

Batch 129490, train_perplexity=18.658188, train_loss=2.926285

Batch 129500, train_perplexity=18.658192, train_loss=2.9262853

Batch 129510, train_perplexity=18.658182, train_loss=2.9262848

Batch 129520, train_perplexity=18.658192, train_loss=2.9262853

Batch 129530, train_perplexity=18.658182, train_loss=2.9262848

Batch 129540, train_perplexity=18.658182, train_loss=2.9262848

Batch 129550, train_perplexity=18.658192, train_loss=2.9262853

Batch 129560, train_perplexity=18.658188, train_loss=2.926285

Batch 129570, train_perplexity=18.658182, train_loss=2.9262848

Batch 129580, train_perplexity=18.658182, train_loss=2.9262848

Batch 129590, train_perplexity=18.658182, train_loss=2.9262848

Batch 129600, train_perplexity=18.658192, train_loss=2.9262853

Batch 129610, train_perplexity=18.658188, train_loss=2.926285

Batch 129620, train_perplexity=18.658182, train_loss=2.9262848

Batch 129630, train_perplexity=18.658182, train_loss=2.9262848

Batch 129640, train_perplexity=18.658182, train_loss=2.9262848

Batch 129650, train_perplexity=18.658182, train_loss=2.9262848

Batch 129660, train_perplexity=18.658182, train_loss=2.9262848

Batch 129670, train_perplexity=18.658188, train_loss=2.926285

Batch 129680, train_perplexity=18.658182, train_loss=2.9262848

Batch 129690, train_perplexity=18.658182, train_loss=2.9262848

Batch 129700, train_perplexity=18.658182, train_loss=2.9262848

Batch 129710, train_perplexity=18.658182, train_loss=2.9262848

Batch 129720, train_perplexity=18.658182, train_loss=2.9262848

Batch 129730, train_perplexity=18.658175, train_loss=2.9262843

Batch 129740, train_perplexity=18.658178, train_loss=2.9262846

Batch 129750, train_perplexity=18.658182, train_loss=2.9262848

Batch 129760, train_perplexity=18.658178, train_loss=2.9262846

Batch 129770, train_perplexity=18.658178, train_loss=2.9262846

Batch 129780, train_perplexity=18.658175, train_loss=2.9262843

Batch 129790, train_perplexity=18.658182, train_loss=2.9262848

Batch 129800, train_perplexity=18.658182, train_loss=2.9262848

Batch 129810, train_perplexity=18.658182, train_loss=2.9262848

Batch 129820, train_perplexity=18.658175, train_loss=2.9262843

Batch 129830, train_perplexity=18.658175, train_loss=2.9262843

Batch 129840, train_perplexity=18.658178, train_loss=2.9262846

Batch 129850, train_perplexity=18.658175, train_loss=2.9262843

Batch 129860, train_perplexity=18.658178, train_loss=2.9262846

Batch 129870, train_perplexity=18.658175, train_loss=2.9262843

Batch 129880, train_perplexity=18.658175, train_loss=2.9262843

Batch 129890, train_perplexity=18.658175, train_loss=2.9262843

Batch 129900, train_perplexity=18.658175, train_loss=2.9262843

Batch 129910, train_perplexity=18.658175, train_loss=2.9262843

Batch 129920, train_perplexity=18.658169, train_loss=2.926284

Batch 129930, train_perplexity=18.658175, train_loss=2.9262843

Batch 129940, train_perplexity=18.658175, train_loss=2.9262843

Batch 129950, train_perplexity=18.658175, train_loss=2.9262843

Batch 129960, train_perplexity=18.658175, train_loss=2.9262843

Batch 129970, train_perplexity=18.658169, train_loss=2.926284

Batch 129980, train_perplexity=18.658175, train_loss=2.9262843

Batch 129990, train_perplexity=18.658175, train_loss=2.9262843

Batch 130000, train_perplexity=18.658175, train_loss=2.9262843

Batch 130010, train_perplexity=18.658169, train_loss=2.926284

Batch 130020, train_perplexity=18.658169, train_loss=2.926284

Batch 130030, train_perplexity=18.658175, train_loss=2.9262843

Batch 130040, train_perplexity=18.658175, train_loss=2.9262843

Batch 130050, train_perplexity=18.658175, train_loss=2.9262843

Batch 130060, train_perplexity=18.658169, train_loss=2.926284

Batch 130070, train_perplexity=18.658169, train_loss=2.926284

Batch 130080, train_perplexity=18.658175, train_loss=2.9262843

Batch 130090, train_perplexity=18.658175, train_loss=2.9262843

Batch 130100, train_perplexity=18.658169, train_loss=2.926284

Batch 130110, train_perplexity=18.658165, train_loss=2.9262838

Batch 130120, train_perplexity=18.658175, train_loss=2.9262843

Batch 130130, train_perplexity=18.658165, train_loss=2.9262838

Batch 130140, train_perplexity=18.658165, train_loss=2.9262838

Batch 130150, train_perplexity=18.658165, train_loss=2.9262838

Batch 130160, train_perplexity=18.658165, train_loss=2.9262838

Batch 130170, train_perplexity=18.658165, train_loss=2.9262838

Batch 130180, train_perplexity=18.658175, train_loss=2.9262843

Batch 130190, train_perplexity=18.658165, train_loss=2.9262838

Batch 130200, train_perplexity=18.658165, train_loss=2.9262838

Batch 130210, train_perplexity=18.658165, train_loss=2.9262838

Batch 130220, train_perplexity=18.658165, train_loss=2.9262838

Batch 130230, train_perplexity=18.658165, train_loss=2.9262838

Batch 130240, train_perplexity=18.658169, train_loss=2.926284

Batch 130250, train_perplexity=18.658165, train_loss=2.9262838

Batch 130260, train_perplexity=18.658165, train_loss=2.9262838

Batch 130270, train_perplexity=18.658165, train_loss=2.9262838

Batch 130280, train_perplexity=18.658161, train_loss=2.9262836

Batch 130290, train_perplexity=18.658161, train_loss=2.9262836

Batch 130300, train_perplexity=18.658161, train_loss=2.9262836

Batch 130310, train_perplexity=18.658165, train_loss=2.9262838

Batch 130320, train_perplexity=18.658165, train_loss=2.9262838

Batch 130330, train_perplexity=18.658161, train_loss=2.9262836

Batch 130340, train_perplexity=18.658161, train_loss=2.9262836

Batch 130350, train_perplexity=18.658161, train_loss=2.9262836

Batch 130360, train_perplexity=18.658161, train_loss=2.9262836

Batch 130370, train_perplexity=18.658161, train_loss=2.9262836

Batch 130380, train_perplexity=18.658161, train_loss=2.9262836

Batch 130390, train_perplexity=18.658161, train_loss=2.9262836

Batch 130400, train_perplexity=18.658155, train_loss=2.9262834

Batch 130410, train_perplexity=18.658155, train_loss=2.9262834

Batch 130420, train_perplexity=18.658161, train_loss=2.9262836

Batch 130430, train_perplexity=18.658161, train_loss=2.9262836

Batch 130440, train_perplexity=18.658155, train_loss=2.9262834

Batch 130450, train_perplexity=18.658165, train_loss=2.9262838

Batch 130460, train_perplexity=18.658155, train_loss=2.9262834
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 130470, train_perplexity=18.658155, train_loss=2.9262834

Batch 130480, train_perplexity=18.658155, train_loss=2.9262834

Batch 130490, train_perplexity=18.658161, train_loss=2.9262836

Batch 130500, train_perplexity=18.658155, train_loss=2.9262834

Batch 130510, train_perplexity=18.658155, train_loss=2.9262834

Batch 130520, train_perplexity=18.658155, train_loss=2.9262834

Batch 130530, train_perplexity=18.658155, train_loss=2.9262834

Batch 130540, train_perplexity=18.658155, train_loss=2.9262834

Batch 130550, train_perplexity=18.658155, train_loss=2.9262834

Batch 130560, train_perplexity=18.658155, train_loss=2.9262834

Batch 130570, train_perplexity=18.658155, train_loss=2.9262834

Batch 130580, train_perplexity=18.658155, train_loss=2.9262834

Batch 130590, train_perplexity=18.658155, train_loss=2.9262834

Batch 130600, train_perplexity=18.658155, train_loss=2.9262834

Batch 130610, train_perplexity=18.658155, train_loss=2.9262834

Batch 130620, train_perplexity=18.658152, train_loss=2.9262831

Batch 130630, train_perplexity=18.658152, train_loss=2.9262831

Batch 130640, train_perplexity=18.658155, train_loss=2.9262834

Batch 130650, train_perplexity=18.658155, train_loss=2.9262834

Batch 130660, train_perplexity=18.658152, train_loss=2.9262831

Batch 130670, train_perplexity=18.658155, train_loss=2.9262834

Batch 130680, train_perplexity=18.658152, train_loss=2.9262831

Batch 130690, train_perplexity=18.658148, train_loss=2.926283

Batch 130700, train_perplexity=18.658155, train_loss=2.9262834

Batch 130710, train_perplexity=18.658152, train_loss=2.9262831

Batch 130720, train_perplexity=18.658152, train_loss=2.9262831

Batch 130730, train_perplexity=18.658148, train_loss=2.926283

Batch 130740, train_perplexity=18.658152, train_loss=2.9262831

Batch 130750, train_perplexity=18.658148, train_loss=2.926283

Batch 130760, train_perplexity=18.658148, train_loss=2.926283

Batch 130770, train_perplexity=18.658148, train_loss=2.926283

Batch 130780, train_perplexity=18.658148, train_loss=2.926283

Batch 130790, train_perplexity=18.658155, train_loss=2.9262834

Batch 130800, train_perplexity=18.658152, train_loss=2.9262831

Batch 130810, train_perplexity=18.658148, train_loss=2.926283

Batch 130820, train_perplexity=18.658148, train_loss=2.926283

Batch 130830, train_perplexity=18.658148, train_loss=2.926283

Batch 130840, train_perplexity=18.658148, train_loss=2.926283

Batch 130850, train_perplexity=18.658148, train_loss=2.926283

Batch 130860, train_perplexity=18.658148, train_loss=2.926283

Batch 130870, train_perplexity=18.658142, train_loss=2.9262826

Batch 130880, train_perplexity=18.658142, train_loss=2.9262826

Batch 130890, train_perplexity=18.658148, train_loss=2.926283

Batch 130900, train_perplexity=18.658142, train_loss=2.9262826

Batch 130910, train_perplexity=18.658142, train_loss=2.9262826

Batch 130920, train_perplexity=18.658148, train_loss=2.926283

Batch 130930, train_perplexity=18.658148, train_loss=2.926283

Batch 130940, train_perplexity=18.658148, train_loss=2.926283

Batch 130950, train_perplexity=18.658142, train_loss=2.9262826

Batch 130960, train_perplexity=18.658138, train_loss=2.9262824

Batch 130970, train_perplexity=18.658142, train_loss=2.9262826

Batch 130980, train_perplexity=18.658142, train_loss=2.9262826

Batch 130990, train_perplexity=18.658148, train_loss=2.926283

Batch 131000, train_perplexity=18.658138, train_loss=2.9262824

Batch 131010, train_perplexity=18.658142, train_loss=2.9262826

Batch 131020, train_perplexity=18.658138, train_loss=2.9262824

Batch 131030, train_perplexity=18.658138, train_loss=2.9262824

Batch 131040, train_perplexity=18.658142, train_loss=2.9262826

Batch 131050, train_perplexity=18.658148, train_loss=2.926283

Batch 131060, train_perplexity=18.658138, train_loss=2.9262824

Batch 131070, train_perplexity=18.658138, train_loss=2.9262824

Batch 131080, train_perplexity=18.658138, train_loss=2.9262824

Batch 131090, train_perplexity=18.658138, train_loss=2.9262824

Batch 131100, train_perplexity=18.658138, train_loss=2.9262824

Batch 131110, train_perplexity=18.658142, train_loss=2.9262826

Batch 131120, train_perplexity=18.658138, train_loss=2.9262824

Batch 131130, train_perplexity=18.658138, train_loss=2.9262824

Batch 131140, train_perplexity=18.658138, train_loss=2.9262824

Batch 131150, train_perplexity=18.658138, train_loss=2.9262824

Batch 131160, train_perplexity=18.658134, train_loss=2.9262822

Batch 131170, train_perplexity=18.658138, train_loss=2.9262824

Batch 131180, train_perplexity=18.658138, train_loss=2.9262824

Batch 131190, train_perplexity=18.658138, train_loss=2.9262824

Batch 131200, train_perplexity=18.658138, train_loss=2.9262824

Batch 131210, train_perplexity=18.658138, train_loss=2.9262824

Batch 131220, train_perplexity=18.658138, train_loss=2.9262824

Batch 131230, train_perplexity=18.658134, train_loss=2.9262822

Batch 131240, train_perplexity=18.658138, train_loss=2.9262824

Batch 131250, train_perplexity=18.658138, train_loss=2.9262824

Batch 131260, train_perplexity=18.658138, train_loss=2.9262824

Batch 131270, train_perplexity=18.658134, train_loss=2.9262822

Batch 131280, train_perplexity=18.658138, train_loss=2.9262824

Batch 131290, train_perplexity=18.658138, train_loss=2.9262824

Batch 131300, train_perplexity=18.658129, train_loss=2.926282

Batch 131310, train_perplexity=18.658129, train_loss=2.926282

Batch 131320, train_perplexity=18.658134, train_loss=2.9262822

Batch 131330, train_perplexity=18.658129, train_loss=2.926282

Batch 131340, train_perplexity=18.658134, train_loss=2.9262822

Batch 131350, train_perplexity=18.658129, train_loss=2.926282

Batch 131360, train_perplexity=18.658134, train_loss=2.9262822

Batch 131370, train_perplexity=18.658134, train_loss=2.9262822

Batch 131380, train_perplexity=18.658129, train_loss=2.926282

Batch 131390, train_perplexity=18.658129, train_loss=2.926282

Batch 131400, train_perplexity=18.658129, train_loss=2.926282

Batch 131410, train_perplexity=18.658134, train_loss=2.9262822

Batch 131420, train_perplexity=18.658129, train_loss=2.926282

Batch 131430, train_perplexity=18.658129, train_loss=2.926282

Batch 131440, train_perplexity=18.658129, train_loss=2.926282

Batch 131450, train_perplexity=18.658134, train_loss=2.9262822

Batch 131460, train_perplexity=18.658129, train_loss=2.926282

Batch 131470, train_perplexity=18.658125, train_loss=2.9262817

Batch 131480, train_perplexity=18.658125, train_loss=2.9262817

Batch 131490, train_perplexity=18.658129, train_loss=2.926282

Batch 131500, train_perplexity=18.658125, train_loss=2.9262817

Batch 131510, train_perplexity=18.658129, train_loss=2.926282

Batch 131520, train_perplexity=18.658129, train_loss=2.926282

Batch 131530, train_perplexity=18.658125, train_loss=2.9262817

Batch 131540, train_perplexity=18.658129, train_loss=2.926282

Batch 131550, train_perplexity=18.658129, train_loss=2.926282

Batch 131560, train_perplexity=18.658129, train_loss=2.926282

Batch 131570, train_perplexity=18.658125, train_loss=2.9262817

Batch 131580, train_perplexity=18.658129, train_loss=2.926282

Batch 131590, train_perplexity=18.658125, train_loss=2.9262817

Batch 131600, train_perplexity=18.658125, train_loss=2.9262817

Batch 131610, train_perplexity=18.658125, train_loss=2.9262817

Batch 131620, train_perplexity=18.658121, train_loss=2.9262815

Batch 131630, train_perplexity=18.658121, train_loss=2.9262815

Batch 131640, train_perplexity=18.658125, train_loss=2.9262817

Batch 131650, train_perplexity=18.658121, train_loss=2.9262815

Batch 131660, train_perplexity=18.658121, train_loss=2.9262815

Batch 131670, train_perplexity=18.658125, train_loss=2.9262817

Batch 131680, train_perplexity=18.658121, train_loss=2.9262815

Batch 131690, train_perplexity=18.658125, train_loss=2.9262817

Batch 131700, train_perplexity=18.658121, train_loss=2.9262815

Batch 131710, train_perplexity=18.658129, train_loss=2.926282

Batch 131720, train_perplexity=18.658121, train_loss=2.9262815

Batch 131730, train_perplexity=18.658121, train_loss=2.9262815

Batch 131740, train_perplexity=18.658121, train_loss=2.9262815

Batch 131750, train_perplexity=18.658121, train_loss=2.9262815
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 131760, train_perplexity=18.658121, train_loss=2.9262815

Batch 131770, train_perplexity=18.658121, train_loss=2.9262815

Batch 131780, train_perplexity=18.658121, train_loss=2.9262815

Batch 131790, train_perplexity=18.658121, train_loss=2.9262815

Batch 131800, train_perplexity=18.658121, train_loss=2.9262815

Batch 131810, train_perplexity=18.658115, train_loss=2.9262812

Batch 131820, train_perplexity=18.658121, train_loss=2.9262815

Batch 131830, train_perplexity=18.658115, train_loss=2.9262812

Batch 131840, train_perplexity=18.658121, train_loss=2.9262815

Batch 131850, train_perplexity=18.658115, train_loss=2.9262812

Batch 131860, train_perplexity=18.658121, train_loss=2.9262815

Batch 131870, train_perplexity=18.658121, train_loss=2.9262815

Batch 131880, train_perplexity=18.658115, train_loss=2.9262812

Batch 131890, train_perplexity=18.658121, train_loss=2.9262815

Batch 131900, train_perplexity=18.658115, train_loss=2.9262812

Batch 131910, train_perplexity=18.658121, train_loss=2.9262815

Batch 131920, train_perplexity=18.658115, train_loss=2.9262812

Batch 131930, train_perplexity=18.658115, train_loss=2.9262812

Batch 131940, train_perplexity=18.658115, train_loss=2.9262812

Batch 131950, train_perplexity=18.658112, train_loss=2.926281

Batch 131960, train_perplexity=18.658112, train_loss=2.926281

Batch 131970, train_perplexity=18.658115, train_loss=2.9262812

Batch 131980, train_perplexity=18.658115, train_loss=2.9262812

Batch 131990, train_perplexity=18.658115, train_loss=2.9262812

Batch 132000, train_perplexity=18.658112, train_loss=2.926281

Batch 132010, train_perplexity=18.658112, train_loss=2.926281

Batch 132020, train_perplexity=18.658112, train_loss=2.926281

Batch 132030, train_perplexity=18.658112, train_loss=2.926281

Batch 132040, train_perplexity=18.658112, train_loss=2.926281

Batch 132050, train_perplexity=18.658112, train_loss=2.926281

Batch 132060, train_perplexity=18.658112, train_loss=2.926281

Batch 132070, train_perplexity=18.658115, train_loss=2.9262812

Batch 132080, train_perplexity=18.658108, train_loss=2.9262807

Batch 132090, train_perplexity=18.658108, train_loss=2.9262807

Batch 132100, train_perplexity=18.658112, train_loss=2.926281

Batch 132110, train_perplexity=18.658112, train_loss=2.926281

Batch 132120, train_perplexity=18.658112, train_loss=2.926281

Batch 132130, train_perplexity=18.658112, train_loss=2.926281

Batch 132140, train_perplexity=18.658112, train_loss=2.926281

Batch 132150, train_perplexity=18.658112, train_loss=2.926281

Batch 132160, train_perplexity=18.658108, train_loss=2.9262807

Batch 132170, train_perplexity=18.658108, train_loss=2.9262807

Batch 132180, train_perplexity=18.658112, train_loss=2.926281

Batch 132190, train_perplexity=18.658108, train_loss=2.9262807

Batch 132200, train_perplexity=18.658112, train_loss=2.926281

Batch 132210, train_perplexity=18.658108, train_loss=2.9262807

Batch 132220, train_perplexity=18.658108, train_loss=2.9262807

Batch 132230, train_perplexity=18.658108, train_loss=2.9262807

Batch 132240, train_perplexity=18.658108, train_loss=2.9262807

Batch 132250, train_perplexity=18.658108, train_loss=2.9262807

Batch 132260, train_perplexity=18.658108, train_loss=2.9262807

Batch 132270, train_perplexity=18.658102, train_loss=2.9262805

Batch 132280, train_perplexity=18.658102, train_loss=2.9262805

Batch 132290, train_perplexity=18.658102, train_loss=2.9262805

Batch 132300, train_perplexity=18.658102, train_loss=2.9262805

Batch 132310, train_perplexity=18.658102, train_loss=2.9262805

Batch 132320, train_perplexity=18.658108, train_loss=2.9262807

Batch 132330, train_perplexity=18.658102, train_loss=2.9262805

Batch 132340, train_perplexity=18.658094, train_loss=2.92628

Batch 132350, train_perplexity=18.658108, train_loss=2.9262807

Batch 132360, train_perplexity=18.658102, train_loss=2.9262805

Batch 132370, train_perplexity=18.658102, train_loss=2.9262805

Batch 132380, train_perplexity=18.658102, train_loss=2.9262805

Batch 132390, train_perplexity=18.658102, train_loss=2.9262805

Batch 132400, train_perplexity=18.658102, train_loss=2.9262805

Batch 132410, train_perplexity=18.658102, train_loss=2.9262805

Batch 132420, train_perplexity=18.658102, train_loss=2.9262805

Batch 132430, train_perplexity=18.658098, train_loss=2.9262803

Batch 132440, train_perplexity=18.658102, train_loss=2.9262805

Batch 132450, train_perplexity=18.658102, train_loss=2.9262805

Batch 132460, train_perplexity=18.658102, train_loss=2.9262805

Batch 132470, train_perplexity=18.658098, train_loss=2.9262803

Batch 132480, train_perplexity=18.658094, train_loss=2.92628

Batch 132490, train_perplexity=18.658094, train_loss=2.92628

Batch 132500, train_perplexity=18.658102, train_loss=2.9262805

Batch 132510, train_perplexity=18.658098, train_loss=2.9262803

Batch 132520, train_perplexity=18.658098, train_loss=2.9262803

Batch 132530, train_perplexity=18.658094, train_loss=2.92628

Batch 132540, train_perplexity=18.658098, train_loss=2.9262803

Batch 132550, train_perplexity=18.658098, train_loss=2.9262803

Batch 132560, train_perplexity=18.658102, train_loss=2.9262805

Batch 132570, train_perplexity=18.658094, train_loss=2.92628

Batch 132580, train_perplexity=18.658094, train_loss=2.92628

Batch 132590, train_perplexity=18.658094, train_loss=2.92628

Batch 132600, train_perplexity=18.658098, train_loss=2.9262803

Batch 132610, train_perplexity=18.658094, train_loss=2.92628

Batch 132620, train_perplexity=18.658094, train_loss=2.92628

Batch 132630, train_perplexity=18.658094, train_loss=2.92628

Batch 132640, train_perplexity=18.658094, train_loss=2.92628

Batch 132650, train_perplexity=18.658094, train_loss=2.92628

Batch 132660, train_perplexity=18.658094, train_loss=2.92628

Batch 132670, train_perplexity=18.658094, train_loss=2.92628

Batch 132680, train_perplexity=18.658094, train_loss=2.92628

Batch 132690, train_perplexity=18.658094, train_loss=2.92628

Batch 132700, train_perplexity=18.658094, train_loss=2.92628

Batch 132710, train_perplexity=18.658089, train_loss=2.9262798

Batch 132720, train_perplexity=18.658085, train_loss=2.9262795

Batch 132730, train_perplexity=18.658094, train_loss=2.92628

Batch 132740, train_perplexity=18.658094, train_loss=2.92628

Batch 132750, train_perplexity=18.658094, train_loss=2.92628

Batch 132760, train_perplexity=18.658094, train_loss=2.92628

Batch 132770, train_perplexity=18.658085, train_loss=2.9262795

Batch 132780, train_perplexity=18.658094, train_loss=2.92628

Batch 132790, train_perplexity=18.658089, train_loss=2.9262798

Batch 132800, train_perplexity=18.658089, train_loss=2.9262798

Batch 132810, train_perplexity=18.658085, train_loss=2.9262795

Batch 132820, train_perplexity=18.658094, train_loss=2.92628

Batch 132830, train_perplexity=18.658085, train_loss=2.9262795

Batch 132840, train_perplexity=18.658085, train_loss=2.9262795

Batch 132850, train_perplexity=18.658085, train_loss=2.9262795

Batch 132860, train_perplexity=18.658085, train_loss=2.9262795

Batch 132870, train_perplexity=18.658089, train_loss=2.9262798

Batch 132880, train_perplexity=18.658085, train_loss=2.9262795

Batch 132890, train_perplexity=18.658085, train_loss=2.9262795

Batch 132900, train_perplexity=18.658085, train_loss=2.9262795

Batch 132910, train_perplexity=18.658085, train_loss=2.9262795

Batch 132920, train_perplexity=18.658085, train_loss=2.9262795

Batch 132930, train_perplexity=18.658089, train_loss=2.9262798

Batch 132940, train_perplexity=18.658089, train_loss=2.9262798

Batch 132950, train_perplexity=18.658085, train_loss=2.9262795

Batch 132960, train_perplexity=18.658081, train_loss=2.9262793

Batch 132970, train_perplexity=18.658085, train_loss=2.9262795

Batch 132980, train_perplexity=18.658085, train_loss=2.9262795

Batch 132990, train_perplexity=18.658085, train_loss=2.9262795

Batch 133000, train_perplexity=18.658085, train_loss=2.9262795

Batch 133010, train_perplexity=18.658081, train_loss=2.9262793

Batch 133020, train_perplexity=18.658085, train_loss=2.9262795

Batch 133030, train_perplexity=18.658085, train_loss=2.9262795

Batch 133040, train_perplexity=18.658085, train_loss=2.9262795
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 133050, train_perplexity=18.658085, train_loss=2.9262795

Batch 133060, train_perplexity=18.658081, train_loss=2.9262793

Batch 133070, train_perplexity=18.658085, train_loss=2.9262795

Batch 133080, train_perplexity=18.658081, train_loss=2.9262793

Batch 133090, train_perplexity=18.658075, train_loss=2.926279

Batch 133100, train_perplexity=18.658085, train_loss=2.9262795

Batch 133110, train_perplexity=18.658085, train_loss=2.9262795

Batch 133120, train_perplexity=18.658081, train_loss=2.9262793

Batch 133130, train_perplexity=18.658075, train_loss=2.926279

Batch 133140, train_perplexity=18.658081, train_loss=2.9262793

Batch 133150, train_perplexity=18.658075, train_loss=2.926279

Batch 133160, train_perplexity=18.658075, train_loss=2.926279

Batch 133170, train_perplexity=18.658081, train_loss=2.9262793

Batch 133180, train_perplexity=18.658081, train_loss=2.9262793

Batch 133190, train_perplexity=18.658075, train_loss=2.926279

Batch 133200, train_perplexity=18.658081, train_loss=2.9262793

Batch 133210, train_perplexity=18.658081, train_loss=2.9262793

Batch 133220, train_perplexity=18.658075, train_loss=2.926279

Batch 133230, train_perplexity=18.658075, train_loss=2.926279

Batch 133240, train_perplexity=18.658081, train_loss=2.9262793

Batch 133250, train_perplexity=18.658075, train_loss=2.926279

Batch 133260, train_perplexity=18.658085, train_loss=2.9262795

Batch 133270, train_perplexity=18.658075, train_loss=2.926279

Batch 133280, train_perplexity=18.658075, train_loss=2.926279

Batch 133290, train_perplexity=18.658075, train_loss=2.926279

Batch 133300, train_perplexity=18.658075, train_loss=2.926279

Batch 133310, train_perplexity=18.658075, train_loss=2.926279

Batch 133320, train_perplexity=18.658072, train_loss=2.9262788

Batch 133330, train_perplexity=18.658075, train_loss=2.926279

Batch 133340, train_perplexity=18.658068, train_loss=2.9262786

Batch 133350, train_perplexity=18.658072, train_loss=2.9262788

Batch 133360, train_perplexity=18.658075, train_loss=2.926279

Batch 133370, train_perplexity=18.658075, train_loss=2.926279

Batch 133380, train_perplexity=18.658075, train_loss=2.926279

Batch 133390, train_perplexity=18.658072, train_loss=2.9262788

Batch 133400, train_perplexity=18.658075, train_loss=2.926279

Batch 133410, train_perplexity=18.658072, train_loss=2.9262788

Batch 133420, train_perplexity=18.658075, train_loss=2.926279

Batch 133430, train_perplexity=18.658072, train_loss=2.9262788

Batch 133440, train_perplexity=18.658068, train_loss=2.9262786

Batch 133450, train_perplexity=18.658072, train_loss=2.9262788

Batch 133460, train_perplexity=18.658068, train_loss=2.9262786

Batch 133470, train_perplexity=18.658072, train_loss=2.9262788

Batch 133480, train_perplexity=18.658072, train_loss=2.9262788

Batch 133490, train_perplexity=18.658068, train_loss=2.9262786

Batch 133500, train_perplexity=18.658068, train_loss=2.9262786

Batch 133510, train_perplexity=18.658072, train_loss=2.9262788

Batch 133520, train_perplexity=18.658068, train_loss=2.9262786

Batch 133530, train_perplexity=18.658068, train_loss=2.9262786

Batch 133540, train_perplexity=18.658068, train_loss=2.9262786

Batch 133550, train_perplexity=18.658068, train_loss=2.9262786

Batch 133560, train_perplexity=18.658062, train_loss=2.9262784

Batch 133570, train_perplexity=18.658068, train_loss=2.9262786

Batch 133580, train_perplexity=18.658068, train_loss=2.9262786

Batch 133590, train_perplexity=18.658062, train_loss=2.9262784

Batch 133600, train_perplexity=18.658068, train_loss=2.9262786

Batch 133610, train_perplexity=18.658068, train_loss=2.9262786

Batch 133620, train_perplexity=18.658068, train_loss=2.9262786

Batch 133630, train_perplexity=18.658068, train_loss=2.9262786

Batch 133640, train_perplexity=18.658068, train_loss=2.9262786

Batch 133650, train_perplexity=18.658062, train_loss=2.9262784

Batch 133660, train_perplexity=18.658068, train_loss=2.9262786

Batch 133670, train_perplexity=18.658058, train_loss=2.926278

Batch 133680, train_perplexity=18.658058, train_loss=2.926278

Batch 133690, train_perplexity=18.658068, train_loss=2.9262786

Batch 133700, train_perplexity=18.658062, train_loss=2.9262784

Batch 133710, train_perplexity=18.658068, train_loss=2.9262786

Batch 133720, train_perplexity=18.658068, train_loss=2.9262786

Batch 133730, train_perplexity=18.658058, train_loss=2.926278

Batch 133740, train_perplexity=18.658068, train_loss=2.9262786

Batch 133750, train_perplexity=18.658062, train_loss=2.9262784

Batch 133760, train_perplexity=18.658058, train_loss=2.926278

Batch 133770, train_perplexity=18.658058, train_loss=2.926278

Batch 133780, train_perplexity=18.658058, train_loss=2.926278

Batch 133790, train_perplexity=18.658062, train_loss=2.9262784

Batch 133800, train_perplexity=18.658062, train_loss=2.9262784

Batch 133810, train_perplexity=18.658058, train_loss=2.926278

Batch 133820, train_perplexity=18.658062, train_loss=2.9262784

Batch 133830, train_perplexity=18.658058, train_loss=2.926278

Batch 133840, train_perplexity=18.658058, train_loss=2.926278

Batch 133850, train_perplexity=18.658058, train_loss=2.926278

Batch 133860, train_perplexity=18.658058, train_loss=2.926278

Batch 133870, train_perplexity=18.658054, train_loss=2.9262779

Batch 133880, train_perplexity=18.658058, train_loss=2.926278

Batch 133890, train_perplexity=18.658058, train_loss=2.926278

Batch 133900, train_perplexity=18.658058, train_loss=2.926278

Batch 133910, train_perplexity=18.658054, train_loss=2.9262779

Batch 133920, train_perplexity=18.658058, train_loss=2.926278

Batch 133930, train_perplexity=18.658058, train_loss=2.926278

Batch 133940, train_perplexity=18.658058, train_loss=2.926278

Batch 133950, train_perplexity=18.658054, train_loss=2.9262779

Batch 133960, train_perplexity=18.658058, train_loss=2.926278

Batch 133970, train_perplexity=18.658058, train_loss=2.926278

Batch 133980, train_perplexity=18.658054, train_loss=2.9262779

Batch 133990, train_perplexity=18.658058, train_loss=2.926278

Batch 134000, train_perplexity=18.658058, train_loss=2.926278

Batch 134010, train_perplexity=18.658049, train_loss=2.9262776

Batch 134020, train_perplexity=18.658049, train_loss=2.9262776

Batch 134030, train_perplexity=18.658049, train_loss=2.9262776

Batch 134040, train_perplexity=18.658049, train_loss=2.9262776

Batch 134050, train_perplexity=18.658054, train_loss=2.9262779

Batch 134060, train_perplexity=18.658054, train_loss=2.9262779

Batch 134070, train_perplexity=18.658049, train_loss=2.9262776

Batch 134080, train_perplexity=18.658049, train_loss=2.9262776

Batch 134090, train_perplexity=18.658049, train_loss=2.9262776

Batch 134100, train_perplexity=18.658049, train_loss=2.9262776

Batch 134110, train_perplexity=18.658054, train_loss=2.9262779

Batch 134120, train_perplexity=18.658049, train_loss=2.9262776

Batch 134130, train_perplexity=18.658049, train_loss=2.9262776

Batch 134140, train_perplexity=18.658049, train_loss=2.9262776

Batch 134150, train_perplexity=18.658049, train_loss=2.9262776

Batch 134160, train_perplexity=18.658049, train_loss=2.9262776

Batch 134170, train_perplexity=18.658049, train_loss=2.9262776

Batch 134180, train_perplexity=18.658049, train_loss=2.9262776

Batch 134190, train_perplexity=18.658049, train_loss=2.9262776

Batch 134200, train_perplexity=18.658045, train_loss=2.9262774

Batch 134210, train_perplexity=18.658049, train_loss=2.9262776

Batch 134220, train_perplexity=18.658049, train_loss=2.9262776

Batch 134230, train_perplexity=18.658049, train_loss=2.9262776

Batch 134240, train_perplexity=18.658049, train_loss=2.9262776

Batch 134250, train_perplexity=18.658049, train_loss=2.9262776

Batch 134260, train_perplexity=18.658049, train_loss=2.9262776

Batch 134270, train_perplexity=18.658041, train_loss=2.9262772

Batch 134280, train_perplexity=18.658045, train_loss=2.9262774

Batch 134290, train_perplexity=18.658049, train_loss=2.9262776

Batch 134300, train_perplexity=18.658041, train_loss=2.9262772

Batch 134310, train_perplexity=18.658041, train_loss=2.9262772

Batch 134320, train_perplexity=18.658041, train_loss=2.9262772

Batch 134330, train_perplexity=18.658045, train_loss=2.9262774
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 134340, train_perplexity=18.658041, train_loss=2.9262772

Batch 134350, train_perplexity=18.658041, train_loss=2.9262772

Batch 134360, train_perplexity=18.658041, train_loss=2.9262772

Batch 134370, train_perplexity=18.658041, train_loss=2.9262772

Batch 134380, train_perplexity=18.658041, train_loss=2.9262772

Batch 134390, train_perplexity=18.658041, train_loss=2.9262772

Batch 134400, train_perplexity=18.658041, train_loss=2.9262772

Batch 134410, train_perplexity=18.658041, train_loss=2.9262772

Batch 134420, train_perplexity=18.658041, train_loss=2.9262772

Batch 134430, train_perplexity=18.658041, train_loss=2.9262772

Batch 134440, train_perplexity=18.658035, train_loss=2.926277

Batch 134450, train_perplexity=18.658041, train_loss=2.9262772

Batch 134460, train_perplexity=18.658041, train_loss=2.9262772

Batch 134470, train_perplexity=18.658041, train_loss=2.9262772

Batch 134480, train_perplexity=18.658035, train_loss=2.926277

Batch 134490, train_perplexity=18.658035, train_loss=2.926277

Batch 134500, train_perplexity=18.658041, train_loss=2.9262772

Batch 134510, train_perplexity=18.658035, train_loss=2.926277

Batch 134520, train_perplexity=18.658031, train_loss=2.9262767

Batch 134530, train_perplexity=18.658041, train_loss=2.9262772

Batch 134540, train_perplexity=18.658041, train_loss=2.9262772

Batch 134550, train_perplexity=18.658041, train_loss=2.9262772

Batch 134560, train_perplexity=18.658035, train_loss=2.926277

Batch 134570, train_perplexity=18.658041, train_loss=2.9262772

Batch 134580, train_perplexity=18.658031, train_loss=2.9262767

Batch 134590, train_perplexity=18.658041, train_loss=2.9262772

Batch 134600, train_perplexity=18.658031, train_loss=2.9262767

Batch 134610, train_perplexity=18.658031, train_loss=2.9262767

Batch 134620, train_perplexity=18.658041, train_loss=2.9262772

Batch 134630, train_perplexity=18.658031, train_loss=2.9262767

Batch 134640, train_perplexity=18.658031, train_loss=2.9262767

Batch 134650, train_perplexity=18.658031, train_loss=2.9262767

Batch 134660, train_perplexity=18.658031, train_loss=2.9262767

Batch 134670, train_perplexity=18.658031, train_loss=2.9262767

Batch 134680, train_perplexity=18.658031, train_loss=2.9262767

Batch 134690, train_perplexity=18.658031, train_loss=2.9262767

Batch 134700, train_perplexity=18.658031, train_loss=2.9262767

Batch 134710, train_perplexity=18.658031, train_loss=2.9262767

Batch 134720, train_perplexity=18.658031, train_loss=2.9262767

Batch 134730, train_perplexity=18.658031, train_loss=2.9262767

Batch 134740, train_perplexity=18.658031, train_loss=2.9262767

Batch 134750, train_perplexity=18.658031, train_loss=2.9262767

Batch 134760, train_perplexity=18.658031, train_loss=2.9262767

Batch 134770, train_perplexity=18.658031, train_loss=2.9262767

Batch 134780, train_perplexity=18.658031, train_loss=2.9262767

Batch 134790, train_perplexity=18.658022, train_loss=2.9262762

Batch 134800, train_perplexity=18.658031, train_loss=2.9262767

Batch 134810, train_perplexity=18.658031, train_loss=2.9262767

Batch 134820, train_perplexity=18.658028, train_loss=2.9262764

Batch 134830, train_perplexity=18.658022, train_loss=2.9262762

Batch 134840, train_perplexity=18.658031, train_loss=2.9262767

Batch 134850, train_perplexity=18.658031, train_loss=2.9262767

Batch 134860, train_perplexity=18.658028, train_loss=2.9262764

Batch 134870, train_perplexity=18.658028, train_loss=2.9262764

Batch 134880, train_perplexity=18.658028, train_loss=2.9262764

Batch 134890, train_perplexity=18.658022, train_loss=2.9262762

Batch 134900, train_perplexity=18.658022, train_loss=2.9262762

Batch 134910, train_perplexity=18.658022, train_loss=2.9262762

Batch 134920, train_perplexity=18.658022, train_loss=2.9262762

Batch 134930, train_perplexity=18.658022, train_loss=2.9262762

Batch 134940, train_perplexity=18.658022, train_loss=2.9262762

Batch 134950, train_perplexity=18.658022, train_loss=2.9262762

Batch 134960, train_perplexity=18.658022, train_loss=2.9262762

Batch 134970, train_perplexity=18.658022, train_loss=2.9262762

Batch 134980, train_perplexity=18.658022, train_loss=2.9262762

Batch 134990, train_perplexity=18.658022, train_loss=2.9262762

Batch 135000, train_perplexity=18.658022, train_loss=2.9262762

Batch 135010, train_perplexity=18.658022, train_loss=2.9262762

Batch 135020, train_perplexity=18.658022, train_loss=2.9262762

Batch 135030, train_perplexity=18.658022, train_loss=2.9262762

Batch 135040, train_perplexity=18.658022, train_loss=2.9262762

Batch 135050, train_perplexity=18.658022, train_loss=2.9262762

Batch 135060, train_perplexity=18.658022, train_loss=2.9262762

Batch 135070, train_perplexity=18.658022, train_loss=2.9262762

Batch 135080, train_perplexity=18.658022, train_loss=2.9262762

Batch 135090, train_perplexity=18.658018, train_loss=2.926276

Batch 135100, train_perplexity=18.658022, train_loss=2.9262762

Batch 135110, train_perplexity=18.658022, train_loss=2.9262762

Batch 135120, train_perplexity=18.658018, train_loss=2.926276

Batch 135130, train_perplexity=18.658022, train_loss=2.9262762

Batch 135140, train_perplexity=18.658022, train_loss=2.9262762

Batch 135150, train_perplexity=18.658022, train_loss=2.9262762

Batch 135160, train_perplexity=18.658018, train_loss=2.926276

Batch 135170, train_perplexity=18.658014, train_loss=2.9262757

Batch 135180, train_perplexity=18.658022, train_loss=2.9262762

Batch 135190, train_perplexity=18.658022, train_loss=2.9262762

Batch 135200, train_perplexity=18.658018, train_loss=2.926276

Batch 135210, train_perplexity=18.658014, train_loss=2.9262757

Batch 135220, train_perplexity=18.658014, train_loss=2.9262757

Batch 135230, train_perplexity=18.658022, train_loss=2.9262762

Batch 135240, train_perplexity=18.658014, train_loss=2.9262757

Batch 135250, train_perplexity=18.658018, train_loss=2.926276

Batch 135260, train_perplexity=18.658018, train_loss=2.926276

Batch 135270, train_perplexity=18.658014, train_loss=2.9262757

Batch 135280, train_perplexity=18.658014, train_loss=2.9262757

Batch 135290, train_perplexity=18.658014, train_loss=2.9262757

Batch 135300, train_perplexity=18.658014, train_loss=2.9262757

Batch 135310, train_perplexity=18.658014, train_loss=2.9262757

Batch 135320, train_perplexity=18.658014, train_loss=2.9262757

Batch 135330, train_perplexity=18.658014, train_loss=2.9262757

Batch 135340, train_perplexity=18.658014, train_loss=2.9262757

Batch 135350, train_perplexity=18.658014, train_loss=2.9262757

Batch 135360, train_perplexity=18.658014, train_loss=2.9262757

Batch 135370, train_perplexity=18.658014, train_loss=2.9262757

Batch 135380, train_perplexity=18.658014, train_loss=2.9262757

Batch 135390, train_perplexity=18.658014, train_loss=2.9262757

Batch 135400, train_perplexity=18.658014, train_loss=2.9262757

Batch 135410, train_perplexity=18.658014, train_loss=2.9262757

Batch 135420, train_perplexity=18.658014, train_loss=2.9262757

Batch 135430, train_perplexity=18.658014, train_loss=2.9262757

Batch 135440, train_perplexity=18.658014, train_loss=2.9262757

Batch 135450, train_perplexity=18.658014, train_loss=2.9262757

Batch 135460, train_perplexity=18.658014, train_loss=2.9262757

Batch 135470, train_perplexity=18.658009, train_loss=2.9262755

Batch 135480, train_perplexity=18.658014, train_loss=2.9262757

Batch 135490, train_perplexity=18.658009, train_loss=2.9262755

Batch 135500, train_perplexity=18.658014, train_loss=2.9262757

Batch 135510, train_perplexity=18.658009, train_loss=2.9262755

Batch 135520, train_perplexity=18.658005, train_loss=2.9262753

Batch 135530, train_perplexity=18.658009, train_loss=2.9262755

Batch 135540, train_perplexity=18.658005, train_loss=2.9262753

Batch 135550, train_perplexity=18.658005, train_loss=2.9262753

Batch 135560, train_perplexity=18.658005, train_loss=2.9262753

Batch 135570, train_perplexity=18.658005, train_loss=2.9262753

Batch 135580, train_perplexity=18.658005, train_loss=2.9262753

Batch 135590, train_perplexity=18.658005, train_loss=2.9262753

Batch 135600, train_perplexity=18.658005, train_loss=2.9262753

Batch 135610, train_perplexity=18.658005, train_loss=2.9262753

Batch 135620, train_perplexity=18.658005, train_loss=2.9262753
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 135630, train_perplexity=18.658005, train_loss=2.9262753

Batch 135640, train_perplexity=18.658005, train_loss=2.9262753

Batch 135650, train_perplexity=18.658005, train_loss=2.9262753

Batch 135660, train_perplexity=18.658005, train_loss=2.9262753

Batch 135670, train_perplexity=18.658005, train_loss=2.9262753

Batch 135680, train_perplexity=18.658005, train_loss=2.9262753

Batch 135690, train_perplexity=18.658005, train_loss=2.9262753

Batch 135700, train_perplexity=18.658005, train_loss=2.9262753

Batch 135710, train_perplexity=18.658005, train_loss=2.9262753

Batch 135720, train_perplexity=18.658005, train_loss=2.9262753

Batch 135730, train_perplexity=18.658005, train_loss=2.9262753

Batch 135740, train_perplexity=18.658005, train_loss=2.9262753

Batch 135750, train_perplexity=18.658005, train_loss=2.9262753

Batch 135760, train_perplexity=18.658005, train_loss=2.9262753

Batch 135770, train_perplexity=18.658, train_loss=2.926275

Batch 135780, train_perplexity=18.658, train_loss=2.926275

Batch 135790, train_perplexity=18.657995, train_loss=2.9262748

Batch 135800, train_perplexity=18.658, train_loss=2.926275

Batch 135810, train_perplexity=18.657995, train_loss=2.9262748

Batch 135820, train_perplexity=18.657995, train_loss=2.9262748

Batch 135830, train_perplexity=18.658005, train_loss=2.9262753

Batch 135840, train_perplexity=18.657995, train_loss=2.9262748

Batch 135850, train_perplexity=18.658005, train_loss=2.9262753

Batch 135860, train_perplexity=18.657995, train_loss=2.9262748

Batch 135870, train_perplexity=18.657995, train_loss=2.9262748

Batch 135880, train_perplexity=18.657995, train_loss=2.9262748

Batch 135890, train_perplexity=18.657995, train_loss=2.9262748

Batch 135900, train_perplexity=18.657995, train_loss=2.9262748

Batch 135910, train_perplexity=18.657995, train_loss=2.9262748

Batch 135920, train_perplexity=18.657995, train_loss=2.9262748

Batch 135930, train_perplexity=18.657995, train_loss=2.9262748

Batch 135940, train_perplexity=18.657995, train_loss=2.9262748

Batch 135950, train_perplexity=18.657995, train_loss=2.9262748

Batch 135960, train_perplexity=18.657995, train_loss=2.9262748

Batch 135970, train_perplexity=18.657995, train_loss=2.9262748

Batch 135980, train_perplexity=18.657995, train_loss=2.9262748

Batch 135990, train_perplexity=18.657995, train_loss=2.9262748

Batch 136000, train_perplexity=18.657995, train_loss=2.9262748

Batch 136010, train_perplexity=18.657995, train_loss=2.9262748

Batch 136020, train_perplexity=18.657995, train_loss=2.9262748

Batch 136030, train_perplexity=18.657995, train_loss=2.9262748

Batch 136040, train_perplexity=18.657991, train_loss=2.9262745

Batch 136050, train_perplexity=18.657995, train_loss=2.9262748

Batch 136060, train_perplexity=18.657995, train_loss=2.9262748

Batch 136070, train_perplexity=18.657995, train_loss=2.9262748

Batch 136080, train_perplexity=18.657988, train_loss=2.9262743

Batch 136090, train_perplexity=18.657995, train_loss=2.9262748

Batch 136100, train_perplexity=18.657995, train_loss=2.9262748

Batch 136110, train_perplexity=18.657991, train_loss=2.9262745

Batch 136120, train_perplexity=18.657988, train_loss=2.9262743

Batch 136130, train_perplexity=18.657995, train_loss=2.9262748

Batch 136140, train_perplexity=18.657988, train_loss=2.9262743

Batch 136150, train_perplexity=18.657991, train_loss=2.9262745

Batch 136160, train_perplexity=18.657988, train_loss=2.9262743

Batch 136170, train_perplexity=18.657991, train_loss=2.9262745

Batch 136180, train_perplexity=18.657988, train_loss=2.9262743

Batch 136190, train_perplexity=18.657988, train_loss=2.9262743

Batch 136200, train_perplexity=18.657988, train_loss=2.9262743

Batch 136210, train_perplexity=18.657988, train_loss=2.9262743

Batch 136220, train_perplexity=18.657988, train_loss=2.9262743

Batch 136230, train_perplexity=18.657988, train_loss=2.9262743

Batch 136240, train_perplexity=18.657988, train_loss=2.9262743

Batch 136250, train_perplexity=18.657988, train_loss=2.9262743

Batch 136260, train_perplexity=18.657988, train_loss=2.9262743

Batch 136270, train_perplexity=18.657982, train_loss=2.926274

Batch 136280, train_perplexity=18.657988, train_loss=2.9262743

Batch 136290, train_perplexity=18.657988, train_loss=2.9262743

Batch 136300, train_perplexity=18.657988, train_loss=2.9262743

Batch 136310, train_perplexity=18.657988, train_loss=2.9262743

Batch 136320, train_perplexity=18.657982, train_loss=2.926274

Batch 136330, train_perplexity=18.657988, train_loss=2.9262743

Batch 136340, train_perplexity=18.657982, train_loss=2.926274

Batch 136350, train_perplexity=18.657988, train_loss=2.9262743

Batch 136360, train_perplexity=18.657988, train_loss=2.9262743

Batch 136370, train_perplexity=18.657988, train_loss=2.9262743

Batch 136380, train_perplexity=18.657988, train_loss=2.9262743

Batch 136390, train_perplexity=18.657988, train_loss=2.9262743

Batch 136400, train_perplexity=18.657988, train_loss=2.9262743

Batch 136410, train_perplexity=18.657982, train_loss=2.926274

Batch 136420, train_perplexity=18.657982, train_loss=2.926274

Batch 136430, train_perplexity=18.657988, train_loss=2.9262743

Batch 136440, train_perplexity=18.657982, train_loss=2.926274

Batch 136450, train_perplexity=18.657982, train_loss=2.926274

Batch 136460, train_perplexity=18.657978, train_loss=2.9262738

Batch 136470, train_perplexity=18.657982, train_loss=2.926274

Batch 136480, train_perplexity=18.657978, train_loss=2.9262738

Batch 136490, train_perplexity=18.657978, train_loss=2.9262738

Batch 136500, train_perplexity=18.657978, train_loss=2.9262738

Batch 136510, train_perplexity=18.657982, train_loss=2.926274

Batch 136520, train_perplexity=18.657978, train_loss=2.9262738

Batch 136530, train_perplexity=18.657978, train_loss=2.9262738

Batch 136540, train_perplexity=18.657982, train_loss=2.926274

Batch 136550, train_perplexity=18.657978, train_loss=2.9262738

Batch 136560, train_perplexity=18.657978, train_loss=2.9262738

Batch 136570, train_perplexity=18.657978, train_loss=2.9262738

Batch 136580, train_perplexity=18.657978, train_loss=2.9262738

Batch 136590, train_perplexity=18.657978, train_loss=2.9262738

Batch 136600, train_perplexity=18.657978, train_loss=2.9262738

Batch 136610, train_perplexity=18.657982, train_loss=2.926274

Batch 136620, train_perplexity=18.657978, train_loss=2.9262738

Batch 136630, train_perplexity=18.657978, train_loss=2.9262738

Batch 136640, train_perplexity=18.657978, train_loss=2.9262738

Batch 136650, train_perplexity=18.657974, train_loss=2.9262736

Batch 136660, train_perplexity=18.657978, train_loss=2.9262738

Batch 136670, train_perplexity=18.657978, train_loss=2.9262738

Batch 136680, train_perplexity=18.657978, train_loss=2.9262738

Batch 136690, train_perplexity=18.657969, train_loss=2.9262733

Batch 136700, train_perplexity=18.657978, train_loss=2.9262738

Batch 136710, train_perplexity=18.657978, train_loss=2.9262738

Batch 136720, train_perplexity=18.657969, train_loss=2.9262733

Batch 136730, train_perplexity=18.657978, train_loss=2.9262738

Batch 136740, train_perplexity=18.657978, train_loss=2.9262738

Batch 136750, train_perplexity=18.657974, train_loss=2.9262736

Batch 136760, train_perplexity=18.657969, train_loss=2.9262733

Batch 136770, train_perplexity=18.657969, train_loss=2.9262733

Batch 136780, train_perplexity=18.657969, train_loss=2.9262733

Batch 136790, train_perplexity=18.657969, train_loss=2.9262733

Batch 136800, train_perplexity=18.657969, train_loss=2.9262733

Batch 136810, train_perplexity=18.657974, train_loss=2.9262736

Batch 136820, train_perplexity=18.657969, train_loss=2.9262733

Batch 136830, train_perplexity=18.657969, train_loss=2.9262733

Batch 136840, train_perplexity=18.657969, train_loss=2.9262733

Batch 136850, train_perplexity=18.657969, train_loss=2.9262733

Batch 136860, train_perplexity=18.657969, train_loss=2.9262733

Batch 136870, train_perplexity=18.657969, train_loss=2.9262733

Batch 136880, train_perplexity=18.657969, train_loss=2.9262733

Batch 136890, train_perplexity=18.657969, train_loss=2.9262733

Batch 136900, train_perplexity=18.657969, train_loss=2.9262733

Batch 136910, train_perplexity=18.657969, train_loss=2.9262733
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 136920, train_perplexity=18.657969, train_loss=2.9262733

Batch 136930, train_perplexity=18.657969, train_loss=2.9262733

Batch 136940, train_perplexity=18.657969, train_loss=2.9262733

Batch 136950, train_perplexity=18.657965, train_loss=2.926273

Batch 136960, train_perplexity=18.657969, train_loss=2.9262733

Batch 136970, train_perplexity=18.657969, train_loss=2.9262733

Batch 136980, train_perplexity=18.657965, train_loss=2.926273

Batch 136990, train_perplexity=18.65796, train_loss=2.9262729

Batch 137000, train_perplexity=18.657965, train_loss=2.926273

Batch 137010, train_perplexity=18.657965, train_loss=2.926273

Batch 137020, train_perplexity=18.657969, train_loss=2.9262733

Batch 137030, train_perplexity=18.657965, train_loss=2.926273

Batch 137040, train_perplexity=18.657965, train_loss=2.926273

Batch 137050, train_perplexity=18.65796, train_loss=2.9262729

Batch 137060, train_perplexity=18.657965, train_loss=2.926273

Batch 137070, train_perplexity=18.657965, train_loss=2.926273

Batch 137080, train_perplexity=18.657965, train_loss=2.926273

Batch 137090, train_perplexity=18.657965, train_loss=2.926273

Batch 137100, train_perplexity=18.657965, train_loss=2.926273

Batch 137110, train_perplexity=18.65796, train_loss=2.9262729

Batch 137120, train_perplexity=18.65796, train_loss=2.9262729

Batch 137130, train_perplexity=18.65796, train_loss=2.9262729

Batch 137140, train_perplexity=18.65796, train_loss=2.9262729

Batch 137150, train_perplexity=18.657965, train_loss=2.926273

Batch 137160, train_perplexity=18.65796, train_loss=2.9262729

Batch 137170, train_perplexity=18.65796, train_loss=2.9262729

Batch 137180, train_perplexity=18.65796, train_loss=2.9262729

Batch 137190, train_perplexity=18.65796, train_loss=2.9262729

Batch 137200, train_perplexity=18.65796, train_loss=2.9262729

Batch 137210, train_perplexity=18.65796, train_loss=2.9262729

Batch 137220, train_perplexity=18.65796, train_loss=2.9262729

Batch 137230, train_perplexity=18.65796, train_loss=2.9262729

Batch 137240, train_perplexity=18.65796, train_loss=2.9262729

Batch 137250, train_perplexity=18.65796, train_loss=2.9262729

Batch 137260, train_perplexity=18.65796, train_loss=2.9262729

Batch 137270, train_perplexity=18.65796, train_loss=2.9262729

Batch 137280, train_perplexity=18.657955, train_loss=2.9262726

Batch 137290, train_perplexity=18.657955, train_loss=2.9262726

Batch 137300, train_perplexity=18.657955, train_loss=2.9262726

Batch 137310, train_perplexity=18.65796, train_loss=2.9262729

Batch 137320, train_perplexity=18.65796, train_loss=2.9262729

Batch 137330, train_perplexity=18.65796, train_loss=2.9262729

Batch 137340, train_perplexity=18.65796, train_loss=2.9262729

Batch 137350, train_perplexity=18.657955, train_loss=2.9262726

Batch 137360, train_perplexity=18.657951, train_loss=2.9262724

Batch 137370, train_perplexity=18.65796, train_loss=2.9262729

Batch 137380, train_perplexity=18.657951, train_loss=2.9262724

Batch 137390, train_perplexity=18.657951, train_loss=2.9262724

Batch 137400, train_perplexity=18.657951, train_loss=2.9262724

Batch 137410, train_perplexity=18.65796, train_loss=2.9262729

Batch 137420, train_perplexity=18.657951, train_loss=2.9262724

Batch 137430, train_perplexity=18.657951, train_loss=2.9262724

Batch 137440, train_perplexity=18.657951, train_loss=2.9262724

Batch 137450, train_perplexity=18.657951, train_loss=2.9262724

Batch 137460, train_perplexity=18.657951, train_loss=2.9262724

Batch 137470, train_perplexity=18.657951, train_loss=2.9262724

Batch 137480, train_perplexity=18.657951, train_loss=2.9262724

Batch 137490, train_perplexity=18.657951, train_loss=2.9262724

Batch 137500, train_perplexity=18.657951, train_loss=2.9262724

Batch 137510, train_perplexity=18.657951, train_loss=2.9262724

Batch 137520, train_perplexity=18.657951, train_loss=2.9262724

Batch 137530, train_perplexity=18.657951, train_loss=2.9262724

Batch 137540, train_perplexity=18.657951, train_loss=2.9262724

Batch 137550, train_perplexity=18.657951, train_loss=2.9262724

Batch 137560, train_perplexity=18.657951, train_loss=2.9262724

Batch 137570, train_perplexity=18.657951, train_loss=2.9262724

Batch 137580, train_perplexity=18.657951, train_loss=2.9262724

Batch 137590, train_perplexity=18.657951, train_loss=2.9262724

Batch 137600, train_perplexity=18.657951, train_loss=2.9262724

Batch 137610, train_perplexity=18.657951, train_loss=2.9262724

Batch 137620, train_perplexity=18.657942, train_loss=2.926272

Batch 137630, train_perplexity=18.657948, train_loss=2.9262722

Batch 137640, train_perplexity=18.657942, train_loss=2.926272

Batch 137650, train_perplexity=18.657948, train_loss=2.9262722

Batch 137660, train_perplexity=18.657948, train_loss=2.9262722

Batch 137670, train_perplexity=18.657948, train_loss=2.9262722

Batch 137680, train_perplexity=18.657942, train_loss=2.926272

Batch 137690, train_perplexity=18.657942, train_loss=2.926272

Batch 137700, train_perplexity=18.657942, train_loss=2.926272

Batch 137710, train_perplexity=18.657948, train_loss=2.9262722

Batch 137720, train_perplexity=18.657942, train_loss=2.926272

Batch 137730, train_perplexity=18.657942, train_loss=2.926272

Batch 137740, train_perplexity=18.657942, train_loss=2.926272

Batch 137750, train_perplexity=18.657948, train_loss=2.9262722

Batch 137760, train_perplexity=18.657942, train_loss=2.926272

Batch 137770, train_perplexity=18.657942, train_loss=2.926272

Batch 137780, train_perplexity=18.657942, train_loss=2.926272

Batch 137790, train_perplexity=18.657942, train_loss=2.926272

Batch 137800, train_perplexity=18.657942, train_loss=2.926272

Batch 137810, train_perplexity=18.657942, train_loss=2.926272

Batch 137820, train_perplexity=18.657942, train_loss=2.926272

Batch 137830, train_perplexity=18.657942, train_loss=2.926272

Batch 137840, train_perplexity=18.657942, train_loss=2.926272

Batch 137850, train_perplexity=18.657942, train_loss=2.926272

Batch 137860, train_perplexity=18.657942, train_loss=2.926272

Batch 137870, train_perplexity=18.657942, train_loss=2.926272

Batch 137880, train_perplexity=18.657942, train_loss=2.926272

Batch 137890, train_perplexity=18.657942, train_loss=2.926272

Batch 137900, train_perplexity=18.657942, train_loss=2.926272

Batch 137910, train_perplexity=18.657942, train_loss=2.926272

Batch 137920, train_perplexity=18.657934, train_loss=2.9262714

Batch 137930, train_perplexity=18.657942, train_loss=2.926272

Batch 137940, train_perplexity=18.657934, train_loss=2.9262714

Batch 137950, train_perplexity=18.657942, train_loss=2.926272

Batch 137960, train_perplexity=18.657942, train_loss=2.926272

Batch 137970, train_perplexity=18.657934, train_loss=2.9262714

Batch 137980, train_perplexity=18.657942, train_loss=2.926272

Batch 137990, train_perplexity=18.657934, train_loss=2.9262714

Batch 138000, train_perplexity=18.657938, train_loss=2.9262717

Batch 138010, train_perplexity=18.657934, train_loss=2.9262714

Batch 138020, train_perplexity=18.657934, train_loss=2.9262714

Batch 138030, train_perplexity=18.657934, train_loss=2.9262714

Batch 138040, train_perplexity=18.657934, train_loss=2.9262714

Batch 138050, train_perplexity=18.657934, train_loss=2.9262714

Batch 138060, train_perplexity=18.657934, train_loss=2.9262714

Batch 138070, train_perplexity=18.657934, train_loss=2.9262714

Batch 138080, train_perplexity=18.657934, train_loss=2.9262714

Batch 138090, train_perplexity=18.657934, train_loss=2.9262714

Batch 138100, train_perplexity=18.657934, train_loss=2.9262714

Batch 138110, train_perplexity=18.657934, train_loss=2.9262714

Batch 138120, train_perplexity=18.657934, train_loss=2.9262714

Batch 138130, train_perplexity=18.657934, train_loss=2.9262714

Batch 138140, train_perplexity=18.657934, train_loss=2.9262714

Batch 138150, train_perplexity=18.657934, train_loss=2.9262714

Batch 138160, train_perplexity=18.657934, train_loss=2.9262714

Batch 138170, train_perplexity=18.657934, train_loss=2.9262714

Batch 138180, train_perplexity=18.657928, train_loss=2.9262712

Batch 138190, train_perplexity=18.657934, train_loss=2.9262714

Batch 138200, train_perplexity=18.657934, train_loss=2.9262714

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 138210, train_perplexity=18.657934, train_loss=2.9262714

Batch 138220, train_perplexity=18.657934, train_loss=2.9262714

Batch 138230, train_perplexity=18.657928, train_loss=2.9262712

Batch 138240, train_perplexity=18.657928, train_loss=2.9262712

Batch 138250, train_perplexity=18.657934, train_loss=2.9262714

Batch 138260, train_perplexity=18.657928, train_loss=2.9262712

Batch 138270, train_perplexity=18.657928, train_loss=2.9262712

Batch 138280, train_perplexity=18.657928, train_loss=2.9262712

Batch 138290, train_perplexity=18.657928, train_loss=2.9262712

Batch 138300, train_perplexity=18.657934, train_loss=2.9262714

Batch 138310, train_perplexity=18.657925, train_loss=2.926271

Batch 138320, train_perplexity=18.657928, train_loss=2.9262712

Batch 138330, train_perplexity=18.657925, train_loss=2.926271

Batch 138340, train_perplexity=18.657928, train_loss=2.9262712

Batch 138350, train_perplexity=18.657925, train_loss=2.926271

Batch 138360, train_perplexity=18.657925, train_loss=2.926271

Batch 138370, train_perplexity=18.657925, train_loss=2.926271

Batch 138380, train_perplexity=18.657925, train_loss=2.926271

Batch 138390, train_perplexity=18.657925, train_loss=2.926271

Batch 138400, train_perplexity=18.657925, train_loss=2.926271

Batch 138410, train_perplexity=18.657928, train_loss=2.9262712

Batch 138420, train_perplexity=18.657925, train_loss=2.926271

Batch 138430, train_perplexity=18.657925, train_loss=2.926271

Batch 138440, train_perplexity=18.657928, train_loss=2.9262712

Batch 138450, train_perplexity=18.657925, train_loss=2.926271

Batch 138460, train_perplexity=18.657925, train_loss=2.926271

Batch 138470, train_perplexity=18.657925, train_loss=2.926271

Batch 138480, train_perplexity=18.657925, train_loss=2.926271

Batch 138490, train_perplexity=18.657925, train_loss=2.926271

Batch 138500, train_perplexity=18.657925, train_loss=2.926271

Batch 138510, train_perplexity=18.65792, train_loss=2.9262707

Batch 138520, train_perplexity=18.657925, train_loss=2.926271

Batch 138530, train_perplexity=18.657925, train_loss=2.926271

Batch 138540, train_perplexity=18.657915, train_loss=2.9262705

Batch 138550, train_perplexity=18.657925, train_loss=2.926271

Batch 138560, train_perplexity=18.657925, train_loss=2.926271

Batch 138570, train_perplexity=18.657915, train_loss=2.9262705

Batch 138580, train_perplexity=18.657925, train_loss=2.926271

Batch 138590, train_perplexity=18.657925, train_loss=2.926271

Batch 138600, train_perplexity=18.65792, train_loss=2.9262707

Batch 138610, train_perplexity=18.657915, train_loss=2.9262705

Batch 138620, train_perplexity=18.657915, train_loss=2.9262705

Batch 138630, train_perplexity=18.657915, train_loss=2.9262705

Batch 138640, train_perplexity=18.657915, train_loss=2.9262705

Batch 138650, train_perplexity=18.657915, train_loss=2.9262705

Batch 138660, train_perplexity=18.657915, train_loss=2.9262705

Batch 138670, train_perplexity=18.657915, train_loss=2.9262705

Batch 138680, train_perplexity=18.657915, train_loss=2.9262705

Batch 138690, train_perplexity=18.657925, train_loss=2.926271

Batch 138700, train_perplexity=18.657915, train_loss=2.9262705

Batch 138710, train_perplexity=18.657915, train_loss=2.9262705

Batch 138720, train_perplexity=18.657915, train_loss=2.9262705

Batch 138730, train_perplexity=18.657915, train_loss=2.9262705

Batch 138740, train_perplexity=18.657915, train_loss=2.9262705

Batch 138750, train_perplexity=18.657915, train_loss=2.9262705

Batch 138760, train_perplexity=18.657915, train_loss=2.9262705

Batch 138770, train_perplexity=18.657915, train_loss=2.9262705

Batch 138780, train_perplexity=18.657915, train_loss=2.9262705

Batch 138790, train_perplexity=18.657915, train_loss=2.9262705

Batch 138800, train_perplexity=18.657915, train_loss=2.9262705

Batch 138810, train_perplexity=18.657915, train_loss=2.9262705

Batch 138820, train_perplexity=18.657915, train_loss=2.9262705

Batch 138830, train_perplexity=18.657907, train_loss=2.92627

Batch 138840, train_perplexity=18.657915, train_loss=2.9262705

Batch 138850, train_perplexity=18.657911, train_loss=2.9262702

Batch 138860, train_perplexity=18.657915, train_loss=2.9262705

Batch 138870, train_perplexity=18.657915, train_loss=2.9262705

Batch 138880, train_perplexity=18.657915, train_loss=2.9262705

Batch 138890, train_perplexity=18.657915, train_loss=2.9262705

Batch 138900, train_perplexity=18.657911, train_loss=2.9262702

Batch 138910, train_perplexity=18.657911, train_loss=2.9262702

Batch 138920, train_perplexity=18.657915, train_loss=2.9262705

Batch 138930, train_perplexity=18.657911, train_loss=2.9262702

Batch 138940, train_perplexity=18.657911, train_loss=2.9262702

Batch 138950, train_perplexity=18.657907, train_loss=2.92627

Batch 138960, train_perplexity=18.657915, train_loss=2.9262705

Batch 138970, train_perplexity=18.657907, train_loss=2.92627

Batch 138980, train_perplexity=18.657911, train_loss=2.9262702

Batch 138990, train_perplexity=18.657907, train_loss=2.92627

Batch 139000, train_perplexity=18.657911, train_loss=2.9262702

Batch 139010, train_perplexity=18.657907, train_loss=2.92627

Batch 139020, train_perplexity=18.657907, train_loss=2.92627

Batch 139030, train_perplexity=18.657907, train_loss=2.92627

Batch 139040, train_perplexity=18.657907, train_loss=2.92627

Batch 139050, train_perplexity=18.657907, train_loss=2.92627

Batch 139060, train_perplexity=18.657907, train_loss=2.92627

Batch 139070, train_perplexity=18.657907, train_loss=2.92627

Batch 139080, train_perplexity=18.657907, train_loss=2.92627

Batch 139090, train_perplexity=18.657907, train_loss=2.92627

Batch 139100, train_perplexity=18.657907, train_loss=2.92627

Batch 139110, train_perplexity=18.657907, train_loss=2.92627

Batch 139120, train_perplexity=18.657907, train_loss=2.92627

Batch 139130, train_perplexity=18.657907, train_loss=2.92627

Batch 139140, train_perplexity=18.657907, train_loss=2.92627

Batch 139150, train_perplexity=18.657902, train_loss=2.9262698

Batch 139160, train_perplexity=18.657902, train_loss=2.9262698

Batch 139170, train_perplexity=18.657902, train_loss=2.9262698

Batch 139180, train_perplexity=18.657907, train_loss=2.92627

Batch 139190, train_perplexity=18.657907, train_loss=2.92627

Batch 139200, train_perplexity=18.657898, train_loss=2.9262695

Batch 139210, train_perplexity=18.657898, train_loss=2.9262695

Batch 139220, train_perplexity=18.657907, train_loss=2.92627

Batch 139230, train_perplexity=18.657898, train_loss=2.9262695

Batch 139240, train_perplexity=18.657902, train_loss=2.9262698

Batch 139250, train_perplexity=18.657898, train_loss=2.9262695

Batch 139260, train_perplexity=18.657902, train_loss=2.9262698

Batch 139270, train_perplexity=18.657902, train_loss=2.9262698

Batch 139280, train_perplexity=18.657898, train_loss=2.9262695

Batch 139290, train_perplexity=18.657898, train_loss=2.9262695

Batch 139300, train_perplexity=18.657907, train_loss=2.92627

Batch 139310, train_perplexity=18.657898, train_loss=2.9262695

Batch 139320, train_perplexity=18.657898, train_loss=2.9262695

Batch 139330, train_perplexity=18.657898, train_loss=2.9262695

Batch 139340, train_perplexity=18.657898, train_loss=2.9262695

Batch 139350, train_perplexity=18.657898, train_loss=2.9262695

Batch 139360, train_perplexity=18.657898, train_loss=2.9262695

Batch 139370, train_perplexity=18.657898, train_loss=2.9262695

Batch 139380, train_perplexity=18.657898, train_loss=2.9262695

Batch 139390, train_perplexity=18.657898, train_loss=2.9262695

Batch 139400, train_perplexity=18.657898, train_loss=2.9262695

Batch 139410, train_perplexity=18.657898, train_loss=2.9262695

Batch 139420, train_perplexity=18.657898, train_loss=2.9262695

Batch 139430, train_perplexity=18.657898, train_loss=2.9262695

Batch 139440, train_perplexity=18.657898, train_loss=2.9262695

Batch 139450, train_perplexity=18.657898, train_loss=2.9262695

Batch 139460, train_perplexity=18.657898, train_loss=2.9262695

Batch 139470, train_perplexity=18.657898, train_loss=2.9262695

Batch 139480, train_perplexity=18.657894, train_loss=2.9262693

Batch 139490, train_perplexity=18.657894, train_loss=2.9262693

Batch 139500, train_perplexity=18.657898, train_loss=2.9262695
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 139510, train_perplexity=18.657898, train_loss=2.9262695

Batch 139520, train_perplexity=18.657894, train_loss=2.9262693

Batch 139530, train_perplexity=18.657894, train_loss=2.9262693

Batch 139540, train_perplexity=18.657888, train_loss=2.926269

Batch 139550, train_perplexity=18.657898, train_loss=2.9262695

Batch 139560, train_perplexity=18.657888, train_loss=2.926269

Batch 139570, train_perplexity=18.657888, train_loss=2.926269

Batch 139580, train_perplexity=18.657888, train_loss=2.926269

Batch 139590, train_perplexity=18.657894, train_loss=2.9262693

Batch 139600, train_perplexity=18.657888, train_loss=2.926269

Batch 139610, train_perplexity=18.657898, train_loss=2.9262695

Batch 139620, train_perplexity=18.657888, train_loss=2.926269

Batch 139630, train_perplexity=18.657888, train_loss=2.926269

Batch 139640, train_perplexity=18.657894, train_loss=2.9262693

Batch 139650, train_perplexity=18.657888, train_loss=2.926269

Batch 139660, train_perplexity=18.657888, train_loss=2.926269

Batch 139670, train_perplexity=18.657888, train_loss=2.926269

Batch 139680, train_perplexity=18.657885, train_loss=2.9262688

Batch 139690, train_perplexity=18.657888, train_loss=2.926269

Batch 139700, train_perplexity=18.657888, train_loss=2.926269

Batch 139710, train_perplexity=18.657888, train_loss=2.926269

Batch 139720, train_perplexity=18.657885, train_loss=2.9262688

Batch 139730, train_perplexity=18.657885, train_loss=2.9262688

Batch 139740, train_perplexity=18.657888, train_loss=2.926269

Batch 139750, train_perplexity=18.657885, train_loss=2.9262688

Batch 139760, train_perplexity=18.657888, train_loss=2.926269

Batch 139770, train_perplexity=18.657888, train_loss=2.926269

Batch 139780, train_perplexity=18.657888, train_loss=2.926269

Batch 139790, train_perplexity=18.65788, train_loss=2.9262686

Batch 139800, train_perplexity=18.65788, train_loss=2.9262686

Batch 139810, train_perplexity=18.657888, train_loss=2.926269

Batch 139820, train_perplexity=18.657888, train_loss=2.926269

Batch 139830, train_perplexity=18.65788, train_loss=2.9262686

Batch 139840, train_perplexity=18.65788, train_loss=2.9262686

Batch 139850, train_perplexity=18.65788, train_loss=2.9262686

Batch 139860, train_perplexity=18.65788, train_loss=2.9262686

Batch 139870, train_perplexity=18.65788, train_loss=2.9262686

Batch 139880, train_perplexity=18.65788, train_loss=2.9262686

Batch 139890, train_perplexity=18.65788, train_loss=2.9262686

Batch 139900, train_perplexity=18.65788, train_loss=2.9262686

Batch 139910, train_perplexity=18.65788, train_loss=2.9262686

Batch 139920, train_perplexity=18.657885, train_loss=2.9262688

Batch 139930, train_perplexity=18.65788, train_loss=2.9262686

Batch 139940, train_perplexity=18.65788, train_loss=2.9262686

Batch 139950, train_perplexity=18.65788, train_loss=2.9262686

Batch 139960, train_perplexity=18.65788, train_loss=2.9262686

Batch 139970, train_perplexity=18.65788, train_loss=2.9262686

Batch 139980, train_perplexity=18.65788, train_loss=2.9262686

Batch 139990, train_perplexity=18.65788, train_loss=2.9262686

Batch 140000, train_perplexity=18.65788, train_loss=2.9262686

Batch 140010, train_perplexity=18.65788, train_loss=2.9262686

Batch 140020, train_perplexity=18.65788, train_loss=2.9262686

Batch 140030, train_perplexity=18.65788, train_loss=2.9262686

Batch 140040, train_perplexity=18.65788, train_loss=2.9262686

Batch 140050, train_perplexity=18.65788, train_loss=2.9262686

Batch 140060, train_perplexity=18.657875, train_loss=2.9262683

Batch 140070, train_perplexity=18.65788, train_loss=2.9262686

Batch 140080, train_perplexity=18.65788, train_loss=2.9262686

Batch 140090, train_perplexity=18.657871, train_loss=2.926268

Batch 140100, train_perplexity=18.657871, train_loss=2.926268

Batch 140110, train_perplexity=18.657871, train_loss=2.926268

Batch 140120, train_perplexity=18.65788, train_loss=2.9262686

Batch 140130, train_perplexity=18.657871, train_loss=2.926268

Batch 140140, train_perplexity=18.65788, train_loss=2.9262686

Batch 140150, train_perplexity=18.65788, train_loss=2.9262686

Batch 140160, train_perplexity=18.65788, train_loss=2.9262686

Batch 140170, train_perplexity=18.65788, train_loss=2.9262686

Batch 140180, train_perplexity=18.657871, train_loss=2.926268

Batch 140190, train_perplexity=18.657871, train_loss=2.926268

Batch 140200, train_perplexity=18.657871, train_loss=2.926268

Batch 140210, train_perplexity=18.657871, train_loss=2.926268

Batch 140220, train_perplexity=18.657871, train_loss=2.926268

Batch 140230, train_perplexity=18.657871, train_loss=2.926268

Batch 140240, train_perplexity=18.657871, train_loss=2.926268

Batch 140250, train_perplexity=18.657871, train_loss=2.926268

Batch 140260, train_perplexity=18.657871, train_loss=2.926268

Batch 140270, train_perplexity=18.657871, train_loss=2.926268

Batch 140280, train_perplexity=18.657871, train_loss=2.926268

Batch 140290, train_perplexity=18.657871, train_loss=2.926268

Batch 140300, train_perplexity=18.657871, train_loss=2.926268

Batch 140310, train_perplexity=18.657867, train_loss=2.9262679

Batch 140320, train_perplexity=18.657871, train_loss=2.926268

Batch 140330, train_perplexity=18.657871, train_loss=2.926268

Batch 140340, train_perplexity=18.657871, train_loss=2.926268

Batch 140350, train_perplexity=18.657867, train_loss=2.9262679

Batch 140360, train_perplexity=18.657871, train_loss=2.926268

Batch 140370, train_perplexity=18.657871, train_loss=2.926268

Batch 140380, train_perplexity=18.657871, train_loss=2.926268

Batch 140390, train_perplexity=18.657867, train_loss=2.9262679

Batch 140400, train_perplexity=18.657871, train_loss=2.926268

Batch 140410, train_perplexity=18.657871, train_loss=2.926268

Batch 140420, train_perplexity=18.657871, train_loss=2.926268

Batch 140430, train_perplexity=18.657867, train_loss=2.9262679

Batch 140440, train_perplexity=18.657867, train_loss=2.9262679

Batch 140450, train_perplexity=18.657867, train_loss=2.9262679

Batch 140460, train_perplexity=18.657871, train_loss=2.926268

Batch 140470, train_perplexity=18.657867, train_loss=2.9262679

Batch 140480, train_perplexity=18.657871, train_loss=2.926268

Batch 140490, train_perplexity=18.657871, train_loss=2.926268

Batch 140500, train_perplexity=18.657867, train_loss=2.9262679

Batch 140510, train_perplexity=18.657871, train_loss=2.926268

Batch 140520, train_perplexity=18.657862, train_loss=2.9262676

Batch 140530, train_perplexity=18.657862, train_loss=2.9262676

Batch 140540, train_perplexity=18.657862, train_loss=2.9262676

Batch 140550, train_perplexity=18.657862, train_loss=2.9262676

Batch 140560, train_perplexity=18.657862, train_loss=2.9262676

Batch 140570, train_perplexity=18.657862, train_loss=2.9262676

Batch 140580, train_perplexity=18.657862, train_loss=2.9262676

Batch 140590, train_perplexity=18.657862, train_loss=2.9262676

Batch 140600, train_perplexity=18.657862, train_loss=2.9262676

Batch 140610, train_perplexity=18.657862, train_loss=2.9262676

Batch 140620, train_perplexity=18.657862, train_loss=2.9262676

Batch 140630, train_perplexity=18.657862, train_loss=2.9262676

Batch 140640, train_perplexity=18.657862, train_loss=2.9262676

Batch 140650, train_perplexity=18.657862, train_loss=2.9262676

Batch 140660, train_perplexity=18.657862, train_loss=2.9262676

Batch 140670, train_perplexity=18.657862, train_loss=2.9262676

Batch 140680, train_perplexity=18.657862, train_loss=2.9262676

Batch 140690, train_perplexity=18.657862, train_loss=2.9262676

Batch 140700, train_perplexity=18.657862, train_loss=2.9262676

Batch 140710, train_perplexity=18.657854, train_loss=2.9262671

Batch 140720, train_perplexity=18.657862, train_loss=2.9262676

Batch 140730, train_perplexity=18.657858, train_loss=2.9262674

Batch 140740, train_perplexity=18.657858, train_loss=2.9262674

Batch 140750, train_perplexity=18.657854, train_loss=2.9262671

Batch 140760, train_perplexity=18.657862, train_loss=2.9262676

Batch 140770, train_perplexity=18.657854, train_loss=2.9262671

Batch 140780, train_perplexity=18.657854, train_loss=2.9262671

Batch 140790, train_perplexity=18.657854, train_loss=2.9262671

Batch 140800, train_perplexity=18.657854, train_loss=2.9262671
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 140810, train_perplexity=18.657858, train_loss=2.9262674

Batch 140820, train_perplexity=18.657854, train_loss=2.9262671

Batch 140830, train_perplexity=18.657854, train_loss=2.9262671

Batch 140840, train_perplexity=18.657854, train_loss=2.9262671

Batch 140850, train_perplexity=18.657854, train_loss=2.9262671

Batch 140860, train_perplexity=18.657854, train_loss=2.9262671

Batch 140870, train_perplexity=18.657854, train_loss=2.9262671

Batch 140880, train_perplexity=18.657854, train_loss=2.9262671

Batch 140890, train_perplexity=18.657854, train_loss=2.9262671

Batch 140900, train_perplexity=18.657854, train_loss=2.9262671

Batch 140910, train_perplexity=18.657854, train_loss=2.9262671

Batch 140920, train_perplexity=18.657854, train_loss=2.9262671

Batch 140930, train_perplexity=18.657854, train_loss=2.9262671

Batch 140940, train_perplexity=18.657854, train_loss=2.9262671

Batch 140950, train_perplexity=18.657854, train_loss=2.9262671

Batch 140960, train_perplexity=18.657854, train_loss=2.9262671

Batch 140970, train_perplexity=18.657854, train_loss=2.9262671

Batch 140980, train_perplexity=18.657854, train_loss=2.9262671

Batch 140990, train_perplexity=18.657854, train_loss=2.9262671

Batch 141000, train_perplexity=18.657854, train_loss=2.9262671

Batch 141010, train_perplexity=18.657848, train_loss=2.926267

Batch 141020, train_perplexity=18.657854, train_loss=2.9262671

Batch 141030, train_perplexity=18.657854, train_loss=2.9262671

Batch 141040, train_perplexity=18.657854, train_loss=2.9262671

Batch 141050, train_perplexity=18.657854, train_loss=2.9262671

Batch 141060, train_perplexity=18.657848, train_loss=2.926267

Batch 141070, train_perplexity=18.657854, train_loss=2.9262671

Batch 141080, train_perplexity=18.657845, train_loss=2.9262667

Batch 141090, train_perplexity=18.657845, train_loss=2.9262667

Batch 141100, train_perplexity=18.657845, train_loss=2.9262667

Batch 141110, train_perplexity=18.657848, train_loss=2.926267

Batch 141120, train_perplexity=18.657848, train_loss=2.926267

Batch 141130, train_perplexity=18.657845, train_loss=2.9262667

Batch 141140, train_perplexity=18.657845, train_loss=2.9262667

Batch 141150, train_perplexity=18.657845, train_loss=2.9262667

Batch 141160, train_perplexity=18.657845, train_loss=2.9262667

Batch 141170, train_perplexity=18.657845, train_loss=2.9262667

Batch 141180, train_perplexity=18.657845, train_loss=2.9262667

Batch 141190, train_perplexity=18.657845, train_loss=2.9262667

Batch 141200, train_perplexity=18.657845, train_loss=2.9262667

Batch 141210, train_perplexity=18.657845, train_loss=2.9262667

Batch 141220, train_perplexity=18.657845, train_loss=2.9262667

Batch 141230, train_perplexity=18.657845, train_loss=2.9262667

Batch 141240, train_perplexity=18.657845, train_loss=2.9262667

Batch 141250, train_perplexity=18.657845, train_loss=2.9262667

Batch 141260, train_perplexity=18.657845, train_loss=2.9262667

Batch 141270, train_perplexity=18.657845, train_loss=2.9262667

Batch 141280, train_perplexity=18.657845, train_loss=2.9262667

Batch 141290, train_perplexity=18.657845, train_loss=2.9262667

Batch 141300, train_perplexity=18.657845, train_loss=2.9262667

Batch 141310, train_perplexity=18.65784, train_loss=2.9262664

Batch 141320, train_perplexity=18.657845, train_loss=2.9262667

Batch 141330, train_perplexity=18.657845, train_loss=2.9262667

Batch 141340, train_perplexity=18.657845, train_loss=2.9262667

Batch 141350, train_perplexity=18.65784, train_loss=2.9262664

Batch 141360, train_perplexity=18.657835, train_loss=2.9262662

Batch 141370, train_perplexity=18.657845, train_loss=2.9262667

Batch 141380, train_perplexity=18.657835, train_loss=2.9262662

Batch 141390, train_perplexity=18.657835, train_loss=2.9262662

Batch 141400, train_perplexity=18.657835, train_loss=2.9262662

Batch 141410, train_perplexity=18.657835, train_loss=2.9262662

Batch 141420, train_perplexity=18.657835, train_loss=2.9262662

Batch 141430, train_perplexity=18.657835, train_loss=2.9262662

Batch 141440, train_perplexity=18.657835, train_loss=2.9262662

Batch 141450, train_perplexity=18.657835, train_loss=2.9262662

Batch 141460, train_perplexity=18.657835, train_loss=2.9262662

Batch 141470, train_perplexity=18.657835, train_loss=2.9262662

Batch 141480, train_perplexity=18.65784, train_loss=2.9262664

Batch 141490, train_perplexity=18.657835, train_loss=2.9262662

Batch 141500, train_perplexity=18.657835, train_loss=2.9262662

Batch 141510, train_perplexity=18.657835, train_loss=2.9262662

Batch 141520, train_perplexity=18.657835, train_loss=2.9262662

Batch 141530, train_perplexity=18.657835, train_loss=2.9262662

Batch 141540, train_perplexity=18.657835, train_loss=2.9262662

Batch 141550, train_perplexity=18.657835, train_loss=2.9262662

Batch 141560, train_perplexity=18.657835, train_loss=2.9262662

Batch 141570, train_perplexity=18.657835, train_loss=2.9262662

Batch 141580, train_perplexity=18.657835, train_loss=2.9262662

Batch 141590, train_perplexity=18.657831, train_loss=2.926266

Batch 141600, train_perplexity=18.657835, train_loss=2.9262662

Batch 141610, train_perplexity=18.657835, train_loss=2.9262662

Batch 141620, train_perplexity=18.657835, train_loss=2.9262662

Batch 141630, train_perplexity=18.657835, train_loss=2.9262662

Batch 141640, train_perplexity=18.657831, train_loss=2.926266

Batch 141650, train_perplexity=18.657831, train_loss=2.926266

Batch 141660, train_perplexity=18.657827, train_loss=2.9262657

Batch 141670, train_perplexity=18.657835, train_loss=2.9262662

Batch 141680, train_perplexity=18.657835, train_loss=2.9262662

Batch 141690, train_perplexity=18.657827, train_loss=2.9262657

Batch 141700, train_perplexity=18.657835, train_loss=2.9262662

Batch 141710, train_perplexity=18.657835, train_loss=2.9262662

Batch 141720, train_perplexity=18.657831, train_loss=2.926266

Batch 141730, train_perplexity=18.657827, train_loss=2.9262657

Batch 141740, train_perplexity=18.657827, train_loss=2.9262657

Batch 141750, train_perplexity=18.657835, train_loss=2.9262662

Batch 141760, train_perplexity=18.657827, train_loss=2.9262657

Batch 141770, train_perplexity=18.657822, train_loss=2.9262655

Batch 141780, train_perplexity=18.657827, train_loss=2.9262657

Batch 141790, train_perplexity=18.657827, train_loss=2.9262657

Batch 141800, train_perplexity=18.657827, train_loss=2.9262657

Batch 141810, train_perplexity=18.657827, train_loss=2.9262657

Batch 141820, train_perplexity=18.657827, train_loss=2.9262657

Batch 141830, train_perplexity=18.657827, train_loss=2.9262657

Batch 141840, train_perplexity=18.657827, train_loss=2.9262657

Batch 141850, train_perplexity=18.657827, train_loss=2.9262657

Batch 141860, train_perplexity=18.657827, train_loss=2.9262657

Batch 141870, train_perplexity=18.657827, train_loss=2.9262657

Batch 141880, train_perplexity=18.657827, train_loss=2.9262657

Batch 141890, train_perplexity=18.657827, train_loss=2.9262657

Batch 141900, train_perplexity=18.657827, train_loss=2.9262657

Batch 141910, train_perplexity=18.657822, train_loss=2.9262655

Batch 141920, train_perplexity=18.657827, train_loss=2.9262657

Batch 141930, train_perplexity=18.657827, train_loss=2.9262657

Batch 141940, train_perplexity=18.657822, train_loss=2.9262655

Batch 141950, train_perplexity=18.657822, train_loss=2.9262655

Batch 141960, train_perplexity=18.657818, train_loss=2.9262652

Batch 141970, train_perplexity=18.657818, train_loss=2.9262652

Batch 141980, train_perplexity=18.657818, train_loss=2.9262652

Batch 141990, train_perplexity=18.657827, train_loss=2.9262657

Batch 142000, train_perplexity=18.657818, train_loss=2.9262652

Batch 142010, train_perplexity=18.657818, train_loss=2.9262652

Batch 142020, train_perplexity=18.657818, train_loss=2.9262652

Batch 142030, train_perplexity=18.657818, train_loss=2.9262652

Batch 142040, train_perplexity=18.657818, train_loss=2.9262652

Batch 142050, train_perplexity=18.657818, train_loss=2.9262652

Batch 142060, train_perplexity=18.657814, train_loss=2.926265

Batch 142070, train_perplexity=18.657822, train_loss=2.9262655

Batch 142080, train_perplexity=18.657818, train_loss=2.9262652

Batch 142090, train_perplexity=18.657818, train_loss=2.9262652
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 142100, train_perplexity=18.657814, train_loss=2.926265

Batch 142110, train_perplexity=18.657818, train_loss=2.9262652

Batch 142120, train_perplexity=18.657818, train_loss=2.9262652

Batch 142130, train_perplexity=18.657818, train_loss=2.9262652

Batch 142140, train_perplexity=18.657818, train_loss=2.9262652

Batch 142150, train_perplexity=18.657818, train_loss=2.9262652

Batch 142160, train_perplexity=18.657818, train_loss=2.9262652

Batch 142170, train_perplexity=18.657818, train_loss=2.9262652

Batch 142180, train_perplexity=18.657818, train_loss=2.9262652

Batch 142190, train_perplexity=18.657818, train_loss=2.9262652

Batch 142200, train_perplexity=18.657818, train_loss=2.9262652

Batch 142210, train_perplexity=18.657818, train_loss=2.9262652

Batch 142220, train_perplexity=18.657818, train_loss=2.9262652

Batch 142230, train_perplexity=18.657818, train_loss=2.9262652

Batch 142240, train_perplexity=18.657818, train_loss=2.9262652

Batch 142250, train_perplexity=18.657818, train_loss=2.9262652

Batch 142260, train_perplexity=18.657814, train_loss=2.926265

Batch 142270, train_perplexity=18.657818, train_loss=2.9262652

Batch 142280, train_perplexity=18.657818, train_loss=2.9262652

Batch 142290, train_perplexity=18.657818, train_loss=2.9262652

Batch 142300, train_perplexity=18.657814, train_loss=2.926265

Batch 142310, train_perplexity=18.657808, train_loss=2.9262648

Batch 142320, train_perplexity=18.657818, train_loss=2.9262652

Batch 142330, train_perplexity=18.657808, train_loss=2.9262648

Batch 142340, train_perplexity=18.657814, train_loss=2.926265

Batch 142350, train_perplexity=18.657814, train_loss=2.926265

Batch 142360, train_perplexity=18.657814, train_loss=2.926265

Batch 142370, train_perplexity=18.657814, train_loss=2.926265

Batch 142380, train_perplexity=18.657808, train_loss=2.9262648

Batch 142390, train_perplexity=18.657808, train_loss=2.9262648

Batch 142400, train_perplexity=18.657808, train_loss=2.9262648

Batch 142410, train_perplexity=18.657808, train_loss=2.9262648

Batch 142420, train_perplexity=18.657808, train_loss=2.9262648

Batch 142430, train_perplexity=18.657808, train_loss=2.9262648

Batch 142440, train_perplexity=18.657804, train_loss=2.9262645

Batch 142450, train_perplexity=18.657808, train_loss=2.9262648

Batch 142460, train_perplexity=18.657814, train_loss=2.926265

Batch 142470, train_perplexity=18.657808, train_loss=2.9262648

Batch 142480, train_perplexity=18.657808, train_loss=2.9262648

Batch 142490, train_perplexity=18.657808, train_loss=2.9262648

Batch 142500, train_perplexity=18.657808, train_loss=2.9262648

Batch 142510, train_perplexity=18.657808, train_loss=2.9262648

Batch 142520, train_perplexity=18.657808, train_loss=2.9262648

Batch 142530, train_perplexity=18.657808, train_loss=2.9262648

Batch 142540, train_perplexity=18.657808, train_loss=2.9262648

Batch 142550, train_perplexity=18.657808, train_loss=2.9262648

Batch 142560, train_perplexity=18.657804, train_loss=2.9262645

Batch 142570, train_perplexity=18.657804, train_loss=2.9262645

Batch 142580, train_perplexity=18.657804, train_loss=2.9262645

Batch 142590, train_perplexity=18.657808, train_loss=2.9262648

Batch 142600, train_perplexity=18.6578, train_loss=2.9262643

Batch 142610, train_perplexity=18.657804, train_loss=2.9262645

Batch 142620, train_perplexity=18.657804, train_loss=2.9262645

Batch 142630, train_perplexity=18.6578, train_loss=2.9262643

Batch 142640, train_perplexity=18.6578, train_loss=2.9262643

Batch 142650, train_perplexity=18.657804, train_loss=2.9262645

Batch 142660, train_perplexity=18.6578, train_loss=2.9262643

Batch 142670, train_perplexity=18.6578, train_loss=2.9262643

Batch 142680, train_perplexity=18.657804, train_loss=2.9262645

Batch 142690, train_perplexity=18.6578, train_loss=2.9262643

Batch 142700, train_perplexity=18.657808, train_loss=2.9262648

Batch 142710, train_perplexity=18.6578, train_loss=2.9262643

Batch 142720, train_perplexity=18.6578, train_loss=2.9262643

Batch 142730, train_perplexity=18.6578, train_loss=2.9262643

Batch 142740, train_perplexity=18.6578, train_loss=2.9262643

Batch 142750, train_perplexity=18.6578, train_loss=2.9262643

Batch 142760, train_perplexity=18.6578, train_loss=2.9262643

Batch 142770, train_perplexity=18.6578, train_loss=2.9262643

Batch 142780, train_perplexity=18.657795, train_loss=2.926264

Batch 142790, train_perplexity=18.6578, train_loss=2.9262643

Batch 142800, train_perplexity=18.6578, train_loss=2.9262643

Batch 142810, train_perplexity=18.6578, train_loss=2.9262643

Batch 142820, train_perplexity=18.6578, train_loss=2.9262643

Batch 142830, train_perplexity=18.6578, train_loss=2.9262643

Batch 142840, train_perplexity=18.6578, train_loss=2.9262643

Batch 142850, train_perplexity=18.6578, train_loss=2.9262643

Batch 142860, train_perplexity=18.6578, train_loss=2.9262643

Batch 142870, train_perplexity=18.657795, train_loss=2.926264

Batch 142880, train_perplexity=18.6578, train_loss=2.9262643

Batch 142890, train_perplexity=18.6578, train_loss=2.9262643

Batch 142900, train_perplexity=18.657795, train_loss=2.926264

Batch 142910, train_perplexity=18.657791, train_loss=2.9262638

Batch 142920, train_perplexity=18.657791, train_loss=2.9262638

Batch 142930, train_perplexity=18.657791, train_loss=2.9262638

Batch 142940, train_perplexity=18.657791, train_loss=2.9262638

Batch 142950, train_perplexity=18.657795, train_loss=2.926264

Batch 142960, train_perplexity=18.657795, train_loss=2.926264

Batch 142970, train_perplexity=18.657791, train_loss=2.9262638

Batch 142980, train_perplexity=18.657791, train_loss=2.9262638

Batch 142990, train_perplexity=18.657791, train_loss=2.9262638

Batch 143000, train_perplexity=18.657791, train_loss=2.9262638

Batch 143010, train_perplexity=18.657791, train_loss=2.9262638

Batch 143020, train_perplexity=18.657791, train_loss=2.9262638

Batch 143030, train_perplexity=18.657791, train_loss=2.9262638

Batch 143040, train_perplexity=18.657791, train_loss=2.9262638

Batch 143050, train_perplexity=18.657791, train_loss=2.9262638

Batch 143060, train_perplexity=18.657791, train_loss=2.9262638

Batch 143070, train_perplexity=18.657791, train_loss=2.9262638

Batch 143080, train_perplexity=18.657791, train_loss=2.9262638

Batch 143090, train_perplexity=18.657791, train_loss=2.9262638

Batch 143100, train_perplexity=18.657791, train_loss=2.9262638

Batch 143110, train_perplexity=18.657791, train_loss=2.9262638

Batch 143120, train_perplexity=18.657791, train_loss=2.9262638

Batch 143130, train_perplexity=18.657791, train_loss=2.9262638

Batch 143140, train_perplexity=18.657791, train_loss=2.9262638

Batch 143150, train_perplexity=18.657791, train_loss=2.9262638

Batch 143160, train_perplexity=18.657787, train_loss=2.9262636

Batch 143170, train_perplexity=18.657791, train_loss=2.9262638

Batch 143180, train_perplexity=18.657791, train_loss=2.9262638

Batch 143190, train_perplexity=18.657782, train_loss=2.9262633

Batch 143200, train_perplexity=18.657787, train_loss=2.9262636

Batch 143210, train_perplexity=18.657791, train_loss=2.9262638

Batch 143220, train_perplexity=18.657782, train_loss=2.9262633

Batch 143230, train_perplexity=18.657791, train_loss=2.9262638

Batch 143240, train_perplexity=18.657791, train_loss=2.9262638

Batch 143250, train_perplexity=18.657787, train_loss=2.9262636

Batch 143260, train_perplexity=18.657782, train_loss=2.9262633

Batch 143270, train_perplexity=18.657791, train_loss=2.9262638

Batch 143280, train_perplexity=18.657782, train_loss=2.9262633

Batch 143290, train_perplexity=18.657791, train_loss=2.9262638

Batch 143300, train_perplexity=18.657782, train_loss=2.9262633

Batch 143310, train_perplexity=18.657782, train_loss=2.9262633

Batch 143320, train_perplexity=18.657782, train_loss=2.9262633

Batch 143330, train_perplexity=18.657782, train_loss=2.9262633

Batch 143340, train_perplexity=18.657782, train_loss=2.9262633

Batch 143350, train_perplexity=18.657782, train_loss=2.9262633

Batch 143360, train_perplexity=18.657782, train_loss=2.9262633

Batch 143370, train_perplexity=18.657782, train_loss=2.9262633

Batch 143380, train_perplexity=18.657782, train_loss=2.9262633
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 143390, train_perplexity=18.657782, train_loss=2.9262633

Batch 143400, train_perplexity=18.657782, train_loss=2.9262633

Batch 143410, train_perplexity=18.657782, train_loss=2.9262633

Batch 143420, train_perplexity=18.657782, train_loss=2.9262633

Batch 143430, train_perplexity=18.657778, train_loss=2.926263

Batch 143440, train_perplexity=18.657782, train_loss=2.9262633

Batch 143450, train_perplexity=18.657782, train_loss=2.9262633

Batch 143460, train_perplexity=18.657778, train_loss=2.926263

Batch 143470, train_perplexity=18.657774, train_loss=2.9262629

Batch 143480, train_perplexity=18.657778, train_loss=2.926263

Batch 143490, train_perplexity=18.657774, train_loss=2.9262629

Batch 143500, train_perplexity=18.657774, train_loss=2.9262629

Batch 143510, train_perplexity=18.657778, train_loss=2.926263

Batch 143520, train_perplexity=18.657782, train_loss=2.9262633

Batch 143530, train_perplexity=18.657778, train_loss=2.926263

Batch 143540, train_perplexity=18.657782, train_loss=2.9262633

Batch 143550, train_perplexity=18.657774, train_loss=2.9262629

Batch 143560, train_perplexity=18.657778, train_loss=2.926263

Batch 143570, train_perplexity=18.657778, train_loss=2.926263

Batch 143580, train_perplexity=18.657774, train_loss=2.9262629

Batch 143590, train_perplexity=18.657774, train_loss=2.9262629

Batch 143600, train_perplexity=18.657774, train_loss=2.9262629

Batch 143610, train_perplexity=18.657774, train_loss=2.9262629

Batch 143620, train_perplexity=18.657774, train_loss=2.9262629

Batch 143630, train_perplexity=18.657774, train_loss=2.9262629

Batch 143640, train_perplexity=18.657774, train_loss=2.9262629

Batch 143650, train_perplexity=18.657774, train_loss=2.9262629

Batch 143660, train_perplexity=18.657774, train_loss=2.9262629

Batch 143670, train_perplexity=18.657774, train_loss=2.9262629

Batch 143680, train_perplexity=18.657774, train_loss=2.9262629

Batch 143690, train_perplexity=18.657774, train_loss=2.9262629

Batch 143700, train_perplexity=18.657774, train_loss=2.9262629

Batch 143710, train_perplexity=18.657774, train_loss=2.9262629

Batch 143720, train_perplexity=18.657774, train_loss=2.9262629

Batch 143730, train_perplexity=18.657774, train_loss=2.9262629

Batch 143740, train_perplexity=18.657768, train_loss=2.9262626

Batch 143750, train_perplexity=18.657774, train_loss=2.9262629

Batch 143760, train_perplexity=18.657774, train_loss=2.9262629

Batch 143770, train_perplexity=18.657768, train_loss=2.9262626

Batch 143780, train_perplexity=18.657774, train_loss=2.9262629

Batch 143790, train_perplexity=18.657768, train_loss=2.9262626

Batch 143800, train_perplexity=18.657768, train_loss=2.9262626

Batch 143810, train_perplexity=18.657768, train_loss=2.9262626

Batch 143820, train_perplexity=18.657764, train_loss=2.9262624

Batch 143830, train_perplexity=18.657774, train_loss=2.9262629

Batch 143840, train_perplexity=18.657774, train_loss=2.9262629

Batch 143850, train_perplexity=18.657764, train_loss=2.9262624

Batch 143860, train_perplexity=18.657764, train_loss=2.9262624

Batch 143870, train_perplexity=18.657774, train_loss=2.9262629

Batch 143880, train_perplexity=18.657764, train_loss=2.9262624

Batch 143890, train_perplexity=18.657774, train_loss=2.9262629

Batch 143900, train_perplexity=18.657774, train_loss=2.9262629

Batch 143910, train_perplexity=18.657764, train_loss=2.9262624

Batch 143920, train_perplexity=18.657768, train_loss=2.9262626

Batch 143930, train_perplexity=18.657764, train_loss=2.9262624

Batch 143940, train_perplexity=18.657764, train_loss=2.9262624

Batch 143950, train_perplexity=18.657764, train_loss=2.9262624

Batch 143960, train_perplexity=18.657764, train_loss=2.9262624

Batch 143970, train_perplexity=18.657764, train_loss=2.9262624

Batch 143980, train_perplexity=18.657764, train_loss=2.9262624

Batch 143990, train_perplexity=18.657764, train_loss=2.9262624

Batch 144000, train_perplexity=18.657764, train_loss=2.9262624

Batch 144010, train_perplexity=18.657764, train_loss=2.9262624

Batch 144020, train_perplexity=18.657764, train_loss=2.9262624

Batch 144030, train_perplexity=18.657764, train_loss=2.9262624

Batch 144040, train_perplexity=18.657764, train_loss=2.9262624

Batch 144050, train_perplexity=18.657764, train_loss=2.9262624

Batch 144060, train_perplexity=18.657764, train_loss=2.9262624

Batch 144070, train_perplexity=18.657764, train_loss=2.9262624

Batch 144080, train_perplexity=18.65776, train_loss=2.9262621

Batch 144090, train_perplexity=18.657755, train_loss=2.926262

Batch 144100, train_perplexity=18.657764, train_loss=2.9262624

Batch 144110, train_perplexity=18.657764, train_loss=2.9262624

Batch 144120, train_perplexity=18.65776, train_loss=2.9262621

Batch 144130, train_perplexity=18.657768, train_loss=2.9262626

Batch 144140, train_perplexity=18.657755, train_loss=2.926262

Batch 144150, train_perplexity=18.65776, train_loss=2.9262621

Batch 144160, train_perplexity=18.65776, train_loss=2.9262621

Batch 144170, train_perplexity=18.65776, train_loss=2.9262621

Batch 144180, train_perplexity=18.657755, train_loss=2.926262

Batch 144190, train_perplexity=18.657755, train_loss=2.926262

Batch 144200, train_perplexity=18.657755, train_loss=2.926262

Batch 144210, train_perplexity=18.657764, train_loss=2.9262624

Batch 144220, train_perplexity=18.657755, train_loss=2.926262

Batch 144230, train_perplexity=18.657755, train_loss=2.926262

Batch 144240, train_perplexity=18.65776, train_loss=2.9262621

Batch 144250, train_perplexity=18.65776, train_loss=2.9262621

Batch 144260, train_perplexity=18.657755, train_loss=2.926262

Batch 144270, train_perplexity=18.657755, train_loss=2.926262

Batch 144280, train_perplexity=18.657755, train_loss=2.926262

Batch 144290, train_perplexity=18.657751, train_loss=2.9262617

Batch 144300, train_perplexity=18.657755, train_loss=2.926262

Batch 144310, train_perplexity=18.657755, train_loss=2.926262

Batch 144320, train_perplexity=18.657755, train_loss=2.926262

Batch 144330, train_perplexity=18.657755, train_loss=2.926262

Batch 144340, train_perplexity=18.657755, train_loss=2.926262

Batch 144350, train_perplexity=18.657755, train_loss=2.926262

Batch 144360, train_perplexity=18.657751, train_loss=2.9262617

Batch 144370, train_perplexity=18.657755, train_loss=2.926262

Batch 144380, train_perplexity=18.657751, train_loss=2.9262617

Batch 144390, train_perplexity=18.657755, train_loss=2.926262

Batch 144400, train_perplexity=18.657755, train_loss=2.926262

Batch 144410, train_perplexity=18.657755, train_loss=2.926262

Batch 144420, train_perplexity=18.657755, train_loss=2.926262

Batch 144430, train_perplexity=18.657747, train_loss=2.9262614

Batch 144440, train_perplexity=18.657755, train_loss=2.926262

Batch 144450, train_perplexity=18.657751, train_loss=2.9262617

Batch 144460, train_perplexity=18.657747, train_loss=2.9262614

Batch 144470, train_perplexity=18.657755, train_loss=2.926262

Batch 144480, train_perplexity=18.657755, train_loss=2.926262

Batch 144490, train_perplexity=18.657747, train_loss=2.9262614

Batch 144500, train_perplexity=18.657747, train_loss=2.9262614

Batch 144510, train_perplexity=18.657755, train_loss=2.926262

Batch 144520, train_perplexity=18.657747, train_loss=2.9262614

Batch 144530, train_perplexity=18.657755, train_loss=2.926262

Batch 144540, train_perplexity=18.657747, train_loss=2.9262614

Batch 144550, train_perplexity=18.657747, train_loss=2.9262614

Batch 144560, train_perplexity=18.657742, train_loss=2.9262612

Batch 144570, train_perplexity=18.657747, train_loss=2.9262614

Batch 144580, train_perplexity=18.657747, train_loss=2.9262614

Batch 144590, train_perplexity=18.657747, train_loss=2.9262614

Batch 144600, train_perplexity=18.657747, train_loss=2.9262614

Batch 144610, train_perplexity=18.657747, train_loss=2.9262614

Batch 144620, train_perplexity=18.657747, train_loss=2.9262614

Batch 144630, train_perplexity=18.657747, train_loss=2.9262614

Batch 144640, train_perplexity=18.657747, train_loss=2.9262614

Batch 144650, train_perplexity=18.657742, train_loss=2.9262612

Batch 144660, train_perplexity=18.657747, train_loss=2.9262614

Batch 144670, train_perplexity=18.657747, train_loss=2.9262614
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 144680, train_perplexity=18.657747, train_loss=2.9262614

Batch 144690, train_perplexity=18.657747, train_loss=2.9262614

Batch 144700, train_perplexity=18.657747, train_loss=2.9262614

Batch 144710, train_perplexity=18.657742, train_loss=2.9262612

Batch 144720, train_perplexity=18.657742, train_loss=2.9262612

Batch 144730, train_perplexity=18.657747, train_loss=2.9262614

Batch 144740, train_perplexity=18.657747, train_loss=2.9262614

Batch 144750, train_perplexity=18.657742, train_loss=2.9262612

Batch 144760, train_perplexity=18.657742, train_loss=2.9262612

Batch 144770, train_perplexity=18.657738, train_loss=2.926261

Batch 144780, train_perplexity=18.657742, train_loss=2.9262612

Batch 144790, train_perplexity=18.657747, train_loss=2.9262614

Batch 144800, train_perplexity=18.657738, train_loss=2.926261

Batch 144810, train_perplexity=18.657738, train_loss=2.926261

Batch 144820, train_perplexity=18.657742, train_loss=2.9262612

Batch 144830, train_perplexity=18.657738, train_loss=2.926261

Batch 144840, train_perplexity=18.657738, train_loss=2.926261

Batch 144850, train_perplexity=18.657738, train_loss=2.926261

Batch 144860, train_perplexity=18.657738, train_loss=2.926261

Batch 144870, train_perplexity=18.657738, train_loss=2.926261

Batch 144880, train_perplexity=18.657742, train_loss=2.9262612

Batch 144890, train_perplexity=18.657738, train_loss=2.926261

Batch 144900, train_perplexity=18.657738, train_loss=2.926261

Batch 144910, train_perplexity=18.657738, train_loss=2.926261

Batch 144920, train_perplexity=18.657738, train_loss=2.926261

Batch 144930, train_perplexity=18.657738, train_loss=2.926261

Batch 144940, train_perplexity=18.657738, train_loss=2.926261

Batch 144950, train_perplexity=18.657734, train_loss=2.9262607

Batch 144960, train_perplexity=18.657738, train_loss=2.926261

Batch 144970, train_perplexity=18.657738, train_loss=2.926261

Batch 144980, train_perplexity=18.657728, train_loss=2.9262605

Batch 144990, train_perplexity=18.657738, train_loss=2.926261

Batch 145000, train_perplexity=18.657728, train_loss=2.9262605

Batch 145010, train_perplexity=18.657734, train_loss=2.9262607

Batch 145020, train_perplexity=18.657728, train_loss=2.9262605

Batch 145030, train_perplexity=18.657738, train_loss=2.926261

Batch 145040, train_perplexity=18.657728, train_loss=2.9262605

Batch 145050, train_perplexity=18.657728, train_loss=2.9262605

Batch 145060, train_perplexity=18.657734, train_loss=2.9262607

Batch 145070, train_perplexity=18.657734, train_loss=2.9262607

Batch 145080, train_perplexity=18.657728, train_loss=2.9262605

Batch 145090, train_perplexity=18.657734, train_loss=2.9262607

Batch 145100, train_perplexity=18.657738, train_loss=2.926261

Batch 145110, train_perplexity=18.657738, train_loss=2.926261

Batch 145120, train_perplexity=18.657728, train_loss=2.9262605

Batch 145130, train_perplexity=18.657728, train_loss=2.9262605

Batch 145140, train_perplexity=18.657734, train_loss=2.9262607

Batch 145150, train_perplexity=18.657734, train_loss=2.9262607

Batch 145160, train_perplexity=18.657734, train_loss=2.9262607

Batch 145170, train_perplexity=18.657728, train_loss=2.9262605

Batch 145180, train_perplexity=18.657728, train_loss=2.9262605

Batch 145190, train_perplexity=18.657728, train_loss=2.9262605

Batch 145200, train_perplexity=18.657734, train_loss=2.9262607

Batch 145210, train_perplexity=18.657728, train_loss=2.9262605

Batch 145220, train_perplexity=18.657724, train_loss=2.9262602

Batch 145230, train_perplexity=18.657728, train_loss=2.9262605

Batch 145240, train_perplexity=18.657728, train_loss=2.9262605

Batch 145250, train_perplexity=18.65772, train_loss=2.92626

Batch 145260, train_perplexity=18.657728, train_loss=2.9262605

Batch 145270, train_perplexity=18.657724, train_loss=2.9262602

Batch 145280, train_perplexity=18.657728, train_loss=2.9262605

Batch 145290, train_perplexity=18.657728, train_loss=2.9262605

Batch 145300, train_perplexity=18.657728, train_loss=2.9262605

Batch 145310, train_perplexity=18.65772, train_loss=2.92626

Batch 145320, train_perplexity=18.657724, train_loss=2.9262602

Batch 145330, train_perplexity=18.657724, train_loss=2.9262602

Batch 145340, train_perplexity=18.65772, train_loss=2.92626

Batch 145350, train_perplexity=18.657728, train_loss=2.9262605

Batch 145360, train_perplexity=18.657728, train_loss=2.9262605

Batch 145370, train_perplexity=18.65772, train_loss=2.92626

Batch 145380, train_perplexity=18.65772, train_loss=2.92626

Batch 145390, train_perplexity=18.657724, train_loss=2.9262602

Batch 145400, train_perplexity=18.657724, train_loss=2.9262602

Batch 145410, train_perplexity=18.65772, train_loss=2.92626

Batch 145420, train_perplexity=18.657724, train_loss=2.9262602

Batch 145430, train_perplexity=18.657724, train_loss=2.9262602

Batch 145440, train_perplexity=18.657724, train_loss=2.9262602

Batch 145450, train_perplexity=18.657728, train_loss=2.9262605

Batch 145460, train_perplexity=18.65772, train_loss=2.92626

Batch 145470, train_perplexity=18.65772, train_loss=2.92626

Batch 145480, train_perplexity=18.65772, train_loss=2.92626

Batch 145490, train_perplexity=18.657724, train_loss=2.9262602

Batch 145500, train_perplexity=18.65772, train_loss=2.92626

Batch 145510, train_perplexity=18.65772, train_loss=2.92626

Batch 145520, train_perplexity=18.657715, train_loss=2.9262598

Batch 145530, train_perplexity=18.65772, train_loss=2.92626

Batch 145540, train_perplexity=18.65772, train_loss=2.92626

Batch 145550, train_perplexity=18.65772, train_loss=2.92626

Batch 145560, train_perplexity=18.65772, train_loss=2.92626

Batch 145570, train_perplexity=18.65772, train_loss=2.92626

Batch 145580, train_perplexity=18.657711, train_loss=2.9262595

Batch 145590, train_perplexity=18.65772, train_loss=2.92626

Batch 145600, train_perplexity=18.65772, train_loss=2.92626

Batch 145610, train_perplexity=18.65772, train_loss=2.92626

Batch 145620, train_perplexity=18.65772, train_loss=2.92626

Batch 145630, train_perplexity=18.65772, train_loss=2.92626

Batch 145640, train_perplexity=18.657715, train_loss=2.9262598

Batch 145650, train_perplexity=18.65772, train_loss=2.92626

Batch 145660, train_perplexity=18.65772, train_loss=2.92626

Batch 145670, train_perplexity=18.65772, train_loss=2.92626

Batch 145680, train_perplexity=18.657715, train_loss=2.9262598

Batch 145690, train_perplexity=18.657715, train_loss=2.9262598

Batch 145700, train_perplexity=18.657711, train_loss=2.9262595

Batch 145710, train_perplexity=18.657711, train_loss=2.9262595

Batch 145720, train_perplexity=18.657711, train_loss=2.9262595

Batch 145730, train_perplexity=18.657715, train_loss=2.9262598

Batch 145740, train_perplexity=18.657711, train_loss=2.9262595

Batch 145750, train_perplexity=18.657711, train_loss=2.9262595

Batch 145760, train_perplexity=18.657711, train_loss=2.9262595

Batch 145770, train_perplexity=18.657711, train_loss=2.9262595

Batch 145780, train_perplexity=18.657711, train_loss=2.9262595

Batch 145790, train_perplexity=18.657711, train_loss=2.9262595

Batch 145800, train_perplexity=18.657711, train_loss=2.9262595

Batch 145810, train_perplexity=18.657711, train_loss=2.9262595

Batch 145820, train_perplexity=18.65772, train_loss=2.92626

Batch 145830, train_perplexity=18.657711, train_loss=2.9262595

Batch 145840, train_perplexity=18.657711, train_loss=2.9262595

Batch 145850, train_perplexity=18.657711, train_loss=2.9262595

Batch 145860, train_perplexity=18.657711, train_loss=2.9262595

Batch 145870, train_perplexity=18.657711, train_loss=2.9262595

Batch 145880, train_perplexity=18.657711, train_loss=2.9262595

Batch 145890, train_perplexity=18.657707, train_loss=2.9262593

Batch 145900, train_perplexity=18.657701, train_loss=2.926259

Batch 145910, train_perplexity=18.657707, train_loss=2.9262593

Batch 145920, train_perplexity=18.657711, train_loss=2.9262595

Batch 145930, train_perplexity=18.657711, train_loss=2.9262595

Batch 145940, train_perplexity=18.657711, train_loss=2.9262595

Batch 145950, train_perplexity=18.657711, train_loss=2.9262595

Batch 145960, train_perplexity=18.657707, train_loss=2.9262593

Batch 145970, train_perplexity=18.657711, train_loss=2.9262595
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 145980, train_perplexity=18.657701, train_loss=2.926259

Batch 145990, train_perplexity=18.657707, train_loss=2.9262593

Batch 146000, train_perplexity=18.657707, train_loss=2.9262593

Batch 146010, train_perplexity=18.657707, train_loss=2.9262593

Batch 146020, train_perplexity=18.657707, train_loss=2.9262593

Batch 146030, train_perplexity=18.657707, train_loss=2.9262593

Batch 146040, train_perplexity=18.657701, train_loss=2.926259

Batch 146050, train_perplexity=18.657701, train_loss=2.926259

Batch 146060, train_perplexity=18.657707, train_loss=2.9262593

Batch 146070, train_perplexity=18.657701, train_loss=2.926259

Batch 146080, train_perplexity=18.657701, train_loss=2.926259

Batch 146090, train_perplexity=18.657701, train_loss=2.926259

Batch 146100, train_perplexity=18.657711, train_loss=2.9262595

Batch 146110, train_perplexity=18.657701, train_loss=2.926259

Batch 146120, train_perplexity=18.657701, train_loss=2.926259

Batch 146130, train_perplexity=18.657701, train_loss=2.926259

Batch 146140, train_perplexity=18.657701, train_loss=2.926259

Batch 146150, train_perplexity=18.657701, train_loss=2.926259

Batch 146160, train_perplexity=18.657698, train_loss=2.9262588

Batch 146170, train_perplexity=18.657701, train_loss=2.926259

Batch 146180, train_perplexity=18.657701, train_loss=2.926259

Batch 146190, train_perplexity=18.657701, train_loss=2.926259

Batch 146200, train_perplexity=18.657698, train_loss=2.9262588

Batch 146210, train_perplexity=18.657701, train_loss=2.926259

Batch 146220, train_perplexity=18.657701, train_loss=2.926259

Batch 146230, train_perplexity=18.657701, train_loss=2.926259

Batch 146240, train_perplexity=18.657701, train_loss=2.926259

Batch 146250, train_perplexity=18.657701, train_loss=2.926259

Batch 146260, train_perplexity=18.657694, train_loss=2.9262586

Batch 146270, train_perplexity=18.657701, train_loss=2.926259

Batch 146280, train_perplexity=18.657698, train_loss=2.9262588

Batch 146290, train_perplexity=18.657698, train_loss=2.9262588

Batch 146300, train_perplexity=18.657701, train_loss=2.926259

Batch 146310, train_perplexity=18.657694, train_loss=2.9262586

Batch 146320, train_perplexity=18.657694, train_loss=2.9262586

Batch 146330, train_perplexity=18.657701, train_loss=2.926259

Batch 146340, train_perplexity=18.657694, train_loss=2.9262586

Batch 146350, train_perplexity=18.657694, train_loss=2.9262586

Batch 146360, train_perplexity=18.657698, train_loss=2.9262588

Batch 146370, train_perplexity=18.657698, train_loss=2.9262588

Batch 146380, train_perplexity=18.657694, train_loss=2.9262586

Batch 146390, train_perplexity=18.657698, train_loss=2.9262588

Batch 146400, train_perplexity=18.657694, train_loss=2.9262586

Batch 146410, train_perplexity=18.657694, train_loss=2.9262586

Batch 146420, train_perplexity=18.657694, train_loss=2.9262586

Batch 146430, train_perplexity=18.657694, train_loss=2.9262586

Batch 146440, train_perplexity=18.657694, train_loss=2.9262586

Batch 146450, train_perplexity=18.657688, train_loss=2.9262583

Batch 146460, train_perplexity=18.657688, train_loss=2.9262583

Batch 146470, train_perplexity=18.657694, train_loss=2.9262586

Batch 146480, train_perplexity=18.657694, train_loss=2.9262586

Batch 146490, train_perplexity=18.657694, train_loss=2.9262586

Batch 146500, train_perplexity=18.657694, train_loss=2.9262586

Batch 146510, train_perplexity=18.657694, train_loss=2.9262586

Batch 146520, train_perplexity=18.657688, train_loss=2.9262583

Batch 146530, train_perplexity=18.657694, train_loss=2.9262586

Batch 146540, train_perplexity=18.657694, train_loss=2.9262586

Batch 146550, train_perplexity=18.657688, train_loss=2.9262583

Batch 146560, train_perplexity=18.657688, train_loss=2.9262583

Batch 146570, train_perplexity=18.657694, train_loss=2.9262586

Batch 146580, train_perplexity=18.657694, train_loss=2.9262586

Batch 146590, train_perplexity=18.657694, train_loss=2.9262586

Batch 146600, train_perplexity=18.657684, train_loss=2.926258

Batch 146610, train_perplexity=18.657688, train_loss=2.9262583

Batch 146620, train_perplexity=18.657694, train_loss=2.9262586

Batch 146630, train_perplexity=18.657694, train_loss=2.9262586

Batch 146640, train_perplexity=18.657684, train_loss=2.926258

Batch 146650, train_perplexity=18.657688, train_loss=2.9262583

Batch 146660, train_perplexity=18.657684, train_loss=2.926258

Batch 146670, train_perplexity=18.657684, train_loss=2.926258

Batch 146680, train_perplexity=18.657684, train_loss=2.926258

Batch 146690, train_perplexity=18.657684, train_loss=2.926258

Batch 146700, train_perplexity=18.657684, train_loss=2.926258

Batch 146710, train_perplexity=18.657684, train_loss=2.926258

Batch 146720, train_perplexity=18.657684, train_loss=2.926258

Batch 146730, train_perplexity=18.657688, train_loss=2.9262583

Batch 146740, train_perplexity=18.65768, train_loss=2.9262578

Batch 146750, train_perplexity=18.65768, train_loss=2.9262578

Batch 146760, train_perplexity=18.657684, train_loss=2.926258

Batch 146770, train_perplexity=18.65768, train_loss=2.9262578

Batch 146780, train_perplexity=18.65768, train_loss=2.9262578

Batch 146790, train_perplexity=18.657684, train_loss=2.926258

Batch 146800, train_perplexity=18.657675, train_loss=2.9262576

Batch 146810, train_perplexity=18.65768, train_loss=2.9262578

Batch 146820, train_perplexity=18.657684, train_loss=2.926258

Batch 146830, train_perplexity=18.657684, train_loss=2.926258

Batch 146840, train_perplexity=18.65768, train_loss=2.9262578

Batch 146850, train_perplexity=18.657684, train_loss=2.926258

Batch 146860, train_perplexity=18.657675, train_loss=2.9262576

Batch 146870, train_perplexity=18.657675, train_loss=2.9262576

Batch 146880, train_perplexity=18.65768, train_loss=2.9262578

Batch 146890, train_perplexity=18.65768, train_loss=2.9262578

Batch 146900, train_perplexity=18.657675, train_loss=2.9262576

Batch 146910, train_perplexity=18.65768, train_loss=2.9262578

Batch 146920, train_perplexity=18.657675, train_loss=2.9262576

Batch 146930, train_perplexity=18.657675, train_loss=2.9262576

Batch 146940, train_perplexity=18.657675, train_loss=2.9262576

Batch 146950, train_perplexity=18.657675, train_loss=2.9262576

Batch 146960, train_perplexity=18.657675, train_loss=2.9262576

Batch 146970, train_perplexity=18.657675, train_loss=2.9262576

Batch 146980, train_perplexity=18.657675, train_loss=2.9262576

Batch 146990, train_perplexity=18.657675, train_loss=2.9262576

Batch 147000, train_perplexity=18.657675, train_loss=2.9262576

Batch 147010, train_perplexity=18.657675, train_loss=2.9262576

Batch 147020, train_perplexity=18.657667, train_loss=2.9262571

Batch 147030, train_perplexity=18.657675, train_loss=2.9262576

Batch 147040, train_perplexity=18.657675, train_loss=2.9262576

Batch 147050, train_perplexity=18.657675, train_loss=2.9262576

Batch 147060, train_perplexity=18.657675, train_loss=2.9262576

Batch 147070, train_perplexity=18.657675, train_loss=2.9262576

Batch 147080, train_perplexity=18.657667, train_loss=2.9262571

Batch 147090, train_perplexity=18.657667, train_loss=2.9262571

Batch 147100, train_perplexity=18.657675, train_loss=2.9262576

Batch 147110, train_perplexity=18.657675, train_loss=2.9262576

Batch 147120, train_perplexity=18.657667, train_loss=2.9262571

Batch 147130, train_perplexity=18.657667, train_loss=2.9262571

Batch 147140, train_perplexity=18.657667, train_loss=2.9262571

Batch 147150, train_perplexity=18.657667, train_loss=2.9262571

Batch 147160, train_perplexity=18.657667, train_loss=2.9262571

Batch 147170, train_perplexity=18.657667, train_loss=2.9262571

Batch 147180, train_perplexity=18.657667, train_loss=2.9262571

Batch 147190, train_perplexity=18.657667, train_loss=2.9262571

Batch 147200, train_perplexity=18.657667, train_loss=2.9262571

Batch 147210, train_perplexity=18.657667, train_loss=2.9262571

Batch 147220, train_perplexity=18.657667, train_loss=2.9262571

Batch 147230, train_perplexity=18.657667, train_loss=2.9262571

Batch 147240, train_perplexity=18.657667, train_loss=2.9262571

Batch 147250, train_perplexity=18.657661, train_loss=2.926257

Batch 147260, train_perplexity=18.657667, train_loss=2.9262571
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 147270, train_perplexity=18.657667, train_loss=2.9262571

Batch 147280, train_perplexity=18.657667, train_loss=2.9262571

Batch 147290, train_perplexity=18.657667, train_loss=2.9262571

Batch 147300, train_perplexity=18.657661, train_loss=2.926257

Batch 147310, train_perplexity=18.657667, train_loss=2.9262571

Batch 147320, train_perplexity=18.657661, train_loss=2.926257

Batch 147330, train_perplexity=18.657667, train_loss=2.9262571

Batch 147340, train_perplexity=18.657667, train_loss=2.9262571

Batch 147350, train_perplexity=18.657658, train_loss=2.9262567

Batch 147360, train_perplexity=18.657661, train_loss=2.926257

Batch 147370, train_perplexity=18.657661, train_loss=2.926257

Batch 147380, train_perplexity=18.657667, train_loss=2.9262571

Batch 147390, train_perplexity=18.657658, train_loss=2.9262567

Batch 147400, train_perplexity=18.657658, train_loss=2.9262567

Batch 147410, train_perplexity=18.657661, train_loss=2.926257

Batch 147420, train_perplexity=18.657658, train_loss=2.9262567

Batch 147430, train_perplexity=18.657658, train_loss=2.9262567

Batch 147440, train_perplexity=18.657658, train_loss=2.9262567

Batch 147450, train_perplexity=18.657658, train_loss=2.9262567

Batch 147460, train_perplexity=18.657658, train_loss=2.9262567

Batch 147470, train_perplexity=18.657658, train_loss=2.9262567

Batch 147480, train_perplexity=18.657661, train_loss=2.926257

Batch 147490, train_perplexity=18.657658, train_loss=2.9262567

Batch 147500, train_perplexity=18.657658, train_loss=2.9262567

Batch 147510, train_perplexity=18.657658, train_loss=2.9262567

Batch 147520, train_perplexity=18.657658, train_loss=2.9262567

Batch 147530, train_perplexity=18.657658, train_loss=2.9262567

Batch 147540, train_perplexity=18.657661, train_loss=2.926257

Batch 147550, train_perplexity=18.657654, train_loss=2.9262564

Batch 147560, train_perplexity=18.657658, train_loss=2.9262567

Batch 147570, train_perplexity=18.657658, train_loss=2.9262567

Batch 147580, train_perplexity=18.657654, train_loss=2.9262564

Batch 147590, train_perplexity=18.657658, train_loss=2.9262567

Batch 147600, train_perplexity=18.657654, train_loss=2.9262564

Batch 147610, train_perplexity=18.657658, train_loss=2.9262567

Batch 147620, train_perplexity=18.657648, train_loss=2.9262562

Batch 147630, train_perplexity=18.657658, train_loss=2.9262567

Batch 147640, train_perplexity=18.657654, train_loss=2.9262564

Batch 147650, train_perplexity=18.657658, train_loss=2.9262567

Batch 147660, train_perplexity=18.657648, train_loss=2.9262562

Batch 147670, train_perplexity=18.657658, train_loss=2.9262567

Batch 147680, train_perplexity=18.657648, train_loss=2.9262562

Batch 147690, train_perplexity=18.657658, train_loss=2.9262567

Batch 147700, train_perplexity=18.657648, train_loss=2.9262562

Batch 147710, train_perplexity=18.657654, train_loss=2.9262564

Batch 147720, train_perplexity=18.657648, train_loss=2.9262562

Batch 147730, train_perplexity=18.657654, train_loss=2.9262564

Batch 147740, train_perplexity=18.657648, train_loss=2.9262562

Batch 147750, train_perplexity=18.657648, train_loss=2.9262562

Batch 147760, train_perplexity=18.657648, train_loss=2.9262562

Batch 147770, train_perplexity=18.657648, train_loss=2.9262562

Batch 147780, train_perplexity=18.657648, train_loss=2.9262562

Batch 147790, train_perplexity=18.657648, train_loss=2.9262562

Batch 147800, train_perplexity=18.657648, train_loss=2.9262562

Batch 147810, train_perplexity=18.657648, train_loss=2.9262562

Batch 147820, train_perplexity=18.657648, train_loss=2.9262562

Batch 147830, train_perplexity=18.657648, train_loss=2.9262562

Batch 147840, train_perplexity=18.657648, train_loss=2.9262562

Batch 147850, train_perplexity=18.657648, train_loss=2.9262562

Batch 147860, train_perplexity=18.657648, train_loss=2.9262562

Batch 147870, train_perplexity=18.657648, train_loss=2.9262562

Batch 147880, train_perplexity=18.657648, train_loss=2.9262562

Batch 147890, train_perplexity=18.657644, train_loss=2.926256

Batch 147900, train_perplexity=18.657648, train_loss=2.9262562

Batch 147910, train_perplexity=18.657648, train_loss=2.9262562

Batch 147920, train_perplexity=18.657648, train_loss=2.9262562

Batch 147930, train_perplexity=18.657648, train_loss=2.9262562

Batch 147940, train_perplexity=18.65764, train_loss=2.9262557

Batch 147950, train_perplexity=18.65764, train_loss=2.9262557

Batch 147960, train_perplexity=18.657644, train_loss=2.926256

Batch 147970, train_perplexity=18.657648, train_loss=2.9262562

Batch 147980, train_perplexity=18.65764, train_loss=2.9262557

Batch 147990, train_perplexity=18.657644, train_loss=2.926256

Batch 148000, train_perplexity=18.657648, train_loss=2.9262562

Batch 148010, train_perplexity=18.657648, train_loss=2.9262562

Batch 148020, train_perplexity=18.65764, train_loss=2.9262557

Batch 148030, train_perplexity=18.65764, train_loss=2.9262557

Batch 148040, train_perplexity=18.65764, train_loss=2.9262557

Batch 148050, train_perplexity=18.65764, train_loss=2.9262557

Batch 148060, train_perplexity=18.65764, train_loss=2.9262557

Batch 148070, train_perplexity=18.65764, train_loss=2.9262557

Batch 148080, train_perplexity=18.65764, train_loss=2.9262557

Batch 148090, train_perplexity=18.65764, train_loss=2.9262557

Batch 148100, train_perplexity=18.65764, train_loss=2.9262557

Batch 148110, train_perplexity=18.65764, train_loss=2.9262557

Batch 148120, train_perplexity=18.657635, train_loss=2.9262555

Batch 148130, train_perplexity=18.65764, train_loss=2.9262557

Batch 148140, train_perplexity=18.657635, train_loss=2.9262555

Batch 148150, train_perplexity=18.65764, train_loss=2.9262557

Batch 148160, train_perplexity=18.65764, train_loss=2.9262557

Batch 148170, train_perplexity=18.65764, train_loss=2.9262557

Batch 148180, train_perplexity=18.65764, train_loss=2.9262557

Batch 148190, train_perplexity=18.657635, train_loss=2.9262555

Batch 148200, train_perplexity=18.65763, train_loss=2.9262552

Batch 148210, train_perplexity=18.65764, train_loss=2.9262557

Batch 148220, train_perplexity=18.657635, train_loss=2.9262555

Batch 148230, train_perplexity=18.657635, train_loss=2.9262555

Batch 148240, train_perplexity=18.65764, train_loss=2.9262557

Batch 148250, train_perplexity=18.657635, train_loss=2.9262555

Batch 148260, train_perplexity=18.65763, train_loss=2.9262552

Batch 148270, train_perplexity=18.65763, train_loss=2.9262552

Batch 148280, train_perplexity=18.657635, train_loss=2.9262555

Batch 148290, train_perplexity=18.657635, train_loss=2.9262555

Batch 148300, train_perplexity=18.657635, train_loss=2.9262555

Batch 148310, train_perplexity=18.65763, train_loss=2.9262552

Batch 148320, train_perplexity=18.65763, train_loss=2.9262552

Batch 148330, train_perplexity=18.657635, train_loss=2.9262555

Batch 148340, train_perplexity=18.65763, train_loss=2.9262552

Batch 148350, train_perplexity=18.65763, train_loss=2.9262552

Batch 148360, train_perplexity=18.65763, train_loss=2.9262552

Batch 148370, train_perplexity=18.65763, train_loss=2.9262552

Batch 148380, train_perplexity=18.65763, train_loss=2.9262552

Batch 148390, train_perplexity=18.65763, train_loss=2.9262552

Batch 148400, train_perplexity=18.65763, train_loss=2.9262552

Batch 148410, train_perplexity=18.65763, train_loss=2.9262552

Batch 148420, train_perplexity=18.65763, train_loss=2.9262552

Batch 148430, train_perplexity=18.65763, train_loss=2.9262552

Batch 148440, train_perplexity=18.65763, train_loss=2.9262552

Batch 148450, train_perplexity=18.657627, train_loss=2.926255

Batch 148460, train_perplexity=18.657627, train_loss=2.926255

Batch 148470, train_perplexity=18.65763, train_loss=2.9262552

Batch 148480, train_perplexity=18.657627, train_loss=2.926255

Batch 148490, train_perplexity=18.65763, train_loss=2.9262552

Batch 148500, train_perplexity=18.657627, train_loss=2.926255

Batch 148510, train_perplexity=18.65763, train_loss=2.9262552

Batch 148520, train_perplexity=18.65763, train_loss=2.9262552

Batch 148530, train_perplexity=18.65763, train_loss=2.9262552

Batch 148540, train_perplexity=18.65763, train_loss=2.9262552

Batch 148550, train_perplexity=18.657627, train_loss=2.926255
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 148560, train_perplexity=18.657621, train_loss=2.9262547

Batch 148570, train_perplexity=18.65763, train_loss=2.9262552

Batch 148580, train_perplexity=18.657621, train_loss=2.9262547

Batch 148590, train_perplexity=18.657621, train_loss=2.9262547

Batch 148600, train_perplexity=18.657621, train_loss=2.9262547

Batch 148610, train_perplexity=18.657627, train_loss=2.926255

Batch 148620, train_perplexity=18.657621, train_loss=2.9262547

Batch 148630, train_perplexity=18.657621, train_loss=2.9262547

Batch 148640, train_perplexity=18.657621, train_loss=2.9262547

Batch 148650, train_perplexity=18.657621, train_loss=2.9262547

Batch 148660, train_perplexity=18.657621, train_loss=2.9262547

Batch 148670, train_perplexity=18.657621, train_loss=2.9262547

Batch 148680, train_perplexity=18.657621, train_loss=2.9262547

Batch 148690, train_perplexity=18.657621, train_loss=2.9262547

Batch 148700, train_perplexity=18.657621, train_loss=2.9262547

Batch 148710, train_perplexity=18.657621, train_loss=2.9262547

Batch 148720, train_perplexity=18.657621, train_loss=2.9262547

Batch 148730, train_perplexity=18.657621, train_loss=2.9262547

Batch 148740, train_perplexity=18.657618, train_loss=2.9262545

Batch 148750, train_perplexity=18.657618, train_loss=2.9262545

Batch 148760, train_perplexity=18.657621, train_loss=2.9262547

Batch 148770, train_perplexity=18.657618, train_loss=2.9262545

Batch 148780, train_perplexity=18.657621, train_loss=2.9262547

Batch 148790, train_perplexity=18.657618, train_loss=2.9262545

Batch 148800, train_perplexity=18.657621, train_loss=2.9262547

Batch 148810, train_perplexity=18.657621, train_loss=2.9262547

Batch 148820, train_perplexity=18.657621, train_loss=2.9262547

Batch 148830, train_perplexity=18.657621, train_loss=2.9262547

Batch 148840, train_perplexity=18.657621, train_loss=2.9262547

Batch 148850, train_perplexity=18.657618, train_loss=2.9262545

Batch 148860, train_perplexity=18.657621, train_loss=2.9262547

Batch 148870, train_perplexity=18.657618, train_loss=2.9262545

Batch 148880, train_perplexity=18.657618, train_loss=2.9262545

Batch 148890, train_perplexity=18.657618, train_loss=2.9262545

Batch 148900, train_perplexity=18.657614, train_loss=2.9262543

Batch 148910, train_perplexity=18.657614, train_loss=2.9262543

Batch 148920, train_perplexity=18.657621, train_loss=2.9262547

Batch 148930, train_perplexity=18.657614, train_loss=2.9262543

Batch 148940, train_perplexity=18.657614, train_loss=2.9262543

Batch 148950, train_perplexity=18.657614, train_loss=2.9262543

Batch 148960, train_perplexity=18.657608, train_loss=2.926254

Batch 148970, train_perplexity=18.657618, train_loss=2.9262545

Batch 148980, train_perplexity=18.657614, train_loss=2.9262543

Batch 148990, train_perplexity=18.657618, train_loss=2.9262545

Batch 149000, train_perplexity=18.657608, train_loss=2.926254

Batch 149010, train_perplexity=18.657614, train_loss=2.9262543

Batch 149020, train_perplexity=18.657608, train_loss=2.926254

Batch 149030, train_perplexity=18.657614, train_loss=2.9262543

Batch 149040, train_perplexity=18.657608, train_loss=2.926254

Batch 149050, train_perplexity=18.657614, train_loss=2.9262543

Batch 149060, train_perplexity=18.657614, train_loss=2.9262543

Batch 149070, train_perplexity=18.657614, train_loss=2.9262543

Batch 149080, train_perplexity=18.657614, train_loss=2.9262543

Batch 149090, train_perplexity=18.657614, train_loss=2.9262543

Batch 149100, train_perplexity=18.657608, train_loss=2.926254

Batch 149110, train_perplexity=18.657614, train_loss=2.9262543

Batch 149120, train_perplexity=18.657614, train_loss=2.9262543

Batch 149130, train_perplexity=18.657614, train_loss=2.9262543

Batch 149140, train_perplexity=18.657604, train_loss=2.9262538

Batch 149150, train_perplexity=18.657604, train_loss=2.9262538

Batch 149160, train_perplexity=18.657608, train_loss=2.926254

Batch 149170, train_perplexity=18.657608, train_loss=2.926254

Batch 149180, train_perplexity=18.657608, train_loss=2.926254

Batch 149190, train_perplexity=18.657604, train_loss=2.9262538

Batch 149200, train_perplexity=18.657604, train_loss=2.9262538

Batch 149210, train_perplexity=18.657604, train_loss=2.9262538

Batch 149220, train_perplexity=18.657604, train_loss=2.9262538

Batch 149230, train_perplexity=18.657604, train_loss=2.9262538

Batch 149240, train_perplexity=18.657614, train_loss=2.9262543

Batch 149250, train_perplexity=18.657604, train_loss=2.9262538

Batch 149260, train_perplexity=18.657604, train_loss=2.9262538

Batch 149270, train_perplexity=18.657604, train_loss=2.9262538

Batch 149280, train_perplexity=18.657604, train_loss=2.9262538

Batch 149290, train_perplexity=18.657604, train_loss=2.9262538

Batch 149300, train_perplexity=18.6576, train_loss=2.9262536

Batch 149310, train_perplexity=18.657604, train_loss=2.9262538

Batch 149320, train_perplexity=18.657604, train_loss=2.9262538

Batch 149330, train_perplexity=18.657604, train_loss=2.9262538

Batch 149340, train_perplexity=18.657604, train_loss=2.9262538

Batch 149350, train_perplexity=18.657604, train_loss=2.9262538

Batch 149360, train_perplexity=18.657604, train_loss=2.9262538

Batch 149370, train_perplexity=18.6576, train_loss=2.9262536

Batch 149380, train_perplexity=18.657604, train_loss=2.9262538

Batch 149390, train_perplexity=18.657604, train_loss=2.9262538

Batch 149400, train_perplexity=18.657604, train_loss=2.9262538

Batch 149410, train_perplexity=18.6576, train_loss=2.9262536

Batch 149420, train_perplexity=18.657604, train_loss=2.9262538

Batch 149430, train_perplexity=18.657595, train_loss=2.9262533

Batch 149440, train_perplexity=18.6576, train_loss=2.9262536

Batch 149450, train_perplexity=18.6576, train_loss=2.9262536

Batch 149460, train_perplexity=18.6576, train_loss=2.9262536

Batch 149470, train_perplexity=18.657595, train_loss=2.9262533

Batch 149480, train_perplexity=18.657604, train_loss=2.9262538

Batch 149490, train_perplexity=18.6576, train_loss=2.9262536

Batch 149500, train_perplexity=18.657595, train_loss=2.9262533

Batch 149510, train_perplexity=18.657604, train_loss=2.9262538

Batch 149520, train_perplexity=18.657595, train_loss=2.9262533

Batch 149530, train_perplexity=18.657595, train_loss=2.9262533

Batch 149540, train_perplexity=18.65759, train_loss=2.926253

Batch 149550, train_perplexity=18.6576, train_loss=2.9262536

Batch 149560, train_perplexity=18.657595, train_loss=2.9262533

Batch 149570, train_perplexity=18.6576, train_loss=2.9262536

Batch 149580, train_perplexity=18.6576, train_loss=2.9262536

Batch 149590, train_perplexity=18.65759, train_loss=2.926253

Batch 149600, train_perplexity=18.657595, train_loss=2.9262533

Batch 149610, train_perplexity=18.657595, train_loss=2.9262533

Batch 149620, train_perplexity=18.6576, train_loss=2.9262536

Batch 149630, train_perplexity=18.65759, train_loss=2.926253

Batch 149640, train_perplexity=18.657595, train_loss=2.9262533

Batch 149650, train_perplexity=18.65759, train_loss=2.926253

Batch 149660, train_perplexity=18.657595, train_loss=2.9262533

Batch 149670, train_perplexity=18.657595, train_loss=2.9262533

Batch 149680, train_perplexity=18.657595, train_loss=2.9262533

Batch 149690, train_perplexity=18.65759, train_loss=2.926253

Batch 149700, train_perplexity=18.657595, train_loss=2.9262533

Batch 149710, train_perplexity=18.65759, train_loss=2.926253

Batch 149720, train_perplexity=18.657587, train_loss=2.9262528

Batch 149730, train_perplexity=18.65759, train_loss=2.926253

Batch 149740, train_perplexity=18.657587, train_loss=2.9262528

Batch 149750, train_perplexity=18.657587, train_loss=2.9262528

Batch 149760, train_perplexity=18.657587, train_loss=2.9262528

Batch 149770, train_perplexity=18.657595, train_loss=2.9262533

Batch 149780, train_perplexity=18.657587, train_loss=2.9262528

Batch 149790, train_perplexity=18.657595, train_loss=2.9262533

Batch 149800, train_perplexity=18.657595, train_loss=2.9262533

Batch 149810, train_perplexity=18.657587, train_loss=2.9262528

Batch 149820, train_perplexity=18.657587, train_loss=2.9262528

Batch 149830, train_perplexity=18.657587, train_loss=2.9262528

Batch 149840, train_perplexity=18.657587, train_loss=2.9262528
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 149850, train_perplexity=18.65759, train_loss=2.926253

Batch 149860, train_perplexity=18.657587, train_loss=2.9262528

Batch 149870, train_perplexity=18.657587, train_loss=2.9262528

Batch 149880, train_perplexity=18.657587, train_loss=2.9262528

Batch 149890, train_perplexity=18.657587, train_loss=2.9262528

Batch 149900, train_perplexity=18.657587, train_loss=2.9262528

Batch 149910, train_perplexity=18.657587, train_loss=2.9262528

Batch 149920, train_perplexity=18.657587, train_loss=2.9262528

Batch 149930, train_perplexity=18.657587, train_loss=2.9262528

Batch 149940, train_perplexity=18.657581, train_loss=2.9262526

Batch 149950, train_perplexity=18.657587, train_loss=2.9262528

Batch 149960, train_perplexity=18.657587, train_loss=2.9262528

Batch 149970, train_perplexity=18.657587, train_loss=2.9262528

Batch 149980, train_perplexity=18.657587, train_loss=2.9262528

Batch 149990, train_perplexity=18.657581, train_loss=2.9262526

Batch 150000, train_perplexity=18.657587, train_loss=2.9262528

Batch 150010, train_perplexity=18.657587, train_loss=2.9262528

Batch 150020, train_perplexity=18.657581, train_loss=2.9262526

Batch 150030, train_perplexity=18.657578, train_loss=2.9262524

Batch 150040, train_perplexity=18.657587, train_loss=2.9262528

Batch 150050, train_perplexity=18.657581, train_loss=2.9262526

Batch 150060, train_perplexity=18.657587, train_loss=2.9262528

Batch 150070, train_perplexity=18.657581, train_loss=2.9262526

Batch 150080, train_perplexity=18.657578, train_loss=2.9262524

Batch 150090, train_perplexity=18.657581, train_loss=2.9262526

Batch 150100, train_perplexity=18.657581, train_loss=2.9262526

Batch 150110, train_perplexity=18.657581, train_loss=2.9262526

Batch 150120, train_perplexity=18.657587, train_loss=2.9262528

Batch 150130, train_perplexity=18.657587, train_loss=2.9262528

Batch 150140, train_perplexity=18.657578, train_loss=2.9262524

Batch 150150, train_perplexity=18.657581, train_loss=2.9262526

Batch 150160, train_perplexity=18.657578, train_loss=2.9262524

Batch 150170, train_perplexity=18.657578, train_loss=2.9262524

Batch 150180, train_perplexity=18.657578, train_loss=2.9262524

Batch 150190, train_perplexity=18.657578, train_loss=2.9262524

Batch 150200, train_perplexity=18.657578, train_loss=2.9262524

Batch 150210, train_perplexity=18.657574, train_loss=2.9262521

Batch 150220, train_perplexity=18.657581, train_loss=2.9262526

Batch 150230, train_perplexity=18.657578, train_loss=2.9262524

Batch 150240, train_perplexity=18.657578, train_loss=2.9262524

Batch 150250, train_perplexity=18.657574, train_loss=2.9262521

Batch 150260, train_perplexity=18.657574, train_loss=2.9262521

Batch 150270, train_perplexity=18.657568, train_loss=2.926252

Batch 150280, train_perplexity=18.657578, train_loss=2.9262524

Batch 150290, train_perplexity=18.657578, train_loss=2.9262524

Batch 150300, train_perplexity=18.657578, train_loss=2.9262524

Batch 150310, train_perplexity=18.657574, train_loss=2.9262521

Batch 150320, train_perplexity=18.657578, train_loss=2.9262524

Batch 150330, train_perplexity=18.657578, train_loss=2.9262524

Batch 150340, train_perplexity=18.657578, train_loss=2.9262524

Batch 150350, train_perplexity=18.657578, train_loss=2.9262524

Batch 150360, train_perplexity=18.657574, train_loss=2.9262521

Batch 150370, train_perplexity=18.657574, train_loss=2.9262521

Batch 150380, train_perplexity=18.657568, train_loss=2.926252

Batch 150390, train_perplexity=18.657574, train_loss=2.9262521

Batch 150400, train_perplexity=18.657568, train_loss=2.926252

Batch 150410, train_perplexity=18.657574, train_loss=2.9262521

Batch 150420, train_perplexity=18.657574, train_loss=2.9262521

Batch 150430, train_perplexity=18.657568, train_loss=2.926252

Batch 150440, train_perplexity=18.657568, train_loss=2.926252

Batch 150450, train_perplexity=18.657568, train_loss=2.926252

Batch 150460, train_perplexity=18.657568, train_loss=2.926252

Batch 150470, train_perplexity=18.657568, train_loss=2.926252

Batch 150480, train_perplexity=18.657578, train_loss=2.9262524

Batch 150490, train_perplexity=18.657568, train_loss=2.926252

Batch 150500, train_perplexity=18.657568, train_loss=2.926252

Batch 150510, train_perplexity=18.657568, train_loss=2.926252

Batch 150520, train_perplexity=18.657564, train_loss=2.9262516

Batch 150530, train_perplexity=18.657568, train_loss=2.926252

Batch 150540, train_perplexity=18.657568, train_loss=2.926252

Batch 150550, train_perplexity=18.657568, train_loss=2.926252

Batch 150560, train_perplexity=18.657568, train_loss=2.926252

Batch 150570, train_perplexity=18.657564, train_loss=2.9262516

Batch 150580, train_perplexity=18.657564, train_loss=2.9262516

Batch 150590, train_perplexity=18.657568, train_loss=2.926252

Batch 150600, train_perplexity=18.657564, train_loss=2.9262516

Batch 150610, train_perplexity=18.657568, train_loss=2.926252

Batch 150620, train_perplexity=18.657564, train_loss=2.9262516

Batch 150630, train_perplexity=18.657564, train_loss=2.9262516

Batch 150640, train_perplexity=18.65756, train_loss=2.9262514

Batch 150650, train_perplexity=18.657564, train_loss=2.9262516

Batch 150660, train_perplexity=18.657568, train_loss=2.926252

Batch 150670, train_perplexity=18.65756, train_loss=2.9262514

Batch 150680, train_perplexity=18.657564, train_loss=2.9262516

Batch 150690, train_perplexity=18.65756, train_loss=2.9262514

Batch 150700, train_perplexity=18.65756, train_loss=2.9262514

Batch 150710, train_perplexity=18.657568, train_loss=2.926252

Batch 150720, train_perplexity=18.65756, train_loss=2.9262514

Batch 150730, train_perplexity=18.657555, train_loss=2.9262512

Batch 150740, train_perplexity=18.65756, train_loss=2.9262514

Batch 150750, train_perplexity=18.65756, train_loss=2.9262514

Batch 150760, train_perplexity=18.65756, train_loss=2.9262514

Batch 150770, train_perplexity=18.65756, train_loss=2.9262514

Batch 150780, train_perplexity=18.65756, train_loss=2.9262514

Batch 150790, train_perplexity=18.65756, train_loss=2.9262514

Batch 150800, train_perplexity=18.657555, train_loss=2.9262512

Batch 150810, train_perplexity=18.65756, train_loss=2.9262514

Batch 150820, train_perplexity=18.65756, train_loss=2.9262514

Batch 150830, train_perplexity=18.65756, train_loss=2.9262514

Batch 150840, train_perplexity=18.657555, train_loss=2.9262512

Batch 150850, train_perplexity=18.65756, train_loss=2.9262514

Batch 150860, train_perplexity=18.65756, train_loss=2.9262514

Batch 150870, train_perplexity=18.657555, train_loss=2.9262512

Batch 150880, train_perplexity=18.65756, train_loss=2.9262514

Batch 150890, train_perplexity=18.657555, train_loss=2.9262512

Batch 150900, train_perplexity=18.657555, train_loss=2.9262512

Batch 150910, train_perplexity=18.657555, train_loss=2.9262512

Batch 150920, train_perplexity=18.657555, train_loss=2.9262512

Batch 150930, train_perplexity=18.65755, train_loss=2.926251

Batch 150940, train_perplexity=18.657555, train_loss=2.9262512

Batch 150950, train_perplexity=18.65755, train_loss=2.926251

Batch 150960, train_perplexity=18.657555, train_loss=2.9262512

Batch 150970, train_perplexity=18.657555, train_loss=2.9262512

Batch 150980, train_perplexity=18.65755, train_loss=2.926251

Batch 150990, train_perplexity=18.65756, train_loss=2.9262514

Batch 151000, train_perplexity=18.65756, train_loss=2.9262514

Batch 151010, train_perplexity=18.657555, train_loss=2.9262512

Batch 151020, train_perplexity=18.657555, train_loss=2.9262512

Batch 151030, train_perplexity=18.657555, train_loss=2.9262512

Batch 151040, train_perplexity=18.657555, train_loss=2.9262512

Batch 151050, train_perplexity=18.65755, train_loss=2.926251

Batch 151060, train_perplexity=18.65755, train_loss=2.926251

Batch 151070, train_perplexity=18.65755, train_loss=2.926251

Batch 151080, train_perplexity=18.65755, train_loss=2.926251

Batch 151090, train_perplexity=18.65755, train_loss=2.926251

Batch 151100, train_perplexity=18.65755, train_loss=2.926251

Batch 151110, train_perplexity=18.657555, train_loss=2.9262512

Batch 151120, train_perplexity=18.65755, train_loss=2.926251

Batch 151130, train_perplexity=18.65755, train_loss=2.926251
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 151140, train_perplexity=18.65755, train_loss=2.926251

Batch 151150, train_perplexity=18.65755, train_loss=2.926251

Batch 151160, train_perplexity=18.65755, train_loss=2.926251

Batch 151170, train_perplexity=18.65755, train_loss=2.926251

Batch 151180, train_perplexity=18.657547, train_loss=2.9262507

Batch 151190, train_perplexity=18.65755, train_loss=2.926251

Batch 151200, train_perplexity=18.657547, train_loss=2.9262507

Batch 151210, train_perplexity=18.65755, train_loss=2.926251

Batch 151220, train_perplexity=18.657547, train_loss=2.9262507

Batch 151230, train_perplexity=18.65755, train_loss=2.926251

Batch 151240, train_perplexity=18.657547, train_loss=2.9262507

Batch 151250, train_perplexity=18.657541, train_loss=2.9262505

Batch 151260, train_perplexity=18.657547, train_loss=2.9262507

Batch 151270, train_perplexity=18.657541, train_loss=2.9262505

Batch 151280, train_perplexity=18.65755, train_loss=2.926251

Batch 151290, train_perplexity=18.657541, train_loss=2.9262505

Batch 151300, train_perplexity=18.657547, train_loss=2.9262507

Batch 151310, train_perplexity=18.657541, train_loss=2.9262505

Batch 151320, train_perplexity=18.657547, train_loss=2.9262507

Batch 151330, train_perplexity=18.65755, train_loss=2.926251

Batch 151340, train_perplexity=18.657541, train_loss=2.9262505

Batch 151350, train_perplexity=18.657541, train_loss=2.9262505

Batch 151360, train_perplexity=18.657541, train_loss=2.9262505

Batch 151370, train_perplexity=18.657541, train_loss=2.9262505

Batch 151380, train_perplexity=18.657541, train_loss=2.9262505

Batch 151390, train_perplexity=18.657541, train_loss=2.9262505

Batch 151400, train_perplexity=18.657541, train_loss=2.9262505

Batch 151410, train_perplexity=18.657541, train_loss=2.9262505

Batch 151420, train_perplexity=18.657541, train_loss=2.9262505

Batch 151430, train_perplexity=18.657547, train_loss=2.9262507

Batch 151440, train_perplexity=18.657541, train_loss=2.9262505

Batch 151450, train_perplexity=18.657541, train_loss=2.9262505

Batch 151460, train_perplexity=18.657541, train_loss=2.9262505

Batch 151470, train_perplexity=18.657537, train_loss=2.9262502

Batch 151480, train_perplexity=18.657541, train_loss=2.9262505

Batch 151490, train_perplexity=18.657541, train_loss=2.9262505

Batch 151500, train_perplexity=18.657541, train_loss=2.9262505

Batch 151510, train_perplexity=18.657537, train_loss=2.9262502

Batch 151520, train_perplexity=18.657537, train_loss=2.9262502

Batch 151530, train_perplexity=18.657534, train_loss=2.92625

Batch 151540, train_perplexity=18.657541, train_loss=2.9262505

Batch 151550, train_perplexity=18.657537, train_loss=2.9262502

Batch 151560, train_perplexity=18.657537, train_loss=2.9262502

Batch 151570, train_perplexity=18.657541, train_loss=2.9262505

Batch 151580, train_perplexity=18.657541, train_loss=2.9262505

Batch 151590, train_perplexity=18.657541, train_loss=2.9262505

Batch 151600, train_perplexity=18.657534, train_loss=2.92625

Batch 151610, train_perplexity=18.657541, train_loss=2.9262505

Batch 151620, train_perplexity=18.657537, train_loss=2.9262502

Batch 151630, train_perplexity=18.657534, train_loss=2.92625

Batch 151640, train_perplexity=18.657537, train_loss=2.9262502

Batch 151650, train_perplexity=18.657534, train_loss=2.92625

Batch 151660, train_perplexity=18.657537, train_loss=2.9262502

Batch 151670, train_perplexity=18.657534, train_loss=2.92625

Batch 151680, train_perplexity=18.657534, train_loss=2.92625

Batch 151690, train_perplexity=18.657534, train_loss=2.92625

Batch 151700, train_perplexity=18.657534, train_loss=2.92625

Batch 151710, train_perplexity=18.657534, train_loss=2.92625

Batch 151720, train_perplexity=18.657534, train_loss=2.92625

Batch 151730, train_perplexity=18.657537, train_loss=2.9262502

Batch 151740, train_perplexity=18.657534, train_loss=2.92625

Batch 151750, train_perplexity=18.657528, train_loss=2.9262497

Batch 151760, train_perplexity=18.657534, train_loss=2.92625

Batch 151770, train_perplexity=18.657534, train_loss=2.92625

Batch 151780, train_perplexity=18.657537, train_loss=2.9262502

Batch 151790, train_perplexity=18.657528, train_loss=2.9262497

Batch 151800, train_perplexity=18.657528, train_loss=2.9262497

Batch 151810, train_perplexity=18.657534, train_loss=2.92625

Batch 151820, train_perplexity=18.657534, train_loss=2.92625

Batch 151830, train_perplexity=18.657534, train_loss=2.92625

Batch 151840, train_perplexity=18.657528, train_loss=2.9262497

Batch 151850, train_perplexity=18.657534, train_loss=2.92625

Batch 151860, train_perplexity=18.657524, train_loss=2.9262495

Batch 151870, train_perplexity=18.657528, train_loss=2.9262497

Batch 151880, train_perplexity=18.657528, train_loss=2.9262497

Batch 151890, train_perplexity=18.657534, train_loss=2.92625

Batch 151900, train_perplexity=18.657534, train_loss=2.92625

Batch 151910, train_perplexity=18.657524, train_loss=2.9262495

Batch 151920, train_perplexity=18.657534, train_loss=2.92625

Batch 151930, train_perplexity=18.657534, train_loss=2.92625

Batch 151940, train_perplexity=18.657524, train_loss=2.9262495

Batch 151950, train_perplexity=18.65752, train_loss=2.9262493

Batch 151960, train_perplexity=18.657534, train_loss=2.92625

Batch 151970, train_perplexity=18.657524, train_loss=2.9262495

Batch 151980, train_perplexity=18.657524, train_loss=2.9262495

Batch 151990, train_perplexity=18.657528, train_loss=2.9262497

Batch 152000, train_perplexity=18.657528, train_loss=2.9262497

Batch 152010, train_perplexity=18.657524, train_loss=2.9262495

Batch 152020, train_perplexity=18.657524, train_loss=2.9262495

Batch 152030, train_perplexity=18.657524, train_loss=2.9262495

Batch 152040, train_perplexity=18.65752, train_loss=2.9262493

Batch 152050, train_perplexity=18.657524, train_loss=2.9262495

Batch 152060, train_perplexity=18.657524, train_loss=2.9262495

Batch 152070, train_perplexity=18.65752, train_loss=2.9262493

Batch 152080, train_perplexity=18.657524, train_loss=2.9262495

Batch 152090, train_perplexity=18.657524, train_loss=2.9262495

Batch 152100, train_perplexity=18.65752, train_loss=2.9262493

Batch 152110, train_perplexity=18.65752, train_loss=2.9262493

Batch 152120, train_perplexity=18.65752, train_loss=2.9262493

Batch 152130, train_perplexity=18.657524, train_loss=2.9262495

Batch 152140, train_perplexity=18.657524, train_loss=2.9262495

Batch 152150, train_perplexity=18.657524, train_loss=2.9262495

Batch 152160, train_perplexity=18.65752, train_loss=2.9262493

Batch 152170, train_perplexity=18.65752, train_loss=2.9262493

Batch 152180, train_perplexity=18.65752, train_loss=2.9262493

Batch 152190, train_perplexity=18.657524, train_loss=2.9262495

Batch 152200, train_perplexity=18.657515, train_loss=2.926249

Batch 152210, train_perplexity=18.657515, train_loss=2.926249

Batch 152220, train_perplexity=18.657515, train_loss=2.926249

Batch 152230, train_perplexity=18.657515, train_loss=2.926249

Batch 152240, train_perplexity=18.657515, train_loss=2.926249

Batch 152250, train_perplexity=18.657515, train_loss=2.926249

Batch 152260, train_perplexity=18.657515, train_loss=2.926249

Batch 152270, train_perplexity=18.657515, train_loss=2.926249

Batch 152280, train_perplexity=18.657515, train_loss=2.926249

Batch 152290, train_perplexity=18.657515, train_loss=2.926249

Batch 152300, train_perplexity=18.657515, train_loss=2.926249

Batch 152310, train_perplexity=18.657515, train_loss=2.926249

Batch 152320, train_perplexity=18.657515, train_loss=2.926249

Batch 152330, train_perplexity=18.657515, train_loss=2.926249

Batch 152340, train_perplexity=18.657515, train_loss=2.926249

Batch 152350, train_perplexity=18.657515, train_loss=2.926249

Batch 152360, train_perplexity=18.657515, train_loss=2.926249

Batch 152370, train_perplexity=18.657515, train_loss=2.926249

Batch 152380, train_perplexity=18.65751, train_loss=2.9262488

Batch 152390, train_perplexity=18.657515, train_loss=2.926249

Batch 152400, train_perplexity=18.657515, train_loss=2.926249

Batch 152410, train_perplexity=18.65751, train_loss=2.9262488

Batch 152420, train_perplexity=18.65751, train_loss=2.9262488

Batch 152430, train_perplexity=18.657515, train_loss=2.926249
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 152440, train_perplexity=18.65751, train_loss=2.9262488

Batch 152450, train_perplexity=18.657507, train_loss=2.9262486

Batch 152460, train_perplexity=18.657515, train_loss=2.926249

Batch 152470, train_perplexity=18.657515, train_loss=2.926249

Batch 152480, train_perplexity=18.65751, train_loss=2.9262488

Batch 152490, train_perplexity=18.657515, train_loss=2.926249

Batch 152500, train_perplexity=18.657515, train_loss=2.926249

Batch 152510, train_perplexity=18.657515, train_loss=2.926249

Batch 152520, train_perplexity=18.65751, train_loss=2.9262488

Batch 152530, train_perplexity=18.65751, train_loss=2.9262488

Batch 152540, train_perplexity=18.657515, train_loss=2.926249

Batch 152550, train_perplexity=18.657507, train_loss=2.9262486

Batch 152560, train_perplexity=18.657507, train_loss=2.9262486

Batch 152570, train_perplexity=18.657507, train_loss=2.9262486

Batch 152580, train_perplexity=18.657515, train_loss=2.926249

Batch 152590, train_perplexity=18.657507, train_loss=2.9262486

Batch 152600, train_perplexity=18.657507, train_loss=2.9262486

Batch 152610, train_perplexity=18.657507, train_loss=2.9262486

Batch 152620, train_perplexity=18.657507, train_loss=2.9262486

Batch 152630, train_perplexity=18.657507, train_loss=2.9262486

Batch 152640, train_perplexity=18.657507, train_loss=2.9262486

Batch 152650, train_perplexity=18.657507, train_loss=2.9262486

Batch 152660, train_perplexity=18.657507, train_loss=2.9262486

Batch 152670, train_perplexity=18.657507, train_loss=2.9262486

Batch 152680, train_perplexity=18.657501, train_loss=2.9262483

Batch 152690, train_perplexity=18.657507, train_loss=2.9262486

Batch 152700, train_perplexity=18.657507, train_loss=2.9262486

Batch 152710, train_perplexity=18.657507, train_loss=2.9262486

Batch 152720, train_perplexity=18.657497, train_loss=2.926248

Batch 152730, train_perplexity=18.657501, train_loss=2.9262483

Batch 152740, train_perplexity=18.657507, train_loss=2.9262486

Batch 152750, train_perplexity=18.657507, train_loss=2.9262486

Batch 152760, train_perplexity=18.657497, train_loss=2.926248

Batch 152770, train_perplexity=18.657507, train_loss=2.9262486

Batch 152780, train_perplexity=18.657497, train_loss=2.926248

Batch 152790, train_perplexity=18.657501, train_loss=2.9262483

Batch 152800, train_perplexity=18.657501, train_loss=2.9262483

Batch 152810, train_perplexity=18.657501, train_loss=2.9262483

Batch 152820, train_perplexity=18.657501, train_loss=2.9262483

Batch 152830, train_perplexity=18.657497, train_loss=2.926248

Batch 152840, train_perplexity=18.657497, train_loss=2.926248

Batch 152850, train_perplexity=18.657497, train_loss=2.926248

Batch 152860, train_perplexity=18.657501, train_loss=2.9262483

Batch 152870, train_perplexity=18.657497, train_loss=2.926248

Batch 152880, train_perplexity=18.657497, train_loss=2.926248

Batch 152890, train_perplexity=18.657501, train_loss=2.9262483

Batch 152900, train_perplexity=18.657497, train_loss=2.926248

Batch 152910, train_perplexity=18.657507, train_loss=2.9262486

Batch 152920, train_perplexity=18.657497, train_loss=2.926248

Batch 152930, train_perplexity=18.657497, train_loss=2.926248

Batch 152940, train_perplexity=18.657497, train_loss=2.926248

Batch 152950, train_perplexity=18.657501, train_loss=2.9262483

Batch 152960, train_perplexity=18.657497, train_loss=2.926248

Batch 152970, train_perplexity=18.657497, train_loss=2.926248

Batch 152980, train_perplexity=18.657494, train_loss=2.9262478

Batch 152990, train_perplexity=18.657497, train_loss=2.926248

Batch 153000, train_perplexity=18.657497, train_loss=2.926248

Batch 153010, train_perplexity=18.657494, train_loss=2.9262478

Batch 153020, train_perplexity=18.657497, train_loss=2.926248

Batch 153030, train_perplexity=18.657494, train_loss=2.9262478

Batch 153040, train_perplexity=18.657494, train_loss=2.9262478

Batch 153050, train_perplexity=18.657494, train_loss=2.9262478

Batch 153060, train_perplexity=18.657497, train_loss=2.926248

Batch 153070, train_perplexity=18.657497, train_loss=2.926248

Batch 153080, train_perplexity=18.657494, train_loss=2.9262478

Batch 153090, train_perplexity=18.657494, train_loss=2.9262478

Batch 153100, train_perplexity=18.657494, train_loss=2.9262478

Batch 153110, train_perplexity=18.657488, train_loss=2.9262476

Batch 153120, train_perplexity=18.657494, train_loss=2.9262478

Batch 153130, train_perplexity=18.657494, train_loss=2.9262478

Batch 153140, train_perplexity=18.657494, train_loss=2.9262478

Batch 153150, train_perplexity=18.657494, train_loss=2.9262478

Batch 153160, train_perplexity=18.657494, train_loss=2.9262478

Batch 153170, train_perplexity=18.657488, train_loss=2.9262476

Batch 153180, train_perplexity=18.657494, train_loss=2.9262478

Batch 153190, train_perplexity=18.657488, train_loss=2.9262476

Batch 153200, train_perplexity=18.657494, train_loss=2.9262478

Batch 153210, train_perplexity=18.657494, train_loss=2.9262478

Batch 153220, train_perplexity=18.657488, train_loss=2.9262476

Batch 153230, train_perplexity=18.657488, train_loss=2.9262476

Batch 153240, train_perplexity=18.657484, train_loss=2.9262474

Batch 153250, train_perplexity=18.657488, train_loss=2.9262476

Batch 153260, train_perplexity=18.657488, train_loss=2.9262476

Batch 153270, train_perplexity=18.657494, train_loss=2.9262478

Batch 153280, train_perplexity=18.657497, train_loss=2.926248

Batch 153290, train_perplexity=18.657484, train_loss=2.9262474

Batch 153300, train_perplexity=18.657484, train_loss=2.9262474

Batch 153310, train_perplexity=18.657488, train_loss=2.9262476

Batch 153320, train_perplexity=18.657488, train_loss=2.9262476

Batch 153330, train_perplexity=18.657488, train_loss=2.9262476

Batch 153340, train_perplexity=18.657488, train_loss=2.9262476

Batch 153350, train_perplexity=18.657488, train_loss=2.9262476

Batch 153360, train_perplexity=18.657488, train_loss=2.9262476

Batch 153370, train_perplexity=18.657484, train_loss=2.9262474

Batch 153380, train_perplexity=18.657484, train_loss=2.9262474

Batch 153390, train_perplexity=18.657488, train_loss=2.9262476

Batch 153400, train_perplexity=18.657484, train_loss=2.9262474

Batch 153410, train_perplexity=18.65748, train_loss=2.9262471

Batch 153420, train_perplexity=18.657484, train_loss=2.9262474

Batch 153430, train_perplexity=18.657484, train_loss=2.9262474

Batch 153440, train_perplexity=18.65748, train_loss=2.9262471

Batch 153450, train_perplexity=18.657488, train_loss=2.9262476

Batch 153460, train_perplexity=18.65748, train_loss=2.9262471

Batch 153470, train_perplexity=18.65748, train_loss=2.9262471

Batch 153480, train_perplexity=18.657488, train_loss=2.9262476

Batch 153490, train_perplexity=18.65748, train_loss=2.9262471

Batch 153500, train_perplexity=18.657484, train_loss=2.9262474

Batch 153510, train_perplexity=18.65748, train_loss=2.9262471

Batch 153520, train_perplexity=18.65748, train_loss=2.9262471

Batch 153530, train_perplexity=18.65748, train_loss=2.9262471

Batch 153540, train_perplexity=18.65748, train_loss=2.9262471

Batch 153550, train_perplexity=18.65748, train_loss=2.9262471

Batch 153560, train_perplexity=18.65748, train_loss=2.9262471

Batch 153570, train_perplexity=18.65748, train_loss=2.9262471

Batch 153580, train_perplexity=18.657484, train_loss=2.9262474

Batch 153590, train_perplexity=18.65748, train_loss=2.9262471

Batch 153600, train_perplexity=18.657475, train_loss=2.926247

Batch 153610, train_perplexity=18.65748, train_loss=2.9262471

Batch 153620, train_perplexity=18.65748, train_loss=2.9262471

Batch 153630, train_perplexity=18.65748, train_loss=2.9262471

Batch 153640, train_perplexity=18.657475, train_loss=2.926247

Batch 153650, train_perplexity=18.65748, train_loss=2.9262471

Batch 153660, train_perplexity=18.657475, train_loss=2.926247

Batch 153670, train_perplexity=18.657475, train_loss=2.926247

Batch 153680, train_perplexity=18.657475, train_loss=2.926247

Batch 153690, train_perplexity=18.65748, train_loss=2.9262471

Batch 153700, train_perplexity=18.657475, train_loss=2.926247

Batch 153710, train_perplexity=18.657475, train_loss=2.926247

Batch 153720, train_perplexity=18.65748, train_loss=2.9262471
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 153730, train_perplexity=18.657475, train_loss=2.926247

Batch 153740, train_perplexity=18.65748, train_loss=2.9262471

Batch 153750, train_perplexity=18.65748, train_loss=2.9262471

Batch 153760, train_perplexity=18.65748, train_loss=2.9262471

Batch 153770, train_perplexity=18.65747, train_loss=2.9262466

Batch 153780, train_perplexity=18.657475, train_loss=2.926247

Batch 153790, train_perplexity=18.65747, train_loss=2.9262466

Batch 153800, train_perplexity=18.657475, train_loss=2.926247

Batch 153810, train_perplexity=18.657475, train_loss=2.926247

Batch 153820, train_perplexity=18.65747, train_loss=2.9262466

Batch 153830, train_perplexity=18.65748, train_loss=2.9262471

Batch 153840, train_perplexity=18.65747, train_loss=2.9262466

Batch 153850, train_perplexity=18.65747, train_loss=2.9262466

Batch 153860, train_perplexity=18.65747, train_loss=2.9262466

Batch 153870, train_perplexity=18.657467, train_loss=2.9262464

Batch 153880, train_perplexity=18.65747, train_loss=2.9262466

Batch 153890, train_perplexity=18.65747, train_loss=2.9262466

Batch 153900, train_perplexity=18.65747, train_loss=2.9262466

Batch 153910, train_perplexity=18.657467, train_loss=2.9262464

Batch 153920, train_perplexity=18.65747, train_loss=2.9262466

Batch 153930, train_perplexity=18.65747, train_loss=2.9262466

Batch 153940, train_perplexity=18.65747, train_loss=2.9262466

Batch 153950, train_perplexity=18.65747, train_loss=2.9262466

Batch 153960, train_perplexity=18.65747, train_loss=2.9262466

Batch 153970, train_perplexity=18.657467, train_loss=2.9262464

Batch 153980, train_perplexity=18.65747, train_loss=2.9262466

Batch 153990, train_perplexity=18.657467, train_loss=2.9262464

Batch 154000, train_perplexity=18.65747, train_loss=2.9262466

Batch 154010, train_perplexity=18.657467, train_loss=2.9262464

Batch 154020, train_perplexity=18.657461, train_loss=2.9262462

Batch 154030, train_perplexity=18.657467, train_loss=2.9262464

Batch 154040, train_perplexity=18.657467, train_loss=2.9262464

Batch 154050, train_perplexity=18.657461, train_loss=2.9262462

Batch 154060, train_perplexity=18.65747, train_loss=2.9262466

Batch 154070, train_perplexity=18.657467, train_loss=2.9262464

Batch 154080, train_perplexity=18.657467, train_loss=2.9262464

Batch 154090, train_perplexity=18.657467, train_loss=2.9262464

Batch 154100, train_perplexity=18.657467, train_loss=2.9262464

Batch 154110, train_perplexity=18.657461, train_loss=2.9262462

Batch 154120, train_perplexity=18.657461, train_loss=2.9262462

Batch 154130, train_perplexity=18.657461, train_loss=2.9262462

Batch 154140, train_perplexity=18.657461, train_loss=2.9262462

Batch 154150, train_perplexity=18.657461, train_loss=2.9262462

Batch 154160, train_perplexity=18.657461, train_loss=2.9262462

Batch 154170, train_perplexity=18.657454, train_loss=2.9262457

Batch 154180, train_perplexity=18.657461, train_loss=2.9262462

Batch 154190, train_perplexity=18.657461, train_loss=2.9262462

Batch 154200, train_perplexity=18.657461, train_loss=2.9262462

Batch 154210, train_perplexity=18.657461, train_loss=2.9262462

Batch 154220, train_perplexity=18.657461, train_loss=2.9262462

Batch 154230, train_perplexity=18.657461, train_loss=2.9262462

Batch 154240, train_perplexity=18.657461, train_loss=2.9262462

Batch 154250, train_perplexity=18.657461, train_loss=2.9262462

Batch 154260, train_perplexity=18.657461, train_loss=2.9262462

Batch 154270, train_perplexity=18.657461, train_loss=2.9262462

Batch 154280, train_perplexity=18.657461, train_loss=2.9262462

Batch 154290, train_perplexity=18.657457, train_loss=2.926246

Batch 154300, train_perplexity=18.657457, train_loss=2.926246

Batch 154310, train_perplexity=18.657461, train_loss=2.9262462

Batch 154320, train_perplexity=18.657461, train_loss=2.9262462

Batch 154330, train_perplexity=18.657454, train_loss=2.9262457

Batch 154340, train_perplexity=18.657461, train_loss=2.9262462

Batch 154350, train_perplexity=18.657454, train_loss=2.9262457

Batch 154360, train_perplexity=18.657457, train_loss=2.926246

Batch 154370, train_perplexity=18.657457, train_loss=2.926246

Batch 154380, train_perplexity=18.657457, train_loss=2.926246

Batch 154390, train_perplexity=18.657454, train_loss=2.9262457

Batch 154400, train_perplexity=18.657457, train_loss=2.926246

Batch 154410, train_perplexity=18.657457, train_loss=2.926246

Batch 154420, train_perplexity=18.657454, train_loss=2.9262457

Batch 154430, train_perplexity=18.657457, train_loss=2.926246

Batch 154440, train_perplexity=18.657454, train_loss=2.9262457

Batch 154450, train_perplexity=18.657454, train_loss=2.9262457

Batch 154460, train_perplexity=18.657454, train_loss=2.9262457

Batch 154470, train_perplexity=18.657454, train_loss=2.9262457

Batch 154480, train_perplexity=18.657454, train_loss=2.9262457

Batch 154490, train_perplexity=18.657454, train_loss=2.9262457

Batch 154500, train_perplexity=18.657454, train_loss=2.9262457

Batch 154510, train_perplexity=18.657457, train_loss=2.926246

Batch 154520, train_perplexity=18.657454, train_loss=2.9262457

Batch 154530, train_perplexity=18.657454, train_loss=2.9262457

Batch 154540, train_perplexity=18.657454, train_loss=2.9262457

Batch 154550, train_perplexity=18.657454, train_loss=2.9262457

Batch 154560, train_perplexity=18.657454, train_loss=2.9262457

Batch 154570, train_perplexity=18.657448, train_loss=2.9262455

Batch 154580, train_perplexity=18.657454, train_loss=2.9262457

Batch 154590, train_perplexity=18.657448, train_loss=2.9262455

Batch 154600, train_perplexity=18.657448, train_loss=2.9262455

Batch 154610, train_perplexity=18.657448, train_loss=2.9262455

Batch 154620, train_perplexity=18.657448, train_loss=2.9262455

Batch 154630, train_perplexity=18.657444, train_loss=2.9262452

Batch 154640, train_perplexity=18.657448, train_loss=2.9262455

Batch 154650, train_perplexity=18.657454, train_loss=2.9262457

Batch 154660, train_perplexity=18.657454, train_loss=2.9262457

Batch 154670, train_perplexity=18.657444, train_loss=2.9262452

Batch 154680, train_perplexity=18.657454, train_loss=2.9262457

Batch 154690, train_perplexity=18.657444, train_loss=2.9262452

Batch 154700, train_perplexity=18.657444, train_loss=2.9262452

Batch 154710, train_perplexity=18.657448, train_loss=2.9262455

Batch 154720, train_perplexity=18.657444, train_loss=2.9262452

Batch 154730, train_perplexity=18.657448, train_loss=2.9262455

Batch 154740, train_perplexity=18.657444, train_loss=2.9262452

Batch 154750, train_perplexity=18.657444, train_loss=2.9262452

Batch 154760, train_perplexity=18.657444, train_loss=2.9262452

Batch 154770, train_perplexity=18.657448, train_loss=2.9262455

Batch 154780, train_perplexity=18.657444, train_loss=2.9262452

Batch 154790, train_perplexity=18.657444, train_loss=2.9262452

Batch 154800, train_perplexity=18.657444, train_loss=2.9262452

Batch 154810, train_perplexity=18.657448, train_loss=2.9262455

Batch 154820, train_perplexity=18.657444, train_loss=2.9262452

Batch 154830, train_perplexity=18.657444, train_loss=2.9262452

Batch 154840, train_perplexity=18.65744, train_loss=2.926245

Batch 154850, train_perplexity=18.657444, train_loss=2.9262452

Batch 154860, train_perplexity=18.657444, train_loss=2.9262452

Batch 154870, train_perplexity=18.657444, train_loss=2.9262452

Batch 154880, train_perplexity=18.657444, train_loss=2.9262452

Batch 154890, train_perplexity=18.657444, train_loss=2.9262452

Batch 154900, train_perplexity=18.657444, train_loss=2.9262452

Batch 154910, train_perplexity=18.657444, train_loss=2.9262452

Batch 154920, train_perplexity=18.657434, train_loss=2.9262447

Batch 154930, train_perplexity=18.657444, train_loss=2.9262452

Batch 154940, train_perplexity=18.657434, train_loss=2.9262447

Batch 154950, train_perplexity=18.65744, train_loss=2.926245

Batch 154960, train_perplexity=18.65744, train_loss=2.926245

Batch 154970, train_perplexity=18.657444, train_loss=2.9262452

Batch 154980, train_perplexity=18.65744, train_loss=2.926245

Batch 154990, train_perplexity=18.657434, train_loss=2.9262447

Batch 155000, train_perplexity=18.65744, train_loss=2.926245

Batch 155010, train_perplexity=18.65744, train_loss=2.926245
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 155020, train_perplexity=18.657434, train_loss=2.9262447

Batch 155030, train_perplexity=18.657434, train_loss=2.9262447

Batch 155040, train_perplexity=18.657434, train_loss=2.9262447

Batch 155050, train_perplexity=18.657434, train_loss=2.9262447

Batch 155060, train_perplexity=18.657434, train_loss=2.9262447

Batch 155070, train_perplexity=18.657434, train_loss=2.9262447

Batch 155080, train_perplexity=18.65744, train_loss=2.926245

Batch 155090, train_perplexity=18.657434, train_loss=2.9262447

Batch 155100, train_perplexity=18.657434, train_loss=2.9262447

Batch 155110, train_perplexity=18.657434, train_loss=2.9262447

Batch 155120, train_perplexity=18.657434, train_loss=2.9262447

Batch 155130, train_perplexity=18.657434, train_loss=2.9262447

Batch 155140, train_perplexity=18.65744, train_loss=2.926245

Batch 155150, train_perplexity=18.657434, train_loss=2.9262447

Batch 155160, train_perplexity=18.657434, train_loss=2.9262447

Batch 155170, train_perplexity=18.657434, train_loss=2.9262447

Batch 155180, train_perplexity=18.657434, train_loss=2.9262447

Batch 155190, train_perplexity=18.65743, train_loss=2.9262445

Batch 155200, train_perplexity=18.657434, train_loss=2.9262447

Batch 155210, train_perplexity=18.657434, train_loss=2.9262447

Batch 155220, train_perplexity=18.65743, train_loss=2.9262445

Batch 155230, train_perplexity=18.65743, train_loss=2.9262445

Batch 155240, train_perplexity=18.657427, train_loss=2.9262443

Batch 155250, train_perplexity=18.65743, train_loss=2.9262445

Batch 155260, train_perplexity=18.65743, train_loss=2.9262445

Batch 155270, train_perplexity=18.65743, train_loss=2.9262445

Batch 155280, train_perplexity=18.65743, train_loss=2.9262445

Batch 155290, train_perplexity=18.65743, train_loss=2.9262445

Batch 155300, train_perplexity=18.65743, train_loss=2.9262445

Batch 155310, train_perplexity=18.65743, train_loss=2.9262445

Batch 155320, train_perplexity=18.657434, train_loss=2.9262447

Batch 155330, train_perplexity=18.657427, train_loss=2.9262443

Batch 155340, train_perplexity=18.657427, train_loss=2.9262443

Batch 155350, train_perplexity=18.65743, train_loss=2.9262445

Batch 155360, train_perplexity=18.657427, train_loss=2.9262443

Batch 155370, train_perplexity=18.657427, train_loss=2.9262443

Batch 155380, train_perplexity=18.657427, train_loss=2.9262443

Batch 155390, train_perplexity=18.657427, train_loss=2.9262443

Batch 155400, train_perplexity=18.657427, train_loss=2.9262443

Batch 155410, train_perplexity=18.657427, train_loss=2.9262443

Batch 155420, train_perplexity=18.657427, train_loss=2.9262443

Batch 155430, train_perplexity=18.657417, train_loss=2.9262438

Batch 155440, train_perplexity=18.657427, train_loss=2.9262443

Batch 155450, train_perplexity=18.657427, train_loss=2.9262443

Batch 155460, train_perplexity=18.657427, train_loss=2.9262443

Batch 155470, train_perplexity=18.657427, train_loss=2.9262443

Batch 155480, train_perplexity=18.657427, train_loss=2.9262443

Batch 155490, train_perplexity=18.657427, train_loss=2.9262443

Batch 155500, train_perplexity=18.657427, train_loss=2.9262443

Batch 155510, train_perplexity=18.657427, train_loss=2.9262443

Batch 155520, train_perplexity=18.657417, train_loss=2.9262438

Batch 155530, train_perplexity=18.657421, train_loss=2.926244

Batch 155540, train_perplexity=18.657421, train_loss=2.926244

Batch 155550, train_perplexity=18.657421, train_loss=2.926244

Batch 155560, train_perplexity=18.657421, train_loss=2.926244

Batch 155570, train_perplexity=18.657427, train_loss=2.9262443

Batch 155580, train_perplexity=18.657421, train_loss=2.926244

Batch 155590, train_perplexity=18.657427, train_loss=2.9262443

Batch 155600, train_perplexity=18.657417, train_loss=2.9262438

Batch 155610, train_perplexity=18.657421, train_loss=2.926244

Batch 155620, train_perplexity=18.657421, train_loss=2.926244

Batch 155630, train_perplexity=18.657417, train_loss=2.9262438

Batch 155640, train_perplexity=18.657417, train_loss=2.9262438

Batch 155650, train_perplexity=18.657417, train_loss=2.9262438

Batch 155660, train_perplexity=18.657421, train_loss=2.926244

Batch 155670, train_perplexity=18.657417, train_loss=2.9262438

Batch 155680, train_perplexity=18.657417, train_loss=2.9262438

Batch 155690, train_perplexity=18.657417, train_loss=2.9262438

Batch 155700, train_perplexity=18.657421, train_loss=2.926244

Batch 155710, train_perplexity=18.657417, train_loss=2.9262438

Batch 155720, train_perplexity=18.657417, train_loss=2.9262438

Batch 155730, train_perplexity=18.657417, train_loss=2.9262438

Batch 155740, train_perplexity=18.657421, train_loss=2.926244

Batch 155750, train_perplexity=18.657417, train_loss=2.9262438

Batch 155760, train_perplexity=18.657417, train_loss=2.9262438

Batch 155770, train_perplexity=18.657417, train_loss=2.9262438

Batch 155780, train_perplexity=18.657417, train_loss=2.9262438

Batch 155790, train_perplexity=18.657413, train_loss=2.9262435

Batch 155800, train_perplexity=18.657417, train_loss=2.9262438

Batch 155810, train_perplexity=18.657413, train_loss=2.9262435

Batch 155820, train_perplexity=18.657408, train_loss=2.9262433

Batch 155830, train_perplexity=18.657413, train_loss=2.9262435

Batch 155840, train_perplexity=18.657417, train_loss=2.9262438

Batch 155850, train_perplexity=18.657408, train_loss=2.9262433

Batch 155860, train_perplexity=18.657413, train_loss=2.9262435

Batch 155870, train_perplexity=18.657417, train_loss=2.9262438

Batch 155880, train_perplexity=18.657413, train_loss=2.9262435

Batch 155890, train_perplexity=18.657413, train_loss=2.9262435

Batch 155900, train_perplexity=18.657408, train_loss=2.9262433

Batch 155910, train_perplexity=18.657408, train_loss=2.9262433

Batch 155920, train_perplexity=18.657413, train_loss=2.9262435

Batch 155930, train_perplexity=18.657408, train_loss=2.9262433

Batch 155940, train_perplexity=18.657408, train_loss=2.9262433

Batch 155950, train_perplexity=18.657408, train_loss=2.9262433

Batch 155960, train_perplexity=18.657408, train_loss=2.9262433

Batch 155970, train_perplexity=18.657408, train_loss=2.9262433

Batch 155980, train_perplexity=18.657413, train_loss=2.9262435

Batch 155990, train_perplexity=18.657417, train_loss=2.9262438

Batch 156000, train_perplexity=18.657408, train_loss=2.9262433

Batch 156010, train_perplexity=18.657408, train_loss=2.9262433

Batch 156020, train_perplexity=18.657413, train_loss=2.9262435

Batch 156030, train_perplexity=18.657408, train_loss=2.9262433

Batch 156040, train_perplexity=18.657408, train_loss=2.9262433

Batch 156050, train_perplexity=18.657408, train_loss=2.9262433

Batch 156060, train_perplexity=18.657408, train_loss=2.9262433

Batch 156070, train_perplexity=18.657408, train_loss=2.9262433

Batch 156080, train_perplexity=18.6574, train_loss=2.9262428

Batch 156090, train_perplexity=18.657408, train_loss=2.9262433

Batch 156100, train_perplexity=18.657408, train_loss=2.9262433

Batch 156110, train_perplexity=18.657408, train_loss=2.9262433

Batch 156120, train_perplexity=18.6574, train_loss=2.9262428

Batch 156130, train_perplexity=18.657404, train_loss=2.926243

Batch 156140, train_perplexity=18.657408, train_loss=2.9262433

Batch 156150, train_perplexity=18.6574, train_loss=2.9262428

Batch 156160, train_perplexity=18.657408, train_loss=2.9262433

Batch 156170, train_perplexity=18.657404, train_loss=2.926243

Batch 156180, train_perplexity=18.657404, train_loss=2.926243

Batch 156190, train_perplexity=18.6574, train_loss=2.9262428

Batch 156200, train_perplexity=18.657404, train_loss=2.926243

Batch 156210, train_perplexity=18.657408, train_loss=2.9262433

Batch 156220, train_perplexity=18.6574, train_loss=2.9262428

Batch 156230, train_perplexity=18.657408, train_loss=2.9262433

Batch 156240, train_perplexity=18.6574, train_loss=2.9262428

Batch 156250, train_perplexity=18.6574, train_loss=2.9262428

Batch 156260, train_perplexity=18.657404, train_loss=2.926243

Batch 156270, train_perplexity=18.657404, train_loss=2.926243

Batch 156280, train_perplexity=18.657404, train_loss=2.926243

Batch 156290, train_perplexity=18.6574, train_loss=2.9262428

Batch 156300, train_perplexity=18.6574, train_loss=2.9262428
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 156310, train_perplexity=18.657404, train_loss=2.926243

Batch 156320, train_perplexity=18.657404, train_loss=2.926243

Batch 156330, train_perplexity=18.6574, train_loss=2.9262428

Batch 156340, train_perplexity=18.657404, train_loss=2.926243

Batch 156350, train_perplexity=18.6574, train_loss=2.9262428

Batch 156360, train_perplexity=18.6574, train_loss=2.9262428

Batch 156370, train_perplexity=18.657404, train_loss=2.926243

Batch 156380, train_perplexity=18.6574, train_loss=2.9262428

Batch 156390, train_perplexity=18.6574, train_loss=2.9262428

Batch 156400, train_perplexity=18.6574, train_loss=2.9262428

Batch 156410, train_perplexity=18.6574, train_loss=2.9262428

Batch 156420, train_perplexity=18.65739, train_loss=2.9262424

Batch 156430, train_perplexity=18.6574, train_loss=2.9262428

Batch 156440, train_perplexity=18.6574, train_loss=2.9262428

Batch 156450, train_perplexity=18.6574, train_loss=2.9262428

Batch 156460, train_perplexity=18.6574, train_loss=2.9262428

Batch 156470, train_perplexity=18.657394, train_loss=2.9262426

Batch 156480, train_perplexity=18.657394, train_loss=2.9262426

Batch 156490, train_perplexity=18.6574, train_loss=2.9262428

Batch 156500, train_perplexity=18.65739, train_loss=2.9262424

Batch 156510, train_perplexity=18.657394, train_loss=2.9262426

Batch 156520, train_perplexity=18.65739, train_loss=2.9262424

Batch 156530, train_perplexity=18.6574, train_loss=2.9262428

Batch 156540, train_perplexity=18.65739, train_loss=2.9262424

Batch 156550, train_perplexity=18.657394, train_loss=2.9262426

Batch 156560, train_perplexity=18.65739, train_loss=2.9262424

Batch 156570, train_perplexity=18.65739, train_loss=2.9262424

Batch 156580, train_perplexity=18.65739, train_loss=2.9262424

Batch 156590, train_perplexity=18.65739, train_loss=2.9262424

Batch 156600, train_perplexity=18.65739, train_loss=2.9262424

Batch 156610, train_perplexity=18.65739, train_loss=2.9262424

Batch 156620, train_perplexity=18.65739, train_loss=2.9262424

Batch 156630, train_perplexity=18.65739, train_loss=2.9262424

Batch 156640, train_perplexity=18.657394, train_loss=2.9262426

Batch 156650, train_perplexity=18.65739, train_loss=2.9262424

Batch 156660, train_perplexity=18.65739, train_loss=2.9262424

Batch 156670, train_perplexity=18.65739, train_loss=2.9262424

Batch 156680, train_perplexity=18.65739, train_loss=2.9262424

Batch 156690, train_perplexity=18.65739, train_loss=2.9262424

Batch 156700, train_perplexity=18.65739, train_loss=2.9262424

Batch 156710, train_perplexity=18.65739, train_loss=2.9262424

Batch 156720, train_perplexity=18.65739, train_loss=2.9262424

Batch 156730, train_perplexity=18.65739, train_loss=2.9262424

Batch 156740, train_perplexity=18.65739, train_loss=2.9262424

Batch 156750, train_perplexity=18.65739, train_loss=2.9262424

Batch 156760, train_perplexity=18.65739, train_loss=2.9262424

Batch 156770, train_perplexity=18.65739, train_loss=2.9262424

Batch 156780, train_perplexity=18.657387, train_loss=2.926242

Batch 156790, train_perplexity=18.657387, train_loss=2.926242

Batch 156800, train_perplexity=18.657381, train_loss=2.9262419

Batch 156810, train_perplexity=18.657387, train_loss=2.926242

Batch 156820, train_perplexity=18.657387, train_loss=2.926242

Batch 156830, train_perplexity=18.657387, train_loss=2.926242

Batch 156840, train_perplexity=18.657387, train_loss=2.926242

Batch 156850, train_perplexity=18.657387, train_loss=2.926242

Batch 156860, train_perplexity=18.657387, train_loss=2.926242

Batch 156870, train_perplexity=18.657381, train_loss=2.9262419

Batch 156880, train_perplexity=18.657381, train_loss=2.9262419

Batch 156890, train_perplexity=18.657387, train_loss=2.926242

Batch 156900, train_perplexity=18.657381, train_loss=2.9262419

Batch 156910, train_perplexity=18.657381, train_loss=2.9262419

Batch 156920, train_perplexity=18.657381, train_loss=2.9262419

Batch 156930, train_perplexity=18.657381, train_loss=2.9262419

Batch 156940, train_perplexity=18.657381, train_loss=2.9262419

Batch 156950, train_perplexity=18.657381, train_loss=2.9262419

Batch 156960, train_perplexity=18.657381, train_loss=2.9262419

Batch 156970, train_perplexity=18.657381, train_loss=2.9262419

Batch 156980, train_perplexity=18.657381, train_loss=2.9262419

Batch 156990, train_perplexity=18.657381, train_loss=2.9262419

Batch 157000, train_perplexity=18.657381, train_loss=2.9262419

Batch 157010, train_perplexity=18.657377, train_loss=2.9262416

Batch 157020, train_perplexity=18.657381, train_loss=2.9262419

Batch 157030, train_perplexity=18.657377, train_loss=2.9262416

Batch 157040, train_perplexity=18.657381, train_loss=2.9262419

Batch 157050, train_perplexity=18.657381, train_loss=2.9262419

Batch 157060, train_perplexity=18.657377, train_loss=2.9262416

Batch 157070, train_perplexity=18.657381, train_loss=2.9262419

Batch 157080, train_perplexity=18.657381, train_loss=2.9262419

Batch 157090, train_perplexity=18.657381, train_loss=2.9262419

Batch 157100, train_perplexity=18.657377, train_loss=2.9262416

Batch 157110, train_perplexity=18.657373, train_loss=2.9262414

Batch 157120, train_perplexity=18.657377, train_loss=2.9262416

Batch 157130, train_perplexity=18.657381, train_loss=2.9262419

Batch 157140, train_perplexity=18.657381, train_loss=2.9262419

Batch 157150, train_perplexity=18.657377, train_loss=2.9262416

Batch 157160, train_perplexity=18.657377, train_loss=2.9262416

Batch 157170, train_perplexity=18.657377, train_loss=2.9262416

Batch 157180, train_perplexity=18.657377, train_loss=2.9262416

Batch 157190, train_perplexity=18.657373, train_loss=2.9262414

Batch 157200, train_perplexity=18.657373, train_loss=2.9262414

Batch 157210, train_perplexity=18.657373, train_loss=2.9262414

Batch 157220, train_perplexity=18.657377, train_loss=2.9262416

Batch 157230, train_perplexity=18.657368, train_loss=2.9262412

Batch 157240, train_perplexity=18.657373, train_loss=2.9262414

Batch 157250, train_perplexity=18.657373, train_loss=2.9262414

Batch 157260, train_perplexity=18.657373, train_loss=2.9262414

Batch 157270, train_perplexity=18.657373, train_loss=2.9262414

Batch 157280, train_perplexity=18.657377, train_loss=2.9262416

Batch 157290, train_perplexity=18.657373, train_loss=2.9262414

Batch 157300, train_perplexity=18.657373, train_loss=2.9262414

Batch 157310, train_perplexity=18.657373, train_loss=2.9262414

Batch 157320, train_perplexity=18.657373, train_loss=2.9262414

Batch 157330, train_perplexity=18.657373, train_loss=2.9262414

Batch 157340, train_perplexity=18.657364, train_loss=2.926241

Batch 157350, train_perplexity=18.657373, train_loss=2.9262414

Batch 157360, train_perplexity=18.657373, train_loss=2.9262414

Batch 157370, train_perplexity=18.657373, train_loss=2.9262414

Batch 157380, train_perplexity=18.657368, train_loss=2.9262412

Batch 157390, train_perplexity=18.657368, train_loss=2.9262412

Batch 157400, train_perplexity=18.657368, train_loss=2.9262412

Batch 157410, train_perplexity=18.657373, train_loss=2.9262414

Batch 157420, train_perplexity=18.657373, train_loss=2.9262414

Batch 157430, train_perplexity=18.657373, train_loss=2.9262414

Batch 157440, train_perplexity=18.657364, train_loss=2.926241

Batch 157450, train_perplexity=18.657368, train_loss=2.9262412

Batch 157460, train_perplexity=18.657373, train_loss=2.9262414

Batch 157470, train_perplexity=18.657373, train_loss=2.9262414

Batch 157480, train_perplexity=18.657368, train_loss=2.9262412

Batch 157490, train_perplexity=18.657368, train_loss=2.9262412

Batch 157500, train_perplexity=18.657368, train_loss=2.9262412

Batch 157510, train_perplexity=18.657373, train_loss=2.9262414

Batch 157520, train_perplexity=18.657364, train_loss=2.926241

Batch 157530, train_perplexity=18.657364, train_loss=2.926241

Batch 157540, train_perplexity=18.657364, train_loss=2.926241

Batch 157550, train_perplexity=18.657368, train_loss=2.9262412

Batch 157560, train_perplexity=18.657364, train_loss=2.926241

Batch 157570, train_perplexity=18.65736, train_loss=2.9262407

Batch 157580, train_perplexity=18.657364, train_loss=2.926241

Batch 157590, train_perplexity=18.657364, train_loss=2.926241

Batch 157600, train_perplexity=18.657364, train_loss=2.926241
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 157610, train_perplexity=18.657368, train_loss=2.9262412

Batch 157620, train_perplexity=18.657364, train_loss=2.926241

Batch 157630, train_perplexity=18.657364, train_loss=2.926241

Batch 157640, train_perplexity=18.657364, train_loss=2.926241

Batch 157650, train_perplexity=18.65736, train_loss=2.9262407

Batch 157660, train_perplexity=18.657364, train_loss=2.926241

Batch 157670, train_perplexity=18.65736, train_loss=2.9262407

Batch 157680, train_perplexity=18.65736, train_loss=2.9262407

Batch 157690, train_perplexity=18.65736, train_loss=2.9262407

Batch 157700, train_perplexity=18.657364, train_loss=2.926241

Batch 157710, train_perplexity=18.657354, train_loss=2.9262404

Batch 157720, train_perplexity=18.65736, train_loss=2.9262407

Batch 157730, train_perplexity=18.657354, train_loss=2.9262404

Batch 157740, train_perplexity=18.657364, train_loss=2.926241

Batch 157750, train_perplexity=18.65736, train_loss=2.9262407

Batch 157760, train_perplexity=18.65736, train_loss=2.9262407

Batch 157770, train_perplexity=18.65736, train_loss=2.9262407

Batch 157780, train_perplexity=18.65736, train_loss=2.9262407

Batch 157790, train_perplexity=18.65736, train_loss=2.9262407

Batch 157800, train_perplexity=18.657354, train_loss=2.9262404

Batch 157810, train_perplexity=18.657354, train_loss=2.9262404

Batch 157820, train_perplexity=18.65736, train_loss=2.9262407

Batch 157830, train_perplexity=18.657354, train_loss=2.9262404

Batch 157840, train_perplexity=18.657354, train_loss=2.9262404

Batch 157850, train_perplexity=18.657354, train_loss=2.9262404

Batch 157860, train_perplexity=18.65736, train_loss=2.9262407

Batch 157870, train_perplexity=18.65736, train_loss=2.9262407

Batch 157880, train_perplexity=18.657354, train_loss=2.9262404

Batch 157890, train_perplexity=18.65736, train_loss=2.9262407

Batch 157900, train_perplexity=18.65735, train_loss=2.9262402

Batch 157910, train_perplexity=18.657354, train_loss=2.9262404

Batch 157920, train_perplexity=18.65735, train_loss=2.9262402

Batch 157930, train_perplexity=18.657354, train_loss=2.9262404

Batch 157940, train_perplexity=18.65736, train_loss=2.9262407

Batch 157950, train_perplexity=18.657354, train_loss=2.9262404

Batch 157960, train_perplexity=18.657354, train_loss=2.9262404

Batch 157970, train_perplexity=18.657354, train_loss=2.9262404

Batch 157980, train_perplexity=18.657354, train_loss=2.9262404

Batch 157990, train_perplexity=18.65735, train_loss=2.9262402

Batch 158000, train_perplexity=18.65735, train_loss=2.9262402

Batch 158010, train_perplexity=18.657354, train_loss=2.9262404

Batch 158020, train_perplexity=18.657354, train_loss=2.9262404

Batch 158030, train_perplexity=18.657354, train_loss=2.9262404

Batch 158040, train_perplexity=18.65735, train_loss=2.9262402

Batch 158050, train_perplexity=18.65735, train_loss=2.9262402

Batch 158060, train_perplexity=18.65735, train_loss=2.9262402

Batch 158070, train_perplexity=18.657347, train_loss=2.92624

Batch 158080, train_perplexity=18.657347, train_loss=2.92624

Batch 158090, train_perplexity=18.657354, train_loss=2.9262404

Batch 158100, train_perplexity=18.657347, train_loss=2.92624

Batch 158110, train_perplexity=18.65735, train_loss=2.9262402

Batch 158120, train_perplexity=18.65735, train_loss=2.9262402

Batch 158130, train_perplexity=18.657347, train_loss=2.92624

Batch 158140, train_perplexity=18.657347, train_loss=2.92624

Batch 158150, train_perplexity=18.65735, train_loss=2.9262402

Batch 158160, train_perplexity=18.657347, train_loss=2.92624

Batch 158170, train_perplexity=18.657347, train_loss=2.92624

Batch 158180, train_perplexity=18.657347, train_loss=2.92624

Batch 158190, train_perplexity=18.657347, train_loss=2.92624

Batch 158200, train_perplexity=18.657347, train_loss=2.92624

Batch 158210, train_perplexity=18.657347, train_loss=2.92624

Batch 158220, train_perplexity=18.657347, train_loss=2.92624

Batch 158230, train_perplexity=18.657347, train_loss=2.92624

Batch 158240, train_perplexity=18.657347, train_loss=2.92624

Batch 158250, train_perplexity=18.657347, train_loss=2.92624

Batch 158260, train_perplexity=18.657347, train_loss=2.92624

Batch 158270, train_perplexity=18.657341, train_loss=2.9262397

Batch 158280, train_perplexity=18.657347, train_loss=2.92624

Batch 158290, train_perplexity=18.657341, train_loss=2.9262397

Batch 158300, train_perplexity=18.657347, train_loss=2.92624

Batch 158310, train_perplexity=18.657347, train_loss=2.92624

Batch 158320, train_perplexity=18.657347, train_loss=2.92624

Batch 158330, train_perplexity=18.657347, train_loss=2.92624

Batch 158340, train_perplexity=18.657337, train_loss=2.9262395

Batch 158350, train_perplexity=18.657341, train_loss=2.9262397

Batch 158360, train_perplexity=18.657341, train_loss=2.9262397

Batch 158370, train_perplexity=18.657337, train_loss=2.9262395

Batch 158380, train_perplexity=18.657347, train_loss=2.92624

Batch 158390, train_perplexity=18.657347, train_loss=2.92624

Batch 158400, train_perplexity=18.657337, train_loss=2.9262395

Batch 158410, train_perplexity=18.657337, train_loss=2.9262395

Batch 158420, train_perplexity=18.657341, train_loss=2.9262397

Batch 158430, train_perplexity=18.657341, train_loss=2.9262397

Batch 158440, train_perplexity=18.657337, train_loss=2.9262395

Batch 158450, train_perplexity=18.657337, train_loss=2.9262395

Batch 158460, train_perplexity=18.657341, train_loss=2.9262397

Batch 158470, train_perplexity=18.657337, train_loss=2.9262395

Batch 158480, train_perplexity=18.657337, train_loss=2.9262395

Batch 158490, train_perplexity=18.657337, train_loss=2.9262395

Batch 158500, train_perplexity=18.657341, train_loss=2.9262397

Batch 158510, train_perplexity=18.657337, train_loss=2.9262395

Batch 158520, train_perplexity=18.657337, train_loss=2.9262395

Batch 158530, train_perplexity=18.657337, train_loss=2.9262395

Batch 158540, train_perplexity=18.657337, train_loss=2.9262395

Batch 158550, train_perplexity=18.657337, train_loss=2.9262395

Batch 158560, train_perplexity=18.657337, train_loss=2.9262395

Batch 158570, train_perplexity=18.657337, train_loss=2.9262395

Batch 158580, train_perplexity=18.657337, train_loss=2.9262395

Batch 158590, train_perplexity=18.657337, train_loss=2.9262395

Batch 158600, train_perplexity=18.657333, train_loss=2.9262393

Batch 158610, train_perplexity=18.657337, train_loss=2.9262395

Batch 158620, train_perplexity=18.657337, train_loss=2.9262395

Batch 158630, train_perplexity=18.657337, train_loss=2.9262395

Batch 158640, train_perplexity=18.657337, train_loss=2.9262395

Batch 158650, train_perplexity=18.657328, train_loss=2.926239

Batch 158660, train_perplexity=18.657337, train_loss=2.9262395

Batch 158670, train_perplexity=18.657333, train_loss=2.9262393

Batch 158680, train_perplexity=18.657333, train_loss=2.9262393

Batch 158690, train_perplexity=18.657337, train_loss=2.9262395

Batch 158700, train_perplexity=18.657337, train_loss=2.9262395

Batch 158710, train_perplexity=18.657328, train_loss=2.926239

Batch 158720, train_perplexity=18.657333, train_loss=2.9262393

Batch 158730, train_perplexity=18.657333, train_loss=2.9262393

Batch 158740, train_perplexity=18.657337, train_loss=2.9262395

Batch 158750, train_perplexity=18.657333, train_loss=2.9262393

Batch 158760, train_perplexity=18.657328, train_loss=2.926239

Batch 158770, train_perplexity=18.657328, train_loss=2.926239

Batch 158780, train_perplexity=18.657328, train_loss=2.926239

Batch 158790, train_perplexity=18.657324, train_loss=2.9262388

Batch 158800, train_perplexity=18.657333, train_loss=2.9262393

Batch 158810, train_perplexity=18.657324, train_loss=2.9262388

Batch 158820, train_perplexity=18.657328, train_loss=2.926239

Batch 158830, train_perplexity=18.657328, train_loss=2.926239

Batch 158840, train_perplexity=18.657328, train_loss=2.926239

Batch 158850, train_perplexity=18.657328, train_loss=2.926239

Batch 158860, train_perplexity=18.657328, train_loss=2.926239

Batch 158870, train_perplexity=18.657328, train_loss=2.926239

Batch 158880, train_perplexity=18.657328, train_loss=2.926239

Batch 158890, train_perplexity=18.657328, train_loss=2.926239

Batch 158900, train_perplexity=18.65732, train_loss=2.9262385
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 158910, train_perplexity=18.657328, train_loss=2.926239

Batch 158920, train_perplexity=18.657328, train_loss=2.926239

Batch 158930, train_perplexity=18.657328, train_loss=2.926239

Batch 158940, train_perplexity=18.657328, train_loss=2.926239

Batch 158950, train_perplexity=18.657324, train_loss=2.9262388

Batch 158960, train_perplexity=18.657328, train_loss=2.926239

Batch 158970, train_perplexity=18.65732, train_loss=2.9262385

Batch 158980, train_perplexity=18.657328, train_loss=2.926239

Batch 158990, train_perplexity=18.65732, train_loss=2.9262385

Batch 159000, train_perplexity=18.657324, train_loss=2.9262388

Batch 159010, train_perplexity=18.657324, train_loss=2.9262388

Batch 159020, train_perplexity=18.65732, train_loss=2.9262385

Batch 159030, train_perplexity=18.657324, train_loss=2.9262388

Batch 159040, train_perplexity=18.657324, train_loss=2.9262388

Batch 159050, train_perplexity=18.65732, train_loss=2.9262385

Batch 159060, train_perplexity=18.65732, train_loss=2.9262385

Batch 159070, train_perplexity=18.657324, train_loss=2.9262388

Batch 159080, train_perplexity=18.65732, train_loss=2.9262385

Batch 159090, train_perplexity=18.65732, train_loss=2.9262385

Batch 159100, train_perplexity=18.65732, train_loss=2.9262385

Batch 159110, train_perplexity=18.65732, train_loss=2.9262385

Batch 159120, train_perplexity=18.65732, train_loss=2.9262385

Batch 159130, train_perplexity=18.65732, train_loss=2.9262385

Batch 159140, train_perplexity=18.65732, train_loss=2.9262385

Batch 159150, train_perplexity=18.65732, train_loss=2.9262385

Batch 159160, train_perplexity=18.65732, train_loss=2.9262385

Batch 159170, train_perplexity=18.65732, train_loss=2.9262385

Batch 159180, train_perplexity=18.65732, train_loss=2.9262385

Batch 159190, train_perplexity=18.65731, train_loss=2.926238

Batch 159200, train_perplexity=18.65732, train_loss=2.9262385

Batch 159210, train_perplexity=18.65732, train_loss=2.9262385

Batch 159220, train_perplexity=18.65731, train_loss=2.926238

Batch 159230, train_perplexity=18.65732, train_loss=2.9262385

Batch 159240, train_perplexity=18.657314, train_loss=2.9262383

Batch 159250, train_perplexity=18.657314, train_loss=2.9262383

Batch 159260, train_perplexity=18.65731, train_loss=2.926238

Batch 159270, train_perplexity=18.65732, train_loss=2.9262385

Batch 159280, train_perplexity=18.657314, train_loss=2.9262383

Batch 159290, train_perplexity=18.657314, train_loss=2.9262383

Batch 159300, train_perplexity=18.65731, train_loss=2.926238

Batch 159310, train_perplexity=18.65732, train_loss=2.9262385

Batch 159320, train_perplexity=18.657314, train_loss=2.9262383

Batch 159330, train_perplexity=18.65732, train_loss=2.9262385

Batch 159340, train_perplexity=18.65732, train_loss=2.9262385

Batch 159350, train_perplexity=18.657314, train_loss=2.9262383

Batch 159360, train_perplexity=18.657314, train_loss=2.9262383

Batch 159370, train_perplexity=18.65731, train_loss=2.926238

Batch 159380, train_perplexity=18.65731, train_loss=2.926238

Batch 159390, train_perplexity=18.657314, train_loss=2.9262383

Batch 159400, train_perplexity=18.65731, train_loss=2.926238

Batch 159410, train_perplexity=18.65731, train_loss=2.926238

Batch 159420, train_perplexity=18.65731, train_loss=2.926238

Batch 159430, train_perplexity=18.65731, train_loss=2.926238

Batch 159440, train_perplexity=18.65731, train_loss=2.926238

Batch 159450, train_perplexity=18.657314, train_loss=2.9262383

Batch 159460, train_perplexity=18.657307, train_loss=2.9262378

Batch 159470, train_perplexity=18.65731, train_loss=2.926238

Batch 159480, train_perplexity=18.65731, train_loss=2.926238

Batch 159490, train_perplexity=18.65731, train_loss=2.926238

Batch 159500, train_perplexity=18.65731, train_loss=2.926238

Batch 159510, train_perplexity=18.65731, train_loss=2.926238

Batch 159520, train_perplexity=18.657307, train_loss=2.9262378

Batch 159530, train_perplexity=18.65731, train_loss=2.926238

Batch 159540, train_perplexity=18.657307, train_loss=2.9262378

Batch 159550, train_perplexity=18.65731, train_loss=2.926238

Batch 159560, train_perplexity=18.657307, train_loss=2.9262378

Batch 159570, train_perplexity=18.657307, train_loss=2.9262378

Batch 159580, train_perplexity=18.657307, train_loss=2.9262378

Batch 159590, train_perplexity=18.65731, train_loss=2.926238

Batch 159600, train_perplexity=18.6573, train_loss=2.9262376

Batch 159610, train_perplexity=18.657307, train_loss=2.9262378

Batch 159620, train_perplexity=18.657307, train_loss=2.9262378

Batch 159630, train_perplexity=18.6573, train_loss=2.9262376

Batch 159640, train_perplexity=18.657307, train_loss=2.9262378

Batch 159650, train_perplexity=18.6573, train_loss=2.9262376

Batch 159660, train_perplexity=18.6573, train_loss=2.9262376

Batch 159670, train_perplexity=18.6573, train_loss=2.9262376

Batch 159680, train_perplexity=18.6573, train_loss=2.9262376

Batch 159690, train_perplexity=18.657307, train_loss=2.9262378

Batch 159700, train_perplexity=18.6573, train_loss=2.9262376

Batch 159710, train_perplexity=18.6573, train_loss=2.9262376

Batch 159720, train_perplexity=18.657307, train_loss=2.9262378

Batch 159730, train_perplexity=18.6573, train_loss=2.9262376

Batch 159740, train_perplexity=18.6573, train_loss=2.9262376

Batch 159750, train_perplexity=18.6573, train_loss=2.9262376

Batch 159760, train_perplexity=18.657297, train_loss=2.9262373

Batch 159770, train_perplexity=18.6573, train_loss=2.9262376

Batch 159780, train_perplexity=18.6573, train_loss=2.9262376

Batch 159790, train_perplexity=18.6573, train_loss=2.9262376

Batch 159800, train_perplexity=18.6573, train_loss=2.9262376

Batch 159810, train_perplexity=18.6573, train_loss=2.9262376

Batch 159820, train_perplexity=18.6573, train_loss=2.9262376

Batch 159830, train_perplexity=18.6573, train_loss=2.9262376

Batch 159840, train_perplexity=18.6573, train_loss=2.9262376

Batch 159850, train_perplexity=18.657297, train_loss=2.9262373

Batch 159860, train_perplexity=18.657293, train_loss=2.926237

Batch 159870, train_perplexity=18.657297, train_loss=2.9262373

Batch 159880, train_perplexity=18.657297, train_loss=2.9262373

Batch 159890, train_perplexity=18.6573, train_loss=2.9262376

Batch 159900, train_perplexity=18.657297, train_loss=2.9262373

Batch 159910, train_perplexity=18.657293, train_loss=2.926237

Batch 159920, train_perplexity=18.657297, train_loss=2.9262373

Batch 159930, train_perplexity=18.657293, train_loss=2.926237

Batch 159940, train_perplexity=18.657293, train_loss=2.926237

Batch 159950, train_perplexity=18.657293, train_loss=2.926237

Batch 159960, train_perplexity=18.657297, train_loss=2.9262373

Batch 159970, train_perplexity=18.657297, train_loss=2.9262373

Batch 159980, train_perplexity=18.657297, train_loss=2.9262373

Batch 159990, train_perplexity=18.657293, train_loss=2.926237

Batch 160000, train_perplexity=18.657293, train_loss=2.926237

Batch 160010, train_perplexity=18.657297, train_loss=2.9262373

Batch 160020, train_perplexity=18.657293, train_loss=2.926237

Batch 160030, train_perplexity=18.657293, train_loss=2.926237

Batch 160040, train_perplexity=18.657293, train_loss=2.926237

Batch 160050, train_perplexity=18.657293, train_loss=2.926237

Batch 160060, train_perplexity=18.657293, train_loss=2.926237

Batch 160070, train_perplexity=18.657288, train_loss=2.9262369

Batch 160080, train_perplexity=18.657288, train_loss=2.9262369

Batch 160090, train_perplexity=18.657293, train_loss=2.926237

Batch 160100, train_perplexity=18.657288, train_loss=2.9262369

Batch 160110, train_perplexity=18.657293, train_loss=2.926237

Batch 160120, train_perplexity=18.657293, train_loss=2.926237

Batch 160130, train_perplexity=18.657293, train_loss=2.926237

Batch 160140, train_perplexity=18.657288, train_loss=2.9262369

Batch 160150, train_perplexity=18.657293, train_loss=2.926237

Batch 160160, train_perplexity=18.657293, train_loss=2.926237

Batch 160170, train_perplexity=18.657293, train_loss=2.926237

Batch 160180, train_perplexity=18.657284, train_loss=2.9262366

Batch 160190, train_perplexity=18.657288, train_loss=2.9262369

Batch 160200, train_perplexity=18.657293, train_loss=2.926237
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 160210, train_perplexity=18.657288, train_loss=2.9262369

Batch 160220, train_perplexity=18.657288, train_loss=2.9262369

Batch 160230, train_perplexity=18.657288, train_loss=2.9262369

Batch 160240, train_perplexity=18.657284, train_loss=2.9262366

Batch 160250, train_perplexity=18.657288, train_loss=2.9262369

Batch 160260, train_perplexity=18.657284, train_loss=2.9262366

Batch 160270, train_perplexity=18.657284, train_loss=2.9262366

Batch 160280, train_perplexity=18.657288, train_loss=2.9262369

Batch 160290, train_perplexity=18.657284, train_loss=2.9262366

Batch 160300, train_perplexity=18.657284, train_loss=2.9262366

Batch 160310, train_perplexity=18.657284, train_loss=2.9262366

Batch 160320, train_perplexity=18.657284, train_loss=2.9262366

Batch 160330, train_perplexity=18.657284, train_loss=2.9262366

Batch 160340, train_perplexity=18.657288, train_loss=2.9262369

Batch 160350, train_perplexity=18.657284, train_loss=2.9262366

Batch 160360, train_perplexity=18.657288, train_loss=2.9262369

Batch 160370, train_perplexity=18.657284, train_loss=2.9262366

Batch 160380, train_perplexity=18.657284, train_loss=2.9262366

Batch 160390, train_perplexity=18.657284, train_loss=2.9262366

Batch 160400, train_perplexity=18.657284, train_loss=2.9262366

Batch 160410, train_perplexity=18.657284, train_loss=2.9262366

Batch 160420, train_perplexity=18.657284, train_loss=2.9262366

Batch 160430, train_perplexity=18.65728, train_loss=2.9262364

Batch 160440, train_perplexity=18.657284, train_loss=2.9262366

Batch 160450, train_perplexity=18.657284, train_loss=2.9262366

Batch 160460, train_perplexity=18.657284, train_loss=2.9262366

Batch 160470, train_perplexity=18.657284, train_loss=2.9262366

Batch 160480, train_perplexity=18.657284, train_loss=2.9262366

Batch 160490, train_perplexity=18.65728, train_loss=2.9262364

Batch 160500, train_perplexity=18.65728, train_loss=2.9262364

Batch 160510, train_perplexity=18.657274, train_loss=2.9262362

Batch 160520, train_perplexity=18.65728, train_loss=2.9262364

Batch 160530, train_perplexity=18.657274, train_loss=2.9262362

Batch 160540, train_perplexity=18.65728, train_loss=2.9262364

Batch 160550, train_perplexity=18.657284, train_loss=2.9262366

Batch 160560, train_perplexity=18.657274, train_loss=2.9262362

Batch 160570, train_perplexity=18.65728, train_loss=2.9262364

Batch 160580, train_perplexity=18.65728, train_loss=2.9262364

Batch 160590, train_perplexity=18.657274, train_loss=2.9262362

Batch 160600, train_perplexity=18.657274, train_loss=2.9262362

Batch 160610, train_perplexity=18.657274, train_loss=2.9262362

Batch 160620, train_perplexity=18.65728, train_loss=2.9262364

Batch 160630, train_perplexity=18.65728, train_loss=2.9262364

Batch 160640, train_perplexity=18.65727, train_loss=2.926236

Batch 160650, train_perplexity=18.65728, train_loss=2.9262364

Batch 160660, train_perplexity=18.65728, train_loss=2.9262364

Batch 160670, train_perplexity=18.657274, train_loss=2.9262362

Batch 160680, train_perplexity=18.65728, train_loss=2.9262364

Batch 160690, train_perplexity=18.65727, train_loss=2.926236

Batch 160700, train_perplexity=18.657274, train_loss=2.9262362

Batch 160710, train_perplexity=18.657274, train_loss=2.9262362

Batch 160720, train_perplexity=18.657274, train_loss=2.9262362

Batch 160730, train_perplexity=18.657274, train_loss=2.9262362

Batch 160740, train_perplexity=18.657274, train_loss=2.9262362

Batch 160750, train_perplexity=18.65727, train_loss=2.926236

Batch 160760, train_perplexity=18.657267, train_loss=2.9262357

Batch 160770, train_perplexity=18.65727, train_loss=2.926236

Batch 160780, train_perplexity=18.657274, train_loss=2.9262362

Batch 160790, train_perplexity=18.65727, train_loss=2.926236

Batch 160800, train_perplexity=18.657267, train_loss=2.9262357

Batch 160810, train_perplexity=18.65727, train_loss=2.926236

Batch 160820, train_perplexity=18.65727, train_loss=2.926236

Batch 160830, train_perplexity=18.657267, train_loss=2.9262357

Batch 160840, train_perplexity=18.65727, train_loss=2.926236

Batch 160850, train_perplexity=18.65727, train_loss=2.926236

Batch 160860, train_perplexity=18.657267, train_loss=2.9262357

Batch 160870, train_perplexity=18.65727, train_loss=2.926236

Batch 160880, train_perplexity=18.657267, train_loss=2.9262357

Batch 160890, train_perplexity=18.65727, train_loss=2.926236

Batch 160900, train_perplexity=18.657267, train_loss=2.9262357

Batch 160910, train_perplexity=18.657267, train_loss=2.9262357

Batch 160920, train_perplexity=18.65727, train_loss=2.926236

Batch 160930, train_perplexity=18.65727, train_loss=2.926236

Batch 160940, train_perplexity=18.65727, train_loss=2.926236

Batch 160950, train_perplexity=18.65727, train_loss=2.926236

Batch 160960, train_perplexity=18.657267, train_loss=2.9262357

Batch 160970, train_perplexity=18.65727, train_loss=2.926236

Batch 160980, train_perplexity=18.657267, train_loss=2.9262357

Batch 160990, train_perplexity=18.657267, train_loss=2.9262357

Batch 161000, train_perplexity=18.657267, train_loss=2.9262357

Batch 161010, train_perplexity=18.657267, train_loss=2.9262357

Batch 161020, train_perplexity=18.65726, train_loss=2.9262354

Batch 161030, train_perplexity=18.657267, train_loss=2.9262357

Batch 161040, train_perplexity=18.657257, train_loss=2.9262352

Batch 161050, train_perplexity=18.657267, train_loss=2.9262357

Batch 161060, train_perplexity=18.65726, train_loss=2.9262354

Batch 161070, train_perplexity=18.657267, train_loss=2.9262357

Batch 161080, train_perplexity=18.657267, train_loss=2.9262357

Batch 161090, train_perplexity=18.657267, train_loss=2.9262357

Batch 161100, train_perplexity=18.65726, train_loss=2.9262354

Batch 161110, train_perplexity=18.65726, train_loss=2.9262354

Batch 161120, train_perplexity=18.657267, train_loss=2.9262357

Batch 161130, train_perplexity=18.657267, train_loss=2.9262357

Batch 161140, train_perplexity=18.65726, train_loss=2.9262354

Batch 161150, train_perplexity=18.657257, train_loss=2.9262352

Batch 161160, train_perplexity=18.657267, train_loss=2.9262357

Batch 161170, train_perplexity=18.657267, train_loss=2.9262357

Batch 161180, train_perplexity=18.657257, train_loss=2.9262352

Batch 161190, train_perplexity=18.657257, train_loss=2.9262352

Batch 161200, train_perplexity=18.65726, train_loss=2.9262354

Batch 161210, train_perplexity=18.657257, train_loss=2.9262352

Batch 161220, train_perplexity=18.657257, train_loss=2.9262352

Batch 161230, train_perplexity=18.65726, train_loss=2.9262354

Batch 161240, train_perplexity=18.657257, train_loss=2.9262352

Batch 161250, train_perplexity=18.657257, train_loss=2.9262352

Batch 161260, train_perplexity=18.657257, train_loss=2.9262352

Batch 161270, train_perplexity=18.65726, train_loss=2.9262354

Batch 161280, train_perplexity=18.65726, train_loss=2.9262354

Batch 161290, train_perplexity=18.65726, train_loss=2.9262354

Batch 161300, train_perplexity=18.657257, train_loss=2.9262352

Batch 161310, train_perplexity=18.65726, train_loss=2.9262354

Batch 161320, train_perplexity=18.657257, train_loss=2.9262352

Batch 161330, train_perplexity=18.657257, train_loss=2.9262352

Batch 161340, train_perplexity=18.657257, train_loss=2.9262352

Batch 161350, train_perplexity=18.657257, train_loss=2.9262352

Batch 161360, train_perplexity=18.657257, train_loss=2.9262352

Batch 161370, train_perplexity=18.657257, train_loss=2.9262352

Batch 161380, train_perplexity=18.657253, train_loss=2.926235

Batch 161390, train_perplexity=18.657257, train_loss=2.9262352

Batch 161400, train_perplexity=18.657257, train_loss=2.9262352

Batch 161410, train_perplexity=18.657257, train_loss=2.9262352

Batch 161420, train_perplexity=18.657253, train_loss=2.926235

Batch 161430, train_perplexity=18.657248, train_loss=2.9262347

Batch 161440, train_perplexity=18.657248, train_loss=2.9262347

Batch 161450, train_perplexity=18.657253, train_loss=2.926235

Batch 161460, train_perplexity=18.657257, train_loss=2.9262352

Batch 161470, train_perplexity=18.657253, train_loss=2.926235

Batch 161480, train_perplexity=18.657248, train_loss=2.9262347

Batch 161490, train_perplexity=18.657248, train_loss=2.9262347
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 161500, train_perplexity=18.657248, train_loss=2.9262347

Batch 161510, train_perplexity=18.657257, train_loss=2.9262352

Batch 161520, train_perplexity=18.657248, train_loss=2.9262347

Batch 161530, train_perplexity=18.657248, train_loss=2.9262347

Batch 161540, train_perplexity=18.657248, train_loss=2.9262347

Batch 161550, train_perplexity=18.657248, train_loss=2.9262347

Batch 161560, train_perplexity=18.657248, train_loss=2.9262347

Batch 161570, train_perplexity=18.657248, train_loss=2.9262347

Batch 161580, train_perplexity=18.657248, train_loss=2.9262347

Batch 161590, train_perplexity=18.657248, train_loss=2.9262347

Batch 161600, train_perplexity=18.657253, train_loss=2.926235

Batch 161610, train_perplexity=18.657248, train_loss=2.9262347

Batch 161620, train_perplexity=18.657248, train_loss=2.9262347

Batch 161630, train_perplexity=18.657248, train_loss=2.9262347

Batch 161640, train_perplexity=18.657248, train_loss=2.9262347

Batch 161650, train_perplexity=18.657244, train_loss=2.9262345

Batch 161660, train_perplexity=18.657248, train_loss=2.9262347

Batch 161670, train_perplexity=18.657248, train_loss=2.9262347

Batch 161680, train_perplexity=18.657244, train_loss=2.9262345

Batch 161690, train_perplexity=18.657248, train_loss=2.9262347

Batch 161700, train_perplexity=18.657248, train_loss=2.9262347

Batch 161710, train_perplexity=18.657248, train_loss=2.9262347

Batch 161720, train_perplexity=18.657244, train_loss=2.9262345

Batch 161730, train_perplexity=18.657244, train_loss=2.9262345

Batch 161740, train_perplexity=18.657248, train_loss=2.9262347

Batch 161750, train_perplexity=18.657248, train_loss=2.9262347

Batch 161760, train_perplexity=18.657248, train_loss=2.9262347

Batch 161770, train_perplexity=18.65724, train_loss=2.9262342

Batch 161780, train_perplexity=18.65724, train_loss=2.9262342

Batch 161790, train_perplexity=18.657244, train_loss=2.9262345

Batch 161800, train_perplexity=18.65724, train_loss=2.9262342

Batch 161810, train_perplexity=18.657244, train_loss=2.9262345

Batch 161820, train_perplexity=18.657244, train_loss=2.9262345

Batch 161830, train_perplexity=18.657244, train_loss=2.9262345

Batch 161840, train_perplexity=18.657244, train_loss=2.9262345

Batch 161850, train_perplexity=18.657248, train_loss=2.9262347

Batch 161860, train_perplexity=18.657244, train_loss=2.9262345

Batch 161870, train_perplexity=18.65724, train_loss=2.9262342

Batch 161880, train_perplexity=18.65724, train_loss=2.9262342

Batch 161890, train_perplexity=18.65724, train_loss=2.9262342

Batch 161900, train_perplexity=18.65724, train_loss=2.9262342

Batch 161910, train_perplexity=18.657244, train_loss=2.9262345

Batch 161920, train_perplexity=18.657234, train_loss=2.926234

Batch 161930, train_perplexity=18.65724, train_loss=2.9262342

Batch 161940, train_perplexity=18.65724, train_loss=2.9262342

Batch 161950, train_perplexity=18.65724, train_loss=2.9262342

Batch 161960, train_perplexity=18.65724, train_loss=2.9262342

Batch 161970, train_perplexity=18.65724, train_loss=2.9262342

Batch 161980, train_perplexity=18.657234, train_loss=2.926234

Batch 161990, train_perplexity=18.657234, train_loss=2.926234

Batch 162000, train_perplexity=18.65724, train_loss=2.9262342

Batch 162010, train_perplexity=18.657234, train_loss=2.926234

Batch 162020, train_perplexity=18.65724, train_loss=2.9262342

Batch 162030, train_perplexity=18.65724, train_loss=2.9262342

Batch 162040, train_perplexity=18.657234, train_loss=2.926234

Batch 162050, train_perplexity=18.65724, train_loss=2.9262342

Batch 162060, train_perplexity=18.65724, train_loss=2.9262342

Batch 162070, train_perplexity=18.657234, train_loss=2.926234

Batch 162080, train_perplexity=18.657234, train_loss=2.926234

Batch 162090, train_perplexity=18.657234, train_loss=2.926234

Batch 162100, train_perplexity=18.65723, train_loss=2.9262338

Batch 162110, train_perplexity=18.65724, train_loss=2.9262342

Batch 162120, train_perplexity=18.65723, train_loss=2.9262338

Batch 162130, train_perplexity=18.65723, train_loss=2.9262338

Batch 162140, train_perplexity=18.657234, train_loss=2.926234

Batch 162150, train_perplexity=18.65723, train_loss=2.9262338

Batch 162160, train_perplexity=18.65723, train_loss=2.9262338

Batch 162170, train_perplexity=18.65723, train_loss=2.9262338

Batch 162180, train_perplexity=18.65723, train_loss=2.9262338

Batch 162190, train_perplexity=18.65723, train_loss=2.9262338

Batch 162200, train_perplexity=18.65723, train_loss=2.9262338

Batch 162210, train_perplexity=18.65723, train_loss=2.9262338

Batch 162220, train_perplexity=18.65723, train_loss=2.9262338

Batch 162230, train_perplexity=18.65723, train_loss=2.9262338

Batch 162240, train_perplexity=18.65723, train_loss=2.9262338

Batch 162250, train_perplexity=18.65723, train_loss=2.9262338

Batch 162260, train_perplexity=18.657234, train_loss=2.926234

Batch 162270, train_perplexity=18.65723, train_loss=2.9262338

Batch 162280, train_perplexity=18.657227, train_loss=2.9262335

Batch 162290, train_perplexity=18.65723, train_loss=2.9262338

Batch 162300, train_perplexity=18.65723, train_loss=2.9262338

Batch 162310, train_perplexity=18.65722, train_loss=2.9262333

Batch 162320, train_perplexity=18.657227, train_loss=2.9262335

Batch 162330, train_perplexity=18.65723, train_loss=2.9262338

Batch 162340, train_perplexity=18.65723, train_loss=2.9262338

Batch 162350, train_perplexity=18.65723, train_loss=2.9262338

Batch 162360, train_perplexity=18.65723, train_loss=2.9262338

Batch 162370, train_perplexity=18.65723, train_loss=2.9262338

Batch 162380, train_perplexity=18.65722, train_loss=2.9262333

Batch 162390, train_perplexity=18.65722, train_loss=2.9262333

Batch 162400, train_perplexity=18.657227, train_loss=2.9262335

Batch 162410, train_perplexity=18.65722, train_loss=2.9262333

Batch 162420, train_perplexity=18.65722, train_loss=2.9262333

Batch 162430, train_perplexity=18.65722, train_loss=2.9262333

Batch 162440, train_perplexity=18.657227, train_loss=2.9262335

Batch 162450, train_perplexity=18.65722, train_loss=2.9262333

Batch 162460, train_perplexity=18.65722, train_loss=2.9262333

Batch 162470, train_perplexity=18.657227, train_loss=2.9262335

Batch 162480, train_perplexity=18.65722, train_loss=2.9262333

Batch 162490, train_perplexity=18.657227, train_loss=2.9262335

Batch 162500, train_perplexity=18.65722, train_loss=2.9262333

Batch 162510, train_perplexity=18.65722, train_loss=2.9262333

Batch 162520, train_perplexity=18.65722, train_loss=2.9262333

Batch 162530, train_perplexity=18.65722, train_loss=2.9262333

Batch 162540, train_perplexity=18.65722, train_loss=2.9262333

Batch 162550, train_perplexity=18.657227, train_loss=2.9262335

Batch 162560, train_perplexity=18.65722, train_loss=2.9262333

Batch 162570, train_perplexity=18.657227, train_loss=2.9262335

Batch 162580, train_perplexity=18.65722, train_loss=2.9262333

Batch 162590, train_perplexity=18.657217, train_loss=2.926233

Batch 162600, train_perplexity=18.65722, train_loss=2.9262333

Batch 162610, train_perplexity=18.65722, train_loss=2.9262333

Batch 162620, train_perplexity=18.657213, train_loss=2.9262328

Batch 162630, train_perplexity=18.65722, train_loss=2.9262333

Batch 162640, train_perplexity=18.657213, train_loss=2.9262328

Batch 162650, train_perplexity=18.657217, train_loss=2.926233

Batch 162660, train_perplexity=18.657217, train_loss=2.926233

Batch 162670, train_perplexity=18.657213, train_loss=2.9262328

Batch 162680, train_perplexity=18.657213, train_loss=2.9262328

Batch 162690, train_perplexity=18.65722, train_loss=2.9262333

Batch 162700, train_perplexity=18.657217, train_loss=2.926233

Batch 162710, train_perplexity=18.657217, train_loss=2.926233

Batch 162720, train_perplexity=18.657213, train_loss=2.9262328

Batch 162730, train_perplexity=18.657217, train_loss=2.926233

Batch 162740, train_perplexity=18.657217, train_loss=2.926233

Batch 162750, train_perplexity=18.657217, train_loss=2.926233

Batch 162760, train_perplexity=18.657217, train_loss=2.926233

Batch 162770, train_perplexity=18.657213, train_loss=2.9262328

Batch 162780, train_perplexity=18.657217, train_loss=2.926233

Batch 162790, train_perplexity=18.657213, train_loss=2.9262328
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 162800, train_perplexity=18.657213, train_loss=2.9262328

Batch 162810, train_perplexity=18.657217, train_loss=2.926233

Batch 162820, train_perplexity=18.657213, train_loss=2.9262328

Batch 162830, train_perplexity=18.657213, train_loss=2.9262328

Batch 162840, train_perplexity=18.657213, train_loss=2.9262328

Batch 162850, train_perplexity=18.657207, train_loss=2.9262326

Batch 162860, train_perplexity=18.657213, train_loss=2.9262328

Batch 162870, train_perplexity=18.657213, train_loss=2.9262328

Batch 162880, train_perplexity=18.657213, train_loss=2.9262328

Batch 162890, train_perplexity=18.657213, train_loss=2.9262328

Batch 162900, train_perplexity=18.657213, train_loss=2.9262328

Batch 162910, train_perplexity=18.657213, train_loss=2.9262328

Batch 162920, train_perplexity=18.657213, train_loss=2.9262328

Batch 162930, train_perplexity=18.657204, train_loss=2.9262323

Batch 162940, train_perplexity=18.657213, train_loss=2.9262328

Batch 162950, train_perplexity=18.657204, train_loss=2.9262323

Batch 162960, train_perplexity=18.657204, train_loss=2.9262323

Batch 162970, train_perplexity=18.657207, train_loss=2.9262326

Batch 162980, train_perplexity=18.657213, train_loss=2.9262328

Batch 162990, train_perplexity=18.657207, train_loss=2.9262326

Batch 163000, train_perplexity=18.657213, train_loss=2.9262328

Batch 163010, train_perplexity=18.657207, train_loss=2.9262326

Batch 163020, train_perplexity=18.657204, train_loss=2.9262323

Batch 163030, train_perplexity=18.657213, train_loss=2.9262328

Batch 163040, train_perplexity=18.657204, train_loss=2.9262323

Batch 163050, train_perplexity=18.657204, train_loss=2.9262323

Batch 163060, train_perplexity=18.657207, train_loss=2.9262326

Batch 163070, train_perplexity=18.657204, train_loss=2.9262323

Batch 163080, train_perplexity=18.657207, train_loss=2.9262326

Batch 163090, train_perplexity=18.657204, train_loss=2.9262323

Batch 163100, train_perplexity=18.657204, train_loss=2.9262323

Batch 163110, train_perplexity=18.657204, train_loss=2.9262323

Batch 163120, train_perplexity=18.657204, train_loss=2.9262323

Batch 163130, train_perplexity=18.657204, train_loss=2.9262323

Batch 163140, train_perplexity=18.657207, train_loss=2.9262326

Batch 163150, train_perplexity=18.657204, train_loss=2.9262323

Batch 163160, train_perplexity=18.657204, train_loss=2.9262323

Batch 163170, train_perplexity=18.657207, train_loss=2.9262326

Batch 163180, train_perplexity=18.657204, train_loss=2.9262323

Batch 163190, train_perplexity=18.6572, train_loss=2.926232

Batch 163200, train_perplexity=18.657207, train_loss=2.9262326

Batch 163210, train_perplexity=18.6572, train_loss=2.926232

Batch 163220, train_perplexity=18.657204, train_loss=2.9262323

Batch 163230, train_perplexity=18.657204, train_loss=2.9262323

Batch 163240, train_perplexity=18.657204, train_loss=2.9262323

Batch 163250, train_perplexity=18.6572, train_loss=2.926232

Batch 163260, train_perplexity=18.657204, train_loss=2.9262323

Batch 163270, train_perplexity=18.657204, train_loss=2.9262323

Batch 163280, train_perplexity=18.6572, train_loss=2.926232

Batch 163290, train_perplexity=18.6572, train_loss=2.926232

Batch 163300, train_perplexity=18.657194, train_loss=2.9262319

Batch 163310, train_perplexity=18.657204, train_loss=2.9262323

Batch 163320, train_perplexity=18.657194, train_loss=2.9262319

Batch 163330, train_perplexity=18.657194, train_loss=2.9262319

Batch 163340, train_perplexity=18.657194, train_loss=2.9262319

Batch 163350, train_perplexity=18.6572, train_loss=2.926232

Batch 163360, train_perplexity=18.657194, train_loss=2.9262319

Batch 163370, train_perplexity=18.657194, train_loss=2.9262319

Batch 163380, train_perplexity=18.6572, train_loss=2.926232

Batch 163390, train_perplexity=18.657194, train_loss=2.9262319

Batch 163400, train_perplexity=18.657194, train_loss=2.9262319

Batch 163410, train_perplexity=18.657194, train_loss=2.9262319

Batch 163420, train_perplexity=18.657194, train_loss=2.9262319

Batch 163430, train_perplexity=18.657194, train_loss=2.9262319

Batch 163440, train_perplexity=18.657194, train_loss=2.9262319

Batch 163450, train_perplexity=18.6572, train_loss=2.926232

Batch 163460, train_perplexity=18.657194, train_loss=2.9262319

Batch 163470, train_perplexity=18.657194, train_loss=2.9262319

Batch 163480, train_perplexity=18.657194, train_loss=2.9262319

Batch 163490, train_perplexity=18.657187, train_loss=2.9262314

Batch 163500, train_perplexity=18.6572, train_loss=2.926232

Batch 163510, train_perplexity=18.657187, train_loss=2.9262314

Batch 163520, train_perplexity=18.657194, train_loss=2.9262319

Batch 163530, train_perplexity=18.657194, train_loss=2.9262319

Batch 163540, train_perplexity=18.657194, train_loss=2.9262319

Batch 163550, train_perplexity=18.657194, train_loss=2.9262319

Batch 163560, train_perplexity=18.65719, train_loss=2.9262316

Batch 163570, train_perplexity=18.65719, train_loss=2.9262316

Batch 163580, train_perplexity=18.657187, train_loss=2.9262314

Batch 163590, train_perplexity=18.657194, train_loss=2.9262319

Batch 163600, train_perplexity=18.657187, train_loss=2.9262314

Batch 163610, train_perplexity=18.657194, train_loss=2.9262319

Batch 163620, train_perplexity=18.65719, train_loss=2.9262316

Batch 163630, train_perplexity=18.65719, train_loss=2.9262316

Batch 163640, train_perplexity=18.657194, train_loss=2.9262319

Batch 163650, train_perplexity=18.657187, train_loss=2.9262314

Batch 163660, train_perplexity=18.657194, train_loss=2.9262319

Batch 163670, train_perplexity=18.657194, train_loss=2.9262319

Batch 163680, train_perplexity=18.657187, train_loss=2.9262314

Batch 163690, train_perplexity=18.657187, train_loss=2.9262314

Batch 163700, train_perplexity=18.65719, train_loss=2.9262316

Batch 163710, train_perplexity=18.657187, train_loss=2.9262314

Batch 163720, train_perplexity=18.657194, train_loss=2.9262319

Batch 163730, train_perplexity=18.657187, train_loss=2.9262314

Batch 163740, train_perplexity=18.657187, train_loss=2.9262314

Batch 163750, train_perplexity=18.657187, train_loss=2.9262314

Batch 163760, train_perplexity=18.657187, train_loss=2.9262314

Batch 163770, train_perplexity=18.657187, train_loss=2.9262314

Batch 163780, train_perplexity=18.657187, train_loss=2.9262314

Batch 163790, train_perplexity=18.657187, train_loss=2.9262314

Batch 163800, train_perplexity=18.657187, train_loss=2.9262314

Batch 163810, train_perplexity=18.657187, train_loss=2.9262314

Batch 163820, train_perplexity=18.657187, train_loss=2.9262314

Batch 163830, train_perplexity=18.65719, train_loss=2.9262316

Batch 163840, train_perplexity=18.657187, train_loss=2.9262314

Batch 163850, train_perplexity=18.657187, train_loss=2.9262314

Batch 163860, train_perplexity=18.65718, train_loss=2.9262311

Batch 163870, train_perplexity=18.657187, train_loss=2.9262314

Batch 163880, train_perplexity=18.65718, train_loss=2.9262311

Batch 163890, train_perplexity=18.657187, train_loss=2.9262314

Batch 163900, train_perplexity=18.657187, train_loss=2.9262314

Batch 163910, train_perplexity=18.65718, train_loss=2.9262311

Batch 163920, train_perplexity=18.657177, train_loss=2.926231

Batch 163930, train_perplexity=18.657177, train_loss=2.926231

Batch 163940, train_perplexity=18.65718, train_loss=2.9262311

Batch 163950, train_perplexity=18.657187, train_loss=2.9262314

Batch 163960, train_perplexity=18.657177, train_loss=2.926231

Batch 163970, train_perplexity=18.657187, train_loss=2.9262314

Batch 163980, train_perplexity=18.657177, train_loss=2.926231

Batch 163990, train_perplexity=18.657177, train_loss=2.926231

Batch 164000, train_perplexity=18.657177, train_loss=2.926231

Batch 164010, train_perplexity=18.65718, train_loss=2.9262311

Batch 164020, train_perplexity=18.657177, train_loss=2.926231

Batch 164030, train_perplexity=18.657187, train_loss=2.9262314

Batch 164040, train_perplexity=18.65718, train_loss=2.9262311

Batch 164050, train_perplexity=18.657177, train_loss=2.926231

Batch 164060, train_perplexity=18.657177, train_loss=2.926231

Batch 164070, train_perplexity=18.657177, train_loss=2.926231

Batch 164080, train_perplexity=18.65718, train_loss=2.9262311
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 164090, train_perplexity=18.657177, train_loss=2.926231

Batch 164100, train_perplexity=18.657177, train_loss=2.926231

Batch 164110, train_perplexity=18.657177, train_loss=2.926231

Batch 164120, train_perplexity=18.657177, train_loss=2.926231

Batch 164130, train_perplexity=18.657177, train_loss=2.926231

Batch 164140, train_perplexity=18.657177, train_loss=2.926231

Batch 164150, train_perplexity=18.657177, train_loss=2.926231

Batch 164160, train_perplexity=18.657177, train_loss=2.926231

Batch 164170, train_perplexity=18.657173, train_loss=2.9262307

Batch 164180, train_perplexity=18.657177, train_loss=2.926231

Batch 164190, train_perplexity=18.657173, train_loss=2.9262307

Batch 164200, train_perplexity=18.657173, train_loss=2.9262307

Batch 164210, train_perplexity=18.657177, train_loss=2.926231

Batch 164220, train_perplexity=18.657177, train_loss=2.926231

Batch 164230, train_perplexity=18.657167, train_loss=2.9262304

Batch 164240, train_perplexity=18.657173, train_loss=2.9262307

Batch 164250, train_perplexity=18.657167, train_loss=2.9262304

Batch 164260, train_perplexity=18.657167, train_loss=2.9262304

Batch 164270, train_perplexity=18.657167, train_loss=2.9262304

Batch 164280, train_perplexity=18.657173, train_loss=2.9262307

Batch 164290, train_perplexity=18.657167, train_loss=2.9262304

Batch 164300, train_perplexity=18.657167, train_loss=2.9262304

Batch 164310, train_perplexity=18.657167, train_loss=2.9262304

Batch 164320, train_perplexity=18.657173, train_loss=2.9262307

Batch 164330, train_perplexity=18.657167, train_loss=2.9262304

Batch 164340, train_perplexity=18.657173, train_loss=2.9262307

Batch 164350, train_perplexity=18.657167, train_loss=2.9262304

Batch 164360, train_perplexity=18.657167, train_loss=2.9262304

Batch 164370, train_perplexity=18.657173, train_loss=2.9262307

Batch 164380, train_perplexity=18.657167, train_loss=2.9262304

Batch 164390, train_perplexity=18.657167, train_loss=2.9262304

Batch 164400, train_perplexity=18.657173, train_loss=2.9262307

Batch 164410, train_perplexity=18.657173, train_loss=2.9262307

Batch 164420, train_perplexity=18.657167, train_loss=2.9262304

Batch 164430, train_perplexity=18.657167, train_loss=2.9262304

Batch 164440, train_perplexity=18.657167, train_loss=2.9262304

Batch 164450, train_perplexity=18.657164, train_loss=2.9262302

Batch 164460, train_perplexity=18.657164, train_loss=2.9262302

Batch 164470, train_perplexity=18.657164, train_loss=2.9262302

Batch 164480, train_perplexity=18.657167, train_loss=2.9262304

Batch 164490, train_perplexity=18.65716, train_loss=2.92623

Batch 164500, train_perplexity=18.657164, train_loss=2.9262302

Batch 164510, train_perplexity=18.657164, train_loss=2.9262302

Batch 164520, train_perplexity=18.65716, train_loss=2.92623

Batch 164530, train_perplexity=18.657164, train_loss=2.9262302

Batch 164540, train_perplexity=18.65716, train_loss=2.92623

Batch 164550, train_perplexity=18.65716, train_loss=2.92623

Batch 164560, train_perplexity=18.65716, train_loss=2.92623

Batch 164570, train_perplexity=18.65716, train_loss=2.92623

Batch 164580, train_perplexity=18.65716, train_loss=2.92623

Batch 164590, train_perplexity=18.65716, train_loss=2.92623

Batch 164600, train_perplexity=18.65716, train_loss=2.92623

Batch 164610, train_perplexity=18.657164, train_loss=2.9262302

Batch 164620, train_perplexity=18.65716, train_loss=2.92623

Batch 164630, train_perplexity=18.657164, train_loss=2.9262302

Batch 164640, train_perplexity=18.65716, train_loss=2.92623

Batch 164650, train_perplexity=18.65716, train_loss=2.92623

Batch 164660, train_perplexity=18.65716, train_loss=2.92623

Batch 164670, train_perplexity=18.65716, train_loss=2.92623

Batch 164680, train_perplexity=18.657164, train_loss=2.9262302

Batch 164690, train_perplexity=18.657164, train_loss=2.9262302

Batch 164700, train_perplexity=18.65716, train_loss=2.92623

Batch 164710, train_perplexity=18.65716, train_loss=2.92623

Batch 164720, train_perplexity=18.65716, train_loss=2.92623

Batch 164730, train_perplexity=18.65716, train_loss=2.92623

Batch 164740, train_perplexity=18.65716, train_loss=2.92623

Batch 164750, train_perplexity=18.65716, train_loss=2.92623

Batch 164760, train_perplexity=18.65716, train_loss=2.92623

Batch 164770, train_perplexity=18.65716, train_loss=2.92623

Batch 164780, train_perplexity=18.65716, train_loss=2.92623

Batch 164790, train_perplexity=18.657154, train_loss=2.9262297

Batch 164800, train_perplexity=18.65715, train_loss=2.9262295

Batch 164810, train_perplexity=18.65716, train_loss=2.92623

Batch 164820, train_perplexity=18.65716, train_loss=2.92623

Batch 164830, train_perplexity=18.65716, train_loss=2.92623

Batch 164840, train_perplexity=18.657154, train_loss=2.9262297

Batch 164850, train_perplexity=18.657154, train_loss=2.9262297

Batch 164860, train_perplexity=18.65715, train_loss=2.9262295

Batch 164870, train_perplexity=18.65715, train_loss=2.9262295

Batch 164880, train_perplexity=18.657154, train_loss=2.9262297

Batch 164890, train_perplexity=18.65715, train_loss=2.9262295

Batch 164900, train_perplexity=18.657154, train_loss=2.9262297

Batch 164910, train_perplexity=18.65715, train_loss=2.9262295

Batch 164920, train_perplexity=18.65715, train_loss=2.9262295

Batch 164930, train_perplexity=18.65715, train_loss=2.9262295

Batch 164940, train_perplexity=18.65715, train_loss=2.9262295

Batch 164950, train_perplexity=18.65715, train_loss=2.9262295

Batch 164960, train_perplexity=18.65715, train_loss=2.9262295

Batch 164970, train_perplexity=18.657154, train_loss=2.9262297

Batch 164980, train_perplexity=18.65715, train_loss=2.9262295

Batch 164990, train_perplexity=18.65715, train_loss=2.9262295

Batch 165000, train_perplexity=18.65715, train_loss=2.9262295

Batch 165010, train_perplexity=18.65715, train_loss=2.9262295

Batch 165020, train_perplexity=18.65715, train_loss=2.9262295

Batch 165030, train_perplexity=18.65715, train_loss=2.9262295

Batch 165040, train_perplexity=18.65715, train_loss=2.9262295

Batch 165050, train_perplexity=18.65715, train_loss=2.9262295

Batch 165060, train_perplexity=18.65715, train_loss=2.9262295

Batch 165070, train_perplexity=18.657146, train_loss=2.9262292

Batch 165080, train_perplexity=18.65715, train_loss=2.9262295

Batch 165090, train_perplexity=18.65715, train_loss=2.9262295

Batch 165100, train_perplexity=18.657146, train_loss=2.9262292

Batch 165110, train_perplexity=18.65715, train_loss=2.9262295

Batch 165120, train_perplexity=18.65715, train_loss=2.9262295

Batch 165130, train_perplexity=18.65715, train_loss=2.9262295

Batch 165140, train_perplexity=18.65714, train_loss=2.926229

Batch 165150, train_perplexity=18.65714, train_loss=2.926229

Batch 165160, train_perplexity=18.657146, train_loss=2.9262292

Batch 165170, train_perplexity=18.657146, train_loss=2.9262292

Batch 165180, train_perplexity=18.65714, train_loss=2.926229

Batch 165190, train_perplexity=18.65714, train_loss=2.926229

Batch 165200, train_perplexity=18.657146, train_loss=2.9262292

Batch 165210, train_perplexity=18.65714, train_loss=2.926229

Batch 165220, train_perplexity=18.657146, train_loss=2.9262292

Batch 165230, train_perplexity=18.65714, train_loss=2.926229

Batch 165240, train_perplexity=18.65714, train_loss=2.926229

Batch 165250, train_perplexity=18.65714, train_loss=2.926229

Batch 165260, train_perplexity=18.65714, train_loss=2.926229

Batch 165270, train_perplexity=18.65714, train_loss=2.926229

Batch 165280, train_perplexity=18.65714, train_loss=2.926229

Batch 165290, train_perplexity=18.65714, train_loss=2.926229

Batch 165300, train_perplexity=18.65714, train_loss=2.926229

Batch 165310, train_perplexity=18.65714, train_loss=2.926229

Batch 165320, train_perplexity=18.65714, train_loss=2.926229

Batch 165330, train_perplexity=18.65714, train_loss=2.926229

Batch 165340, train_perplexity=18.657146, train_loss=2.9262292

Batch 165350, train_perplexity=18.65714, train_loss=2.926229

Batch 165360, train_perplexity=18.65714, train_loss=2.926229

Batch 165370, train_perplexity=18.65714, train_loss=2.926229

Batch 165380, train_perplexity=18.65714, train_loss=2.926229

Batch 165390, train_perplexity=18.65714, train_loss=2.926229
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 165400, train_perplexity=18.657133, train_loss=2.9262285

Batch 165410, train_perplexity=18.65714, train_loss=2.926229

Batch 165420, train_perplexity=18.65714, train_loss=2.926229

Batch 165430, train_perplexity=18.65714, train_loss=2.926229

Batch 165440, train_perplexity=18.657137, train_loss=2.9262288

Batch 165450, train_perplexity=18.657133, train_loss=2.9262285

Batch 165460, train_perplexity=18.657133, train_loss=2.9262285

Batch 165470, train_perplexity=18.65714, train_loss=2.926229

Batch 165480, train_perplexity=18.65714, train_loss=2.926229

Batch 165490, train_perplexity=18.657133, train_loss=2.9262285

Batch 165500, train_perplexity=18.657133, train_loss=2.9262285

Batch 165510, train_perplexity=18.657133, train_loss=2.9262285

Batch 165520, train_perplexity=18.657137, train_loss=2.9262288

Batch 165530, train_perplexity=18.657133, train_loss=2.9262285

Batch 165540, train_perplexity=18.657133, train_loss=2.9262285

Batch 165550, train_perplexity=18.657137, train_loss=2.9262288

Batch 165560, train_perplexity=18.657133, train_loss=2.9262285

Batch 165570, train_perplexity=18.65714, train_loss=2.926229

Batch 165580, train_perplexity=18.657133, train_loss=2.9262285

Batch 165590, train_perplexity=18.657133, train_loss=2.9262285

Batch 165600, train_perplexity=18.657133, train_loss=2.9262285

Batch 165610, train_perplexity=18.657133, train_loss=2.9262285

Batch 165620, train_perplexity=18.657137, train_loss=2.9262288

Batch 165630, train_perplexity=18.657133, train_loss=2.9262285

Batch 165640, train_perplexity=18.657133, train_loss=2.9262285

Batch 165650, train_perplexity=18.657133, train_loss=2.9262285

Batch 165660, train_perplexity=18.657133, train_loss=2.9262285

Batch 165670, train_perplexity=18.657127, train_loss=2.9262283

Batch 165680, train_perplexity=18.657133, train_loss=2.9262285

Batch 165690, train_perplexity=18.657133, train_loss=2.9262285

Batch 165700, train_perplexity=18.657133, train_loss=2.9262285

Batch 165710, train_perplexity=18.657133, train_loss=2.9262285

Batch 165720, train_perplexity=18.657127, train_loss=2.9262283

Batch 165730, train_perplexity=18.657127, train_loss=2.9262283

Batch 165740, train_perplexity=18.657124, train_loss=2.926228

Batch 165750, train_perplexity=18.657133, train_loss=2.9262285

Batch 165760, train_perplexity=18.657133, train_loss=2.9262285

Batch 165770, train_perplexity=18.657127, train_loss=2.9262283

Batch 165780, train_perplexity=18.657127, train_loss=2.9262283

Batch 165790, train_perplexity=18.657124, train_loss=2.926228

Batch 165800, train_perplexity=18.657124, train_loss=2.926228

Batch 165810, train_perplexity=18.657124, train_loss=2.926228

Batch 165820, train_perplexity=18.657124, train_loss=2.926228

Batch 165830, train_perplexity=18.657124, train_loss=2.926228

Batch 165840, train_perplexity=18.657124, train_loss=2.926228

Batch 165850, train_perplexity=18.657124, train_loss=2.926228

Batch 165860, train_perplexity=18.657133, train_loss=2.9262285

Batch 165870, train_perplexity=18.657124, train_loss=2.926228

Batch 165880, train_perplexity=18.657124, train_loss=2.926228

Batch 165890, train_perplexity=18.657127, train_loss=2.9262283

Batch 165900, train_perplexity=18.657124, train_loss=2.926228

Batch 165910, train_perplexity=18.657124, train_loss=2.926228

Batch 165920, train_perplexity=18.657124, train_loss=2.926228

Batch 165930, train_perplexity=18.657124, train_loss=2.926228

Batch 165940, train_perplexity=18.657124, train_loss=2.926228

Batch 165950, train_perplexity=18.657124, train_loss=2.926228

Batch 165960, train_perplexity=18.657127, train_loss=2.9262283

Batch 165970, train_perplexity=18.657124, train_loss=2.926228

Batch 165980, train_perplexity=18.657124, train_loss=2.926228

Batch 165990, train_perplexity=18.657124, train_loss=2.926228

Batch 166000, train_perplexity=18.657124, train_loss=2.926228

Batch 166010, train_perplexity=18.657124, train_loss=2.926228

Batch 166020, train_perplexity=18.657114, train_loss=2.9262276

Batch 166030, train_perplexity=18.657114, train_loss=2.9262276

Batch 166040, train_perplexity=18.657124, train_loss=2.926228

Batch 166050, train_perplexity=18.657124, train_loss=2.926228

Batch 166060, train_perplexity=18.65712, train_loss=2.9262278

Batch 166070, train_perplexity=18.65712, train_loss=2.9262278

Batch 166080, train_perplexity=18.657124, train_loss=2.926228

Batch 166090, train_perplexity=18.657114, train_loss=2.9262276

Batch 166100, train_perplexity=18.657114, train_loss=2.9262276

Batch 166110, train_perplexity=18.657114, train_loss=2.9262276

Batch 166120, train_perplexity=18.657114, train_loss=2.9262276

Batch 166130, train_perplexity=18.65712, train_loss=2.9262278

Batch 166140, train_perplexity=18.657114, train_loss=2.9262276

Batch 166150, train_perplexity=18.65712, train_loss=2.9262278

Batch 166160, train_perplexity=18.657114, train_loss=2.9262276

Batch 166170, train_perplexity=18.657114, train_loss=2.9262276

Batch 166180, train_perplexity=18.657114, train_loss=2.9262276

Batch 166190, train_perplexity=18.657114, train_loss=2.9262276

Batch 166200, train_perplexity=18.65712, train_loss=2.9262278

Batch 166210, train_perplexity=18.657114, train_loss=2.9262276

Batch 166220, train_perplexity=18.657114, train_loss=2.9262276

Batch 166230, train_perplexity=18.657114, train_loss=2.9262276

Batch 166240, train_perplexity=18.657114, train_loss=2.9262276

Batch 166250, train_perplexity=18.657114, train_loss=2.9262276

Batch 166260, train_perplexity=18.657114, train_loss=2.9262276

Batch 166270, train_perplexity=18.657114, train_loss=2.9262276

Batch 166280, train_perplexity=18.657114, train_loss=2.9262276

Batch 166290, train_perplexity=18.657114, train_loss=2.9262276

Batch 166300, train_perplexity=18.657114, train_loss=2.9262276

Batch 166310, train_perplexity=18.65711, train_loss=2.9262273

Batch 166320, train_perplexity=18.657114, train_loss=2.9262276

Batch 166330, train_perplexity=18.657114, train_loss=2.9262276

Batch 166340, train_perplexity=18.657106, train_loss=2.926227

Batch 166350, train_perplexity=18.657114, train_loss=2.9262276

Batch 166360, train_perplexity=18.65711, train_loss=2.9262273

Batch 166370, train_perplexity=18.65711, train_loss=2.9262273

Batch 166380, train_perplexity=18.65711, train_loss=2.9262273

Batch 166390, train_perplexity=18.657106, train_loss=2.926227

Batch 166400, train_perplexity=18.657106, train_loss=2.926227

Batch 166410, train_perplexity=18.657106, train_loss=2.926227

Batch 166420, train_perplexity=18.657106, train_loss=2.926227

Batch 166430, train_perplexity=18.657106, train_loss=2.926227

Batch 166440, train_perplexity=18.65711, train_loss=2.9262273

Batch 166450, train_perplexity=18.657106, train_loss=2.926227

Batch 166460, train_perplexity=18.657106, train_loss=2.926227

Batch 166470, train_perplexity=18.65711, train_loss=2.9262273

Batch 166480, train_perplexity=18.657106, train_loss=2.926227

Batch 166490, train_perplexity=18.657106, train_loss=2.926227

Batch 166500, train_perplexity=18.657106, train_loss=2.926227

Batch 166510, train_perplexity=18.657106, train_loss=2.926227

Batch 166520, train_perplexity=18.657106, train_loss=2.926227

Batch 166530, train_perplexity=18.657106, train_loss=2.926227

Batch 166540, train_perplexity=18.657106, train_loss=2.926227

Batch 166550, train_perplexity=18.657106, train_loss=2.926227

Batch 166560, train_perplexity=18.657106, train_loss=2.926227

Batch 166570, train_perplexity=18.657106, train_loss=2.926227

Batch 166580, train_perplexity=18.657106, train_loss=2.926227

Batch 166590, train_perplexity=18.657106, train_loss=2.926227

Batch 166600, train_perplexity=18.657106, train_loss=2.926227

Batch 166610, train_perplexity=18.657106, train_loss=2.926227

Batch 166620, train_perplexity=18.657097, train_loss=2.9262266

Batch 166630, train_perplexity=18.6571, train_loss=2.9262269

Batch 166640, train_perplexity=18.6571, train_loss=2.9262269

Batch 166650, train_perplexity=18.657106, train_loss=2.926227

Batch 166660, train_perplexity=18.657097, train_loss=2.9262266

Batch 166670, train_perplexity=18.657097, train_loss=2.9262266

Batch 166680, train_perplexity=18.657097, train_loss=2.9262266

Batch 166690, train_perplexity=18.657106, train_loss=2.926227
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 166700, train_perplexity=18.657097, train_loss=2.9262266

Batch 166710, train_perplexity=18.657097, train_loss=2.9262266

Batch 166720, train_perplexity=18.657097, train_loss=2.9262266

Batch 166730, train_perplexity=18.6571, train_loss=2.9262269

Batch 166740, train_perplexity=18.6571, train_loss=2.9262269

Batch 166750, train_perplexity=18.6571, train_loss=2.9262269

Batch 166760, train_perplexity=18.657097, train_loss=2.9262266

Batch 166770, train_perplexity=18.657097, train_loss=2.9262266

Batch 166780, train_perplexity=18.657097, train_loss=2.9262266

Batch 166790, train_perplexity=18.657097, train_loss=2.9262266

Batch 166800, train_perplexity=18.657097, train_loss=2.9262266

Batch 166810, train_perplexity=18.657097, train_loss=2.9262266

Batch 166820, train_perplexity=18.657097, train_loss=2.9262266

Batch 166830, train_perplexity=18.657097, train_loss=2.9262266

Batch 166840, train_perplexity=18.657097, train_loss=2.9262266

Batch 166850, train_perplexity=18.657097, train_loss=2.9262266

Batch 166860, train_perplexity=18.657097, train_loss=2.9262266

Batch 166870, train_perplexity=18.657097, train_loss=2.9262266

Batch 166880, train_perplexity=18.657097, train_loss=2.9262266

Batch 166890, train_perplexity=18.657097, train_loss=2.9262266

Batch 166900, train_perplexity=18.657097, train_loss=2.9262266

Batch 166910, train_perplexity=18.657093, train_loss=2.9262264

Batch 166920, train_perplexity=18.657097, train_loss=2.9262266

Batch 166930, train_perplexity=18.657087, train_loss=2.9262261

Batch 166940, train_perplexity=18.657093, train_loss=2.9262264

Batch 166950, train_perplexity=18.657093, train_loss=2.9262264

Batch 166960, train_perplexity=18.657093, train_loss=2.9262264

Batch 166970, train_perplexity=18.657093, train_loss=2.9262264

Batch 166980, train_perplexity=18.657093, train_loss=2.9262264

Batch 166990, train_perplexity=18.657097, train_loss=2.9262266

Batch 167000, train_perplexity=18.657087, train_loss=2.9262261

Batch 167010, train_perplexity=18.657087, train_loss=2.9262261

Batch 167020, train_perplexity=18.657093, train_loss=2.9262264

Batch 167030, train_perplexity=18.657087, train_loss=2.9262261

Batch 167040, train_perplexity=18.657087, train_loss=2.9262261

Batch 167050, train_perplexity=18.657087, train_loss=2.9262261

Batch 167060, train_perplexity=18.657087, train_loss=2.9262261

Batch 167070, train_perplexity=18.657087, train_loss=2.9262261

Batch 167080, train_perplexity=18.657087, train_loss=2.9262261

Batch 167090, train_perplexity=18.657087, train_loss=2.9262261

Batch 167100, train_perplexity=18.657087, train_loss=2.9262261

Batch 167110, train_perplexity=18.657087, train_loss=2.9262261

Batch 167120, train_perplexity=18.657087, train_loss=2.9262261

Batch 167130, train_perplexity=18.657087, train_loss=2.9262261

Batch 167140, train_perplexity=18.657087, train_loss=2.9262261

Batch 167150, train_perplexity=18.657087, train_loss=2.9262261

Batch 167160, train_perplexity=18.657087, train_loss=2.9262261

Batch 167170, train_perplexity=18.657087, train_loss=2.9262261

Batch 167180, train_perplexity=18.657087, train_loss=2.9262261

Batch 167190, train_perplexity=18.657087, train_loss=2.9262261

Batch 167200, train_perplexity=18.657087, train_loss=2.9262261

Batch 167210, train_perplexity=18.657087, train_loss=2.9262261

Batch 167220, train_perplexity=18.657087, train_loss=2.9262261

Batch 167230, train_perplexity=18.657084, train_loss=2.926226

Batch 167240, train_perplexity=18.657087, train_loss=2.9262261

Batch 167250, train_perplexity=18.65708, train_loss=2.9262257

Batch 167260, train_perplexity=18.657087, train_loss=2.9262261

Batch 167270, train_perplexity=18.657084, train_loss=2.926226

Batch 167280, train_perplexity=18.657084, train_loss=2.926226

Batch 167290, train_perplexity=18.657084, train_loss=2.926226

Batch 167300, train_perplexity=18.657084, train_loss=2.926226

Batch 167310, train_perplexity=18.657084, train_loss=2.926226

Batch 167320, train_perplexity=18.657087, train_loss=2.9262261

Batch 167330, train_perplexity=18.65708, train_loss=2.9262257

Batch 167340, train_perplexity=18.65708, train_loss=2.9262257

Batch 167350, train_perplexity=18.65708, train_loss=2.9262257

Batch 167360, train_perplexity=18.65708, train_loss=2.9262257

Batch 167370, train_perplexity=18.65708, train_loss=2.9262257

Batch 167380, train_perplexity=18.65708, train_loss=2.9262257

Batch 167390, train_perplexity=18.65708, train_loss=2.9262257

Batch 167400, train_perplexity=18.65708, train_loss=2.9262257

Batch 167410, train_perplexity=18.65708, train_loss=2.9262257

Batch 167420, train_perplexity=18.65708, train_loss=2.9262257

Batch 167430, train_perplexity=18.65708, train_loss=2.9262257

Batch 167440, train_perplexity=18.65708, train_loss=2.9262257

Batch 167450, train_perplexity=18.65708, train_loss=2.9262257

Batch 167460, train_perplexity=18.65708, train_loss=2.9262257

Batch 167470, train_perplexity=18.65708, train_loss=2.9262257

Batch 167480, train_perplexity=18.65708, train_loss=2.9262257

Batch 167490, train_perplexity=18.65708, train_loss=2.9262257

Batch 167500, train_perplexity=18.65708, train_loss=2.9262257

Batch 167510, train_perplexity=18.657074, train_loss=2.9262254

Batch 167520, train_perplexity=18.657074, train_loss=2.9262254

Batch 167530, train_perplexity=18.65708, train_loss=2.9262257

Batch 167540, train_perplexity=18.65708, train_loss=2.9262257

Batch 167550, train_perplexity=18.65708, train_loss=2.9262257

Batch 167560, train_perplexity=18.65708, train_loss=2.9262257

Batch 167570, train_perplexity=18.657074, train_loss=2.9262254

Batch 167580, train_perplexity=18.657074, train_loss=2.9262254

Batch 167590, train_perplexity=18.657074, train_loss=2.9262254

Batch 167600, train_perplexity=18.65707, train_loss=2.9262252

Batch 167610, train_perplexity=18.657074, train_loss=2.9262254

Batch 167620, train_perplexity=18.65707, train_loss=2.9262252

Batch 167630, train_perplexity=18.657074, train_loss=2.9262254

Batch 167640, train_perplexity=18.657074, train_loss=2.9262254

Batch 167650, train_perplexity=18.65707, train_loss=2.9262252

Batch 167660, train_perplexity=18.65707, train_loss=2.9262252

Batch 167670, train_perplexity=18.65707, train_loss=2.9262252

Batch 167680, train_perplexity=18.65707, train_loss=2.9262252

Batch 167690, train_perplexity=18.65707, train_loss=2.9262252

Batch 167700, train_perplexity=18.65707, train_loss=2.9262252

Batch 167710, train_perplexity=18.65707, train_loss=2.9262252

Batch 167720, train_perplexity=18.657074, train_loss=2.9262254

Batch 167730, train_perplexity=18.65707, train_loss=2.9262252

Batch 167740, train_perplexity=18.65707, train_loss=2.9262252

Batch 167750, train_perplexity=18.65707, train_loss=2.9262252

Batch 167760, train_perplexity=18.65707, train_loss=2.9262252

Batch 167770, train_perplexity=18.65707, train_loss=2.9262252

Batch 167780, train_perplexity=18.657066, train_loss=2.926225

Batch 167790, train_perplexity=18.65707, train_loss=2.9262252

Batch 167800, train_perplexity=18.65707, train_loss=2.9262252

Batch 167810, train_perplexity=18.65707, train_loss=2.9262252

Batch 167820, train_perplexity=18.65707, train_loss=2.9262252

Batch 167830, train_perplexity=18.65707, train_loss=2.9262252

Batch 167840, train_perplexity=18.65707, train_loss=2.9262252

Batch 167850, train_perplexity=18.65706, train_loss=2.9262247

Batch 167860, train_perplexity=18.65707, train_loss=2.9262252

Batch 167870, train_perplexity=18.657066, train_loss=2.926225

Batch 167880, train_perplexity=18.65707, train_loss=2.9262252

Batch 167890, train_perplexity=18.65707, train_loss=2.9262252

Batch 167900, train_perplexity=18.65706, train_loss=2.9262247

Batch 167910, train_perplexity=18.65706, train_loss=2.9262247

Batch 167920, train_perplexity=18.657066, train_loss=2.926225

Batch 167930, train_perplexity=18.65706, train_loss=2.9262247

Batch 167940, train_perplexity=18.65706, train_loss=2.9262247

Batch 167950, train_perplexity=18.657066, train_loss=2.926225

Batch 167960, train_perplexity=18.65706, train_loss=2.9262247

Batch 167970, train_perplexity=18.65706, train_loss=2.9262247

Batch 167980, train_perplexity=18.65706, train_loss=2.9262247

Batch 167990, train_perplexity=18.65706, train_loss=2.9262247
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 168000, train_perplexity=18.65706, train_loss=2.9262247

Batch 168010, train_perplexity=18.65706, train_loss=2.9262247

Batch 168020, train_perplexity=18.65706, train_loss=2.9262247

Batch 168030, train_perplexity=18.65706, train_loss=2.9262247

Batch 168040, train_perplexity=18.65706, train_loss=2.9262247

Batch 168050, train_perplexity=18.65706, train_loss=2.9262247

Batch 168060, train_perplexity=18.65706, train_loss=2.9262247

Batch 168070, train_perplexity=18.65706, train_loss=2.9262247

Batch 168080, train_perplexity=18.65706, train_loss=2.9262247

Batch 168090, train_perplexity=18.65706, train_loss=2.9262247

Batch 168100, train_perplexity=18.65706, train_loss=2.9262247

Batch 168110, train_perplexity=18.65706, train_loss=2.9262247

Batch 168120, train_perplexity=18.65706, train_loss=2.9262247

Batch 168130, train_perplexity=18.65706, train_loss=2.9262247

Batch 168140, train_perplexity=18.65706, train_loss=2.9262247

Batch 168150, train_perplexity=18.657057, train_loss=2.9262245

Batch 168160, train_perplexity=18.657053, train_loss=2.9262242

Batch 168170, train_perplexity=18.657057, train_loss=2.9262245

Batch 168180, train_perplexity=18.65706, train_loss=2.9262247

Batch 168190, train_perplexity=18.657057, train_loss=2.9262245

Batch 168200, train_perplexity=18.657057, train_loss=2.9262245

Batch 168210, train_perplexity=18.65706, train_loss=2.9262247

Batch 168220, train_perplexity=18.657053, train_loss=2.9262242

Batch 168230, train_perplexity=18.657057, train_loss=2.9262245

Batch 168240, train_perplexity=18.657057, train_loss=2.9262245

Batch 168250, train_perplexity=18.657057, train_loss=2.9262245

Batch 168260, train_perplexity=18.657053, train_loss=2.9262242

Batch 168270, train_perplexity=18.657053, train_loss=2.9262242

Batch 168280, train_perplexity=18.657053, train_loss=2.9262242

Batch 168290, train_perplexity=18.657057, train_loss=2.9262245

Batch 168300, train_perplexity=18.657053, train_loss=2.9262242

Batch 168310, train_perplexity=18.657053, train_loss=2.9262242

Batch 168320, train_perplexity=18.657053, train_loss=2.9262242

Batch 168330, train_perplexity=18.657053, train_loss=2.9262242

Batch 168340, train_perplexity=18.657053, train_loss=2.9262242

Batch 168350, train_perplexity=18.657053, train_loss=2.9262242

Batch 168360, train_perplexity=18.657053, train_loss=2.9262242

Batch 168370, train_perplexity=18.657053, train_loss=2.9262242

Batch 168380, train_perplexity=18.657053, train_loss=2.9262242

Batch 168390, train_perplexity=18.657053, train_loss=2.9262242

Batch 168400, train_perplexity=18.657047, train_loss=2.926224

Batch 168410, train_perplexity=18.657053, train_loss=2.9262242

Batch 168420, train_perplexity=18.657053, train_loss=2.9262242

Batch 168430, train_perplexity=18.657047, train_loss=2.926224

Batch 168440, train_perplexity=18.657043, train_loss=2.9262238

Batch 168450, train_perplexity=18.657047, train_loss=2.926224

Batch 168460, train_perplexity=18.657047, train_loss=2.926224

Batch 168470, train_perplexity=18.657047, train_loss=2.926224

Batch 168480, train_perplexity=18.657053, train_loss=2.9262242

Batch 168490, train_perplexity=18.657043, train_loss=2.9262238

Batch 168500, train_perplexity=18.657047, train_loss=2.926224

Batch 168510, train_perplexity=18.657047, train_loss=2.926224

Batch 168520, train_perplexity=18.657047, train_loss=2.926224

Batch 168530, train_perplexity=18.657047, train_loss=2.926224

Batch 168540, train_perplexity=18.657053, train_loss=2.9262242

Batch 168550, train_perplexity=18.657043, train_loss=2.9262238

Batch 168560, train_perplexity=18.657043, train_loss=2.9262238

Batch 168570, train_perplexity=18.657047, train_loss=2.926224

Batch 168580, train_perplexity=18.657043, train_loss=2.9262238

Batch 168590, train_perplexity=18.657043, train_loss=2.9262238

Batch 168600, train_perplexity=18.657043, train_loss=2.9262238

Batch 168610, train_perplexity=18.657043, train_loss=2.9262238

Batch 168620, train_perplexity=18.657043, train_loss=2.9262238

Batch 168630, train_perplexity=18.657043, train_loss=2.9262238

Batch 168640, train_perplexity=18.657043, train_loss=2.9262238

Batch 168650, train_perplexity=18.657043, train_loss=2.9262238

Batch 168660, train_perplexity=18.657043, train_loss=2.9262238

Batch 168670, train_perplexity=18.657043, train_loss=2.9262238

Batch 168680, train_perplexity=18.657043, train_loss=2.9262238

Batch 168690, train_perplexity=18.657043, train_loss=2.9262238

Batch 168700, train_perplexity=18.657043, train_loss=2.9262238

Batch 168710, train_perplexity=18.657043, train_loss=2.9262238

Batch 168720, train_perplexity=18.657043, train_loss=2.9262238

Batch 168730, train_perplexity=18.657043, train_loss=2.9262238

Batch 168740, train_perplexity=18.657034, train_loss=2.9262233

Batch 168750, train_perplexity=18.657043, train_loss=2.9262238

Batch 168760, train_perplexity=18.657043, train_loss=2.9262238

Batch 168770, train_perplexity=18.657043, train_loss=2.9262238

Batch 168780, train_perplexity=18.65704, train_loss=2.9262235

Batch 168790, train_perplexity=18.657034, train_loss=2.9262233

Batch 168800, train_perplexity=18.65704, train_loss=2.9262235

Batch 168810, train_perplexity=18.657043, train_loss=2.9262238

Batch 168820, train_perplexity=18.657043, train_loss=2.9262238

Batch 168830, train_perplexity=18.657043, train_loss=2.9262238

Batch 168840, train_perplexity=18.657034, train_loss=2.9262233

Batch 168850, train_perplexity=18.65704, train_loss=2.9262235

Batch 168860, train_perplexity=18.657034, train_loss=2.9262233

Batch 168870, train_perplexity=18.65704, train_loss=2.9262235

Batch 168880, train_perplexity=18.657034, train_loss=2.9262233

Batch 168890, train_perplexity=18.65704, train_loss=2.9262235

Batch 168900, train_perplexity=18.65704, train_loss=2.9262235

Batch 168910, train_perplexity=18.657034, train_loss=2.9262233

Batch 168920, train_perplexity=18.657034, train_loss=2.9262233

Batch 168930, train_perplexity=18.657034, train_loss=2.9262233

Batch 168940, train_perplexity=18.657034, train_loss=2.9262233

Batch 168950, train_perplexity=18.657034, train_loss=2.9262233

Batch 168960, train_perplexity=18.657034, train_loss=2.9262233

Batch 168970, train_perplexity=18.657034, train_loss=2.9262233

Batch 168980, train_perplexity=18.657034, train_loss=2.9262233

Batch 168990, train_perplexity=18.657034, train_loss=2.9262233

Batch 169000, train_perplexity=18.657034, train_loss=2.9262233

Batch 169010, train_perplexity=18.657034, train_loss=2.9262233

Batch 169020, train_perplexity=18.657034, train_loss=2.9262233

Batch 169030, train_perplexity=18.657026, train_loss=2.9262228

Batch 169040, train_perplexity=18.657034, train_loss=2.9262233

Batch 169050, train_perplexity=18.657034, train_loss=2.9262233

Batch 169060, train_perplexity=18.657034, train_loss=2.9262233

Batch 169070, train_perplexity=18.657034, train_loss=2.9262233

Batch 169080, train_perplexity=18.657034, train_loss=2.9262233

Batch 169090, train_perplexity=18.65703, train_loss=2.926223

Batch 169100, train_perplexity=18.65703, train_loss=2.926223

Batch 169110, train_perplexity=18.657034, train_loss=2.9262233

Batch 169120, train_perplexity=18.65703, train_loss=2.926223

Batch 169130, train_perplexity=18.657034, train_loss=2.9262233

Batch 169140, train_perplexity=18.657026, train_loss=2.9262228

Batch 169150, train_perplexity=18.657026, train_loss=2.9262228

Batch 169160, train_perplexity=18.657026, train_loss=2.9262228

Batch 169170, train_perplexity=18.657034, train_loss=2.9262233

Batch 169180, train_perplexity=18.657026, train_loss=2.9262228

Batch 169190, train_perplexity=18.657026, train_loss=2.9262228

Batch 169200, train_perplexity=18.657026, train_loss=2.9262228

Batch 169210, train_perplexity=18.657026, train_loss=2.9262228

Batch 169220, train_perplexity=18.657026, train_loss=2.9262228

Batch 169230, train_perplexity=18.657026, train_loss=2.9262228

Batch 169240, train_perplexity=18.657026, train_loss=2.9262228

Batch 169250, train_perplexity=18.657026, train_loss=2.9262228

Batch 169260, train_perplexity=18.657026, train_loss=2.9262228

Batch 169270, train_perplexity=18.657026, train_loss=2.9262228

Batch 169280, train_perplexity=18.657026, train_loss=2.9262228
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 169290, train_perplexity=18.657026, train_loss=2.9262228

Batch 169300, train_perplexity=18.657026, train_loss=2.9262228

Batch 169310, train_perplexity=18.657026, train_loss=2.9262228

Batch 169320, train_perplexity=18.657026, train_loss=2.9262228

Batch 169330, train_perplexity=18.657026, train_loss=2.9262228

Batch 169340, train_perplexity=18.657026, train_loss=2.9262228

Batch 169350, train_perplexity=18.657026, train_loss=2.9262228

Batch 169360, train_perplexity=18.657026, train_loss=2.9262228

Batch 169370, train_perplexity=18.65702, train_loss=2.9262226

Batch 169380, train_perplexity=18.657026, train_loss=2.9262228

Batch 169390, train_perplexity=18.65702, train_loss=2.9262226

Batch 169400, train_perplexity=18.65702, train_loss=2.9262226

Batch 169410, train_perplexity=18.657026, train_loss=2.9262228

Batch 169420, train_perplexity=18.657026, train_loss=2.9262228

Batch 169430, train_perplexity=18.65702, train_loss=2.9262226

Batch 169440, train_perplexity=18.657026, train_loss=2.9262228

Batch 169450, train_perplexity=18.657017, train_loss=2.9262223

Batch 169460, train_perplexity=18.657026, train_loss=2.9262228

Batch 169470, train_perplexity=18.657017, train_loss=2.9262223

Batch 169480, train_perplexity=18.657017, train_loss=2.9262223

Batch 169490, train_perplexity=18.657017, train_loss=2.9262223

Batch 169500, train_perplexity=18.657017, train_loss=2.9262223

Batch 169510, train_perplexity=18.65702, train_loss=2.9262226

Batch 169520, train_perplexity=18.657017, train_loss=2.9262223

Batch 169530, train_perplexity=18.657017, train_loss=2.9262223

Batch 169540, train_perplexity=18.657017, train_loss=2.9262223

Batch 169550, train_perplexity=18.657017, train_loss=2.9262223

Batch 169560, train_perplexity=18.657017, train_loss=2.9262223

Batch 169570, train_perplexity=18.657017, train_loss=2.9262223

Batch 169580, train_perplexity=18.657017, train_loss=2.9262223

Batch 169590, train_perplexity=18.657017, train_loss=2.9262223

Batch 169600, train_perplexity=18.657017, train_loss=2.9262223

Batch 169610, train_perplexity=18.657017, train_loss=2.9262223

Batch 169620, train_perplexity=18.657017, train_loss=2.9262223

Batch 169630, train_perplexity=18.657017, train_loss=2.9262223

Batch 169640, train_perplexity=18.657017, train_loss=2.9262223

Batch 169650, train_perplexity=18.657017, train_loss=2.9262223

Batch 169660, train_perplexity=18.657017, train_loss=2.9262223

Batch 169670, train_perplexity=18.657017, train_loss=2.9262223

Batch 169680, train_perplexity=18.657017, train_loss=2.9262223

Batch 169690, train_perplexity=18.657017, train_loss=2.9262223

Batch 169700, train_perplexity=18.657007, train_loss=2.9262218

Batch 169710, train_perplexity=18.657017, train_loss=2.9262223

Batch 169720, train_perplexity=18.657013, train_loss=2.926222

Batch 169730, train_perplexity=18.657007, train_loss=2.9262218

Batch 169740, train_perplexity=18.657017, train_loss=2.9262223

Batch 169750, train_perplexity=18.657017, train_loss=2.9262223

Batch 169760, train_perplexity=18.657013, train_loss=2.926222

Batch 169770, train_perplexity=18.657007, train_loss=2.9262218

Batch 169780, train_perplexity=18.657013, train_loss=2.926222

Batch 169790, train_perplexity=18.657013, train_loss=2.926222

Batch 169800, train_perplexity=18.657013, train_loss=2.926222

Batch 169810, train_perplexity=18.657007, train_loss=2.9262218

Batch 169820, train_perplexity=18.657007, train_loss=2.9262218

Batch 169830, train_perplexity=18.657007, train_loss=2.9262218

Batch 169840, train_perplexity=18.657007, train_loss=2.9262218

Batch 169850, train_perplexity=18.657007, train_loss=2.9262218

Batch 169860, train_perplexity=18.657007, train_loss=2.9262218

Batch 169870, train_perplexity=18.657007, train_loss=2.9262218

Batch 169880, train_perplexity=18.657007, train_loss=2.9262218

Batch 169890, train_perplexity=18.657007, train_loss=2.9262218

Batch 169900, train_perplexity=18.657007, train_loss=2.9262218

Batch 169910, train_perplexity=18.657007, train_loss=2.9262218

Batch 169920, train_perplexity=18.657007, train_loss=2.9262218

Batch 169930, train_perplexity=18.657007, train_loss=2.9262218

Batch 169940, train_perplexity=18.657007, train_loss=2.9262218

Batch 169950, train_perplexity=18.657007, train_loss=2.9262218

Batch 169960, train_perplexity=18.657007, train_loss=2.9262218

Batch 169970, train_perplexity=18.657007, train_loss=2.9262218

Batch 169980, train_perplexity=18.657003, train_loss=2.9262216

Batch 169990, train_perplexity=18.657, train_loss=2.9262214

Batch 170000, train_perplexity=18.657007, train_loss=2.9262218

Batch 170010, train_perplexity=18.657003, train_loss=2.9262216

Batch 170020, train_perplexity=18.657007, train_loss=2.9262218

Batch 170030, train_perplexity=18.657003, train_loss=2.9262216

Batch 170040, train_perplexity=18.657, train_loss=2.9262214

Batch 170050, train_perplexity=18.657007, train_loss=2.9262218

Batch 170060, train_perplexity=18.657003, train_loss=2.9262216

Batch 170070, train_perplexity=18.657003, train_loss=2.9262216

Batch 170080, train_perplexity=18.657003, train_loss=2.9262216

Batch 170090, train_perplexity=18.657003, train_loss=2.9262216

Batch 170100, train_perplexity=18.657, train_loss=2.9262214

Batch 170110, train_perplexity=18.657, train_loss=2.9262214

Batch 170120, train_perplexity=18.657, train_loss=2.9262214

Batch 170130, train_perplexity=18.657, train_loss=2.9262214

Batch 170140, train_perplexity=18.657003, train_loss=2.9262216

Batch 170150, train_perplexity=18.657, train_loss=2.9262214

Batch 170160, train_perplexity=18.657, train_loss=2.9262214

Batch 170170, train_perplexity=18.657, train_loss=2.9262214

Batch 170180, train_perplexity=18.657, train_loss=2.9262214

Batch 170190, train_perplexity=18.657, train_loss=2.9262214

Batch 170200, train_perplexity=18.657, train_loss=2.9262214

Batch 170210, train_perplexity=18.657, train_loss=2.9262214

Batch 170220, train_perplexity=18.657, train_loss=2.9262214

Batch 170230, train_perplexity=18.656994, train_loss=2.9262211

Batch 170240, train_perplexity=18.657, train_loss=2.9262214

Batch 170250, train_perplexity=18.657, train_loss=2.9262214

Batch 170260, train_perplexity=18.657, train_loss=2.9262214

Batch 170270, train_perplexity=18.657, train_loss=2.9262214

Batch 170280, train_perplexity=18.657, train_loss=2.9262214

Batch 170290, train_perplexity=18.656994, train_loss=2.9262211

Batch 170300, train_perplexity=18.657, train_loss=2.9262214

Batch 170310, train_perplexity=18.65699, train_loss=2.926221

Batch 170320, train_perplexity=18.657, train_loss=2.9262214

Batch 170330, train_perplexity=18.656994, train_loss=2.9262211

Batch 170340, train_perplexity=18.657, train_loss=2.9262214

Batch 170350, train_perplexity=18.657, train_loss=2.9262214

Batch 170360, train_perplexity=18.656994, train_loss=2.9262211

Batch 170370, train_perplexity=18.65699, train_loss=2.926221

Batch 170380, train_perplexity=18.656994, train_loss=2.9262211

Batch 170390, train_perplexity=18.656994, train_loss=2.9262211

Batch 170400, train_perplexity=18.656994, train_loss=2.9262211

Batch 170410, train_perplexity=18.65699, train_loss=2.926221

Batch 170420, train_perplexity=18.65699, train_loss=2.926221

Batch 170430, train_perplexity=18.656994, train_loss=2.9262211

Batch 170440, train_perplexity=18.65699, train_loss=2.926221

Batch 170450, train_perplexity=18.65699, train_loss=2.926221

Batch 170460, train_perplexity=18.65699, train_loss=2.926221

Batch 170470, train_perplexity=18.65699, train_loss=2.926221

Batch 170480, train_perplexity=18.65699, train_loss=2.926221

Batch 170490, train_perplexity=18.65699, train_loss=2.926221

Batch 170500, train_perplexity=18.65699, train_loss=2.926221

Batch 170510, train_perplexity=18.65699, train_loss=2.926221

Batch 170520, train_perplexity=18.65699, train_loss=2.926221

Batch 170530, train_perplexity=18.65699, train_loss=2.926221

Batch 170540, train_perplexity=18.65699, train_loss=2.926221

Batch 170550, train_perplexity=18.65699, train_loss=2.926221

Batch 170560, train_perplexity=18.65699, train_loss=2.926221

Batch 170570, train_perplexity=18.65699, train_loss=2.926221

Batch 170580, train_perplexity=18.65699, train_loss=2.926221
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 170590, train_perplexity=18.65699, train_loss=2.926221

Batch 170600, train_perplexity=18.65699, train_loss=2.926221

Batch 170610, train_perplexity=18.65699, train_loss=2.926221

Batch 170620, train_perplexity=18.65699, train_loss=2.926221

Batch 170630, train_perplexity=18.656986, train_loss=2.9262207

Batch 170640, train_perplexity=18.65699, train_loss=2.926221

Batch 170650, train_perplexity=18.656986, train_loss=2.9262207

Batch 170660, train_perplexity=18.656986, train_loss=2.9262207

Batch 170670, train_perplexity=18.65698, train_loss=2.9262204

Batch 170680, train_perplexity=18.65698, train_loss=2.9262204

Batch 170690, train_perplexity=18.656986, train_loss=2.9262207

Batch 170700, train_perplexity=18.65698, train_loss=2.9262204

Batch 170710, train_perplexity=18.656986, train_loss=2.9262207

Batch 170720, train_perplexity=18.65698, train_loss=2.9262204

Batch 170730, train_perplexity=18.65698, train_loss=2.9262204

Batch 170740, train_perplexity=18.656986, train_loss=2.9262207

Batch 170750, train_perplexity=18.65698, train_loss=2.9262204

Batch 170760, train_perplexity=18.65698, train_loss=2.9262204

Batch 170770, train_perplexity=18.65698, train_loss=2.9262204

Batch 170780, train_perplexity=18.65698, train_loss=2.9262204

Batch 170790, train_perplexity=18.65698, train_loss=2.9262204

Batch 170800, train_perplexity=18.65698, train_loss=2.9262204

Batch 170810, train_perplexity=18.65698, train_loss=2.9262204

Batch 170820, train_perplexity=18.65698, train_loss=2.9262204

Batch 170830, train_perplexity=18.65698, train_loss=2.9262204

Batch 170840, train_perplexity=18.656986, train_loss=2.9262207

Batch 170850, train_perplexity=18.65698, train_loss=2.9262204

Batch 170860, train_perplexity=18.65698, train_loss=2.9262204

Batch 170870, train_perplexity=18.65698, train_loss=2.9262204

Batch 170880, train_perplexity=18.656977, train_loss=2.9262202

Batch 170890, train_perplexity=18.65698, train_loss=2.9262204

Batch 170900, train_perplexity=18.65698, train_loss=2.9262204

Batch 170910, train_perplexity=18.65698, train_loss=2.9262204

Batch 170920, train_perplexity=18.656977, train_loss=2.9262202

Batch 170930, train_perplexity=18.656977, train_loss=2.9262202

Batch 170940, train_perplexity=18.656977, train_loss=2.9262202

Batch 170950, train_perplexity=18.65698, train_loss=2.9262204

Batch 170960, train_perplexity=18.656977, train_loss=2.9262202

Batch 170970, train_perplexity=18.65698, train_loss=2.9262204

Batch 170980, train_perplexity=18.656973, train_loss=2.92622

Batch 170990, train_perplexity=18.656973, train_loss=2.92622

Batch 171000, train_perplexity=18.656973, train_loss=2.92622

Batch 171010, train_perplexity=18.656973, train_loss=2.92622

Batch 171020, train_perplexity=18.656977, train_loss=2.9262202

Batch 171030, train_perplexity=18.656977, train_loss=2.9262202

Batch 171040, train_perplexity=18.656977, train_loss=2.9262202

Batch 171050, train_perplexity=18.656977, train_loss=2.9262202

Batch 171060, train_perplexity=18.656973, train_loss=2.92622

Batch 171070, train_perplexity=18.656973, train_loss=2.92622

Batch 171080, train_perplexity=18.656973, train_loss=2.92622

Batch 171090, train_perplexity=18.656973, train_loss=2.92622

Batch 171100, train_perplexity=18.656973, train_loss=2.92622

Batch 171110, train_perplexity=18.656977, train_loss=2.9262202

Batch 171120, train_perplexity=18.656973, train_loss=2.92622

Batch 171130, train_perplexity=18.656973, train_loss=2.92622

Batch 171140, train_perplexity=18.656973, train_loss=2.92622

Batch 171150, train_perplexity=18.656973, train_loss=2.92622

Batch 171160, train_perplexity=18.656973, train_loss=2.92622

Batch 171170, train_perplexity=18.656973, train_loss=2.92622

Batch 171180, train_perplexity=18.656973, train_loss=2.92622

Batch 171190, train_perplexity=18.656973, train_loss=2.92622

Batch 171200, train_perplexity=18.656973, train_loss=2.92622

Batch 171210, train_perplexity=18.656973, train_loss=2.92622

Batch 171220, train_perplexity=18.656973, train_loss=2.92622

Batch 171230, train_perplexity=18.656973, train_loss=2.92622

Batch 171240, train_perplexity=18.656973, train_loss=2.92622

Batch 171250, train_perplexity=18.656973, train_loss=2.92622

Batch 171260, train_perplexity=18.656973, train_loss=2.92622

Batch 171270, train_perplexity=18.656973, train_loss=2.92622

Batch 171280, train_perplexity=18.656973, train_loss=2.92622

Batch 171290, train_perplexity=18.656967, train_loss=2.9262197

Batch 171300, train_perplexity=18.656963, train_loss=2.9262195

Batch 171310, train_perplexity=18.656973, train_loss=2.92622

Batch 171320, train_perplexity=18.656963, train_loss=2.9262195

Batch 171330, train_perplexity=18.656963, train_loss=2.9262195

Batch 171340, train_perplexity=18.656963, train_loss=2.9262195

Batch 171350, train_perplexity=18.656967, train_loss=2.9262197

Batch 171360, train_perplexity=18.656967, train_loss=2.9262197

Batch 171370, train_perplexity=18.656967, train_loss=2.9262197

Batch 171380, train_perplexity=18.656963, train_loss=2.9262195

Batch 171390, train_perplexity=18.656963, train_loss=2.9262195

Batch 171400, train_perplexity=18.656967, train_loss=2.9262197

Batch 171410, train_perplexity=18.656963, train_loss=2.9262195

Batch 171420, train_perplexity=18.656963, train_loss=2.9262195

Batch 171430, train_perplexity=18.656963, train_loss=2.9262195

Batch 171440, train_perplexity=18.656963, train_loss=2.9262195

Batch 171450, train_perplexity=18.656963, train_loss=2.9262195

Batch 171460, train_perplexity=18.65696, train_loss=2.9262192

Batch 171470, train_perplexity=18.656963, train_loss=2.9262195

Batch 171480, train_perplexity=18.656963, train_loss=2.9262195

Batch 171490, train_perplexity=18.656963, train_loss=2.9262195

Batch 171500, train_perplexity=18.656963, train_loss=2.9262195

Batch 171510, train_perplexity=18.656963, train_loss=2.9262195

Batch 171520, train_perplexity=18.656963, train_loss=2.9262195

Batch 171530, train_perplexity=18.656956, train_loss=2.926219

Batch 171540, train_perplexity=18.65696, train_loss=2.9262192

Batch 171550, train_perplexity=18.656963, train_loss=2.9262195

Batch 171560, train_perplexity=18.656963, train_loss=2.9262195

Batch 171570, train_perplexity=18.656963, train_loss=2.9262195

Batch 171580, train_perplexity=18.65696, train_loss=2.9262192

Batch 171590, train_perplexity=18.656963, train_loss=2.9262195

Batch 171600, train_perplexity=18.65696, train_loss=2.9262192

Batch 171610, train_perplexity=18.656956, train_loss=2.926219

Batch 171620, train_perplexity=18.65696, train_loss=2.9262192

Batch 171630, train_perplexity=18.656963, train_loss=2.9262195

Batch 171640, train_perplexity=18.65696, train_loss=2.9262192

Batch 171650, train_perplexity=18.65696, train_loss=2.9262192

Batch 171660, train_perplexity=18.65696, train_loss=2.9262192

Batch 171670, train_perplexity=18.656956, train_loss=2.926219

Batch 171680, train_perplexity=18.65696, train_loss=2.9262192

Batch 171690, train_perplexity=18.656956, train_loss=2.926219

Batch 171700, train_perplexity=18.656956, train_loss=2.926219

Batch 171710, train_perplexity=18.656956, train_loss=2.926219

Batch 171720, train_perplexity=18.65696, train_loss=2.9262192

Batch 171730, train_perplexity=18.656956, train_loss=2.926219

Batch 171740, train_perplexity=18.656956, train_loss=2.926219

Batch 171750, train_perplexity=18.656956, train_loss=2.926219

Batch 171760, train_perplexity=18.656956, train_loss=2.926219

Batch 171770, train_perplexity=18.656956, train_loss=2.926219

Batch 171780, train_perplexity=18.656956, train_loss=2.926219

Batch 171790, train_perplexity=18.656956, train_loss=2.926219

Batch 171800, train_perplexity=18.656956, train_loss=2.926219

Batch 171810, train_perplexity=18.65695, train_loss=2.9262187

Batch 171820, train_perplexity=18.656956, train_loss=2.926219

Batch 171830, train_perplexity=18.656956, train_loss=2.926219

Batch 171840, train_perplexity=18.656956, train_loss=2.926219

Batch 171850, train_perplexity=18.656946, train_loss=2.9262185

Batch 171860, train_perplexity=18.656956, train_loss=2.926219

Batch 171870, train_perplexity=18.656946, train_loss=2.9262185

Batch 171880, train_perplexity=18.656956, train_loss=2.926219
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 171890, train_perplexity=18.65695, train_loss=2.9262187

Batch 171900, train_perplexity=18.656956, train_loss=2.926219

Batch 171910, train_perplexity=18.656956, train_loss=2.926219

Batch 171920, train_perplexity=18.656946, train_loss=2.9262185

Batch 171930, train_perplexity=18.656956, train_loss=2.926219

Batch 171940, train_perplexity=18.65695, train_loss=2.9262187

Batch 171950, train_perplexity=18.65695, train_loss=2.9262187

Batch 171960, train_perplexity=18.656946, train_loss=2.9262185

Batch 171970, train_perplexity=18.656946, train_loss=2.9262185

Batch 171980, train_perplexity=18.65695, train_loss=2.9262187

Batch 171990, train_perplexity=18.65695, train_loss=2.9262187

Batch 172000, train_perplexity=18.656946, train_loss=2.9262185

Batch 172010, train_perplexity=18.656946, train_loss=2.9262185

Batch 172020, train_perplexity=18.656946, train_loss=2.9262185

Batch 172030, train_perplexity=18.65695, train_loss=2.9262187

Batch 172040, train_perplexity=18.656946, train_loss=2.9262185

Batch 172050, train_perplexity=18.656946, train_loss=2.9262185

Batch 172060, train_perplexity=18.656946, train_loss=2.9262185

Batch 172070, train_perplexity=18.656946, train_loss=2.9262185

Batch 172080, train_perplexity=18.656946, train_loss=2.9262185

Batch 172090, train_perplexity=18.656946, train_loss=2.9262185

Batch 172100, train_perplexity=18.656946, train_loss=2.9262185

Batch 172110, train_perplexity=18.656946, train_loss=2.9262185

Batch 172120, train_perplexity=18.656946, train_loss=2.9262185

Batch 172130, train_perplexity=18.656946, train_loss=2.9262185

Batch 172140, train_perplexity=18.656946, train_loss=2.9262185

Batch 172150, train_perplexity=18.656942, train_loss=2.9262183

Batch 172160, train_perplexity=18.656946, train_loss=2.9262185

Batch 172170, train_perplexity=18.656946, train_loss=2.9262185

Batch 172180, train_perplexity=18.656946, train_loss=2.9262185

Batch 172190, train_perplexity=18.656942, train_loss=2.9262183

Batch 172200, train_perplexity=18.656946, train_loss=2.9262185

Batch 172210, train_perplexity=18.656946, train_loss=2.9262185

Batch 172220, train_perplexity=18.656942, train_loss=2.9262183

Batch 172230, train_perplexity=18.656942, train_loss=2.9262183

Batch 172240, train_perplexity=18.656937, train_loss=2.926218

Batch 172250, train_perplexity=18.656942, train_loss=2.9262183

Batch 172260, train_perplexity=18.656937, train_loss=2.926218

Batch 172270, train_perplexity=18.656937, train_loss=2.926218

Batch 172280, train_perplexity=18.656942, train_loss=2.9262183

Batch 172290, train_perplexity=18.656937, train_loss=2.926218

Batch 172300, train_perplexity=18.656937, train_loss=2.926218

Batch 172310, train_perplexity=18.656946, train_loss=2.9262185

Batch 172320, train_perplexity=18.656946, train_loss=2.9262185

Batch 172330, train_perplexity=18.656937, train_loss=2.926218

Batch 172340, train_perplexity=18.656937, train_loss=2.926218

Batch 172350, train_perplexity=18.656937, train_loss=2.926218

Batch 172360, train_perplexity=18.656937, train_loss=2.926218

Batch 172370, train_perplexity=18.656937, train_loss=2.926218

Batch 172380, train_perplexity=18.656937, train_loss=2.926218

Batch 172390, train_perplexity=18.656937, train_loss=2.926218

Batch 172400, train_perplexity=18.656937, train_loss=2.926218

Batch 172410, train_perplexity=18.656937, train_loss=2.926218

Batch 172420, train_perplexity=18.656937, train_loss=2.926218

Batch 172430, train_perplexity=18.656937, train_loss=2.926218

Batch 172440, train_perplexity=18.656937, train_loss=2.926218

Batch 172450, train_perplexity=18.656937, train_loss=2.926218

Batch 172460, train_perplexity=18.656937, train_loss=2.926218

Batch 172470, train_perplexity=18.656937, train_loss=2.926218

Batch 172480, train_perplexity=18.656937, train_loss=2.926218

Batch 172490, train_perplexity=18.656937, train_loss=2.926218

Batch 172500, train_perplexity=18.656933, train_loss=2.9262178

Batch 172510, train_perplexity=18.656937, train_loss=2.926218

Batch 172520, train_perplexity=18.656937, train_loss=2.926218

Batch 172530, train_perplexity=18.656933, train_loss=2.9262178

Batch 172540, train_perplexity=18.656937, train_loss=2.926218

Batch 172550, train_perplexity=18.656937, train_loss=2.926218

Batch 172560, train_perplexity=18.656929, train_loss=2.9262176

Batch 172570, train_perplexity=18.656929, train_loss=2.9262176

Batch 172580, train_perplexity=18.656929, train_loss=2.9262176

Batch 172590, train_perplexity=18.656929, train_loss=2.9262176

Batch 172600, train_perplexity=18.656929, train_loss=2.9262176

Batch 172610, train_perplexity=18.656937, train_loss=2.926218

Batch 172620, train_perplexity=18.656933, train_loss=2.9262178

Batch 172630, train_perplexity=18.656933, train_loss=2.9262178

Batch 172640, train_perplexity=18.656929, train_loss=2.9262176

Batch 172650, train_perplexity=18.656929, train_loss=2.9262176

Batch 172660, train_perplexity=18.656929, train_loss=2.9262176

Batch 172670, train_perplexity=18.656929, train_loss=2.9262176

Batch 172680, train_perplexity=18.656929, train_loss=2.9262176

Batch 172690, train_perplexity=18.656929, train_loss=2.9262176

Batch 172700, train_perplexity=18.656929, train_loss=2.9262176

Batch 172710, train_perplexity=18.656923, train_loss=2.9262173

Batch 172720, train_perplexity=18.656929, train_loss=2.9262176

Batch 172730, train_perplexity=18.656929, train_loss=2.9262176

Batch 172740, train_perplexity=18.656929, train_loss=2.9262176

Batch 172750, train_perplexity=18.656929, train_loss=2.9262176

Batch 172760, train_perplexity=18.656929, train_loss=2.9262176

Batch 172770, train_perplexity=18.656929, train_loss=2.9262176

Batch 172780, train_perplexity=18.656929, train_loss=2.9262176

Batch 172790, train_perplexity=18.656929, train_loss=2.9262176

Batch 172800, train_perplexity=18.656929, train_loss=2.9262176

Batch 172810, train_perplexity=18.656929, train_loss=2.9262176

Batch 172820, train_perplexity=18.656923, train_loss=2.9262173

Batch 172830, train_perplexity=18.656929, train_loss=2.9262176

Batch 172840, train_perplexity=18.656923, train_loss=2.9262173

Batch 172850, train_perplexity=18.65692, train_loss=2.926217

Batch 172860, train_perplexity=18.65692, train_loss=2.926217

Batch 172870, train_perplexity=18.656923, train_loss=2.9262173

Batch 172880, train_perplexity=18.656923, train_loss=2.9262173

Batch 172890, train_perplexity=18.656923, train_loss=2.9262173

Batch 172900, train_perplexity=18.65692, train_loss=2.926217

Batch 172910, train_perplexity=18.656929, train_loss=2.9262176

Batch 172920, train_perplexity=18.656929, train_loss=2.9262176

Batch 172930, train_perplexity=18.656929, train_loss=2.9262176

Batch 172940, train_perplexity=18.65692, train_loss=2.926217

Batch 172950, train_perplexity=18.656923, train_loss=2.9262173

Batch 172960, train_perplexity=18.65692, train_loss=2.926217

Batch 172970, train_perplexity=18.65692, train_loss=2.926217

Batch 172980, train_perplexity=18.656923, train_loss=2.9262173

Batch 172990, train_perplexity=18.65692, train_loss=2.926217

Batch 173000, train_perplexity=18.65692, train_loss=2.926217

Batch 173010, train_perplexity=18.656923, train_loss=2.9262173

Batch 173020, train_perplexity=18.65692, train_loss=2.926217

Batch 173030, train_perplexity=18.65692, train_loss=2.926217

Batch 173040, train_perplexity=18.65692, train_loss=2.926217

Batch 173050, train_perplexity=18.65692, train_loss=2.926217

Batch 173060, train_perplexity=18.65692, train_loss=2.926217

Batch 173070, train_perplexity=18.65692, train_loss=2.926217

Batch 173080, train_perplexity=18.65692, train_loss=2.926217

Batch 173090, train_perplexity=18.656916, train_loss=2.9262168

Batch 173100, train_perplexity=18.65692, train_loss=2.926217

Batch 173110, train_perplexity=18.65692, train_loss=2.926217

Batch 173120, train_perplexity=18.65692, train_loss=2.926217

Batch 173130, train_perplexity=18.65692, train_loss=2.926217

Batch 173140, train_perplexity=18.65692, train_loss=2.926217

Batch 173150, train_perplexity=18.65692, train_loss=2.926217

Batch 173160, train_perplexity=18.65691, train_loss=2.9262166

Batch 173170, train_perplexity=18.65691, train_loss=2.9262166

Batch 173180, train_perplexity=18.656916, train_loss=2.9262168
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 173190, train_perplexity=18.65692, train_loss=2.926217

Batch 173200, train_perplexity=18.65691, train_loss=2.9262166

Batch 173210, train_perplexity=18.65691, train_loss=2.9262166

Batch 173220, train_perplexity=18.656916, train_loss=2.9262168

Batch 173230, train_perplexity=18.65691, train_loss=2.9262166

Batch 173240, train_perplexity=18.65691, train_loss=2.9262166

Batch 173250, train_perplexity=18.65691, train_loss=2.9262166

Batch 173260, train_perplexity=18.65691, train_loss=2.9262166

Batch 173270, train_perplexity=18.65691, train_loss=2.9262166

Batch 173280, train_perplexity=18.65691, train_loss=2.9262166

Batch 173290, train_perplexity=18.65691, train_loss=2.9262166

Batch 173300, train_perplexity=18.65691, train_loss=2.9262166

Batch 173310, train_perplexity=18.65691, train_loss=2.9262166

Batch 173320, train_perplexity=18.65691, train_loss=2.9262166

Batch 173330, train_perplexity=18.656906, train_loss=2.9262164

Batch 173340, train_perplexity=18.65691, train_loss=2.9262166

Batch 173350, train_perplexity=18.65691, train_loss=2.9262166

Batch 173360, train_perplexity=18.65691, train_loss=2.9262166

Batch 173370, train_perplexity=18.65691, train_loss=2.9262166

Batch 173380, train_perplexity=18.65691, train_loss=2.9262166

Batch 173390, train_perplexity=18.65691, train_loss=2.9262166

Batch 173400, train_perplexity=18.65691, train_loss=2.9262166

Batch 173410, train_perplexity=18.65691, train_loss=2.9262166

Batch 173420, train_perplexity=18.656906, train_loss=2.9262164

Batch 173430, train_perplexity=18.656902, train_loss=2.9262161

Batch 173440, train_perplexity=18.656906, train_loss=2.9262164

Batch 173450, train_perplexity=18.65691, train_loss=2.9262166

Batch 173460, train_perplexity=18.656906, train_loss=2.9262164

Batch 173470, train_perplexity=18.65691, train_loss=2.9262166

Batch 173480, train_perplexity=18.65691, train_loss=2.9262166

Batch 173490, train_perplexity=18.656902, train_loss=2.9262161

Batch 173500, train_perplexity=18.656902, train_loss=2.9262161

Batch 173510, train_perplexity=18.656902, train_loss=2.9262161

Batch 173520, train_perplexity=18.65691, train_loss=2.9262166

Batch 173530, train_perplexity=18.656902, train_loss=2.9262161

Batch 173540, train_perplexity=18.656902, train_loss=2.9262161

Batch 173550, train_perplexity=18.656902, train_loss=2.9262161

Batch 173560, train_perplexity=18.656902, train_loss=2.9262161

Batch 173570, train_perplexity=18.656902, train_loss=2.9262161

Batch 173580, train_perplexity=18.656906, train_loss=2.9262164

Batch 173590, train_perplexity=18.65691, train_loss=2.9262166

Batch 173600, train_perplexity=18.656902, train_loss=2.9262161

Batch 173610, train_perplexity=18.656902, train_loss=2.9262161

Batch 173620, train_perplexity=18.656902, train_loss=2.9262161

Batch 173630, train_perplexity=18.656902, train_loss=2.9262161

Batch 173640, train_perplexity=18.656902, train_loss=2.9262161

Batch 173650, train_perplexity=18.656902, train_loss=2.9262161

Batch 173660, train_perplexity=18.656902, train_loss=2.9262161

Batch 173670, train_perplexity=18.656902, train_loss=2.9262161

Batch 173680, train_perplexity=18.656902, train_loss=2.9262161

Batch 173690, train_perplexity=18.656902, train_loss=2.9262161

Batch 173700, train_perplexity=18.656893, train_loss=2.9262156

Batch 173710, train_perplexity=18.656902, train_loss=2.9262161

Batch 173720, train_perplexity=18.656902, train_loss=2.9262161

Batch 173730, train_perplexity=18.656902, train_loss=2.9262161

Batch 173740, train_perplexity=18.656902, train_loss=2.9262161

Batch 173750, train_perplexity=18.656893, train_loss=2.9262156

Batch 173760, train_perplexity=18.656897, train_loss=2.926216

Batch 173770, train_perplexity=18.656902, train_loss=2.9262161

Batch 173780, train_perplexity=18.656897, train_loss=2.926216

Batch 173790, train_perplexity=18.656897, train_loss=2.926216

Batch 173800, train_perplexity=18.656893, train_loss=2.9262156

Batch 173810, train_perplexity=18.656897, train_loss=2.926216

Batch 173820, train_perplexity=18.656897, train_loss=2.926216

Batch 173830, train_perplexity=18.656897, train_loss=2.926216

Batch 173840, train_perplexity=18.656893, train_loss=2.9262156

Batch 173850, train_perplexity=18.656893, train_loss=2.9262156

Batch 173860, train_perplexity=18.656897, train_loss=2.926216

Batch 173870, train_perplexity=18.656893, train_loss=2.9262156

Batch 173880, train_perplexity=18.656893, train_loss=2.9262156

Batch 173890, train_perplexity=18.656893, train_loss=2.9262156

Batch 173900, train_perplexity=18.656893, train_loss=2.9262156

Batch 173910, train_perplexity=18.656893, train_loss=2.9262156

Batch 173920, train_perplexity=18.656893, train_loss=2.9262156

Batch 173930, train_perplexity=18.656893, train_loss=2.9262156

Batch 173940, train_perplexity=18.656893, train_loss=2.9262156

Batch 173950, train_perplexity=18.656893, train_loss=2.9262156

Batch 173960, train_perplexity=18.656893, train_loss=2.9262156

Batch 173970, train_perplexity=18.656893, train_loss=2.9262156

Batch 173980, train_perplexity=18.656893, train_loss=2.9262156

Batch 173990, train_perplexity=18.656893, train_loss=2.9262156

Batch 174000, train_perplexity=18.656893, train_loss=2.9262156

Batch 174010, train_perplexity=18.656893, train_loss=2.9262156

Batch 174020, train_perplexity=18.656893, train_loss=2.9262156

Batch 174030, train_perplexity=18.656889, train_loss=2.9262154

Batch 174040, train_perplexity=18.656893, train_loss=2.9262156

Batch 174050, train_perplexity=18.656893, train_loss=2.9262156

Batch 174060, train_perplexity=18.656893, train_loss=2.9262156

Batch 174070, train_perplexity=18.656889, train_loss=2.9262154

Batch 174080, train_perplexity=18.656889, train_loss=2.9262154

Batch 174090, train_perplexity=18.656889, train_loss=2.9262154

Batch 174100, train_perplexity=18.656893, train_loss=2.9262156

Batch 174110, train_perplexity=18.656889, train_loss=2.9262154

Batch 174120, train_perplexity=18.656883, train_loss=2.9262152

Batch 174130, train_perplexity=18.656883, train_loss=2.9262152

Batch 174140, train_perplexity=18.656893, train_loss=2.9262156

Batch 174150, train_perplexity=18.656893, train_loss=2.9262156

Batch 174160, train_perplexity=18.656883, train_loss=2.9262152

Batch 174170, train_perplexity=18.656883, train_loss=2.9262152

Batch 174180, train_perplexity=18.656883, train_loss=2.9262152

Batch 174190, train_perplexity=18.656883, train_loss=2.9262152

Batch 174200, train_perplexity=18.656883, train_loss=2.9262152

Batch 174210, train_perplexity=18.656883, train_loss=2.9262152

Batch 174220, train_perplexity=18.656883, train_loss=2.9262152

Batch 174230, train_perplexity=18.656883, train_loss=2.9262152

Batch 174240, train_perplexity=18.656883, train_loss=2.9262152

Batch 174250, train_perplexity=18.656883, train_loss=2.9262152

Batch 174260, train_perplexity=18.65688, train_loss=2.926215

Batch 174270, train_perplexity=18.656883, train_loss=2.9262152

Batch 174280, train_perplexity=18.656883, train_loss=2.9262152

Batch 174290, train_perplexity=18.656883, train_loss=2.9262152

Batch 174300, train_perplexity=18.656883, train_loss=2.9262152

Batch 174310, train_perplexity=18.656883, train_loss=2.9262152

Batch 174320, train_perplexity=18.656883, train_loss=2.9262152

Batch 174330, train_perplexity=18.656883, train_loss=2.9262152

Batch 174340, train_perplexity=18.656883, train_loss=2.9262152

Batch 174350, train_perplexity=18.656883, train_loss=2.9262152

Batch 174360, train_perplexity=18.656883, train_loss=2.9262152

Batch 174370, train_perplexity=18.656876, train_loss=2.9262147

Batch 174380, train_perplexity=18.656883, train_loss=2.9262152

Batch 174390, train_perplexity=18.656876, train_loss=2.9262147

Batch 174400, train_perplexity=18.656876, train_loss=2.9262147

Batch 174410, train_perplexity=18.656883, train_loss=2.9262152

Batch 174420, train_perplexity=18.656876, train_loss=2.9262147

Batch 174430, train_perplexity=18.65688, train_loss=2.926215

Batch 174440, train_perplexity=18.656876, train_loss=2.9262147

Batch 174450, train_perplexity=18.656876, train_loss=2.9262147

Batch 174460, train_perplexity=18.656883, train_loss=2.9262152

Batch 174470, train_perplexity=18.65688, train_loss=2.926215
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 174480, train_perplexity=18.65688, train_loss=2.926215

Batch 174490, train_perplexity=18.656876, train_loss=2.9262147

Batch 174500, train_perplexity=18.656876, train_loss=2.9262147

Batch 174510, train_perplexity=18.65687, train_loss=2.9262145

Batch 174520, train_perplexity=18.656876, train_loss=2.9262147

Batch 174530, train_perplexity=18.656876, train_loss=2.9262147

Batch 174540, train_perplexity=18.656876, train_loss=2.9262147

Batch 174550, train_perplexity=18.656876, train_loss=2.9262147

Batch 174560, train_perplexity=18.656876, train_loss=2.9262147

Batch 174570, train_perplexity=18.656876, train_loss=2.9262147

Batch 174580, train_perplexity=18.656876, train_loss=2.9262147

Batch 174590, train_perplexity=18.656876, train_loss=2.9262147

Batch 174600, train_perplexity=18.656876, train_loss=2.9262147

Batch 174610, train_perplexity=18.656876, train_loss=2.9262147

Batch 174620, train_perplexity=18.656876, train_loss=2.9262147

Batch 174630, train_perplexity=18.656876, train_loss=2.9262147

Batch 174640, train_perplexity=18.656876, train_loss=2.9262147

Batch 174650, train_perplexity=18.656876, train_loss=2.9262147

Batch 174660, train_perplexity=18.656876, train_loss=2.9262147

Batch 174670, train_perplexity=18.656876, train_loss=2.9262147

Batch 174680, train_perplexity=18.656876, train_loss=2.9262147

Batch 174690, train_perplexity=18.656866, train_loss=2.9262142

Batch 174700, train_perplexity=18.65687, train_loss=2.9262145

Batch 174710, train_perplexity=18.656876, train_loss=2.9262147

Batch 174720, train_perplexity=18.65687, train_loss=2.9262145

Batch 174730, train_perplexity=18.656866, train_loss=2.9262142

Batch 174740, train_perplexity=18.656876, train_loss=2.9262147

Batch 174750, train_perplexity=18.656866, train_loss=2.9262142

Batch 174760, train_perplexity=18.656866, train_loss=2.9262142

Batch 174770, train_perplexity=18.656866, train_loss=2.9262142

Batch 174780, train_perplexity=18.65687, train_loss=2.9262145

Batch 174790, train_perplexity=18.65687, train_loss=2.9262145

Batch 174800, train_perplexity=18.656866, train_loss=2.9262142

Batch 174810, train_perplexity=18.656866, train_loss=2.9262142

Batch 174820, train_perplexity=18.65687, train_loss=2.9262145

Batch 174830, train_perplexity=18.65687, train_loss=2.9262145

Batch 174840, train_perplexity=18.656866, train_loss=2.9262142

Batch 174850, train_perplexity=18.656866, train_loss=2.9262142

Batch 174860, train_perplexity=18.656866, train_loss=2.9262142

Batch 174870, train_perplexity=18.656866, train_loss=2.9262142

Batch 174880, train_perplexity=18.656866, train_loss=2.9262142

Batch 174890, train_perplexity=18.656866, train_loss=2.9262142

Batch 174900, train_perplexity=18.656866, train_loss=2.9262142

Batch 174910, train_perplexity=18.656866, train_loss=2.9262142

Batch 174920, train_perplexity=18.656866, train_loss=2.9262142

Batch 174930, train_perplexity=18.656866, train_loss=2.9262142

Batch 174940, train_perplexity=18.656866, train_loss=2.9262142

Batch 174950, train_perplexity=18.656862, train_loss=2.926214

Batch 174960, train_perplexity=18.656866, train_loss=2.9262142

Batch 174970, train_perplexity=18.656866, train_loss=2.9262142

Batch 174980, train_perplexity=18.656866, train_loss=2.9262142

Batch 174990, train_perplexity=18.656866, train_loss=2.9262142

Batch 175000, train_perplexity=18.656862, train_loss=2.926214

Batch 175010, train_perplexity=18.656866, train_loss=2.9262142

Batch 175020, train_perplexity=18.656862, train_loss=2.926214

Batch 175030, train_perplexity=18.656866, train_loss=2.9262142

Batch 175040, train_perplexity=18.656857, train_loss=2.9262137

Batch 175050, train_perplexity=18.656857, train_loss=2.9262137

Batch 175060, train_perplexity=18.656866, train_loss=2.9262142

Batch 175070, train_perplexity=18.656862, train_loss=2.926214

Batch 175080, train_perplexity=18.656857, train_loss=2.9262137

Batch 175090, train_perplexity=18.656857, train_loss=2.9262137

Batch 175100, train_perplexity=18.656857, train_loss=2.9262137

Batch 175110, train_perplexity=18.656862, train_loss=2.926214

Batch 175120, train_perplexity=18.656857, train_loss=2.9262137

Batch 175130, train_perplexity=18.656862, train_loss=2.926214

Batch 175140, train_perplexity=18.656857, train_loss=2.9262137

Batch 175150, train_perplexity=18.656857, train_loss=2.9262137

Batch 175160, train_perplexity=18.656857, train_loss=2.9262137

Batch 175170, train_perplexity=18.656862, train_loss=2.926214

Batch 175180, train_perplexity=18.656857, train_loss=2.9262137

Batch 175190, train_perplexity=18.656857, train_loss=2.9262137

Batch 175200, train_perplexity=18.656857, train_loss=2.9262137

Batch 175210, train_perplexity=18.656857, train_loss=2.9262137

Batch 175220, train_perplexity=18.656857, train_loss=2.9262137

Batch 175230, train_perplexity=18.656857, train_loss=2.9262137

Batch 175240, train_perplexity=18.656857, train_loss=2.9262137

Batch 175250, train_perplexity=18.656857, train_loss=2.9262137

Batch 175260, train_perplexity=18.656857, train_loss=2.9262137

Batch 175270, train_perplexity=18.656853, train_loss=2.9262135

Batch 175280, train_perplexity=18.656857, train_loss=2.9262137

Batch 175290, train_perplexity=18.656853, train_loss=2.9262135

Batch 175300, train_perplexity=18.656853, train_loss=2.9262135

Batch 175310, train_perplexity=18.656857, train_loss=2.9262137

Batch 175320, train_perplexity=18.656857, train_loss=2.9262137

Batch 175330, train_perplexity=18.656849, train_loss=2.9262133

Batch 175340, train_perplexity=18.656849, train_loss=2.9262133

Batch 175350, train_perplexity=18.656849, train_loss=2.9262133

Batch 175360, train_perplexity=18.656849, train_loss=2.9262133

Batch 175370, train_perplexity=18.656857, train_loss=2.9262137

Batch 175380, train_perplexity=18.656853, train_loss=2.9262135

Batch 175390, train_perplexity=18.656853, train_loss=2.9262135

Batch 175400, train_perplexity=18.656849, train_loss=2.9262133

Batch 175410, train_perplexity=18.656849, train_loss=2.9262133

Batch 175420, train_perplexity=18.656849, train_loss=2.9262133

Batch 175430, train_perplexity=18.656849, train_loss=2.9262133

Batch 175440, train_perplexity=18.656853, train_loss=2.9262135

Batch 175450, train_perplexity=18.656849, train_loss=2.9262133

Batch 175460, train_perplexity=18.656849, train_loss=2.9262133

Batch 175470, train_perplexity=18.656849, train_loss=2.9262133

Batch 175480, train_perplexity=18.656849, train_loss=2.9262133

Batch 175490, train_perplexity=18.656849, train_loss=2.9262133

Batch 175500, train_perplexity=18.656849, train_loss=2.9262133

Batch 175510, train_perplexity=18.656849, train_loss=2.9262133

Batch 175520, train_perplexity=18.656849, train_loss=2.9262133

Batch 175530, train_perplexity=18.656849, train_loss=2.9262133

Batch 175540, train_perplexity=18.656849, train_loss=2.9262133

Batch 175550, train_perplexity=18.656849, train_loss=2.9262133

Batch 175560, train_perplexity=18.656849, train_loss=2.9262133

Batch 175570, train_perplexity=18.656849, train_loss=2.9262133

Batch 175580, train_perplexity=18.656849, train_loss=2.9262133

Batch 175590, train_perplexity=18.656849, train_loss=2.9262133

Batch 175600, train_perplexity=18.656843, train_loss=2.926213

Batch 175610, train_perplexity=18.656849, train_loss=2.9262133

Batch 175620, train_perplexity=18.656849, train_loss=2.9262133

Batch 175630, train_perplexity=18.65684, train_loss=2.9262128

Batch 175640, train_perplexity=18.65684, train_loss=2.9262128

Batch 175650, train_perplexity=18.65684, train_loss=2.9262128

Batch 175660, train_perplexity=18.656843, train_loss=2.926213

Batch 175670, train_perplexity=18.656843, train_loss=2.926213

Batch 175680, train_perplexity=18.656849, train_loss=2.9262133

Batch 175690, train_perplexity=18.65684, train_loss=2.9262128

Batch 175700, train_perplexity=18.65684, train_loss=2.9262128

Batch 175710, train_perplexity=18.656843, train_loss=2.926213

Batch 175720, train_perplexity=18.656843, train_loss=2.926213

Batch 175730, train_perplexity=18.65684, train_loss=2.9262128

Batch 175740, train_perplexity=18.65684, train_loss=2.9262128

Batch 175750, train_perplexity=18.65684, train_loss=2.9262128

Batch 175760, train_perplexity=18.65684, train_loss=2.9262128
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 175770, train_perplexity=18.65684, train_loss=2.9262128

Batch 175780, train_perplexity=18.65684, train_loss=2.9262128

Batch 175790, train_perplexity=18.656849, train_loss=2.9262133

Batch 175800, train_perplexity=18.65684, train_loss=2.9262128

Batch 175810, train_perplexity=18.65684, train_loss=2.9262128

Batch 175820, train_perplexity=18.65684, train_loss=2.9262128

Batch 175830, train_perplexity=18.65684, train_loss=2.9262128

Batch 175840, train_perplexity=18.65684, train_loss=2.9262128

Batch 175850, train_perplexity=18.65684, train_loss=2.9262128

Batch 175860, train_perplexity=18.65684, train_loss=2.9262128

Batch 175870, train_perplexity=18.65684, train_loss=2.9262128

Batch 175880, train_perplexity=18.65684, train_loss=2.9262128

Batch 175890, train_perplexity=18.65684, train_loss=2.9262128

Batch 175900, train_perplexity=18.65684, train_loss=2.9262128

Batch 175910, train_perplexity=18.656836, train_loss=2.9262125

Batch 175920, train_perplexity=18.656836, train_loss=2.9262125

Batch 175930, train_perplexity=18.65684, train_loss=2.9262128

Batch 175940, train_perplexity=18.65684, train_loss=2.9262128

Batch 175950, train_perplexity=18.656836, train_loss=2.9262125

Batch 175960, train_perplexity=18.65683, train_loss=2.9262123

Batch 175970, train_perplexity=18.65684, train_loss=2.9262128

Batch 175980, train_perplexity=18.65684, train_loss=2.9262128

Batch 175990, train_perplexity=18.656836, train_loss=2.9262125

Batch 176000, train_perplexity=18.65684, train_loss=2.9262128

Batch 176010, train_perplexity=18.65683, train_loss=2.9262123

Batch 176020, train_perplexity=18.656836, train_loss=2.9262125

Batch 176030, train_perplexity=18.65683, train_loss=2.9262123

Batch 176040, train_perplexity=18.65683, train_loss=2.9262123

Batch 176050, train_perplexity=18.65683, train_loss=2.9262123

Batch 176060, train_perplexity=18.656836, train_loss=2.9262125

Batch 176070, train_perplexity=18.656836, train_loss=2.9262125

Batch 176080, train_perplexity=18.65683, train_loss=2.9262123

Batch 176090, train_perplexity=18.65683, train_loss=2.9262123

Batch 176100, train_perplexity=18.65683, train_loss=2.9262123

Batch 176110, train_perplexity=18.65683, train_loss=2.9262123

Batch 176120, train_perplexity=18.65683, train_loss=2.9262123

Batch 176130, train_perplexity=18.65683, train_loss=2.9262123

Batch 176140, train_perplexity=18.65683, train_loss=2.9262123

Batch 176150, train_perplexity=18.65683, train_loss=2.9262123

Batch 176160, train_perplexity=18.65683, train_loss=2.9262123

Batch 176170, train_perplexity=18.656826, train_loss=2.926212

Batch 176180, train_perplexity=18.65683, train_loss=2.9262123

Batch 176190, train_perplexity=18.65683, train_loss=2.9262123

Batch 176200, train_perplexity=18.65683, train_loss=2.9262123

Batch 176210, train_perplexity=18.65683, train_loss=2.9262123

Batch 176220, train_perplexity=18.65683, train_loss=2.9262123

Batch 176230, train_perplexity=18.656822, train_loss=2.9262118

Batch 176240, train_perplexity=18.656826, train_loss=2.926212

Batch 176250, train_perplexity=18.65683, train_loss=2.9262123

Batch 176260, train_perplexity=18.65683, train_loss=2.9262123

Batch 176270, train_perplexity=18.65683, train_loss=2.9262123

Batch 176280, train_perplexity=18.65683, train_loss=2.9262123

Batch 176290, train_perplexity=18.656822, train_loss=2.9262118

Batch 176300, train_perplexity=18.65683, train_loss=2.9262123

Batch 176310, train_perplexity=18.65683, train_loss=2.9262123

Batch 176320, train_perplexity=18.656826, train_loss=2.926212

Batch 176330, train_perplexity=18.656822, train_loss=2.9262118

Batch 176340, train_perplexity=18.656822, train_loss=2.9262118

Batch 176350, train_perplexity=18.656826, train_loss=2.926212

Batch 176360, train_perplexity=18.656826, train_loss=2.926212

Batch 176370, train_perplexity=18.656822, train_loss=2.9262118

Batch 176380, train_perplexity=18.656822, train_loss=2.9262118

Batch 176390, train_perplexity=18.656822, train_loss=2.9262118

Batch 176400, train_perplexity=18.656822, train_loss=2.9262118

Batch 176410, train_perplexity=18.656822, train_loss=2.9262118

Batch 176420, train_perplexity=18.656822, train_loss=2.9262118

Batch 176430, train_perplexity=18.656822, train_loss=2.9262118

Batch 176440, train_perplexity=18.656822, train_loss=2.9262118

Batch 176450, train_perplexity=18.656822, train_loss=2.9262118

Batch 176460, train_perplexity=18.656822, train_loss=2.9262118

Batch 176470, train_perplexity=18.656822, train_loss=2.9262118

Batch 176480, train_perplexity=18.656816, train_loss=2.9262116

Batch 176490, train_perplexity=18.656822, train_loss=2.9262118

Batch 176500, train_perplexity=18.656822, train_loss=2.9262118

Batch 176510, train_perplexity=18.656822, train_loss=2.9262118

Batch 176520, train_perplexity=18.656822, train_loss=2.9262118

Batch 176530, train_perplexity=18.656822, train_loss=2.9262118

Batch 176540, train_perplexity=18.656822, train_loss=2.9262118

Batch 176550, train_perplexity=18.656816, train_loss=2.9262116

Batch 176560, train_perplexity=18.656822, train_loss=2.9262118

Batch 176570, train_perplexity=18.656822, train_loss=2.9262118

Batch 176580, train_perplexity=18.656816, train_loss=2.9262116

Batch 176590, train_perplexity=18.656822, train_loss=2.9262118

Batch 176600, train_perplexity=18.656822, train_loss=2.9262118

Batch 176610, train_perplexity=18.656813, train_loss=2.9262114

Batch 176620, train_perplexity=18.656813, train_loss=2.9262114

Batch 176630, train_perplexity=18.656822, train_loss=2.9262118

Batch 176640, train_perplexity=18.656813, train_loss=2.9262114

Batch 176650, train_perplexity=18.656813, train_loss=2.9262114

Batch 176660, train_perplexity=18.656813, train_loss=2.9262114

Batch 176670, train_perplexity=18.656816, train_loss=2.9262116

Batch 176680, train_perplexity=18.656816, train_loss=2.9262116

Batch 176690, train_perplexity=18.656813, train_loss=2.9262114

Batch 176700, train_perplexity=18.656813, train_loss=2.9262114

Batch 176710, train_perplexity=18.656816, train_loss=2.9262116

Batch 176720, train_perplexity=18.656816, train_loss=2.9262116

Batch 176730, train_perplexity=18.656813, train_loss=2.9262114

Batch 176740, train_perplexity=18.656813, train_loss=2.9262114

Batch 176750, train_perplexity=18.656813, train_loss=2.9262114

Batch 176760, train_perplexity=18.656813, train_loss=2.9262114

Batch 176770, train_perplexity=18.656813, train_loss=2.9262114

Batch 176780, train_perplexity=18.656809, train_loss=2.926211

Batch 176790, train_perplexity=18.656813, train_loss=2.9262114

Batch 176800, train_perplexity=18.656813, train_loss=2.9262114

Batch 176810, train_perplexity=18.656813, train_loss=2.9262114

Batch 176820, train_perplexity=18.656813, train_loss=2.9262114

Batch 176830, train_perplexity=18.656813, train_loss=2.9262114

Batch 176840, train_perplexity=18.656813, train_loss=2.9262114

Batch 176850, train_perplexity=18.656813, train_loss=2.9262114

Batch 176860, train_perplexity=18.656809, train_loss=2.926211

Batch 176870, train_perplexity=18.656813, train_loss=2.9262114

Batch 176880, train_perplexity=18.656813, train_loss=2.9262114

Batch 176890, train_perplexity=18.656809, train_loss=2.926211

Batch 176900, train_perplexity=18.656803, train_loss=2.9262109

Batch 176910, train_perplexity=18.656803, train_loss=2.9262109

Batch 176920, train_perplexity=18.656813, train_loss=2.9262114

Batch 176930, train_perplexity=18.656803, train_loss=2.9262109

Batch 176940, train_perplexity=18.656803, train_loss=2.9262109

Batch 176950, train_perplexity=18.656803, train_loss=2.9262109

Batch 176960, train_perplexity=18.656803, train_loss=2.9262109

Batch 176970, train_perplexity=18.656803, train_loss=2.9262109

Batch 176980, train_perplexity=18.656803, train_loss=2.9262109

Batch 176990, train_perplexity=18.656803, train_loss=2.9262109

Batch 177000, train_perplexity=18.656803, train_loss=2.9262109

Batch 177010, train_perplexity=18.656803, train_loss=2.9262109

Batch 177020, train_perplexity=18.656809, train_loss=2.926211

Batch 177030, train_perplexity=18.656803, train_loss=2.9262109

Batch 177040, train_perplexity=18.656803, train_loss=2.9262109

Batch 177050, train_perplexity=18.656803, train_loss=2.9262109
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 177060, train_perplexity=18.656803, train_loss=2.9262109

Batch 177070, train_perplexity=18.656803, train_loss=2.9262109

Batch 177080, train_perplexity=18.656803, train_loss=2.9262109

Batch 177090, train_perplexity=18.6568, train_loss=2.9262106

Batch 177100, train_perplexity=18.656803, train_loss=2.9262109

Batch 177110, train_perplexity=18.656803, train_loss=2.9262109

Batch 177120, train_perplexity=18.656803, train_loss=2.9262109

Batch 177130, train_perplexity=18.6568, train_loss=2.9262106

Batch 177140, train_perplexity=18.656803, train_loss=2.9262109

Batch 177150, train_perplexity=18.6568, train_loss=2.9262106

Batch 177160, train_perplexity=18.6568, train_loss=2.9262106

Batch 177170, train_perplexity=18.6568, train_loss=2.9262106

Batch 177180, train_perplexity=18.656803, train_loss=2.9262109

Batch 177190, train_perplexity=18.6568, train_loss=2.9262106

Batch 177200, train_perplexity=18.656803, train_loss=2.9262109

Batch 177210, train_perplexity=18.656803, train_loss=2.9262109

Batch 177220, train_perplexity=18.656803, train_loss=2.9262109

Batch 177230, train_perplexity=18.656796, train_loss=2.9262104

Batch 177240, train_perplexity=18.6568, train_loss=2.9262106

Batch 177250, train_perplexity=18.6568, train_loss=2.9262106

Batch 177260, train_perplexity=18.6568, train_loss=2.9262106

Batch 177270, train_perplexity=18.6568, train_loss=2.9262106

Batch 177280, train_perplexity=18.656796, train_loss=2.9262104

Batch 177290, train_perplexity=18.656796, train_loss=2.9262104

Batch 177300, train_perplexity=18.656796, train_loss=2.9262104

Batch 177310, train_perplexity=18.656796, train_loss=2.9262104

Batch 177320, train_perplexity=18.656796, train_loss=2.9262104

Batch 177330, train_perplexity=18.656796, train_loss=2.9262104

Batch 177340, train_perplexity=18.656796, train_loss=2.9262104

Batch 177350, train_perplexity=18.656796, train_loss=2.9262104

Batch 177360, train_perplexity=18.656796, train_loss=2.9262104

Batch 177370, train_perplexity=18.656796, train_loss=2.9262104

Batch 177380, train_perplexity=18.656796, train_loss=2.9262104

Batch 177390, train_perplexity=18.65679, train_loss=2.9262102

Batch 177400, train_perplexity=18.656796, train_loss=2.9262104

Batch 177410, train_perplexity=18.656796, train_loss=2.9262104

Batch 177420, train_perplexity=18.656796, train_loss=2.9262104

Batch 177430, train_perplexity=18.656796, train_loss=2.9262104

Batch 177440, train_perplexity=18.656796, train_loss=2.9262104

Batch 177450, train_perplexity=18.656796, train_loss=2.9262104

Batch 177460, train_perplexity=18.65679, train_loss=2.9262102

Batch 177470, train_perplexity=18.656796, train_loss=2.9262104

Batch 177480, train_perplexity=18.656786, train_loss=2.92621

Batch 177490, train_perplexity=18.656796, train_loss=2.9262104

Batch 177500, train_perplexity=18.656796, train_loss=2.9262104

Batch 177510, train_perplexity=18.656796, train_loss=2.9262104

Batch 177520, train_perplexity=18.656786, train_loss=2.92621

Batch 177530, train_perplexity=18.656786, train_loss=2.92621

Batch 177540, train_perplexity=18.656786, train_loss=2.92621

Batch 177550, train_perplexity=18.65679, train_loss=2.9262102

Batch 177560, train_perplexity=18.65679, train_loss=2.9262102

Batch 177570, train_perplexity=18.65679, train_loss=2.9262102

Batch 177580, train_perplexity=18.65679, train_loss=2.9262102

Batch 177590, train_perplexity=18.656786, train_loss=2.92621

Batch 177600, train_perplexity=18.656786, train_loss=2.92621

Batch 177610, train_perplexity=18.65679, train_loss=2.9262102

Batch 177620, train_perplexity=18.656786, train_loss=2.92621

Batch 177630, train_perplexity=18.656786, train_loss=2.92621

Batch 177640, train_perplexity=18.656786, train_loss=2.92621

Batch 177650, train_perplexity=18.65679, train_loss=2.9262102

Batch 177660, train_perplexity=18.656786, train_loss=2.92621

Batch 177670, train_perplexity=18.65679, train_loss=2.9262102

Batch 177680, train_perplexity=18.656786, train_loss=2.92621

Batch 177690, train_perplexity=18.656786, train_loss=2.92621

Batch 177700, train_perplexity=18.656786, train_loss=2.92621

Batch 177710, train_perplexity=18.656786, train_loss=2.92621

Batch 177720, train_perplexity=18.656786, train_loss=2.92621

Batch 177730, train_perplexity=18.656786, train_loss=2.92621

Batch 177740, train_perplexity=18.656782, train_loss=2.9262097

Batch 177750, train_perplexity=18.656786, train_loss=2.92621

Batch 177760, train_perplexity=18.656786, train_loss=2.92621

Batch 177770, train_perplexity=18.656786, train_loss=2.92621

Batch 177780, train_perplexity=18.656786, train_loss=2.92621

Batch 177790, train_perplexity=18.656782, train_loss=2.9262097

Batch 177800, train_perplexity=18.656782, train_loss=2.9262097

Batch 177810, train_perplexity=18.656782, train_loss=2.9262097

Batch 177820, train_perplexity=18.656782, train_loss=2.9262097

Batch 177830, train_perplexity=18.656786, train_loss=2.92621

Batch 177840, train_perplexity=18.656776, train_loss=2.9262094

Batch 177850, train_perplexity=18.656782, train_loss=2.9262097

Batch 177860, train_perplexity=18.656782, train_loss=2.9262097

Batch 177870, train_perplexity=18.656786, train_loss=2.92621

Batch 177880, train_perplexity=18.656776, train_loss=2.9262094

Batch 177890, train_perplexity=18.656786, train_loss=2.92621

Batch 177900, train_perplexity=18.656786, train_loss=2.92621

Batch 177910, train_perplexity=18.656786, train_loss=2.92621

Batch 177920, train_perplexity=18.656776, train_loss=2.9262094

Batch 177930, train_perplexity=18.656782, train_loss=2.9262097

Batch 177940, train_perplexity=18.656776, train_loss=2.9262094

Batch 177950, train_perplexity=18.656776, train_loss=2.9262094

Batch 177960, train_perplexity=18.656782, train_loss=2.9262097

Batch 177970, train_perplexity=18.656776, train_loss=2.9262094

Batch 177980, train_perplexity=18.656773, train_loss=2.9262092

Batch 177990, train_perplexity=18.656776, train_loss=2.9262094

Batch 178000, train_perplexity=18.656776, train_loss=2.9262094

Batch 178010, train_perplexity=18.656773, train_loss=2.9262092

Batch 178020, train_perplexity=18.656776, train_loss=2.9262094

Batch 178030, train_perplexity=18.656776, train_loss=2.9262094

Batch 178040, train_perplexity=18.656776, train_loss=2.9262094

Batch 178050, train_perplexity=18.656776, train_loss=2.9262094

Batch 178060, train_perplexity=18.656776, train_loss=2.9262094

Batch 178070, train_perplexity=18.656776, train_loss=2.9262094

Batch 178080, train_perplexity=18.656776, train_loss=2.9262094

Batch 178090, train_perplexity=18.656776, train_loss=2.9262094

Batch 178100, train_perplexity=18.656776, train_loss=2.9262094

Batch 178110, train_perplexity=18.656776, train_loss=2.9262094

Batch 178120, train_perplexity=18.656776, train_loss=2.9262094

Batch 178130, train_perplexity=18.656773, train_loss=2.9262092

Batch 178140, train_perplexity=18.656773, train_loss=2.9262092

Batch 178150, train_perplexity=18.656773, train_loss=2.9262092

Batch 178160, train_perplexity=18.656776, train_loss=2.9262094

Batch 178170, train_perplexity=18.656776, train_loss=2.9262094

Batch 178180, train_perplexity=18.656776, train_loss=2.9262094

Batch 178190, train_perplexity=18.656773, train_loss=2.9262092

Batch 178200, train_perplexity=18.656769, train_loss=2.926209

Batch 178210, train_perplexity=18.656769, train_loss=2.926209

Batch 178220, train_perplexity=18.656769, train_loss=2.926209

Batch 178230, train_perplexity=18.656773, train_loss=2.9262092

Batch 178240, train_perplexity=18.656769, train_loss=2.926209

Batch 178250, train_perplexity=18.656769, train_loss=2.926209

Batch 178260, train_perplexity=18.656769, train_loss=2.926209

Batch 178270, train_perplexity=18.656769, train_loss=2.926209

Batch 178280, train_perplexity=18.656769, train_loss=2.926209

Batch 178290, train_perplexity=18.656769, train_loss=2.926209

Batch 178300, train_perplexity=18.656773, train_loss=2.9262092

Batch 178310, train_perplexity=18.656769, train_loss=2.926209

Batch 178320, train_perplexity=18.656769, train_loss=2.926209

Batch 178330, train_perplexity=18.656769, train_loss=2.926209

Batch 178340, train_perplexity=18.656769, train_loss=2.926209

Batch 178350, train_perplexity=18.656769, train_loss=2.926209
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 178360, train_perplexity=18.656769, train_loss=2.926209

Batch 178370, train_perplexity=18.656763, train_loss=2.9262087

Batch 178380, train_perplexity=18.65676, train_loss=2.9262085

Batch 178390, train_perplexity=18.656763, train_loss=2.9262087

Batch 178400, train_perplexity=18.656769, train_loss=2.926209

Batch 178410, train_perplexity=18.656769, train_loss=2.926209

Batch 178420, train_perplexity=18.65676, train_loss=2.9262085

Batch 178430, train_perplexity=18.656763, train_loss=2.9262087

Batch 178440, train_perplexity=18.656769, train_loss=2.926209

Batch 178450, train_perplexity=18.656763, train_loss=2.9262087

Batch 178460, train_perplexity=18.65676, train_loss=2.9262085

Batch 178470, train_perplexity=18.656763, train_loss=2.9262087

Batch 178480, train_perplexity=18.65676, train_loss=2.9262085

Batch 178490, train_perplexity=18.656769, train_loss=2.926209

Batch 178500, train_perplexity=18.65676, train_loss=2.9262085

Batch 178510, train_perplexity=18.656769, train_loss=2.926209

Batch 178520, train_perplexity=18.656763, train_loss=2.9262087

Batch 178530, train_perplexity=18.65676, train_loss=2.9262085

Batch 178540, train_perplexity=18.65676, train_loss=2.9262085

Batch 178550, train_perplexity=18.65676, train_loss=2.9262085

Batch 178560, train_perplexity=18.656763, train_loss=2.9262087

Batch 178570, train_perplexity=18.65676, train_loss=2.9262085

Batch 178580, train_perplexity=18.65676, train_loss=2.9262085

Batch 178590, train_perplexity=18.65676, train_loss=2.9262085

Batch 178600, train_perplexity=18.65676, train_loss=2.9262085

Batch 178610, train_perplexity=18.65676, train_loss=2.9262085

Batch 178620, train_perplexity=18.65676, train_loss=2.9262085

Batch 178630, train_perplexity=18.65676, train_loss=2.9262085

Batch 178640, train_perplexity=18.65676, train_loss=2.9262085

Batch 178650, train_perplexity=18.65676, train_loss=2.9262085

Batch 178660, train_perplexity=18.65676, train_loss=2.9262085

Batch 178670, train_perplexity=18.65676, train_loss=2.9262085

Batch 178680, train_perplexity=18.65676, train_loss=2.9262085

Batch 178690, train_perplexity=18.65676, train_loss=2.9262085

Batch 178700, train_perplexity=18.656755, train_loss=2.9262083

Batch 178710, train_perplexity=18.65676, train_loss=2.9262085

Batch 178720, train_perplexity=18.65675, train_loss=2.926208

Batch 178730, train_perplexity=18.65676, train_loss=2.9262085

Batch 178740, train_perplexity=18.65676, train_loss=2.9262085

Batch 178750, train_perplexity=18.65675, train_loss=2.926208

Batch 178760, train_perplexity=18.65676, train_loss=2.9262085

Batch 178770, train_perplexity=18.656755, train_loss=2.9262083

Batch 178780, train_perplexity=18.65675, train_loss=2.926208

Batch 178790, train_perplexity=18.65675, train_loss=2.926208

Batch 178800, train_perplexity=18.65676, train_loss=2.9262085

Batch 178810, train_perplexity=18.656755, train_loss=2.9262083

Batch 178820, train_perplexity=18.65675, train_loss=2.926208

Batch 178830, train_perplexity=18.65676, train_loss=2.9262085

Batch 178840, train_perplexity=18.65675, train_loss=2.926208

Batch 178850, train_perplexity=18.65675, train_loss=2.926208

Batch 178860, train_perplexity=18.65675, train_loss=2.926208

Batch 178870, train_perplexity=18.65675, train_loss=2.926208

Batch 178880, train_perplexity=18.65676, train_loss=2.9262085

Batch 178890, train_perplexity=18.65675, train_loss=2.926208

Batch 178900, train_perplexity=18.65675, train_loss=2.926208

Batch 178910, train_perplexity=18.65675, train_loss=2.926208

Batch 178920, train_perplexity=18.65675, train_loss=2.926208

Batch 178930, train_perplexity=18.65675, train_loss=2.926208

Batch 178940, train_perplexity=18.65675, train_loss=2.926208

Batch 178950, train_perplexity=18.65675, train_loss=2.926208

Batch 178960, train_perplexity=18.65675, train_loss=2.926208

Batch 178970, train_perplexity=18.65675, train_loss=2.926208

Batch 178980, train_perplexity=18.65675, train_loss=2.926208

Batch 178990, train_perplexity=18.65675, train_loss=2.926208

Batch 179000, train_perplexity=18.65675, train_loss=2.926208

Batch 179010, train_perplexity=18.65675, train_loss=2.926208

Batch 179020, train_perplexity=18.656746, train_loss=2.9262078

Batch 179030, train_perplexity=18.65675, train_loss=2.926208

Batch 179040, train_perplexity=18.65675, train_loss=2.926208

Batch 179050, train_perplexity=18.656746, train_loss=2.9262078

Batch 179060, train_perplexity=18.656742, train_loss=2.9262075

Batch 179070, train_perplexity=18.656746, train_loss=2.9262078

Batch 179080, train_perplexity=18.656742, train_loss=2.9262075

Batch 179090, train_perplexity=18.656742, train_loss=2.9262075

Batch 179100, train_perplexity=18.656742, train_loss=2.9262075

Batch 179110, train_perplexity=18.656746, train_loss=2.9262078

Batch 179120, train_perplexity=18.656746, train_loss=2.9262078

Batch 179130, train_perplexity=18.656742, train_loss=2.9262075

Batch 179140, train_perplexity=18.656742, train_loss=2.9262075

Batch 179150, train_perplexity=18.656746, train_loss=2.9262078

Batch 179160, train_perplexity=18.656746, train_loss=2.9262078

Batch 179170, train_perplexity=18.656742, train_loss=2.9262075

Batch 179180, train_perplexity=18.656742, train_loss=2.9262075

Batch 179190, train_perplexity=18.656742, train_loss=2.9262075

Batch 179200, train_perplexity=18.656742, train_loss=2.9262075

Batch 179210, train_perplexity=18.656742, train_loss=2.9262075

Batch 179220, train_perplexity=18.656742, train_loss=2.9262075

Batch 179230, train_perplexity=18.656742, train_loss=2.9262075

Batch 179240, train_perplexity=18.656742, train_loss=2.9262075

Batch 179250, train_perplexity=18.656742, train_loss=2.9262075

Batch 179260, train_perplexity=18.656742, train_loss=2.9262075

Batch 179270, train_perplexity=18.656742, train_loss=2.9262075

Batch 179280, train_perplexity=18.656746, train_loss=2.9262078

Batch 179290, train_perplexity=18.656742, train_loss=2.9262075

Batch 179300, train_perplexity=18.656742, train_loss=2.9262075

Batch 179310, train_perplexity=18.656742, train_loss=2.9262075

Batch 179320, train_perplexity=18.656742, train_loss=2.9262075

Batch 179330, train_perplexity=18.656736, train_loss=2.9262073

Batch 179340, train_perplexity=18.656742, train_loss=2.9262075

Batch 179350, train_perplexity=18.656736, train_loss=2.9262073

Batch 179360, train_perplexity=18.656736, train_loss=2.9262073

Batch 179370, train_perplexity=18.656736, train_loss=2.9262073

Batch 179380, train_perplexity=18.656736, train_loss=2.9262073

Batch 179390, train_perplexity=18.656736, train_loss=2.9262073

Batch 179400, train_perplexity=18.656733, train_loss=2.926207

Batch 179410, train_perplexity=18.656733, train_loss=2.926207

Batch 179420, train_perplexity=18.656733, train_loss=2.926207

Batch 179430, train_perplexity=18.656733, train_loss=2.926207

Batch 179440, train_perplexity=18.656736, train_loss=2.9262073

Batch 179450, train_perplexity=18.656733, train_loss=2.926207

Batch 179460, train_perplexity=18.656733, train_loss=2.926207

Batch 179470, train_perplexity=18.656733, train_loss=2.926207

Batch 179480, train_perplexity=18.656736, train_loss=2.9262073

Batch 179490, train_perplexity=18.656733, train_loss=2.926207

Batch 179500, train_perplexity=18.656733, train_loss=2.926207

Batch 179510, train_perplexity=18.656733, train_loss=2.926207

Batch 179520, train_perplexity=18.656733, train_loss=2.926207

Batch 179530, train_perplexity=18.656736, train_loss=2.9262073

Batch 179540, train_perplexity=18.656733, train_loss=2.926207

Batch 179550, train_perplexity=18.656733, train_loss=2.926207

Batch 179560, train_perplexity=18.656733, train_loss=2.926207

Batch 179570, train_perplexity=18.656733, train_loss=2.926207

Batch 179580, train_perplexity=18.656729, train_loss=2.9262068

Batch 179590, train_perplexity=18.656733, train_loss=2.926207

Batch 179600, train_perplexity=18.656733, train_loss=2.926207

Batch 179610, train_perplexity=18.656733, train_loss=2.926207

Batch 179620, train_perplexity=18.656729, train_loss=2.9262068

Batch 179630, train_perplexity=18.656733, train_loss=2.926207

Batch 179640, train_perplexity=18.656733, train_loss=2.926207

Batch 179650, train_perplexity=18.656733, train_loss=2.926207
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 179660, train_perplexity=18.656729, train_loss=2.9262068

Batch 179670, train_perplexity=18.656733, train_loss=2.926207

Batch 179680, train_perplexity=18.656733, train_loss=2.926207

Batch 179690, train_perplexity=18.656733, train_loss=2.926207

Batch 179700, train_perplexity=18.656729, train_loss=2.9262068

Batch 179710, train_perplexity=18.656733, train_loss=2.926207

Batch 179720, train_perplexity=18.656733, train_loss=2.926207

Batch 179730, train_perplexity=18.656729, train_loss=2.9262068

Batch 179740, train_perplexity=18.656729, train_loss=2.9262068

Batch 179750, train_perplexity=18.656729, train_loss=2.9262068

Batch 179760, train_perplexity=18.656729, train_loss=2.9262068

Batch 179770, train_perplexity=18.656729, train_loss=2.9262068

Batch 179780, train_perplexity=18.656723, train_loss=2.9262066

Batch 179790, train_perplexity=18.656723, train_loss=2.9262066

Batch 179800, train_perplexity=18.656723, train_loss=2.9262066

Batch 179810, train_perplexity=18.656723, train_loss=2.9262066

Batch 179820, train_perplexity=18.656723, train_loss=2.9262066

Batch 179830, train_perplexity=18.656723, train_loss=2.9262066

Batch 179840, train_perplexity=18.656723, train_loss=2.9262066

Batch 179850, train_perplexity=18.656723, train_loss=2.9262066

Batch 179860, train_perplexity=18.656723, train_loss=2.9262066

Batch 179870, train_perplexity=18.656723, train_loss=2.9262066

Batch 179880, train_perplexity=18.656723, train_loss=2.9262066

Batch 179890, train_perplexity=18.656723, train_loss=2.9262066

Batch 179900, train_perplexity=18.656723, train_loss=2.9262066

Batch 179910, train_perplexity=18.656723, train_loss=2.9262066

Batch 179920, train_perplexity=18.656723, train_loss=2.9262066

Batch 179930, train_perplexity=18.656723, train_loss=2.9262066

Batch 179940, train_perplexity=18.656723, train_loss=2.9262066

Batch 179950, train_perplexity=18.656723, train_loss=2.9262066

Batch 179960, train_perplexity=18.656723, train_loss=2.9262066

Batch 179970, train_perplexity=18.65672, train_loss=2.9262064

Batch 179980, train_perplexity=18.656723, train_loss=2.9262066

Batch 179990, train_perplexity=18.65672, train_loss=2.9262064

Batch 180000, train_perplexity=18.656723, train_loss=2.9262066

Batch 180010, train_perplexity=18.656723, train_loss=2.9262066

Batch 180020, train_perplexity=18.65672, train_loss=2.9262064

Batch 180030, train_perplexity=18.656715, train_loss=2.926206

Batch 180040, train_perplexity=18.656715, train_loss=2.926206

Batch 180050, train_perplexity=18.656723, train_loss=2.9262066

Batch 180060, train_perplexity=18.656715, train_loss=2.926206

Batch 180070, train_perplexity=18.65672, train_loss=2.9262064

Batch 180080, train_perplexity=18.656715, train_loss=2.926206

Batch 180090, train_perplexity=18.656723, train_loss=2.9262066

Batch 180100, train_perplexity=18.656715, train_loss=2.926206

Batch 180110, train_perplexity=18.656715, train_loss=2.926206

Batch 180120, train_perplexity=18.656715, train_loss=2.926206

Batch 180130, train_perplexity=18.656715, train_loss=2.926206

Batch 180140, train_perplexity=18.656715, train_loss=2.926206

Batch 180150, train_perplexity=18.656715, train_loss=2.926206

Batch 180160, train_perplexity=18.656715, train_loss=2.926206

Batch 180170, train_perplexity=18.656715, train_loss=2.926206

Batch 180180, train_perplexity=18.656715, train_loss=2.926206

Batch 180190, train_perplexity=18.656715, train_loss=2.926206

Batch 180200, train_perplexity=18.656715, train_loss=2.926206

Batch 180210, train_perplexity=18.656715, train_loss=2.926206

Batch 180220, train_perplexity=18.656715, train_loss=2.926206

Batch 180230, train_perplexity=18.656715, train_loss=2.926206

Batch 180240, train_perplexity=18.656715, train_loss=2.926206

Batch 180250, train_perplexity=18.656715, train_loss=2.926206

Batch 180260, train_perplexity=18.656715, train_loss=2.926206

Batch 180270, train_perplexity=18.656715, train_loss=2.926206

Batch 180280, train_perplexity=18.656715, train_loss=2.926206

Batch 180290, train_perplexity=18.65671, train_loss=2.9262059

Batch 180300, train_perplexity=18.65671, train_loss=2.9262059

Batch 180310, train_perplexity=18.656715, train_loss=2.926206

Batch 180320, train_perplexity=18.656715, train_loss=2.926206

Batch 180330, train_perplexity=18.65671, train_loss=2.9262059

Batch 180340, train_perplexity=18.656715, train_loss=2.926206

Batch 180350, train_perplexity=18.65671, train_loss=2.9262059

Batch 180360, train_perplexity=18.65671, train_loss=2.9262059

Batch 180370, train_perplexity=18.65671, train_loss=2.9262059

Batch 180380, train_perplexity=18.656706, train_loss=2.9262056

Batch 180390, train_perplexity=18.656706, train_loss=2.9262056

Batch 180400, train_perplexity=18.656706, train_loss=2.9262056

Batch 180410, train_perplexity=18.656706, train_loss=2.9262056

Batch 180420, train_perplexity=18.656706, train_loss=2.9262056

Batch 180430, train_perplexity=18.656706, train_loss=2.9262056

Batch 180440, train_perplexity=18.65671, train_loss=2.9262059

Batch 180450, train_perplexity=18.65671, train_loss=2.9262059

Batch 180460, train_perplexity=18.656706, train_loss=2.9262056

Batch 180470, train_perplexity=18.65671, train_loss=2.9262059

Batch 180480, train_perplexity=18.656702, train_loss=2.9262054

Batch 180490, train_perplexity=18.656706, train_loss=2.9262056

Batch 180500, train_perplexity=18.656706, train_loss=2.9262056

Batch 180510, train_perplexity=18.656706, train_loss=2.9262056

Batch 180520, train_perplexity=18.656706, train_loss=2.9262056

Batch 180530, train_perplexity=18.656702, train_loss=2.9262054

Batch 180540, train_perplexity=18.656706, train_loss=2.9262056

Batch 180550, train_perplexity=18.656706, train_loss=2.9262056

Batch 180560, train_perplexity=18.656702, train_loss=2.9262054

Batch 180570, train_perplexity=18.656702, train_loss=2.9262054

Batch 180580, train_perplexity=18.656706, train_loss=2.9262056

Batch 180590, train_perplexity=18.656696, train_loss=2.9262052

Batch 180600, train_perplexity=18.656706, train_loss=2.9262056

Batch 180610, train_perplexity=18.656706, train_loss=2.9262056

Batch 180620, train_perplexity=18.656702, train_loss=2.9262054

Batch 180630, train_perplexity=18.656702, train_loss=2.9262054

Batch 180640, train_perplexity=18.656696, train_loss=2.9262052

Batch 180650, train_perplexity=18.656706, train_loss=2.9262056

Batch 180660, train_perplexity=18.656706, train_loss=2.9262056

Batch 180670, train_perplexity=18.656696, train_loss=2.9262052

Batch 180680, train_perplexity=18.656706, train_loss=2.9262056

Batch 180690, train_perplexity=18.656702, train_loss=2.9262054

Batch 180700, train_perplexity=18.656696, train_loss=2.9262052

Batch 180710, train_perplexity=18.656706, train_loss=2.9262056

Batch 180720, train_perplexity=18.656706, train_loss=2.9262056

Batch 180730, train_perplexity=18.656696, train_loss=2.9262052

Batch 180740, train_perplexity=18.656696, train_loss=2.9262052

Batch 180750, train_perplexity=18.656696, train_loss=2.9262052

Batch 180760, train_perplexity=18.656696, train_loss=2.9262052

Batch 180770, train_perplexity=18.656706, train_loss=2.9262056

Batch 180780, train_perplexity=18.656696, train_loss=2.9262052

Batch 180790, train_perplexity=18.656696, train_loss=2.9262052

Batch 180800, train_perplexity=18.656696, train_loss=2.9262052

Batch 180810, train_perplexity=18.656696, train_loss=2.9262052

Batch 180820, train_perplexity=18.656696, train_loss=2.9262052

Batch 180830, train_perplexity=18.656693, train_loss=2.926205

Batch 180840, train_perplexity=18.656696, train_loss=2.9262052

Batch 180850, train_perplexity=18.656696, train_loss=2.9262052

Batch 180860, train_perplexity=18.656696, train_loss=2.9262052

Batch 180870, train_perplexity=18.656696, train_loss=2.9262052

Batch 180880, train_perplexity=18.656696, train_loss=2.9262052

Batch 180890, train_perplexity=18.656696, train_loss=2.9262052

Batch 180900, train_perplexity=18.656696, train_loss=2.9262052

Batch 180910, train_perplexity=18.656696, train_loss=2.9262052

Batch 180920, train_perplexity=18.656689, train_loss=2.9262047

Batch 180930, train_perplexity=18.656696, train_loss=2.9262052

Batch 180940, train_perplexity=18.656693, train_loss=2.926205
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 180950, train_perplexity=18.656693, train_loss=2.926205

Batch 180960, train_perplexity=18.656696, train_loss=2.9262052

Batch 180970, train_perplexity=18.656696, train_loss=2.9262052

Batch 180980, train_perplexity=18.656689, train_loss=2.9262047

Batch 180990, train_perplexity=18.656689, train_loss=2.9262047

Batch 181000, train_perplexity=18.656689, train_loss=2.9262047

Batch 181010, train_perplexity=18.656693, train_loss=2.926205

Batch 181020, train_perplexity=18.656689, train_loss=2.9262047

Batch 181030, train_perplexity=18.656689, train_loss=2.9262047

Batch 181040, train_perplexity=18.656689, train_loss=2.9262047

Batch 181050, train_perplexity=18.656689, train_loss=2.9262047

Batch 181060, train_perplexity=18.656689, train_loss=2.9262047

Batch 181070, train_perplexity=18.656689, train_loss=2.9262047

Batch 181080, train_perplexity=18.656689, train_loss=2.9262047

Batch 181090, train_perplexity=18.656689, train_loss=2.9262047

Batch 181100, train_perplexity=18.656689, train_loss=2.9262047

Batch 181110, train_perplexity=18.656689, train_loss=2.9262047

Batch 181120, train_perplexity=18.656689, train_loss=2.9262047

Batch 181130, train_perplexity=18.656689, train_loss=2.9262047

Batch 181140, train_perplexity=18.656689, train_loss=2.9262047

Batch 181150, train_perplexity=18.656689, train_loss=2.9262047

Batch 181160, train_perplexity=18.656689, train_loss=2.9262047

Batch 181170, train_perplexity=18.656689, train_loss=2.9262047

Batch 181180, train_perplexity=18.656689, train_loss=2.9262047

Batch 181190, train_perplexity=18.656689, train_loss=2.9262047

Batch 181200, train_perplexity=18.65668, train_loss=2.9262042

Batch 181210, train_perplexity=18.656689, train_loss=2.9262047

Batch 181220, train_perplexity=18.656689, train_loss=2.9262047

Batch 181230, train_perplexity=18.656683, train_loss=2.9262044

Batch 181240, train_perplexity=18.656689, train_loss=2.9262047

Batch 181250, train_perplexity=18.656683, train_loss=2.9262044

Batch 181260, train_perplexity=18.656689, train_loss=2.9262047

Batch 181270, train_perplexity=18.656683, train_loss=2.9262044

Batch 181280, train_perplexity=18.656683, train_loss=2.9262044

Batch 181290, train_perplexity=18.656689, train_loss=2.9262047

Batch 181300, train_perplexity=18.656683, train_loss=2.9262044

Batch 181310, train_perplexity=18.65668, train_loss=2.9262042

Batch 181320, train_perplexity=18.65668, train_loss=2.9262042

Batch 181330, train_perplexity=18.65668, train_loss=2.9262042

Batch 181340, train_perplexity=18.65668, train_loss=2.9262042

Batch 181350, train_perplexity=18.65668, train_loss=2.9262042

Batch 181360, train_perplexity=18.65668, train_loss=2.9262042

Batch 181370, train_perplexity=18.65668, train_loss=2.9262042

Batch 181380, train_perplexity=18.656689, train_loss=2.9262047

Batch 181390, train_perplexity=18.65668, train_loss=2.9262042

Batch 181400, train_perplexity=18.65668, train_loss=2.9262042

Batch 181410, train_perplexity=18.65668, train_loss=2.9262042

Batch 181420, train_perplexity=18.65668, train_loss=2.9262042

Batch 181430, train_perplexity=18.65668, train_loss=2.9262042

Batch 181440, train_perplexity=18.65668, train_loss=2.9262042

Batch 181450, train_perplexity=18.65668, train_loss=2.9262042

Batch 181460, train_perplexity=18.65668, train_loss=2.9262042

Batch 181470, train_perplexity=18.65668, train_loss=2.9262042

Batch 181480, train_perplexity=18.65668, train_loss=2.9262042

Batch 181490, train_perplexity=18.656675, train_loss=2.926204

Batch 181500, train_perplexity=18.65668, train_loss=2.9262042

Batch 181510, train_perplexity=18.65668, train_loss=2.9262042

Batch 181520, train_perplexity=18.65668, train_loss=2.9262042

Batch 181530, train_perplexity=18.65667, train_loss=2.9262037

Batch 181540, train_perplexity=18.656675, train_loss=2.926204

Batch 181550, train_perplexity=18.656675, train_loss=2.926204

Batch 181560, train_perplexity=18.65668, train_loss=2.9262042

Batch 181570, train_perplexity=18.65668, train_loss=2.9262042

Batch 181580, train_perplexity=18.656675, train_loss=2.926204

Batch 181590, train_perplexity=18.65668, train_loss=2.9262042

Batch 181600, train_perplexity=18.65668, train_loss=2.9262042

Batch 181610, train_perplexity=18.656675, train_loss=2.926204

Batch 181620, train_perplexity=18.656675, train_loss=2.926204

Batch 181630, train_perplexity=18.65667, train_loss=2.9262037

Batch 181640, train_perplexity=18.65667, train_loss=2.9262037

Batch 181650, train_perplexity=18.65667, train_loss=2.9262037

Batch 181660, train_perplexity=18.65667, train_loss=2.9262037

Batch 181670, train_perplexity=18.65667, train_loss=2.9262037

Batch 181680, train_perplexity=18.656675, train_loss=2.926204

Batch 181690, train_perplexity=18.65667, train_loss=2.9262037

Batch 181700, train_perplexity=18.65668, train_loss=2.9262042

Batch 181710, train_perplexity=18.65667, train_loss=2.9262037

Batch 181720, train_perplexity=18.65667, train_loss=2.9262037

Batch 181730, train_perplexity=18.65667, train_loss=2.9262037

Batch 181740, train_perplexity=18.65667, train_loss=2.9262037

Batch 181750, train_perplexity=18.65667, train_loss=2.9262037

Batch 181760, train_perplexity=18.65667, train_loss=2.9262037

Batch 181770, train_perplexity=18.65667, train_loss=2.9262037

Batch 181780, train_perplexity=18.65667, train_loss=2.9262037

Batch 181790, train_perplexity=18.65667, train_loss=2.9262037

Batch 181800, train_perplexity=18.65667, train_loss=2.9262037

Batch 181810, train_perplexity=18.65667, train_loss=2.9262037

Batch 181820, train_perplexity=18.65667, train_loss=2.9262037

Batch 181830, train_perplexity=18.656662, train_loss=2.9262033

Batch 181840, train_perplexity=18.65667, train_loss=2.9262037

Batch 181850, train_perplexity=18.65667, train_loss=2.9262037

Batch 181860, train_perplexity=18.65667, train_loss=2.9262037

Batch 181870, train_perplexity=18.65667, train_loss=2.9262037

Batch 181880, train_perplexity=18.656666, train_loss=2.9262035

Batch 181890, train_perplexity=18.65667, train_loss=2.9262037

Batch 181900, train_perplexity=18.65667, train_loss=2.9262037

Batch 181910, train_perplexity=18.656662, train_loss=2.9262033

Batch 181920, train_perplexity=18.656666, train_loss=2.9262035

Batch 181930, train_perplexity=18.656662, train_loss=2.9262033

Batch 181940, train_perplexity=18.656662, train_loss=2.9262033

Batch 181950, train_perplexity=18.65667, train_loss=2.9262037

Batch 181960, train_perplexity=18.656662, train_loss=2.9262033

Batch 181970, train_perplexity=18.656662, train_loss=2.9262033

Batch 181980, train_perplexity=18.65667, train_loss=2.9262037

Batch 181990, train_perplexity=18.656662, train_loss=2.9262033

Batch 182000, train_perplexity=18.656662, train_loss=2.9262033

Batch 182010, train_perplexity=18.656662, train_loss=2.9262033

Batch 182020, train_perplexity=18.656666, train_loss=2.9262035

Batch 182030, train_perplexity=18.656662, train_loss=2.9262033

Batch 182040, train_perplexity=18.656662, train_loss=2.9262033

Batch 182050, train_perplexity=18.656662, train_loss=2.9262033

Batch 182060, train_perplexity=18.656662, train_loss=2.9262033

Batch 182070, train_perplexity=18.656662, train_loss=2.9262033

Batch 182080, train_perplexity=18.656662, train_loss=2.9262033

Batch 182090, train_perplexity=18.656662, train_loss=2.9262033

Batch 182100, train_perplexity=18.656662, train_loss=2.9262033

Batch 182110, train_perplexity=18.656662, train_loss=2.9262033

Batch 182120, train_perplexity=18.656666, train_loss=2.9262035

Batch 182130, train_perplexity=18.656652, train_loss=2.9262028

Batch 182140, train_perplexity=18.656656, train_loss=2.926203

Batch 182150, train_perplexity=18.656656, train_loss=2.926203

Batch 182160, train_perplexity=18.656662, train_loss=2.9262033

Batch 182170, train_perplexity=18.656662, train_loss=2.9262033

Batch 182180, train_perplexity=18.656662, train_loss=2.9262033

Batch 182190, train_perplexity=18.656656, train_loss=2.926203

Batch 182200, train_perplexity=18.656652, train_loss=2.9262028

Batch 182210, train_perplexity=18.656652, train_loss=2.9262028

Batch 182220, train_perplexity=18.656656, train_loss=2.926203

Batch 182230, train_perplexity=18.656652, train_loss=2.9262028

Batch 182240, train_perplexity=18.656652, train_loss=2.9262028
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 182250, train_perplexity=18.656662, train_loss=2.9262033

Batch 182260, train_perplexity=18.656652, train_loss=2.9262028

Batch 182270, train_perplexity=18.656652, train_loss=2.9262028

Batch 182280, train_perplexity=18.656652, train_loss=2.9262028

Batch 182290, train_perplexity=18.656656, train_loss=2.926203

Batch 182300, train_perplexity=18.656662, train_loss=2.9262033

Batch 182310, train_perplexity=18.656652, train_loss=2.9262028

Batch 182320, train_perplexity=18.656656, train_loss=2.926203

Batch 182330, train_perplexity=18.656649, train_loss=2.9262025

Batch 182340, train_perplexity=18.656649, train_loss=2.9262025

Batch 182350, train_perplexity=18.656656, train_loss=2.926203

Batch 182360, train_perplexity=18.656643, train_loss=2.9262023

Batch 182370, train_perplexity=18.656649, train_loss=2.9262025

Batch 182380, train_perplexity=18.656656, train_loss=2.926203

Batch 182390, train_perplexity=18.656652, train_loss=2.9262028

Batch 182400, train_perplexity=18.656652, train_loss=2.9262028

Batch 182410, train_perplexity=18.656652, train_loss=2.9262028

Batch 182420, train_perplexity=18.656652, train_loss=2.9262028

Batch 182430, train_perplexity=18.656652, train_loss=2.9262028

Batch 182440, train_perplexity=18.656652, train_loss=2.9262028

Batch 182450, train_perplexity=18.656643, train_loss=2.9262023

Batch 182460, train_perplexity=18.656652, train_loss=2.9262028

Batch 182470, train_perplexity=18.656649, train_loss=2.9262025

Batch 182480, train_perplexity=18.656652, train_loss=2.9262028

Batch 182490, train_perplexity=18.656652, train_loss=2.9262028

Batch 182500, train_perplexity=18.656652, train_loss=2.9262028

Batch 182510, train_perplexity=18.656652, train_loss=2.9262028

Batch 182520, train_perplexity=18.656643, train_loss=2.9262023

Batch 182530, train_perplexity=18.656643, train_loss=2.9262023

Batch 182540, train_perplexity=18.656649, train_loss=2.9262025

Batch 182550, train_perplexity=18.656649, train_loss=2.9262025

Batch 182560, train_perplexity=18.656643, train_loss=2.9262023

Batch 182570, train_perplexity=18.656643, train_loss=2.9262023

Batch 182580, train_perplexity=18.656643, train_loss=2.9262023

Batch 182590, train_perplexity=18.656643, train_loss=2.9262023

Batch 182600, train_perplexity=18.656649, train_loss=2.9262025

Batch 182610, train_perplexity=18.656652, train_loss=2.9262028

Batch 182620, train_perplexity=18.656649, train_loss=2.9262025

Batch 182630, train_perplexity=18.656643, train_loss=2.9262023

Batch 182640, train_perplexity=18.656643, train_loss=2.9262023

Batch 182650, train_perplexity=18.656643, train_loss=2.9262023

Batch 182660, train_perplexity=18.656643, train_loss=2.9262023

Batch 182670, train_perplexity=18.656643, train_loss=2.9262023

Batch 182680, train_perplexity=18.656643, train_loss=2.9262023

Batch 182690, train_perplexity=18.656643, train_loss=2.9262023

Batch 182700, train_perplexity=18.656643, train_loss=2.9262023

Batch 182710, train_perplexity=18.656643, train_loss=2.9262023

Batch 182720, train_perplexity=18.656643, train_loss=2.9262023

Batch 182730, train_perplexity=18.656643, train_loss=2.9262023

Batch 182740, train_perplexity=18.656643, train_loss=2.9262023

Batch 182750, train_perplexity=18.656643, train_loss=2.9262023

Batch 182760, train_perplexity=18.656643, train_loss=2.9262023

Batch 182770, train_perplexity=18.65664, train_loss=2.926202

Batch 182780, train_perplexity=18.65664, train_loss=2.926202

Batch 182790, train_perplexity=18.65664, train_loss=2.926202

Batch 182800, train_perplexity=18.65664, train_loss=2.926202

Batch 182810, train_perplexity=18.656643, train_loss=2.9262023

Batch 182820, train_perplexity=18.65664, train_loss=2.926202

Batch 182830, train_perplexity=18.656643, train_loss=2.9262023

Batch 182840, train_perplexity=18.656643, train_loss=2.9262023

Batch 182850, train_perplexity=18.65664, train_loss=2.926202

Batch 182860, train_perplexity=18.656635, train_loss=2.9262018

Batch 182870, train_perplexity=18.65663, train_loss=2.9262016

Batch 182880, train_perplexity=18.656635, train_loss=2.9262018

Batch 182890, train_perplexity=18.65664, train_loss=2.926202

Batch 182900, train_perplexity=18.656635, train_loss=2.9262018

Batch 182910, train_perplexity=18.656635, train_loss=2.9262018

Batch 182920, train_perplexity=18.656635, train_loss=2.9262018

Batch 182930, train_perplexity=18.656643, train_loss=2.9262023

Batch 182940, train_perplexity=18.65664, train_loss=2.926202

Batch 182950, train_perplexity=18.656635, train_loss=2.9262018

Batch 182960, train_perplexity=18.656635, train_loss=2.9262018

Batch 182970, train_perplexity=18.656635, train_loss=2.9262018

Batch 182980, train_perplexity=18.656635, train_loss=2.9262018

Batch 182990, train_perplexity=18.656635, train_loss=2.9262018

Batch 183000, train_perplexity=18.656635, train_loss=2.9262018

Batch 183010, train_perplexity=18.656635, train_loss=2.9262018

Batch 183020, train_perplexity=18.656635, train_loss=2.9262018

Batch 183030, train_perplexity=18.656626, train_loss=2.9262013

Batch 183040, train_perplexity=18.656635, train_loss=2.9262018

Batch 183050, train_perplexity=18.656635, train_loss=2.9262018

Batch 183060, train_perplexity=18.65663, train_loss=2.9262016

Batch 183070, train_perplexity=18.65663, train_loss=2.9262016

Batch 183080, train_perplexity=18.656635, train_loss=2.9262018

Batch 183090, train_perplexity=18.65663, train_loss=2.9262016

Batch 183100, train_perplexity=18.656626, train_loss=2.9262013

Batch 183110, train_perplexity=18.656635, train_loss=2.9262018

Batch 183120, train_perplexity=18.656635, train_loss=2.9262018

Batch 183130, train_perplexity=18.656626, train_loss=2.9262013

Batch 183140, train_perplexity=18.65663, train_loss=2.9262016

Batch 183150, train_perplexity=18.656626, train_loss=2.9262013

Batch 183160, train_perplexity=18.656626, train_loss=2.9262013

Batch 183170, train_perplexity=18.656626, train_loss=2.9262013

Batch 183180, train_perplexity=18.656626, train_loss=2.9262013

Batch 183190, train_perplexity=18.656626, train_loss=2.9262013

Batch 183200, train_perplexity=18.656626, train_loss=2.9262013

Batch 183210, train_perplexity=18.656626, train_loss=2.9262013

Batch 183220, train_perplexity=18.656622, train_loss=2.926201

Batch 183230, train_perplexity=18.656626, train_loss=2.9262013

Batch 183240, train_perplexity=18.656626, train_loss=2.9262013

Batch 183250, train_perplexity=18.656626, train_loss=2.9262013

Batch 183260, train_perplexity=18.656622, train_loss=2.926201

Batch 183270, train_perplexity=18.656626, train_loss=2.9262013

Batch 183280, train_perplexity=18.656626, train_loss=2.9262013

Batch 183290, train_perplexity=18.656626, train_loss=2.9262013

Batch 183300, train_perplexity=18.656626, train_loss=2.9262013

Batch 183310, train_perplexity=18.656626, train_loss=2.9262013

Batch 183320, train_perplexity=18.656622, train_loss=2.926201

Batch 183330, train_perplexity=18.656626, train_loss=2.9262013

Batch 183340, train_perplexity=18.656616, train_loss=2.9262009

Batch 183350, train_perplexity=18.656626, train_loss=2.9262013

Batch 183360, train_perplexity=18.656626, train_loss=2.9262013

Batch 183370, train_perplexity=18.656626, train_loss=2.9262013

Batch 183380, train_perplexity=18.656622, train_loss=2.926201

Batch 183390, train_perplexity=18.656622, train_loss=2.926201

Batch 183400, train_perplexity=18.656626, train_loss=2.9262013

Batch 183410, train_perplexity=18.656622, train_loss=2.926201

Batch 183420, train_perplexity=18.656626, train_loss=2.9262013

Batch 183430, train_perplexity=18.656626, train_loss=2.9262013

Batch 183440, train_perplexity=18.656626, train_loss=2.9262013

Batch 183450, train_perplexity=18.656622, train_loss=2.926201

Batch 183460, train_perplexity=18.656616, train_loss=2.9262009

Batch 183470, train_perplexity=18.656626, train_loss=2.9262013

Batch 183480, train_perplexity=18.656616, train_loss=2.9262009

Batch 183490, train_perplexity=18.656622, train_loss=2.926201

Batch 183500, train_perplexity=18.656622, train_loss=2.926201

Batch 183510, train_perplexity=18.656616, train_loss=2.9262009

Batch 183520, train_perplexity=18.656616, train_loss=2.9262009

Batch 183530, train_perplexity=18.656616, train_loss=2.9262009
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 183540, train_perplexity=18.656616, train_loss=2.9262009

Batch 183550, train_perplexity=18.656612, train_loss=2.9262006

Batch 183560, train_perplexity=18.656616, train_loss=2.9262009

Batch 183570, train_perplexity=18.656622, train_loss=2.926201

Batch 183580, train_perplexity=18.656616, train_loss=2.9262009

Batch 183590, train_perplexity=18.656616, train_loss=2.9262009

Batch 183600, train_perplexity=18.656616, train_loss=2.9262009

Batch 183610, train_perplexity=18.656616, train_loss=2.9262009

Batch 183620, train_perplexity=18.656612, train_loss=2.9262006

Batch 183630, train_perplexity=18.656616, train_loss=2.9262009

Batch 183640, train_perplexity=18.656616, train_loss=2.9262009

Batch 183650, train_perplexity=18.656616, train_loss=2.9262009

Batch 183660, train_perplexity=18.656616, train_loss=2.9262009

Batch 183670, train_perplexity=18.656612, train_loss=2.9262006

Batch 183680, train_perplexity=18.656612, train_loss=2.9262006

Batch 183690, train_perplexity=18.656612, train_loss=2.9262006

Batch 183700, train_perplexity=18.656616, train_loss=2.9262009

Batch 183710, train_perplexity=18.656616, train_loss=2.9262009

Batch 183720, train_perplexity=18.656616, train_loss=2.9262009

Batch 183730, train_perplexity=18.656616, train_loss=2.9262009

Batch 183740, train_perplexity=18.656616, train_loss=2.9262009

Batch 183750, train_perplexity=18.656609, train_loss=2.9262004

Batch 183760, train_perplexity=18.656612, train_loss=2.9262006

Batch 183770, train_perplexity=18.656612, train_loss=2.9262006

Batch 183780, train_perplexity=18.656612, train_loss=2.9262006

Batch 183790, train_perplexity=18.656609, train_loss=2.9262004

Batch 183800, train_perplexity=18.656612, train_loss=2.9262006

Batch 183810, train_perplexity=18.656609, train_loss=2.9262004

Batch 183820, train_perplexity=18.656609, train_loss=2.9262004

Batch 183830, train_perplexity=18.656609, train_loss=2.9262004

Batch 183840, train_perplexity=18.656609, train_loss=2.9262004

Batch 183850, train_perplexity=18.656609, train_loss=2.9262004

Batch 183860, train_perplexity=18.656609, train_loss=2.9262004

Batch 183870, train_perplexity=18.656612, train_loss=2.9262006

Batch 183880, train_perplexity=18.656609, train_loss=2.9262004

Batch 183890, train_perplexity=18.656609, train_loss=2.9262004

Batch 183900, train_perplexity=18.656609, train_loss=2.9262004

Batch 183910, train_perplexity=18.656609, train_loss=2.9262004

Batch 183920, train_perplexity=18.656609, train_loss=2.9262004

Batch 183930, train_perplexity=18.656609, train_loss=2.9262004

Batch 183940, train_perplexity=18.656603, train_loss=2.9262002

Batch 183950, train_perplexity=18.656609, train_loss=2.9262004

Batch 183960, train_perplexity=18.656609, train_loss=2.9262004

Batch 183970, train_perplexity=18.656609, train_loss=2.9262004

Batch 183980, train_perplexity=18.656603, train_loss=2.9262002

Batch 183990, train_perplexity=18.656609, train_loss=2.9262004

Batch 184000, train_perplexity=18.656609, train_loss=2.9262004

Batch 184010, train_perplexity=18.656603, train_loss=2.9262002

Batch 184020, train_perplexity=18.656603, train_loss=2.9262002

Batch 184030, train_perplexity=18.656603, train_loss=2.9262002

Batch 184040, train_perplexity=18.656603, train_loss=2.9262002

Batch 184050, train_perplexity=18.656609, train_loss=2.9262004

Batch 184060, train_perplexity=18.656609, train_loss=2.9262004

Batch 184070, train_perplexity=18.656603, train_loss=2.9262002

Batch 184080, train_perplexity=18.656599, train_loss=2.9262

Batch 184090, train_perplexity=18.656603, train_loss=2.9262002

Batch 184100, train_perplexity=18.656603, train_loss=2.9262002

Batch 184110, train_perplexity=18.656599, train_loss=2.9262

Batch 184120, train_perplexity=18.656599, train_loss=2.9262

Batch 184130, train_perplexity=18.656599, train_loss=2.9262

Batch 184140, train_perplexity=18.656603, train_loss=2.9262002

Batch 184150, train_perplexity=18.656599, train_loss=2.9262

Batch 184160, train_perplexity=18.656599, train_loss=2.9262

Batch 184170, train_perplexity=18.656599, train_loss=2.9262

Batch 184180, train_perplexity=18.656603, train_loss=2.9262002

Batch 184190, train_perplexity=18.656599, train_loss=2.9262

Batch 184200, train_perplexity=18.656599, train_loss=2.9262

Batch 184210, train_perplexity=18.656599, train_loss=2.9262

Batch 184220, train_perplexity=18.656595, train_loss=2.9261997

Batch 184230, train_perplexity=18.656599, train_loss=2.9262

Batch 184240, train_perplexity=18.656599, train_loss=2.9262

Batch 184250, train_perplexity=18.656603, train_loss=2.9262002

Batch 184260, train_perplexity=18.656599, train_loss=2.9262

Batch 184270, train_perplexity=18.656599, train_loss=2.9262

Batch 184280, train_perplexity=18.656599, train_loss=2.9262

Batch 184290, train_perplexity=18.656599, train_loss=2.9262

Batch 184300, train_perplexity=18.656599, train_loss=2.9262

Batch 184310, train_perplexity=18.656599, train_loss=2.9262

Batch 184320, train_perplexity=18.656595, train_loss=2.9261997

Batch 184330, train_perplexity=18.656595, train_loss=2.9261997

Batch 184340, train_perplexity=18.656599, train_loss=2.9262

Batch 184350, train_perplexity=18.65659, train_loss=2.9261994

Batch 184360, train_perplexity=18.656595, train_loss=2.9261997

Batch 184370, train_perplexity=18.656595, train_loss=2.9261997

Batch 184380, train_perplexity=18.656599, train_loss=2.9262

Batch 184390, train_perplexity=18.65659, train_loss=2.9261994

Batch 184400, train_perplexity=18.656599, train_loss=2.9262

Batch 184410, train_perplexity=18.65659, train_loss=2.9261994

Batch 184420, train_perplexity=18.65659, train_loss=2.9261994

Batch 184430, train_perplexity=18.65659, train_loss=2.9261994

Batch 184440, train_perplexity=18.65659, train_loss=2.9261994

Batch 184450, train_perplexity=18.65659, train_loss=2.9261994

Batch 184460, train_perplexity=18.656595, train_loss=2.9261997

Batch 184470, train_perplexity=18.65659, train_loss=2.9261994

Batch 184480, train_perplexity=18.65659, train_loss=2.9261994

Batch 184490, train_perplexity=18.65659, train_loss=2.9261994

Batch 184500, train_perplexity=18.65659, train_loss=2.9261994

Batch 184510, train_perplexity=18.65659, train_loss=2.9261994

Batch 184520, train_perplexity=18.65659, train_loss=2.9261994

Batch 184530, train_perplexity=18.65659, train_loss=2.9261994

Batch 184540, train_perplexity=18.65659, train_loss=2.9261994

Batch 184550, train_perplexity=18.65659, train_loss=2.9261994

Batch 184560, train_perplexity=18.65659, train_loss=2.9261994

Batch 184570, train_perplexity=18.65659, train_loss=2.9261994

Batch 184580, train_perplexity=18.65659, train_loss=2.9261994

Batch 184590, train_perplexity=18.65659, train_loss=2.9261994

Batch 184600, train_perplexity=18.65659, train_loss=2.9261994

Batch 184610, train_perplexity=18.656586, train_loss=2.9261992

Batch 184620, train_perplexity=18.65659, train_loss=2.9261994

Batch 184630, train_perplexity=18.656586, train_loss=2.9261992

Batch 184640, train_perplexity=18.65659, train_loss=2.9261994

Batch 184650, train_perplexity=18.656586, train_loss=2.9261992

Batch 184660, train_perplexity=18.65659, train_loss=2.9261994

Batch 184670, train_perplexity=18.656586, train_loss=2.9261992

Batch 184680, train_perplexity=18.656582, train_loss=2.926199

Batch 184690, train_perplexity=18.656582, train_loss=2.926199

Batch 184700, train_perplexity=18.656586, train_loss=2.9261992

Batch 184710, train_perplexity=18.656586, train_loss=2.9261992

Batch 184720, train_perplexity=18.656582, train_loss=2.926199

Batch 184730, train_perplexity=18.656586, train_loss=2.9261992

Batch 184740, train_perplexity=18.656582, train_loss=2.926199

Batch 184750, train_perplexity=18.656582, train_loss=2.926199

Batch 184760, train_perplexity=18.656586, train_loss=2.9261992

Batch 184770, train_perplexity=18.656586, train_loss=2.9261992

Batch 184780, train_perplexity=18.656582, train_loss=2.926199

Batch 184790, train_perplexity=18.656582, train_loss=2.926199

Batch 184800, train_perplexity=18.656582, train_loss=2.926199

Batch 184810, train_perplexity=18.656582, train_loss=2.926199

Batch 184820, train_perplexity=18.656582, train_loss=2.926199

Batch 184830, train_perplexity=18.656582, train_loss=2.926199
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 184840, train_perplexity=18.656582, train_loss=2.926199

Batch 184850, train_perplexity=18.656582, train_loss=2.926199

Batch 184860, train_perplexity=18.656582, train_loss=2.926199

Batch 184870, train_perplexity=18.656586, train_loss=2.9261992

Batch 184880, train_perplexity=18.656582, train_loss=2.926199

Batch 184890, train_perplexity=18.656582, train_loss=2.926199

Batch 184900, train_perplexity=18.656576, train_loss=2.9261987

Batch 184910, train_perplexity=18.656582, train_loss=2.926199

Batch 184920, train_perplexity=18.656576, train_loss=2.9261987

Batch 184930, train_perplexity=18.656572, train_loss=2.9261985

Batch 184940, train_perplexity=18.656582, train_loss=2.926199

Batch 184950, train_perplexity=18.656572, train_loss=2.9261985

Batch 184960, train_perplexity=18.656582, train_loss=2.926199

Batch 184970, train_perplexity=18.656582, train_loss=2.926199

Batch 184980, train_perplexity=18.656572, train_loss=2.9261985

Batch 184990, train_perplexity=18.656576, train_loss=2.9261987

Batch 185000, train_perplexity=18.656582, train_loss=2.926199

Batch 185010, train_perplexity=18.656572, train_loss=2.9261985

Batch 185020, train_perplexity=18.656576, train_loss=2.9261987

Batch 185030, train_perplexity=18.656582, train_loss=2.926199

Batch 185040, train_perplexity=18.656572, train_loss=2.9261985

Batch 185050, train_perplexity=18.656572, train_loss=2.9261985

Batch 185060, train_perplexity=18.656572, train_loss=2.9261985

Batch 185070, train_perplexity=18.656572, train_loss=2.9261985

Batch 185080, train_perplexity=18.656572, train_loss=2.9261985

Batch 185090, train_perplexity=18.656572, train_loss=2.9261985

Batch 185100, train_perplexity=18.656572, train_loss=2.9261985

Batch 185110, train_perplexity=18.656582, train_loss=2.926199

Batch 185120, train_perplexity=18.656576, train_loss=2.9261987

Batch 185130, train_perplexity=18.656572, train_loss=2.9261985

Batch 185140, train_perplexity=18.656572, train_loss=2.9261985

Batch 185150, train_perplexity=18.656572, train_loss=2.9261985

Batch 185160, train_perplexity=18.656569, train_loss=2.9261982

Batch 185170, train_perplexity=18.656572, train_loss=2.9261985

Batch 185180, train_perplexity=18.656572, train_loss=2.9261985

Batch 185190, train_perplexity=18.656563, train_loss=2.926198

Batch 185200, train_perplexity=18.656572, train_loss=2.9261985

Batch 185210, train_perplexity=18.656572, train_loss=2.9261985

Batch 185220, train_perplexity=18.656572, train_loss=2.9261985

Batch 185230, train_perplexity=18.656572, train_loss=2.9261985

Batch 185240, train_perplexity=18.656569, train_loss=2.9261982

Batch 185250, train_perplexity=18.656572, train_loss=2.9261985

Batch 185260, train_perplexity=18.656572, train_loss=2.9261985

Batch 185270, train_perplexity=18.656569, train_loss=2.9261982

Batch 185280, train_perplexity=18.656572, train_loss=2.9261985

Batch 185290, train_perplexity=18.656572, train_loss=2.9261985

Batch 185300, train_perplexity=18.656572, train_loss=2.9261985

Batch 185310, train_perplexity=18.656572, train_loss=2.9261985

Batch 185320, train_perplexity=18.656563, train_loss=2.926198

Batch 185330, train_perplexity=18.656563, train_loss=2.926198

Batch 185340, train_perplexity=18.656569, train_loss=2.9261982

Batch 185350, train_perplexity=18.656563, train_loss=2.926198

Batch 185360, train_perplexity=18.656563, train_loss=2.926198

Batch 185370, train_perplexity=18.656563, train_loss=2.926198

Batch 185380, train_perplexity=18.656563, train_loss=2.926198

Batch 185390, train_perplexity=18.656563, train_loss=2.926198

Batch 185400, train_perplexity=18.656569, train_loss=2.9261982

Batch 185410, train_perplexity=18.656563, train_loss=2.926198

Batch 185420, train_perplexity=18.656563, train_loss=2.926198

Batch 185430, train_perplexity=18.656569, train_loss=2.9261982

Batch 185440, train_perplexity=18.656563, train_loss=2.926198

Batch 185450, train_perplexity=18.656563, train_loss=2.926198

Batch 185460, train_perplexity=18.656563, train_loss=2.926198

Batch 185470, train_perplexity=18.656569, train_loss=2.9261982

Batch 185480, train_perplexity=18.656555, train_loss=2.9261975

Batch 185490, train_perplexity=18.656563, train_loss=2.926198

Batch 185500, train_perplexity=18.656563, train_loss=2.926198

Batch 185510, train_perplexity=18.656563, train_loss=2.926198

Batch 185520, train_perplexity=18.656563, train_loss=2.926198

Batch 185530, train_perplexity=18.656559, train_loss=2.9261978

Batch 185540, train_perplexity=18.656563, train_loss=2.926198

Batch 185550, train_perplexity=18.656555, train_loss=2.9261975

Batch 185560, train_perplexity=18.656563, train_loss=2.926198

Batch 185570, train_perplexity=18.656559, train_loss=2.9261978

Batch 185580, train_perplexity=18.656559, train_loss=2.9261978

Batch 185590, train_perplexity=18.656563, train_loss=2.926198

Batch 185600, train_perplexity=18.656555, train_loss=2.9261975

Batch 185610, train_perplexity=18.656563, train_loss=2.926198

Batch 185620, train_perplexity=18.656563, train_loss=2.926198

Batch 185630, train_perplexity=18.656555, train_loss=2.9261975

Batch 185640, train_perplexity=18.656555, train_loss=2.9261975

Batch 185650, train_perplexity=18.656555, train_loss=2.9261975

Batch 185660, train_perplexity=18.656563, train_loss=2.926198

Batch 185670, train_perplexity=18.656555, train_loss=2.9261975

Batch 185680, train_perplexity=18.656555, train_loss=2.9261975

Batch 185690, train_perplexity=18.656555, train_loss=2.9261975

Batch 185700, train_perplexity=18.656555, train_loss=2.9261975

Batch 185710, train_perplexity=18.656559, train_loss=2.9261978

Batch 185720, train_perplexity=18.656555, train_loss=2.9261975

Batch 185730, train_perplexity=18.656555, train_loss=2.9261975

Batch 185740, train_perplexity=18.656555, train_loss=2.9261975

Batch 185750, train_perplexity=18.656555, train_loss=2.9261975

Batch 185760, train_perplexity=18.656555, train_loss=2.9261975

Batch 185770, train_perplexity=18.656555, train_loss=2.9261975

Batch 185780, train_perplexity=18.656555, train_loss=2.9261975

Batch 185790, train_perplexity=18.656555, train_loss=2.9261975

Batch 185800, train_perplexity=18.656555, train_loss=2.9261975

Batch 185810, train_perplexity=18.65655, train_loss=2.9261973

Batch 185820, train_perplexity=18.656555, train_loss=2.9261975

Batch 185830, train_perplexity=18.656546, train_loss=2.926197

Batch 185840, train_perplexity=18.656555, train_loss=2.9261975

Batch 185850, train_perplexity=18.656546, train_loss=2.926197

Batch 185860, train_perplexity=18.656555, train_loss=2.9261975

Batch 185870, train_perplexity=18.656555, train_loss=2.9261975

Batch 185880, train_perplexity=18.65655, train_loss=2.9261973

Batch 185890, train_perplexity=18.656546, train_loss=2.926197

Batch 185900, train_perplexity=18.656555, train_loss=2.9261975

Batch 185910, train_perplexity=18.65655, train_loss=2.9261973

Batch 185920, train_perplexity=18.65655, train_loss=2.9261973

Batch 185930, train_perplexity=18.656555, train_loss=2.9261975

Batch 185940, train_perplexity=18.656546, train_loss=2.926197

Batch 185950, train_perplexity=18.656546, train_loss=2.926197

Batch 185960, train_perplexity=18.656546, train_loss=2.926197

Batch 185970, train_perplexity=18.65655, train_loss=2.9261973

Batch 185980, train_perplexity=18.656546, train_loss=2.926197

Batch 185990, train_perplexity=18.65655, train_loss=2.9261973

Batch 186000, train_perplexity=18.656546, train_loss=2.926197

Batch 186010, train_perplexity=18.65655, train_loss=2.9261973

Batch 186020, train_perplexity=18.656546, train_loss=2.926197

Batch 186030, train_perplexity=18.656542, train_loss=2.9261968

Batch 186040, train_perplexity=18.656546, train_loss=2.926197

Batch 186050, train_perplexity=18.656546, train_loss=2.926197

Batch 186060, train_perplexity=18.656546, train_loss=2.926197

Batch 186070, train_perplexity=18.656546, train_loss=2.926197

Batch 186080, train_perplexity=18.65655, train_loss=2.9261973

Batch 186090, train_perplexity=18.656546, train_loss=2.926197

Batch 186100, train_perplexity=18.656546, train_loss=2.926197

Batch 186110, train_perplexity=18.656546, train_loss=2.926197

Batch 186120, train_perplexity=18.656542, train_loss=2.9261968
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 186130, train_perplexity=18.656546, train_loss=2.926197

Batch 186140, train_perplexity=18.656546, train_loss=2.926197

Batch 186150, train_perplexity=18.656546, train_loss=2.926197

Batch 186160, train_perplexity=18.656546, train_loss=2.926197

Batch 186170, train_perplexity=18.656546, train_loss=2.926197

Batch 186180, train_perplexity=18.656546, train_loss=2.926197

Batch 186190, train_perplexity=18.656546, train_loss=2.926197

Batch 186200, train_perplexity=18.656546, train_loss=2.926197

Batch 186210, train_perplexity=18.656546, train_loss=2.926197

Batch 186220, train_perplexity=18.656546, train_loss=2.926197

Batch 186230, train_perplexity=18.656546, train_loss=2.926197

Batch 186240, train_perplexity=18.656546, train_loss=2.926197

Batch 186250, train_perplexity=18.656536, train_loss=2.9261966

Batch 186260, train_perplexity=18.656536, train_loss=2.9261966

Batch 186270, train_perplexity=18.656536, train_loss=2.9261966

Batch 186280, train_perplexity=18.656546, train_loss=2.926197

Batch 186290, train_perplexity=18.656536, train_loss=2.9261966

Batch 186300, train_perplexity=18.656536, train_loss=2.9261966

Batch 186310, train_perplexity=18.656536, train_loss=2.9261966

Batch 186320, train_perplexity=18.656536, train_loss=2.9261966

Batch 186330, train_perplexity=18.656536, train_loss=2.9261966

Batch 186340, train_perplexity=18.656546, train_loss=2.926197

Batch 186350, train_perplexity=18.656536, train_loss=2.9261966

Batch 186360, train_perplexity=18.656536, train_loss=2.9261966

Batch 186370, train_perplexity=18.656536, train_loss=2.9261966

Batch 186380, train_perplexity=18.656536, train_loss=2.9261966

Batch 186390, train_perplexity=18.656536, train_loss=2.9261966

Batch 186400, train_perplexity=18.656536, train_loss=2.9261966

Batch 186410, train_perplexity=18.656536, train_loss=2.9261966

Batch 186420, train_perplexity=18.656536, train_loss=2.9261966

Batch 186430, train_perplexity=18.656532, train_loss=2.9261963

Batch 186440, train_perplexity=18.656536, train_loss=2.9261966

Batch 186450, train_perplexity=18.656536, train_loss=2.9261966

Batch 186460, train_perplexity=18.656532, train_loss=2.9261963

Batch 186470, train_perplexity=18.656536, train_loss=2.9261966

Batch 186480, train_perplexity=18.656528, train_loss=2.926196

Batch 186490, train_perplexity=18.656536, train_loss=2.9261966

Batch 186500, train_perplexity=18.656528, train_loss=2.926196

Batch 186510, train_perplexity=18.656532, train_loss=2.9261963

Batch 186520, train_perplexity=18.656528, train_loss=2.926196

Batch 186530, train_perplexity=18.656536, train_loss=2.9261966

Batch 186540, train_perplexity=18.656528, train_loss=2.926196

Batch 186550, train_perplexity=18.656532, train_loss=2.9261963

Batch 186560, train_perplexity=18.656536, train_loss=2.9261966

Batch 186570, train_perplexity=18.656532, train_loss=2.9261963

Batch 186580, train_perplexity=18.656536, train_loss=2.9261966

Batch 186590, train_perplexity=18.656536, train_loss=2.9261966

Batch 186600, train_perplexity=18.656536, train_loss=2.9261966

Batch 186610, train_perplexity=18.656528, train_loss=2.926196

Batch 186620, train_perplexity=18.656532, train_loss=2.9261963

Batch 186630, train_perplexity=18.656532, train_loss=2.9261963

Batch 186640, train_perplexity=18.656528, train_loss=2.926196

Batch 186650, train_perplexity=18.656528, train_loss=2.926196

Batch 186660, train_perplexity=18.656528, train_loss=2.926196

Batch 186670, train_perplexity=18.656528, train_loss=2.926196

Batch 186680, train_perplexity=18.656523, train_loss=2.9261959

Batch 186690, train_perplexity=18.656532, train_loss=2.9261963

Batch 186700, train_perplexity=18.656528, train_loss=2.926196

Batch 186710, train_perplexity=18.656528, train_loss=2.926196

Batch 186720, train_perplexity=18.656528, train_loss=2.926196

Batch 186730, train_perplexity=18.656528, train_loss=2.926196

Batch 186740, train_perplexity=18.656528, train_loss=2.926196

Batch 186750, train_perplexity=18.656523, train_loss=2.9261959

Batch 186760, train_perplexity=18.656519, train_loss=2.9261956

Batch 186770, train_perplexity=18.656528, train_loss=2.926196

Batch 186780, train_perplexity=18.656528, train_loss=2.926196

Batch 186790, train_perplexity=18.656523, train_loss=2.9261959

Batch 186800, train_perplexity=18.656523, train_loss=2.9261959

Batch 186810, train_perplexity=18.656523, train_loss=2.9261959

Batch 186820, train_perplexity=18.656528, train_loss=2.926196

Batch 186830, train_perplexity=18.656528, train_loss=2.926196

Batch 186840, train_perplexity=18.656528, train_loss=2.926196

Batch 186850, train_perplexity=18.656523, train_loss=2.9261959

Batch 186860, train_perplexity=18.656528, train_loss=2.926196

Batch 186870, train_perplexity=18.656523, train_loss=2.9261959

Batch 186880, train_perplexity=18.656519, train_loss=2.9261956

Batch 186890, train_perplexity=18.656528, train_loss=2.926196

Batch 186900, train_perplexity=18.656519, train_loss=2.9261956

Batch 186910, train_perplexity=18.656523, train_loss=2.9261959

Batch 186920, train_perplexity=18.656519, train_loss=2.9261956

Batch 186930, train_perplexity=18.656523, train_loss=2.9261959

Batch 186940, train_perplexity=18.656519, train_loss=2.9261956

Batch 186950, train_perplexity=18.656519, train_loss=2.9261956

Batch 186960, train_perplexity=18.656523, train_loss=2.9261959

Batch 186970, train_perplexity=18.656519, train_loss=2.9261956

Batch 186980, train_perplexity=18.656519, train_loss=2.9261956

Batch 186990, train_perplexity=18.656519, train_loss=2.9261956

Batch 187000, train_perplexity=18.656515, train_loss=2.9261954

Batch 187010, train_perplexity=18.656519, train_loss=2.9261956

Batch 187020, train_perplexity=18.656519, train_loss=2.9261956

Batch 187030, train_perplexity=18.656519, train_loss=2.9261956

Batch 187040, train_perplexity=18.656519, train_loss=2.9261956

Batch 187050, train_perplexity=18.656519, train_loss=2.9261956

Batch 187060, train_perplexity=18.656519, train_loss=2.9261956

Batch 187070, train_perplexity=18.656515, train_loss=2.9261954

Batch 187080, train_perplexity=18.656519, train_loss=2.9261956

Batch 187090, train_perplexity=18.656519, train_loss=2.9261956

Batch 187100, train_perplexity=18.656515, train_loss=2.9261954

Batch 187110, train_perplexity=18.656519, train_loss=2.9261956

Batch 187120, train_perplexity=18.65651, train_loss=2.9261951

Batch 187130, train_perplexity=18.656515, train_loss=2.9261954

Batch 187140, train_perplexity=18.656519, train_loss=2.9261956

Batch 187150, train_perplexity=18.656515, train_loss=2.9261954

Batch 187160, train_perplexity=18.656515, train_loss=2.9261954

Batch 187170, train_perplexity=18.65651, train_loss=2.9261951

Batch 187180, train_perplexity=18.656519, train_loss=2.9261956

Batch 187190, train_perplexity=18.65651, train_loss=2.9261951

Batch 187200, train_perplexity=18.65651, train_loss=2.9261951

Batch 187210, train_perplexity=18.65651, train_loss=2.9261951

Batch 187220, train_perplexity=18.656519, train_loss=2.9261956

Batch 187230, train_perplexity=18.656515, train_loss=2.9261954

Batch 187240, train_perplexity=18.65651, train_loss=2.9261951

Batch 187250, train_perplexity=18.65651, train_loss=2.9261951

Batch 187260, train_perplexity=18.65651, train_loss=2.9261951

Batch 187270, train_perplexity=18.65651, train_loss=2.9261951

Batch 187280, train_perplexity=18.65651, train_loss=2.9261951

Batch 187290, train_perplexity=18.65651, train_loss=2.9261951

Batch 187300, train_perplexity=18.65651, train_loss=2.9261951

Batch 187310, train_perplexity=18.65651, train_loss=2.9261951

Batch 187320, train_perplexity=18.65651, train_loss=2.9261951

Batch 187330, train_perplexity=18.65651, train_loss=2.9261951

Batch 187340, train_perplexity=18.65651, train_loss=2.9261951

Batch 187350, train_perplexity=18.65651, train_loss=2.9261951

Batch 187360, train_perplexity=18.656506, train_loss=2.926195

Batch 187370, train_perplexity=18.65651, train_loss=2.9261951

Batch 187380, train_perplexity=18.65651, train_loss=2.9261951

Batch 187390, train_perplexity=18.65651, train_loss=2.9261951

Batch 187400, train_perplexity=18.65651, train_loss=2.9261951

Batch 187410, train_perplexity=18.65651, train_loss=2.9261951
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 187420, train_perplexity=18.65651, train_loss=2.9261951

Batch 187430, train_perplexity=18.65651, train_loss=2.9261951

Batch 187440, train_perplexity=18.656506, train_loss=2.926195

Batch 187450, train_perplexity=18.656502, train_loss=2.9261947

Batch 187460, train_perplexity=18.65651, train_loss=2.9261951

Batch 187470, train_perplexity=18.65651, train_loss=2.9261951

Batch 187480, train_perplexity=18.656506, train_loss=2.926195

Batch 187490, train_perplexity=18.656506, train_loss=2.926195

Batch 187500, train_perplexity=18.656502, train_loss=2.9261947

Batch 187510, train_perplexity=18.656506, train_loss=2.926195

Batch 187520, train_perplexity=18.65651, train_loss=2.9261951

Batch 187530, train_perplexity=18.65651, train_loss=2.9261951

Batch 187540, train_perplexity=18.65651, train_loss=2.9261951

Batch 187550, train_perplexity=18.65651, train_loss=2.9261951

Batch 187560, train_perplexity=18.656502, train_loss=2.9261947

Batch 187570, train_perplexity=18.656502, train_loss=2.9261947

Batch 187580, train_perplexity=18.65651, train_loss=2.9261951

Batch 187590, train_perplexity=18.656502, train_loss=2.9261947

Batch 187600, train_perplexity=18.656502, train_loss=2.9261947

Batch 187610, train_perplexity=18.65651, train_loss=2.9261951

Batch 187620, train_perplexity=18.656496, train_loss=2.9261944

Batch 187630, train_perplexity=18.656502, train_loss=2.9261947

Batch 187640, train_perplexity=18.656502, train_loss=2.9261947

Batch 187650, train_perplexity=18.656502, train_loss=2.9261947

Batch 187660, train_perplexity=18.656496, train_loss=2.9261944

Batch 187670, train_perplexity=18.656496, train_loss=2.9261944

Batch 187680, train_perplexity=18.656496, train_loss=2.9261944

Batch 187690, train_perplexity=18.656502, train_loss=2.9261947

Batch 187700, train_perplexity=18.656502, train_loss=2.9261947

Batch 187710, train_perplexity=18.656502, train_loss=2.9261947

Batch 187720, train_perplexity=18.656492, train_loss=2.9261942

Batch 187730, train_perplexity=18.656502, train_loss=2.9261947

Batch 187740, train_perplexity=18.656496, train_loss=2.9261944

Batch 187750, train_perplexity=18.656492, train_loss=2.9261942

Batch 187760, train_perplexity=18.656496, train_loss=2.9261944

Batch 187770, train_perplexity=18.656492, train_loss=2.9261942

Batch 187780, train_perplexity=18.656492, train_loss=2.9261942

Batch 187790, train_perplexity=18.656502, train_loss=2.9261947

Batch 187800, train_perplexity=18.656496, train_loss=2.9261944

Batch 187810, train_perplexity=18.656492, train_loss=2.9261942

Batch 187820, train_perplexity=18.656492, train_loss=2.9261942

Batch 187830, train_perplexity=18.656492, train_loss=2.9261942

Batch 187840, train_perplexity=18.656502, train_loss=2.9261947

Batch 187850, train_perplexity=18.656492, train_loss=2.9261942

Batch 187860, train_perplexity=18.656492, train_loss=2.9261942

Batch 187870, train_perplexity=18.656496, train_loss=2.9261944

Batch 187880, train_perplexity=18.656492, train_loss=2.9261942

Batch 187890, train_perplexity=18.656492, train_loss=2.9261942

Batch 187900, train_perplexity=18.656488, train_loss=2.926194

Batch 187910, train_perplexity=18.656492, train_loss=2.9261942

Batch 187920, train_perplexity=18.656492, train_loss=2.9261942

Batch 187930, train_perplexity=18.656492, train_loss=2.9261942

Batch 187940, train_perplexity=18.656492, train_loss=2.9261942

Batch 187950, train_perplexity=18.656492, train_loss=2.9261942

Batch 187960, train_perplexity=18.656492, train_loss=2.9261942

Batch 187970, train_perplexity=18.656492, train_loss=2.9261942

Batch 187980, train_perplexity=18.656492, train_loss=2.9261942

Batch 187990, train_perplexity=18.656488, train_loss=2.926194

Batch 188000, train_perplexity=18.656492, train_loss=2.9261942

Batch 188010, train_perplexity=18.656492, train_loss=2.9261942

Batch 188020, train_perplexity=18.656492, train_loss=2.9261942

Batch 188030, train_perplexity=18.656492, train_loss=2.9261942

Batch 188040, train_perplexity=18.656492, train_loss=2.9261942

Batch 188050, train_perplexity=18.656492, train_loss=2.9261942

Batch 188060, train_perplexity=18.656492, train_loss=2.9261942

Batch 188070, train_perplexity=18.656488, train_loss=2.926194

Batch 188080, train_perplexity=18.656483, train_loss=2.9261937

Batch 188090, train_perplexity=18.656488, train_loss=2.926194

Batch 188100, train_perplexity=18.656483, train_loss=2.9261937

Batch 188110, train_perplexity=18.656483, train_loss=2.9261937

Batch 188120, train_perplexity=18.656483, train_loss=2.9261937

Batch 188130, train_perplexity=18.656488, train_loss=2.926194

Batch 188140, train_perplexity=18.656492, train_loss=2.9261942

Batch 188150, train_perplexity=18.656488, train_loss=2.926194

Batch 188160, train_perplexity=18.656492, train_loss=2.9261942

Batch 188170, train_perplexity=18.656483, train_loss=2.9261937

Batch 188180, train_perplexity=18.656483, train_loss=2.9261937

Batch 188190, train_perplexity=18.656488, train_loss=2.926194

Batch 188200, train_perplexity=18.656483, train_loss=2.9261937

Batch 188210, train_perplexity=18.656483, train_loss=2.9261937

Batch 188220, train_perplexity=18.656488, train_loss=2.926194

Batch 188230, train_perplexity=18.656483, train_loss=2.9261937

Batch 188240, train_perplexity=18.656483, train_loss=2.9261937

Batch 188250, train_perplexity=18.656483, train_loss=2.9261937

Batch 188260, train_perplexity=18.656483, train_loss=2.9261937

Batch 188270, train_perplexity=18.656483, train_loss=2.9261937

Batch 188280, train_perplexity=18.656483, train_loss=2.9261937

Batch 188290, train_perplexity=18.656483, train_loss=2.9261937

Batch 188300, train_perplexity=18.656479, train_loss=2.9261935

Batch 188310, train_perplexity=18.656483, train_loss=2.9261937

Batch 188320, train_perplexity=18.656483, train_loss=2.9261937

Batch 188330, train_perplexity=18.656475, train_loss=2.9261932

Batch 188340, train_perplexity=18.656483, train_loss=2.9261937

Batch 188350, train_perplexity=18.656483, train_loss=2.9261937

Batch 188360, train_perplexity=18.656479, train_loss=2.9261935

Batch 188370, train_perplexity=18.656483, train_loss=2.9261937

Batch 188380, train_perplexity=18.656483, train_loss=2.9261937

Batch 188390, train_perplexity=18.656475, train_loss=2.9261932

Batch 188400, train_perplexity=18.656479, train_loss=2.9261935

Batch 188410, train_perplexity=18.656479, train_loss=2.9261935

Batch 188420, train_perplexity=18.656479, train_loss=2.9261935

Batch 188430, train_perplexity=18.656483, train_loss=2.9261937

Batch 188440, train_perplexity=18.656475, train_loss=2.9261932

Batch 188450, train_perplexity=18.656475, train_loss=2.9261932

Batch 188460, train_perplexity=18.656479, train_loss=2.9261935

Batch 188470, train_perplexity=18.656479, train_loss=2.9261935

Batch 188480, train_perplexity=18.656475, train_loss=2.9261932

Batch 188490, train_perplexity=18.656479, train_loss=2.9261935

Batch 188500, train_perplexity=18.656475, train_loss=2.9261932

Batch 188510, train_perplexity=18.656479, train_loss=2.9261935

Batch 188520, train_perplexity=18.656475, train_loss=2.9261932

Batch 188530, train_perplexity=18.656475, train_loss=2.9261932

Batch 188540, train_perplexity=18.656475, train_loss=2.9261932

Batch 188550, train_perplexity=18.656475, train_loss=2.9261932

Batch 188560, train_perplexity=18.656475, train_loss=2.9261932

Batch 188570, train_perplexity=18.656479, train_loss=2.9261935

Batch 188580, train_perplexity=18.656475, train_loss=2.9261932

Batch 188590, train_perplexity=18.656475, train_loss=2.9261932

Batch 188600, train_perplexity=18.65647, train_loss=2.926193

Batch 188610, train_perplexity=18.656475, train_loss=2.9261932

Batch 188620, train_perplexity=18.656475, train_loss=2.9261932

Batch 188630, train_perplexity=18.656475, train_loss=2.9261932

Batch 188640, train_perplexity=18.656475, train_loss=2.9261932

Batch 188650, train_perplexity=18.656475, train_loss=2.9261932

Batch 188660, train_perplexity=18.656466, train_loss=2.9261928

Batch 188670, train_perplexity=18.65647, train_loss=2.926193

Batch 188680, train_perplexity=18.656475, train_loss=2.9261932

Batch 188690, train_perplexity=18.65647, train_loss=2.926193

Batch 188700, train_perplexity=18.656466, train_loss=2.9261928
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 188710, train_perplexity=18.656475, train_loss=2.9261932

Batch 188720, train_perplexity=18.656475, train_loss=2.9261932

Batch 188730, train_perplexity=18.656466, train_loss=2.9261928

Batch 188740, train_perplexity=18.656475, train_loss=2.9261932

Batch 188750, train_perplexity=18.656475, train_loss=2.9261932

Batch 188760, train_perplexity=18.656475, train_loss=2.9261932

Batch 188770, train_perplexity=18.65647, train_loss=2.926193

Batch 188780, train_perplexity=18.656466, train_loss=2.9261928

Batch 188790, train_perplexity=18.656475, train_loss=2.9261932

Batch 188800, train_perplexity=18.656466, train_loss=2.9261928

Batch 188810, train_perplexity=18.656466, train_loss=2.9261928

Batch 188820, train_perplexity=18.656466, train_loss=2.9261928

Batch 188830, train_perplexity=18.65647, train_loss=2.926193

Batch 188840, train_perplexity=18.656466, train_loss=2.9261928

Batch 188850, train_perplexity=18.656466, train_loss=2.9261928

Batch 188860, train_perplexity=18.656466, train_loss=2.9261928

Batch 188870, train_perplexity=18.656466, train_loss=2.9261928

Batch 188880, train_perplexity=18.656466, train_loss=2.9261928

Batch 188890, train_perplexity=18.656466, train_loss=2.9261928

Batch 188900, train_perplexity=18.656462, train_loss=2.9261925

Batch 188910, train_perplexity=18.656456, train_loss=2.9261923

Batch 188920, train_perplexity=18.656462, train_loss=2.9261925

Batch 188930, train_perplexity=18.656466, train_loss=2.9261928

Batch 188940, train_perplexity=18.656466, train_loss=2.9261928

Batch 188950, train_perplexity=18.656466, train_loss=2.9261928

Batch 188960, train_perplexity=18.656466, train_loss=2.9261928

Batch 188970, train_perplexity=18.656456, train_loss=2.9261923

Batch 188980, train_perplexity=18.656466, train_loss=2.9261928

Batch 188990, train_perplexity=18.656466, train_loss=2.9261928

Batch 189000, train_perplexity=18.656466, train_loss=2.9261928

Batch 189010, train_perplexity=18.656456, train_loss=2.9261923

Batch 189020, train_perplexity=18.656462, train_loss=2.9261925

Batch 189030, train_perplexity=18.656456, train_loss=2.9261923

Batch 189040, train_perplexity=18.656456, train_loss=2.9261923

Batch 189050, train_perplexity=18.656466, train_loss=2.9261928

Batch 189060, train_perplexity=18.656462, train_loss=2.9261925

Batch 189070, train_perplexity=18.656456, train_loss=2.9261923

Batch 189080, train_perplexity=18.656462, train_loss=2.9261925

Batch 189090, train_perplexity=18.656462, train_loss=2.9261925

Batch 189100, train_perplexity=18.656456, train_loss=2.9261923

Batch 189110, train_perplexity=18.656456, train_loss=2.9261923

Batch 189120, train_perplexity=18.656456, train_loss=2.9261923

Batch 189130, train_perplexity=18.656456, train_loss=2.9261923

Batch 189140, train_perplexity=18.656456, train_loss=2.9261923

Batch 189150, train_perplexity=18.656456, train_loss=2.9261923

Batch 189160, train_perplexity=18.656452, train_loss=2.926192

Batch 189170, train_perplexity=18.656456, train_loss=2.9261923

Batch 189180, train_perplexity=18.656456, train_loss=2.9261923

Batch 189190, train_perplexity=18.656456, train_loss=2.9261923

Batch 189200, train_perplexity=18.656456, train_loss=2.9261923

Batch 189210, train_perplexity=18.656456, train_loss=2.9261923

Batch 189220, train_perplexity=18.656456, train_loss=2.9261923

Batch 189230, train_perplexity=18.656456, train_loss=2.9261923

Batch 189240, train_perplexity=18.656452, train_loss=2.926192

Batch 189250, train_perplexity=18.656456, train_loss=2.9261923

Batch 189260, train_perplexity=18.656452, train_loss=2.926192

Batch 189270, train_perplexity=18.656456, train_loss=2.9261923

Batch 189280, train_perplexity=18.656456, train_loss=2.9261923

Batch 189290, train_perplexity=18.656448, train_loss=2.9261918

Batch 189300, train_perplexity=18.656452, train_loss=2.926192

Batch 189310, train_perplexity=18.656452, train_loss=2.926192

Batch 189320, train_perplexity=18.656452, train_loss=2.926192

Batch 189330, train_perplexity=18.656448, train_loss=2.9261918

Batch 189340, train_perplexity=18.656448, train_loss=2.9261918

Batch 189350, train_perplexity=18.656456, train_loss=2.9261923

Batch 189360, train_perplexity=18.656452, train_loss=2.926192

Batch 189370, train_perplexity=18.656456, train_loss=2.9261923

Batch 189380, train_perplexity=18.656456, train_loss=2.9261923

Batch 189390, train_perplexity=18.656452, train_loss=2.926192

Batch 189400, train_perplexity=18.656456, train_loss=2.9261923

Batch 189410, train_perplexity=18.656448, train_loss=2.9261918

Batch 189420, train_perplexity=18.656456, train_loss=2.9261923

Batch 189430, train_perplexity=18.656448, train_loss=2.9261918

Batch 189440, train_perplexity=18.656448, train_loss=2.9261918

Batch 189450, train_perplexity=18.656448, train_loss=2.9261918

Batch 189460, train_perplexity=18.656443, train_loss=2.9261916

Batch 189470, train_perplexity=18.656452, train_loss=2.926192

Batch 189480, train_perplexity=18.656443, train_loss=2.9261916

Batch 189490, train_perplexity=18.656448, train_loss=2.9261918

Batch 189500, train_perplexity=18.656448, train_loss=2.9261918

Batch 189510, train_perplexity=18.656448, train_loss=2.9261918

Batch 189520, train_perplexity=18.656456, train_loss=2.9261923

Batch 189530, train_perplexity=18.656448, train_loss=2.9261918

Batch 189540, train_perplexity=18.656448, train_loss=2.9261918

Batch 189550, train_perplexity=18.656448, train_loss=2.9261918

Batch 189560, train_perplexity=18.656448, train_loss=2.9261918

Batch 189570, train_perplexity=18.656448, train_loss=2.9261918

Batch 189580, train_perplexity=18.656448, train_loss=2.9261918

Batch 189590, train_perplexity=18.656439, train_loss=2.9261913

Batch 189600, train_perplexity=18.656448, train_loss=2.9261918

Batch 189610, train_perplexity=18.656443, train_loss=2.9261916

Batch 189620, train_perplexity=18.656443, train_loss=2.9261916

Batch 189630, train_perplexity=18.656448, train_loss=2.9261918

Batch 189640, train_perplexity=18.656443, train_loss=2.9261916

Batch 189650, train_perplexity=18.656439, train_loss=2.9261913

Batch 189660, train_perplexity=18.656439, train_loss=2.9261913

Batch 189670, train_perplexity=18.656443, train_loss=2.9261916

Batch 189680, train_perplexity=18.656439, train_loss=2.9261913

Batch 189690, train_perplexity=18.656439, train_loss=2.9261913

Batch 189700, train_perplexity=18.656443, train_loss=2.9261916

Batch 189710, train_perplexity=18.656448, train_loss=2.9261918

Batch 189720, train_perplexity=18.656439, train_loss=2.9261913

Batch 189730, train_perplexity=18.656439, train_loss=2.9261913

Batch 189740, train_perplexity=18.656443, train_loss=2.9261916

Batch 189750, train_perplexity=18.656439, train_loss=2.9261913

Batch 189760, train_perplexity=18.656439, train_loss=2.9261913

Batch 189770, train_perplexity=18.656439, train_loss=2.9261913

Batch 189780, train_perplexity=18.656439, train_loss=2.9261913

Batch 189790, train_perplexity=18.656435, train_loss=2.926191

Batch 189800, train_perplexity=18.656439, train_loss=2.9261913

Batch 189810, train_perplexity=18.656439, train_loss=2.9261913

Batch 189820, train_perplexity=18.656439, train_loss=2.9261913

Batch 189830, train_perplexity=18.656435, train_loss=2.926191

Batch 189840, train_perplexity=18.656439, train_loss=2.9261913

Batch 189850, train_perplexity=18.656439, train_loss=2.9261913

Batch 189860, train_perplexity=18.656439, train_loss=2.9261913

Batch 189870, train_perplexity=18.656439, train_loss=2.9261913

Batch 189880, train_perplexity=18.656439, train_loss=2.9261913

Batch 189890, train_perplexity=18.656439, train_loss=2.9261913

Batch 189900, train_perplexity=18.656439, train_loss=2.9261913

Batch 189910, train_perplexity=18.656435, train_loss=2.926191

Batch 189920, train_perplexity=18.65643, train_loss=2.9261909

Batch 189930, train_perplexity=18.656439, train_loss=2.9261913

Batch 189940, train_perplexity=18.656439, train_loss=2.9261913

Batch 189950, train_perplexity=18.656435, train_loss=2.926191

Batch 189960, train_perplexity=18.656435, train_loss=2.926191

Batch 189970, train_perplexity=18.65643, train_loss=2.9261909

Batch 189980, train_perplexity=18.656435, train_loss=2.926191

Batch 189990, train_perplexity=18.656439, train_loss=2.9261913
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 190000, train_perplexity=18.656435, train_loss=2.926191

Batch 190010, train_perplexity=18.65643, train_loss=2.9261909

Batch 190020, train_perplexity=18.656435, train_loss=2.926191

Batch 190030, train_perplexity=18.656439, train_loss=2.9261913

Batch 190040, train_perplexity=18.65643, train_loss=2.9261909

Batch 190050, train_perplexity=18.65643, train_loss=2.9261909

Batch 190060, train_perplexity=18.65643, train_loss=2.9261909

Batch 190070, train_perplexity=18.656435, train_loss=2.926191

Batch 190080, train_perplexity=18.65643, train_loss=2.9261909

Batch 190090, train_perplexity=18.656425, train_loss=2.9261906

Batch 190100, train_perplexity=18.65643, train_loss=2.9261909

Batch 190110, train_perplexity=18.65643, train_loss=2.9261909

Batch 190120, train_perplexity=18.656422, train_loss=2.9261904

Batch 190130, train_perplexity=18.65643, train_loss=2.9261909

Batch 190140, train_perplexity=18.656425, train_loss=2.9261906

Batch 190150, train_perplexity=18.656422, train_loss=2.9261904

Batch 190160, train_perplexity=18.656425, train_loss=2.9261906

Batch 190170, train_perplexity=18.65643, train_loss=2.9261909

Batch 190180, train_perplexity=18.656425, train_loss=2.9261906

Batch 190190, train_perplexity=18.65643, train_loss=2.9261909

Batch 190200, train_perplexity=18.656425, train_loss=2.9261906

Batch 190210, train_perplexity=18.65643, train_loss=2.9261909

Batch 190220, train_perplexity=18.65643, train_loss=2.9261909

Batch 190230, train_perplexity=18.656425, train_loss=2.9261906

Batch 190240, train_perplexity=18.656422, train_loss=2.9261904

Batch 190250, train_perplexity=18.656425, train_loss=2.9261906

Batch 190260, train_perplexity=18.656422, train_loss=2.9261904

Batch 190270, train_perplexity=18.656422, train_loss=2.9261904

Batch 190280, train_perplexity=18.656425, train_loss=2.9261906

Batch 190290, train_perplexity=18.65643, train_loss=2.9261909

Batch 190300, train_perplexity=18.65643, train_loss=2.9261909

Batch 190310, train_perplexity=18.65643, train_loss=2.9261909

Batch 190320, train_perplexity=18.656422, train_loss=2.9261904

Batch 190330, train_perplexity=18.656422, train_loss=2.9261904

Batch 190340, train_perplexity=18.656425, train_loss=2.9261906

Batch 190350, train_perplexity=18.656422, train_loss=2.9261904

Batch 190360, train_perplexity=18.656422, train_loss=2.9261904

Batch 190370, train_perplexity=18.656422, train_loss=2.9261904

Batch 190380, train_perplexity=18.656422, train_loss=2.9261904

Batch 190390, train_perplexity=18.656425, train_loss=2.9261906

Batch 190400, train_perplexity=18.656422, train_loss=2.9261904

Batch 190410, train_perplexity=18.656422, train_loss=2.9261904

Batch 190420, train_perplexity=18.656422, train_loss=2.9261904

Batch 190430, train_perplexity=18.656422, train_loss=2.9261904

Batch 190440, train_perplexity=18.656422, train_loss=2.9261904

Batch 190450, train_perplexity=18.656416, train_loss=2.9261901

Batch 190460, train_perplexity=18.656422, train_loss=2.9261904

Batch 190470, train_perplexity=18.656422, train_loss=2.9261904

Batch 190480, train_perplexity=18.656422, train_loss=2.9261904

Batch 190490, train_perplexity=18.656416, train_loss=2.9261901

Batch 190500, train_perplexity=18.656416, train_loss=2.9261901

Batch 190510, train_perplexity=18.656412, train_loss=2.92619

Batch 190520, train_perplexity=18.656422, train_loss=2.9261904

Batch 190530, train_perplexity=18.656422, train_loss=2.9261904

Batch 190540, train_perplexity=18.656416, train_loss=2.9261901

Batch 190550, train_perplexity=18.656422, train_loss=2.9261904

Batch 190560, train_perplexity=18.656416, train_loss=2.9261901

Batch 190570, train_perplexity=18.656416, train_loss=2.9261901

Batch 190580, train_perplexity=18.656422, train_loss=2.9261904

Batch 190590, train_perplexity=18.656416, train_loss=2.9261901

Batch 190600, train_perplexity=18.656416, train_loss=2.9261901

Batch 190610, train_perplexity=18.656412, train_loss=2.92619

Batch 190620, train_perplexity=18.656412, train_loss=2.92619

Batch 190630, train_perplexity=18.656412, train_loss=2.92619

Batch 190640, train_perplexity=18.656416, train_loss=2.9261901

Batch 190650, train_perplexity=18.656412, train_loss=2.92619

Batch 190660, train_perplexity=18.656412, train_loss=2.92619

Batch 190670, train_perplexity=18.656422, train_loss=2.9261904

Batch 190680, train_perplexity=18.656412, train_loss=2.92619

Batch 190690, train_perplexity=18.656412, train_loss=2.92619

Batch 190700, train_perplexity=18.656412, train_loss=2.92619

Batch 190710, train_perplexity=18.656412, train_loss=2.92619

Batch 190720, train_perplexity=18.656412, train_loss=2.92619

Batch 190730, train_perplexity=18.656412, train_loss=2.92619

Batch 190740, train_perplexity=18.656412, train_loss=2.92619

Batch 190750, train_perplexity=18.656412, train_loss=2.92619

Batch 190760, train_perplexity=18.656412, train_loss=2.92619

Batch 190770, train_perplexity=18.656412, train_loss=2.92619

Batch 190780, train_perplexity=18.656412, train_loss=2.92619

Batch 190790, train_perplexity=18.656408, train_loss=2.9261897

Batch 190800, train_perplexity=18.656412, train_loss=2.92619

Batch 190810, train_perplexity=18.656412, train_loss=2.92619

Batch 190820, train_perplexity=18.656412, train_loss=2.92619

Batch 190830, train_perplexity=18.656408, train_loss=2.9261897

Batch 190840, train_perplexity=18.656412, train_loss=2.92619

Batch 190850, train_perplexity=18.656403, train_loss=2.9261894

Batch 190860, train_perplexity=18.656403, train_loss=2.9261894

Batch 190870, train_perplexity=18.656408, train_loss=2.9261897

Batch 190880, train_perplexity=18.656412, train_loss=2.92619

Batch 190890, train_perplexity=18.656403, train_loss=2.9261894

Batch 190900, train_perplexity=18.656403, train_loss=2.9261894

Batch 190910, train_perplexity=18.656408, train_loss=2.9261897

Batch 190920, train_perplexity=18.656403, train_loss=2.9261894

Batch 190930, train_perplexity=18.656408, train_loss=2.9261897

Batch 190940, train_perplexity=18.656403, train_loss=2.9261894

Batch 190950, train_perplexity=18.656403, train_loss=2.9261894

Batch 190960, train_perplexity=18.656403, train_loss=2.9261894

Batch 190970, train_perplexity=18.656403, train_loss=2.9261894

Batch 190980, train_perplexity=18.656403, train_loss=2.9261894

Batch 190990, train_perplexity=18.656403, train_loss=2.9261894

Batch 191000, train_perplexity=18.656403, train_loss=2.9261894

Batch 191010, train_perplexity=18.656403, train_loss=2.9261894

Batch 191020, train_perplexity=18.656399, train_loss=2.9261892

Batch 191030, train_perplexity=18.656395, train_loss=2.926189

Batch 191040, train_perplexity=18.656403, train_loss=2.9261894

Batch 191050, train_perplexity=18.656399, train_loss=2.9261892

Batch 191060, train_perplexity=18.656403, train_loss=2.9261894

Batch 191070, train_perplexity=18.656395, train_loss=2.926189

Batch 191080, train_perplexity=18.656403, train_loss=2.9261894

Batch 191090, train_perplexity=18.656403, train_loss=2.9261894

Batch 191100, train_perplexity=18.656403, train_loss=2.9261894

Batch 191110, train_perplexity=18.656395, train_loss=2.926189

Batch 191120, train_perplexity=18.656395, train_loss=2.926189

Batch 191130, train_perplexity=18.656399, train_loss=2.9261892

Batch 191140, train_perplexity=18.656403, train_loss=2.9261894

Batch 191150, train_perplexity=18.656399, train_loss=2.9261892

Batch 191160, train_perplexity=18.656403, train_loss=2.9261894

Batch 191170, train_perplexity=18.656399, train_loss=2.9261892

Batch 191180, train_perplexity=18.656399, train_loss=2.9261892

Batch 191190, train_perplexity=18.656403, train_loss=2.9261894

Batch 191200, train_perplexity=18.656403, train_loss=2.9261894

Batch 191210, train_perplexity=18.656399, train_loss=2.9261892

Batch 191220, train_perplexity=18.656395, train_loss=2.926189

Batch 191230, train_perplexity=18.656395, train_loss=2.926189

Batch 191240, train_perplexity=18.656395, train_loss=2.926189

Batch 191250, train_perplexity=18.656403, train_loss=2.9261894

Batch 191260, train_perplexity=18.656403, train_loss=2.9261894

Batch 191270, train_perplexity=18.656399, train_loss=2.9261892

Batch 191280, train_perplexity=18.656395, train_loss=2.926189

Batch 191290, train_perplexity=18.656395, train_loss=2.926189
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 191300, train_perplexity=18.656395, train_loss=2.926189

Batch 191310, train_perplexity=18.656395, train_loss=2.926189

Batch 191320, train_perplexity=18.65639, train_loss=2.9261887

Batch 191330, train_perplexity=18.656395, train_loss=2.926189

Batch 191340, train_perplexity=18.656385, train_loss=2.9261885

Batch 191350, train_perplexity=18.656395, train_loss=2.926189

Batch 191360, train_perplexity=18.65639, train_loss=2.9261887

Batch 191370, train_perplexity=18.656395, train_loss=2.926189

Batch 191380, train_perplexity=18.656395, train_loss=2.926189

Batch 191390, train_perplexity=18.656395, train_loss=2.926189

Batch 191400, train_perplexity=18.656385, train_loss=2.9261885

Batch 191410, train_perplexity=18.656385, train_loss=2.9261885

Batch 191420, train_perplexity=18.656395, train_loss=2.926189

Batch 191430, train_perplexity=18.656385, train_loss=2.9261885

Batch 191440, train_perplexity=18.656395, train_loss=2.926189

Batch 191450, train_perplexity=18.65639, train_loss=2.9261887

Batch 191460, train_perplexity=18.656385, train_loss=2.9261885

Batch 191470, train_perplexity=18.656385, train_loss=2.9261885

Batch 191480, train_perplexity=18.656385, train_loss=2.9261885

Batch 191490, train_perplexity=18.656385, train_loss=2.9261885

Batch 191500, train_perplexity=18.656385, train_loss=2.9261885

Batch 191510, train_perplexity=18.65639, train_loss=2.9261887

Batch 191520, train_perplexity=18.65639, train_loss=2.9261887

Batch 191530, train_perplexity=18.656385, train_loss=2.9261885

Batch 191540, train_perplexity=18.656385, train_loss=2.9261885

Batch 191550, train_perplexity=18.656382, train_loss=2.9261882

Batch 191560, train_perplexity=18.656385, train_loss=2.9261885

Batch 191570, train_perplexity=18.656385, train_loss=2.9261885

Batch 191580, train_perplexity=18.656385, train_loss=2.9261885

Batch 191590, train_perplexity=18.656385, train_loss=2.9261885

Batch 191600, train_perplexity=18.656385, train_loss=2.9261885

Batch 191610, train_perplexity=18.656382, train_loss=2.9261882

Batch 191620, train_perplexity=18.656385, train_loss=2.9261885

Batch 191630, train_perplexity=18.656385, train_loss=2.9261885

Batch 191640, train_perplexity=18.656385, train_loss=2.9261885

Batch 191650, train_perplexity=18.656385, train_loss=2.9261885

Batch 191660, train_perplexity=18.656376, train_loss=2.926188

Batch 191670, train_perplexity=18.656385, train_loss=2.9261885

Batch 191680, train_perplexity=18.656385, train_loss=2.9261885

Batch 191690, train_perplexity=18.656385, train_loss=2.9261885

Batch 191700, train_perplexity=18.656376, train_loss=2.926188

Batch 191710, train_perplexity=18.656385, train_loss=2.9261885

Batch 191720, train_perplexity=18.656382, train_loss=2.9261882

Batch 191730, train_perplexity=18.656382, train_loss=2.9261882

Batch 191740, train_perplexity=18.656385, train_loss=2.9261885

Batch 191750, train_perplexity=18.656382, train_loss=2.9261882

Batch 191760, train_perplexity=18.656382, train_loss=2.9261882

Batch 191770, train_perplexity=18.656385, train_loss=2.9261885

Batch 191780, train_perplexity=18.656382, train_loss=2.9261882

Batch 191790, train_perplexity=18.656385, train_loss=2.9261885

Batch 191800, train_perplexity=18.656376, train_loss=2.926188

Batch 191810, train_perplexity=18.656376, train_loss=2.926188

Batch 191820, train_perplexity=18.656382, train_loss=2.9261882

Batch 191830, train_perplexity=18.656376, train_loss=2.926188

Batch 191840, train_perplexity=18.656376, train_loss=2.926188

Batch 191850, train_perplexity=18.656376, train_loss=2.926188

Batch 191860, train_perplexity=18.656376, train_loss=2.926188

Batch 191870, train_perplexity=18.656376, train_loss=2.926188

Batch 191880, train_perplexity=18.656376, train_loss=2.926188

Batch 191890, train_perplexity=18.656376, train_loss=2.926188

Batch 191900, train_perplexity=18.656382, train_loss=2.9261882

Batch 191910, train_perplexity=18.656376, train_loss=2.926188

Batch 191920, train_perplexity=18.656376, train_loss=2.926188

Batch 191930, train_perplexity=18.656376, train_loss=2.926188

Batch 191940, train_perplexity=18.656376, train_loss=2.926188

Batch 191950, train_perplexity=18.656376, train_loss=2.926188

Batch 191960, train_perplexity=18.656382, train_loss=2.9261882

Batch 191970, train_perplexity=18.656376, train_loss=2.926188

Batch 191980, train_perplexity=18.656376, train_loss=2.926188

Batch 191990, train_perplexity=18.656368, train_loss=2.9261875

Batch 192000, train_perplexity=18.656372, train_loss=2.9261878

Batch 192010, train_perplexity=18.656368, train_loss=2.9261875

Batch 192020, train_perplexity=18.656372, train_loss=2.9261878

Batch 192030, train_perplexity=18.656376, train_loss=2.926188

Batch 192040, train_perplexity=18.656368, train_loss=2.9261875

Batch 192050, train_perplexity=18.656376, train_loss=2.926188

Batch 192060, train_perplexity=18.656368, train_loss=2.9261875

Batch 192070, train_perplexity=18.656368, train_loss=2.9261875

Batch 192080, train_perplexity=18.656368, train_loss=2.9261875

Batch 192090, train_perplexity=18.656372, train_loss=2.9261878

Batch 192100, train_perplexity=18.656372, train_loss=2.9261878

Batch 192110, train_perplexity=18.656372, train_loss=2.9261878

Batch 192120, train_perplexity=18.656368, train_loss=2.9261875

Batch 192130, train_perplexity=18.656368, train_loss=2.9261875

Batch 192140, train_perplexity=18.656368, train_loss=2.9261875

Batch 192150, train_perplexity=18.656368, train_loss=2.9261875

Batch 192160, train_perplexity=18.656368, train_loss=2.9261875

Batch 192170, train_perplexity=18.656372, train_loss=2.9261878

Batch 192180, train_perplexity=18.656368, train_loss=2.9261875

Batch 192190, train_perplexity=18.656368, train_loss=2.9261875

Batch 192200, train_perplexity=18.656368, train_loss=2.9261875

Batch 192210, train_perplexity=18.656368, train_loss=2.9261875

Batch 192220, train_perplexity=18.656368, train_loss=2.9261875

Batch 192230, train_perplexity=18.656368, train_loss=2.9261875

Batch 192240, train_perplexity=18.656363, train_loss=2.9261873

Batch 192250, train_perplexity=18.656368, train_loss=2.9261875

Batch 192260, train_perplexity=18.656363, train_loss=2.9261873

Batch 192270, train_perplexity=18.656372, train_loss=2.9261878

Batch 192280, train_perplexity=18.656368, train_loss=2.9261875

Batch 192290, train_perplexity=18.656368, train_loss=2.9261875

Batch 192300, train_perplexity=18.656368, train_loss=2.9261875

Batch 192310, train_perplexity=18.656368, train_loss=2.9261875

Batch 192320, train_perplexity=18.656359, train_loss=2.926187

Batch 192330, train_perplexity=18.656368, train_loss=2.9261875

Batch 192340, train_perplexity=18.656368, train_loss=2.9261875

Batch 192350, train_perplexity=18.656363, train_loss=2.9261873

Batch 192360, train_perplexity=18.656368, train_loss=2.9261875

Batch 192370, train_perplexity=18.656359, train_loss=2.926187

Batch 192380, train_perplexity=18.656368, train_loss=2.9261875

Batch 192390, train_perplexity=18.656363, train_loss=2.9261873

Batch 192400, train_perplexity=18.656368, train_loss=2.9261875

Batch 192410, train_perplexity=18.656363, train_loss=2.9261873

Batch 192420, train_perplexity=18.656359, train_loss=2.926187

Batch 192430, train_perplexity=18.656368, train_loss=2.9261875

Batch 192440, train_perplexity=18.656359, train_loss=2.926187

Batch 192450, train_perplexity=18.656359, train_loss=2.926187

Batch 192460, train_perplexity=18.656368, train_loss=2.9261875

Batch 192470, train_perplexity=18.656368, train_loss=2.9261875

Batch 192480, train_perplexity=18.656359, train_loss=2.926187

Batch 192490, train_perplexity=18.656359, train_loss=2.926187

Batch 192500, train_perplexity=18.656359, train_loss=2.926187

Batch 192510, train_perplexity=18.656359, train_loss=2.926187

Batch 192520, train_perplexity=18.656359, train_loss=2.926187

Batch 192530, train_perplexity=18.656359, train_loss=2.926187

Batch 192540, train_perplexity=18.656359, train_loss=2.926187

Batch 192550, train_perplexity=18.656359, train_loss=2.926187

Batch 192560, train_perplexity=18.656355, train_loss=2.9261868

Batch 192570, train_perplexity=18.656359, train_loss=2.926187

Batch 192580, train_perplexity=18.656359, train_loss=2.926187
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 192590, train_perplexity=18.656359, train_loss=2.926187

Batch 192600, train_perplexity=18.656359, train_loss=2.926187

Batch 192610, train_perplexity=18.65635, train_loss=2.9261866

Batch 192620, train_perplexity=18.656355, train_loss=2.9261868

Batch 192630, train_perplexity=18.656359, train_loss=2.926187

Batch 192640, train_perplexity=18.656359, train_loss=2.926187

Batch 192650, train_perplexity=18.656355, train_loss=2.9261868

Batch 192660, train_perplexity=18.65635, train_loss=2.9261866

Batch 192670, train_perplexity=18.656359, train_loss=2.926187

Batch 192680, train_perplexity=18.656355, train_loss=2.9261868

Batch 192690, train_perplexity=18.65635, train_loss=2.9261866

Batch 192700, train_perplexity=18.65635, train_loss=2.9261866

Batch 192710, train_perplexity=18.656359, train_loss=2.926187

Batch 192720, train_perplexity=18.65635, train_loss=2.9261866

Batch 192730, train_perplexity=18.65635, train_loss=2.9261866

Batch 192740, train_perplexity=18.656355, train_loss=2.9261868

Batch 192750, train_perplexity=18.65635, train_loss=2.9261866

Batch 192760, train_perplexity=18.656355, train_loss=2.9261868

Batch 192770, train_perplexity=18.656355, train_loss=2.9261868

Batch 192780, train_perplexity=18.65635, train_loss=2.9261866

Batch 192790, train_perplexity=18.65635, train_loss=2.9261866

Batch 192800, train_perplexity=18.65635, train_loss=2.9261866

Batch 192810, train_perplexity=18.65635, train_loss=2.9261866

Batch 192820, train_perplexity=18.65635, train_loss=2.9261866

Batch 192830, train_perplexity=18.65635, train_loss=2.9261866

Batch 192840, train_perplexity=18.65635, train_loss=2.9261866

Batch 192850, train_perplexity=18.65635, train_loss=2.9261866

Batch 192860, train_perplexity=18.656345, train_loss=2.9261863

Batch 192870, train_perplexity=18.65635, train_loss=2.9261866

Batch 192880, train_perplexity=18.656345, train_loss=2.9261863

Batch 192890, train_perplexity=18.65635, train_loss=2.9261866

Batch 192900, train_perplexity=18.656345, train_loss=2.9261863

Batch 192910, train_perplexity=18.65635, train_loss=2.9261866

Batch 192920, train_perplexity=18.656345, train_loss=2.9261863

Batch 192930, train_perplexity=18.65635, train_loss=2.9261866

Batch 192940, train_perplexity=18.656345, train_loss=2.9261863

Batch 192950, train_perplexity=18.656345, train_loss=2.9261863

Batch 192960, train_perplexity=18.656342, train_loss=2.926186

Batch 192970, train_perplexity=18.656345, train_loss=2.9261863

Batch 192980, train_perplexity=18.65635, train_loss=2.9261866

Batch 192990, train_perplexity=18.65635, train_loss=2.9261866

Batch 193000, train_perplexity=18.656345, train_loss=2.9261863

Batch 193010, train_perplexity=18.65635, train_loss=2.9261866

Batch 193020, train_perplexity=18.65635, train_loss=2.9261866

Batch 193030, train_perplexity=18.65635, train_loss=2.9261866

Batch 193040, train_perplexity=18.656345, train_loss=2.9261863

Batch 193050, train_perplexity=18.656345, train_loss=2.9261863

Batch 193060, train_perplexity=18.656342, train_loss=2.926186

Batch 193070, train_perplexity=18.656342, train_loss=2.926186

Batch 193080, train_perplexity=18.65635, train_loss=2.9261866

Batch 193090, train_perplexity=18.656345, train_loss=2.9261863

Batch 193100, train_perplexity=18.656342, train_loss=2.926186

Batch 193110, train_perplexity=18.656342, train_loss=2.926186

Batch 193120, train_perplexity=18.656342, train_loss=2.926186

Batch 193130, train_perplexity=18.656342, train_loss=2.926186

Batch 193140, train_perplexity=18.656342, train_loss=2.926186

Batch 193150, train_perplexity=18.656342, train_loss=2.926186

Batch 193160, train_perplexity=18.65635, train_loss=2.9261866

Batch 193170, train_perplexity=18.656342, train_loss=2.926186

Batch 193180, train_perplexity=18.656342, train_loss=2.926186

Batch 193190, train_perplexity=18.656342, train_loss=2.926186

Batch 193200, train_perplexity=18.656342, train_loss=2.926186

Batch 193210, train_perplexity=18.656336, train_loss=2.9261858

Batch 193220, train_perplexity=18.656332, train_loss=2.9261856

Batch 193230, train_perplexity=18.656336, train_loss=2.9261858

Batch 193240, train_perplexity=18.656342, train_loss=2.926186

Batch 193250, train_perplexity=18.656332, train_loss=2.9261856

Batch 193260, train_perplexity=18.656332, train_loss=2.9261856

Batch 193270, train_perplexity=18.656332, train_loss=2.9261856

Batch 193280, train_perplexity=18.656332, train_loss=2.9261856

Batch 193290, train_perplexity=18.656336, train_loss=2.9261858

Batch 193300, train_perplexity=18.656332, train_loss=2.9261856

Batch 193310, train_perplexity=18.656336, train_loss=2.9261858

Batch 193320, train_perplexity=18.656332, train_loss=2.9261856

Batch 193330, train_perplexity=18.656336, train_loss=2.9261858

Batch 193340, train_perplexity=18.656332, train_loss=2.9261856

Batch 193350, train_perplexity=18.656336, train_loss=2.9261858

Batch 193360, train_perplexity=18.656336, train_loss=2.9261858

Batch 193370, train_perplexity=18.656332, train_loss=2.9261856

Batch 193380, train_perplexity=18.656332, train_loss=2.9261856

Batch 193390, train_perplexity=18.656332, train_loss=2.9261856

Batch 193400, train_perplexity=18.656332, train_loss=2.9261856

Batch 193410, train_perplexity=18.656332, train_loss=2.9261856

Batch 193420, train_perplexity=18.656332, train_loss=2.9261856

Batch 193430, train_perplexity=18.656332, train_loss=2.9261856

Batch 193440, train_perplexity=18.656332, train_loss=2.9261856

Batch 193450, train_perplexity=18.656332, train_loss=2.9261856

Batch 193460, train_perplexity=18.656332, train_loss=2.9261856

Batch 193470, train_perplexity=18.656328, train_loss=2.9261854

Batch 193480, train_perplexity=18.656336, train_loss=2.9261858

Batch 193490, train_perplexity=18.656328, train_loss=2.9261854

Batch 193500, train_perplexity=18.656332, train_loss=2.9261856

Batch 193510, train_perplexity=18.656332, train_loss=2.9261856

Batch 193520, train_perplexity=18.656332, train_loss=2.9261856

Batch 193530, train_perplexity=18.656332, train_loss=2.9261856

Batch 193540, train_perplexity=18.656332, train_loss=2.9261856

Batch 193550, train_perplexity=18.656332, train_loss=2.9261856

Batch 193560, train_perplexity=18.656328, train_loss=2.9261854

Batch 193570, train_perplexity=18.656332, train_loss=2.9261856

Batch 193580, train_perplexity=18.656332, train_loss=2.9261856

Batch 193590, train_perplexity=18.656332, train_loss=2.9261856

Batch 193600, train_perplexity=18.656328, train_loss=2.9261854

Batch 193610, train_perplexity=18.656332, train_loss=2.9261856

Batch 193620, train_perplexity=18.656328, train_loss=2.9261854

Batch 193630, train_perplexity=18.656328, train_loss=2.9261854

Batch 193640, train_perplexity=18.656328, train_loss=2.9261854

Batch 193650, train_perplexity=18.656328, train_loss=2.9261854

Batch 193660, train_perplexity=18.656322, train_loss=2.9261851

Batch 193670, train_perplexity=18.656322, train_loss=2.9261851

Batch 193680, train_perplexity=18.656322, train_loss=2.9261851

Batch 193690, train_perplexity=18.656328, train_loss=2.9261854

Batch 193700, train_perplexity=18.656322, train_loss=2.9261851

Batch 193710, train_perplexity=18.656322, train_loss=2.9261851

Batch 193720, train_perplexity=18.656322, train_loss=2.9261851

Batch 193730, train_perplexity=18.656322, train_loss=2.9261851

Batch 193740, train_perplexity=18.656322, train_loss=2.9261851

Batch 193750, train_perplexity=18.656322, train_loss=2.9261851

Batch 193760, train_perplexity=18.656322, train_loss=2.9261851

Batch 193770, train_perplexity=18.656319, train_loss=2.926185

Batch 193780, train_perplexity=18.656319, train_loss=2.926185

Batch 193790, train_perplexity=18.656322, train_loss=2.9261851

Batch 193800, train_perplexity=18.656322, train_loss=2.9261851

Batch 193810, train_perplexity=18.656322, train_loss=2.9261851

Batch 193820, train_perplexity=18.656322, train_loss=2.9261851

Batch 193830, train_perplexity=18.656322, train_loss=2.9261851

Batch 193840, train_perplexity=18.656322, train_loss=2.9261851

Batch 193850, train_perplexity=18.656319, train_loss=2.926185

Batch 193860, train_perplexity=18.656322, train_loss=2.9261851

Batch 193870, train_perplexity=18.656322, train_loss=2.9261851
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 193880, train_perplexity=18.656322, train_loss=2.9261851

Batch 193890, train_perplexity=18.656319, train_loss=2.926185

Batch 193900, train_perplexity=18.656319, train_loss=2.926185

Batch 193910, train_perplexity=18.656322, train_loss=2.9261851

Batch 193920, train_perplexity=18.656319, train_loss=2.926185

Batch 193930, train_perplexity=18.656315, train_loss=2.9261847

Batch 193940, train_perplexity=18.656315, train_loss=2.9261847

Batch 193950, train_perplexity=18.656315, train_loss=2.9261847

Batch 193960, train_perplexity=18.656315, train_loss=2.9261847

Batch 193970, train_perplexity=18.656319, train_loss=2.926185

Batch 193980, train_perplexity=18.656315, train_loss=2.9261847

Batch 193990, train_perplexity=18.656315, train_loss=2.9261847

Batch 194000, train_perplexity=18.656319, train_loss=2.926185

Batch 194010, train_perplexity=18.656319, train_loss=2.926185

Batch 194020, train_perplexity=18.656315, train_loss=2.9261847

Batch 194030, train_perplexity=18.656315, train_loss=2.9261847

Batch 194040, train_perplexity=18.656315, train_loss=2.9261847

Batch 194050, train_perplexity=18.656315, train_loss=2.9261847

Batch 194060, train_perplexity=18.656315, train_loss=2.9261847

Batch 194070, train_perplexity=18.65631, train_loss=2.9261844

Batch 194080, train_perplexity=18.656315, train_loss=2.9261847

Batch 194090, train_perplexity=18.656315, train_loss=2.9261847

Batch 194100, train_perplexity=18.65631, train_loss=2.9261844

Batch 194110, train_perplexity=18.656315, train_loss=2.9261847

Batch 194120, train_perplexity=18.65631, train_loss=2.9261844

Batch 194130, train_perplexity=18.65631, train_loss=2.9261844

Batch 194140, train_perplexity=18.656315, train_loss=2.9261847

Batch 194150, train_perplexity=18.656305, train_loss=2.9261842

Batch 194160, train_perplexity=18.656305, train_loss=2.9261842

Batch 194170, train_perplexity=18.656315, train_loss=2.9261847

Batch 194180, train_perplexity=18.65631, train_loss=2.9261844

Batch 194190, train_perplexity=18.65631, train_loss=2.9261844

Batch 194200, train_perplexity=18.656305, train_loss=2.9261842

Batch 194210, train_perplexity=18.656305, train_loss=2.9261842

Batch 194220, train_perplexity=18.656305, train_loss=2.9261842

Batch 194230, train_perplexity=18.656305, train_loss=2.9261842

Batch 194240, train_perplexity=18.65631, train_loss=2.9261844

Batch 194250, train_perplexity=18.656305, train_loss=2.9261842

Batch 194260, train_perplexity=18.656315, train_loss=2.9261847

Batch 194270, train_perplexity=18.656315, train_loss=2.9261847

Batch 194280, train_perplexity=18.65631, train_loss=2.9261844

Batch 194290, train_perplexity=18.65631, train_loss=2.9261844

Batch 194300, train_perplexity=18.656305, train_loss=2.9261842

Batch 194310, train_perplexity=18.656305, train_loss=2.9261842

Batch 194320, train_perplexity=18.656305, train_loss=2.9261842

Batch 194330, train_perplexity=18.656305, train_loss=2.9261842

Batch 194340, train_perplexity=18.656305, train_loss=2.9261842

Batch 194350, train_perplexity=18.656305, train_loss=2.9261842

Batch 194360, train_perplexity=18.656305, train_loss=2.9261842

Batch 194370, train_perplexity=18.656305, train_loss=2.9261842

Batch 194380, train_perplexity=18.656305, train_loss=2.9261842

Batch 194390, train_perplexity=18.656305, train_loss=2.9261842

Batch 194400, train_perplexity=18.656305, train_loss=2.9261842

Batch 194410, train_perplexity=18.656301, train_loss=2.926184

Batch 194420, train_perplexity=18.656305, train_loss=2.9261842

Batch 194430, train_perplexity=18.656305, train_loss=2.9261842

Batch 194440, train_perplexity=18.65631, train_loss=2.9261844

Batch 194450, train_perplexity=18.656296, train_loss=2.9261837

Batch 194460, train_perplexity=18.656305, train_loss=2.9261842

Batch 194470, train_perplexity=18.656305, train_loss=2.9261842

Batch 194480, train_perplexity=18.656301, train_loss=2.926184

Batch 194490, train_perplexity=18.656305, train_loss=2.9261842

Batch 194500, train_perplexity=18.656301, train_loss=2.926184

Batch 194510, train_perplexity=18.656305, train_loss=2.9261842

Batch 194520, train_perplexity=18.656301, train_loss=2.926184

Batch 194530, train_perplexity=18.656305, train_loss=2.9261842

Batch 194540, train_perplexity=18.656305, train_loss=2.9261842

Batch 194550, train_perplexity=18.656296, train_loss=2.9261837

Batch 194560, train_perplexity=18.656296, train_loss=2.9261837

Batch 194570, train_perplexity=18.656305, train_loss=2.9261842

Batch 194580, train_perplexity=18.656301, train_loss=2.926184

Batch 194590, train_perplexity=18.656296, train_loss=2.9261837

Batch 194600, train_perplexity=18.656296, train_loss=2.9261837

Batch 194610, train_perplexity=18.656301, train_loss=2.926184

Batch 194620, train_perplexity=18.656296, train_loss=2.9261837

Batch 194630, train_perplexity=18.656296, train_loss=2.9261837

Batch 194640, train_perplexity=18.656296, train_loss=2.9261837

Batch 194650, train_perplexity=18.656296, train_loss=2.9261837

Batch 194660, train_perplexity=18.656301, train_loss=2.926184

Batch 194670, train_perplexity=18.656296, train_loss=2.9261837

Batch 194680, train_perplexity=18.656296, train_loss=2.9261837

Batch 194690, train_perplexity=18.656296, train_loss=2.9261837

Batch 194700, train_perplexity=18.656296, train_loss=2.9261837

Batch 194710, train_perplexity=18.656296, train_loss=2.9261837

Batch 194720, train_perplexity=18.656292, train_loss=2.9261835

Batch 194730, train_perplexity=18.656296, train_loss=2.9261837

Batch 194740, train_perplexity=18.656296, train_loss=2.9261837

Batch 194750, train_perplexity=18.656296, train_loss=2.9261837

Batch 194760, train_perplexity=18.656296, train_loss=2.9261837

Batch 194770, train_perplexity=18.656296, train_loss=2.9261837

Batch 194780, train_perplexity=18.656292, train_loss=2.9261835

Batch 194790, train_perplexity=18.656292, train_loss=2.9261835

Batch 194800, train_perplexity=18.656292, train_loss=2.9261835

Batch 194810, train_perplexity=18.656292, train_loss=2.9261835

Batch 194820, train_perplexity=18.656292, train_loss=2.9261835

Batch 194830, train_perplexity=18.656292, train_loss=2.9261835

Batch 194840, train_perplexity=18.656292, train_loss=2.9261835

Batch 194850, train_perplexity=18.656292, train_loss=2.9261835

Batch 194860, train_perplexity=18.656292, train_loss=2.9261835

Batch 194870, train_perplexity=18.656288, train_loss=2.9261832

Batch 194880, train_perplexity=18.656288, train_loss=2.9261832

Batch 194890, train_perplexity=18.656292, train_loss=2.9261835

Batch 194900, train_perplexity=18.656292, train_loss=2.9261835

Batch 194910, train_perplexity=18.656292, train_loss=2.9261835

Batch 194920, train_perplexity=18.656288, train_loss=2.9261832

Batch 194930, train_perplexity=18.656288, train_loss=2.9261832

Batch 194940, train_perplexity=18.656288, train_loss=2.9261832

Batch 194950, train_perplexity=18.656296, train_loss=2.9261837

Batch 194960, train_perplexity=18.656296, train_loss=2.9261837

Batch 194970, train_perplexity=18.656288, train_loss=2.9261832

Batch 194980, train_perplexity=18.656288, train_loss=2.9261832

Batch 194990, train_perplexity=18.656288, train_loss=2.9261832

Batch 195000, train_perplexity=18.656282, train_loss=2.926183

Batch 195010, train_perplexity=18.656292, train_loss=2.9261835

Batch 195020, train_perplexity=18.656288, train_loss=2.9261832

Batch 195030, train_perplexity=18.656288, train_loss=2.9261832

Batch 195040, train_perplexity=18.656288, train_loss=2.9261832

Batch 195050, train_perplexity=18.656288, train_loss=2.9261832

Batch 195060, train_perplexity=18.656282, train_loss=2.926183

Batch 195070, train_perplexity=18.656282, train_loss=2.926183

Batch 195080, train_perplexity=18.656288, train_loss=2.9261832

Batch 195090, train_perplexity=18.656288, train_loss=2.9261832

Batch 195100, train_perplexity=18.656288, train_loss=2.9261832

Batch 195110, train_perplexity=18.656282, train_loss=2.926183

Batch 195120, train_perplexity=18.656282, train_loss=2.926183

Batch 195130, train_perplexity=18.656288, train_loss=2.9261832

Batch 195140, train_perplexity=18.656279, train_loss=2.9261827

Batch 195150, train_perplexity=18.656288, train_loss=2.9261832

Batch 195160, train_perplexity=18.656288, train_loss=2.9261832
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 195170, train_perplexity=18.656282, train_loss=2.926183

Batch 195180, train_perplexity=18.656279, train_loss=2.9261827

Batch 195190, train_perplexity=18.656282, train_loss=2.926183

Batch 195200, train_perplexity=18.656282, train_loss=2.926183

Batch 195210, train_perplexity=18.656282, train_loss=2.926183

Batch 195220, train_perplexity=18.656279, train_loss=2.9261827

Batch 195230, train_perplexity=18.656279, train_loss=2.9261827

Batch 195240, train_perplexity=18.656288, train_loss=2.9261832

Batch 195250, train_perplexity=18.656279, train_loss=2.9261827

Batch 195260, train_perplexity=18.656279, train_loss=2.9261827

Batch 195270, train_perplexity=18.656279, train_loss=2.9261827

Batch 195280, train_perplexity=18.656279, train_loss=2.9261827

Batch 195290, train_perplexity=18.656279, train_loss=2.9261827

Batch 195300, train_perplexity=18.65627, train_loss=2.9261823

Batch 195310, train_perplexity=18.656279, train_loss=2.9261827

Batch 195320, train_perplexity=18.656279, train_loss=2.9261827

Batch 195330, train_perplexity=18.656279, train_loss=2.9261827

Batch 195340, train_perplexity=18.656275, train_loss=2.9261825

Batch 195350, train_perplexity=18.656279, train_loss=2.9261827

Batch 195360, train_perplexity=18.656275, train_loss=2.9261825

Batch 195370, train_perplexity=18.65627, train_loss=2.9261823

Batch 195380, train_perplexity=18.656275, train_loss=2.9261825

Batch 195390, train_perplexity=18.656279, train_loss=2.9261827

Batch 195400, train_perplexity=18.656279, train_loss=2.9261827

Batch 195410, train_perplexity=18.656275, train_loss=2.9261825

Batch 195420, train_perplexity=18.656279, train_loss=2.9261827

Batch 195430, train_perplexity=18.656275, train_loss=2.9261825

Batch 195440, train_perplexity=18.65627, train_loss=2.9261823

Batch 195450, train_perplexity=18.65627, train_loss=2.9261823

Batch 195460, train_perplexity=18.65627, train_loss=2.9261823

Batch 195470, train_perplexity=18.656275, train_loss=2.9261825

Batch 195480, train_perplexity=18.65627, train_loss=2.9261823

Batch 195490, train_perplexity=18.65627, train_loss=2.9261823

Batch 195500, train_perplexity=18.656279, train_loss=2.9261827

Batch 195510, train_perplexity=18.65627, train_loss=2.9261823

Batch 195520, train_perplexity=18.65627, train_loss=2.9261823

Batch 195530, train_perplexity=18.656275, train_loss=2.9261825

Batch 195540, train_perplexity=18.65627, train_loss=2.9261823

Batch 195550, train_perplexity=18.65627, train_loss=2.9261823

Batch 195560, train_perplexity=18.65627, train_loss=2.9261823

Batch 195570, train_perplexity=18.65627, train_loss=2.9261823

Batch 195580, train_perplexity=18.656275, train_loss=2.9261825

Batch 195590, train_perplexity=18.656275, train_loss=2.9261825

Batch 195600, train_perplexity=18.65627, train_loss=2.9261823

Batch 195610, train_perplexity=18.656275, train_loss=2.9261825

Batch 195620, train_perplexity=18.65627, train_loss=2.9261823

Batch 195630, train_perplexity=18.65627, train_loss=2.9261823

Batch 195640, train_perplexity=18.656265, train_loss=2.926182

Batch 195650, train_perplexity=18.65627, train_loss=2.9261823

Batch 195660, train_perplexity=18.65627, train_loss=2.9261823

Batch 195670, train_perplexity=18.65627, train_loss=2.9261823

Batch 195680, train_perplexity=18.656265, train_loss=2.926182

Batch 195690, train_perplexity=18.65627, train_loss=2.9261823

Batch 195700, train_perplexity=18.656261, train_loss=2.9261818

Batch 195710, train_perplexity=18.656265, train_loss=2.926182

Batch 195720, train_perplexity=18.65627, train_loss=2.9261823

Batch 195730, train_perplexity=18.656261, train_loss=2.9261818

Batch 195740, train_perplexity=18.656265, train_loss=2.926182

Batch 195750, train_perplexity=18.656261, train_loss=2.9261818

Batch 195760, train_perplexity=18.656261, train_loss=2.9261818

Batch 195770, train_perplexity=18.65627, train_loss=2.9261823

Batch 195780, train_perplexity=18.656261, train_loss=2.9261818

Batch 195790, train_perplexity=18.656265, train_loss=2.926182

Batch 195800, train_perplexity=18.656261, train_loss=2.9261818

Batch 195810, train_perplexity=18.656261, train_loss=2.9261818

Batch 195820, train_perplexity=18.656261, train_loss=2.9261818

Batch 195830, train_perplexity=18.656265, train_loss=2.926182

Batch 195840, train_perplexity=18.656265, train_loss=2.926182

Batch 195850, train_perplexity=18.656261, train_loss=2.9261818

Batch 195860, train_perplexity=18.656265, train_loss=2.926182

Batch 195870, train_perplexity=18.656261, train_loss=2.9261818

Batch 195880, train_perplexity=18.656265, train_loss=2.926182

Batch 195890, train_perplexity=18.656265, train_loss=2.926182

Batch 195900, train_perplexity=18.656261, train_loss=2.9261818

Batch 195910, train_perplexity=18.656261, train_loss=2.9261818

Batch 195920, train_perplexity=18.656261, train_loss=2.9261818

Batch 195930, train_perplexity=18.656261, train_loss=2.9261818

Batch 195940, train_perplexity=18.656261, train_loss=2.9261818

Batch 195950, train_perplexity=18.656261, train_loss=2.9261818

Batch 195960, train_perplexity=18.656261, train_loss=2.9261818

Batch 195970, train_perplexity=18.656261, train_loss=2.9261818

Batch 195980, train_perplexity=18.656256, train_loss=2.9261816

Batch 195990, train_perplexity=18.656261, train_loss=2.9261818

Batch 196000, train_perplexity=18.656261, train_loss=2.9261818

Batch 196010, train_perplexity=18.656256, train_loss=2.9261816

Batch 196020, train_perplexity=18.656261, train_loss=2.9261818

Batch 196030, train_perplexity=18.656256, train_loss=2.9261816

Batch 196040, train_perplexity=18.656261, train_loss=2.9261818

Batch 196050, train_perplexity=18.656261, train_loss=2.9261818

Batch 196060, train_perplexity=18.656261, train_loss=2.9261818

Batch 196070, train_perplexity=18.656252, train_loss=2.9261813

Batch 196080, train_perplexity=18.656252, train_loss=2.9261813

Batch 196090, train_perplexity=18.656256, train_loss=2.9261816

Batch 196100, train_perplexity=18.656256, train_loss=2.9261816

Batch 196110, train_perplexity=18.656261, train_loss=2.9261818

Batch 196120, train_perplexity=18.656252, train_loss=2.9261813

Batch 196130, train_perplexity=18.656252, train_loss=2.9261813

Batch 196140, train_perplexity=18.656256, train_loss=2.9261816

Batch 196150, train_perplexity=18.656252, train_loss=2.9261813

Batch 196160, train_perplexity=18.656261, train_loss=2.9261818

Batch 196170, train_perplexity=18.656252, train_loss=2.9261813

Batch 196180, train_perplexity=18.656252, train_loss=2.9261813

Batch 196190, train_perplexity=18.656252, train_loss=2.9261813

Batch 196200, train_perplexity=18.656256, train_loss=2.9261816

Batch 196210, train_perplexity=18.656252, train_loss=2.9261813

Batch 196220, train_perplexity=18.656256, train_loss=2.9261816

Batch 196230, train_perplexity=18.656252, train_loss=2.9261813

Batch 196240, train_perplexity=18.656252, train_loss=2.9261813

Batch 196250, train_perplexity=18.656252, train_loss=2.9261813

Batch 196260, train_perplexity=18.656252, train_loss=2.9261813

Batch 196270, train_perplexity=18.656252, train_loss=2.9261813

Batch 196280, train_perplexity=18.656252, train_loss=2.9261813

Batch 196290, train_perplexity=18.656252, train_loss=2.9261813

Batch 196300, train_perplexity=18.656252, train_loss=2.9261813

Batch 196310, train_perplexity=18.656252, train_loss=2.9261813

Batch 196320, train_perplexity=18.656252, train_loss=2.9261813

Batch 196330, train_perplexity=18.656252, train_loss=2.9261813

Batch 196340, train_perplexity=18.656252, train_loss=2.9261813

Batch 196350, train_perplexity=18.656248, train_loss=2.926181

Batch 196360, train_perplexity=18.656252, train_loss=2.9261813

Batch 196370, train_perplexity=18.656252, train_loss=2.9261813

Batch 196380, train_perplexity=18.656242, train_loss=2.9261808

Batch 196390, train_perplexity=18.656242, train_loss=2.9261808

Batch 196400, train_perplexity=18.656242, train_loss=2.9261808

Batch 196410, train_perplexity=18.656248, train_loss=2.926181

Batch 196420, train_perplexity=18.656242, train_loss=2.9261808

Batch 196430, train_perplexity=18.656248, train_loss=2.926181

Batch 196440, train_perplexity=18.656242, train_loss=2.9261808

Batch 196450, train_perplexity=18.656242, train_loss=2.9261808
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 196460, train_perplexity=18.656242, train_loss=2.9261808

Batch 196470, train_perplexity=18.656242, train_loss=2.9261808

Batch 196480, train_perplexity=18.656242, train_loss=2.9261808

Batch 196490, train_perplexity=18.656242, train_loss=2.9261808

Batch 196500, train_perplexity=18.656242, train_loss=2.9261808

Batch 196510, train_perplexity=18.656242, train_loss=2.9261808

Batch 196520, train_perplexity=18.656242, train_loss=2.9261808

Batch 196530, train_perplexity=18.656242, train_loss=2.9261808

Batch 196540, train_perplexity=18.656242, train_loss=2.9261808

Batch 196550, train_perplexity=18.656242, train_loss=2.9261808

Batch 196560, train_perplexity=18.656242, train_loss=2.9261808

Batch 196570, train_perplexity=18.656239, train_loss=2.9261806

Batch 196580, train_perplexity=18.656239, train_loss=2.9261806

Batch 196590, train_perplexity=18.656242, train_loss=2.9261808

Batch 196600, train_perplexity=18.656242, train_loss=2.9261808

Batch 196610, train_perplexity=18.656242, train_loss=2.9261808

Batch 196620, train_perplexity=18.656242, train_loss=2.9261808

Batch 196630, train_perplexity=18.656242, train_loss=2.9261808

Batch 196640, train_perplexity=18.656242, train_loss=2.9261808

Batch 196650, train_perplexity=18.656242, train_loss=2.9261808

Batch 196660, train_perplexity=18.656242, train_loss=2.9261808

Batch 196670, train_perplexity=18.656239, train_loss=2.9261806

Batch 196680, train_perplexity=18.656239, train_loss=2.9261806

Batch 196690, train_perplexity=18.656239, train_loss=2.9261806

Batch 196700, train_perplexity=18.656239, train_loss=2.9261806

Batch 196710, train_perplexity=18.656235, train_loss=2.9261804

Batch 196720, train_perplexity=18.656235, train_loss=2.9261804

Batch 196730, train_perplexity=18.656235, train_loss=2.9261804

Batch 196740, train_perplexity=18.656235, train_loss=2.9261804

Batch 196750, train_perplexity=18.656239, train_loss=2.9261806

Batch 196760, train_perplexity=18.656235, train_loss=2.9261804

Batch 196770, train_perplexity=18.656235, train_loss=2.9261804

Batch 196780, train_perplexity=18.656235, train_loss=2.9261804

Batch 196790, train_perplexity=18.656235, train_loss=2.9261804

Batch 196800, train_perplexity=18.656239, train_loss=2.9261806

Batch 196810, train_perplexity=18.656235, train_loss=2.9261804

Batch 196820, train_perplexity=18.656235, train_loss=2.9261804

Batch 196830, train_perplexity=18.656235, train_loss=2.9261804

Batch 196840, train_perplexity=18.656235, train_loss=2.9261804

Batch 196850, train_perplexity=18.656235, train_loss=2.9261804

Batch 196860, train_perplexity=18.656235, train_loss=2.9261804

Batch 196870, train_perplexity=18.656235, train_loss=2.9261804

Batch 196880, train_perplexity=18.656235, train_loss=2.9261804

Batch 196890, train_perplexity=18.656235, train_loss=2.9261804

Batch 196900, train_perplexity=18.656235, train_loss=2.9261804

Batch 196910, train_perplexity=18.656235, train_loss=2.9261804

Batch 196920, train_perplexity=18.656235, train_loss=2.9261804

Batch 196930, train_perplexity=18.656235, train_loss=2.9261804

Batch 196940, train_perplexity=18.656229, train_loss=2.9261801

Batch 196950, train_perplexity=18.656235, train_loss=2.9261804

Batch 196960, train_perplexity=18.656229, train_loss=2.9261801

Batch 196970, train_perplexity=18.656235, train_loss=2.9261804

Batch 196980, train_perplexity=18.656229, train_loss=2.9261801

Batch 196990, train_perplexity=18.656225, train_loss=2.92618

Batch 197000, train_perplexity=18.656225, train_loss=2.92618

Batch 197010, train_perplexity=18.656225, train_loss=2.92618

Batch 197020, train_perplexity=18.656225, train_loss=2.92618

Batch 197030, train_perplexity=18.656229, train_loss=2.9261801

Batch 197040, train_perplexity=18.656225, train_loss=2.92618

Batch 197050, train_perplexity=18.656235, train_loss=2.9261804

Batch 197060, train_perplexity=18.656235, train_loss=2.9261804

Batch 197070, train_perplexity=18.656225, train_loss=2.92618

Batch 197080, train_perplexity=18.656225, train_loss=2.92618

Batch 197090, train_perplexity=18.656235, train_loss=2.9261804

Batch 197100, train_perplexity=18.656225, train_loss=2.92618

Batch 197110, train_perplexity=18.656225, train_loss=2.92618

Batch 197120, train_perplexity=18.656225, train_loss=2.92618

Batch 197130, train_perplexity=18.656225, train_loss=2.92618

Batch 197140, train_perplexity=18.656225, train_loss=2.92618

Batch 197150, train_perplexity=18.656225, train_loss=2.92618

Batch 197160, train_perplexity=18.656225, train_loss=2.92618

Batch 197170, train_perplexity=18.656225, train_loss=2.92618

Batch 197180, train_perplexity=18.656221, train_loss=2.9261796

Batch 197190, train_perplexity=18.656225, train_loss=2.92618

Batch 197200, train_perplexity=18.656225, train_loss=2.92618

Batch 197210, train_perplexity=18.656225, train_loss=2.92618

Batch 197220, train_perplexity=18.656221, train_loss=2.9261796

Batch 197230, train_perplexity=18.656229, train_loss=2.9261801

Batch 197240, train_perplexity=18.656221, train_loss=2.9261796

Batch 197250, train_perplexity=18.656221, train_loss=2.9261796

Batch 197260, train_perplexity=18.656225, train_loss=2.92618

Batch 197270, train_perplexity=18.656225, train_loss=2.92618

Batch 197280, train_perplexity=18.656216, train_loss=2.9261794

Batch 197290, train_perplexity=18.656221, train_loss=2.9261796

Batch 197300, train_perplexity=18.656225, train_loss=2.92618

Batch 197310, train_perplexity=18.656216, train_loss=2.9261794

Batch 197320, train_perplexity=18.656216, train_loss=2.9261794

Batch 197330, train_perplexity=18.656221, train_loss=2.9261796

Batch 197340, train_perplexity=18.656221, train_loss=2.9261796

Batch 197350, train_perplexity=18.656221, train_loss=2.9261796

Batch 197360, train_perplexity=18.656225, train_loss=2.92618

Batch 197370, train_perplexity=18.656216, train_loss=2.9261794

Batch 197380, train_perplexity=18.656221, train_loss=2.9261796

Batch 197390, train_perplexity=18.656221, train_loss=2.9261796

Batch 197400, train_perplexity=18.656216, train_loss=2.9261794

Batch 197410, train_perplexity=18.656216, train_loss=2.9261794

Batch 197420, train_perplexity=18.656216, train_loss=2.9261794

Batch 197430, train_perplexity=18.656221, train_loss=2.9261796

Batch 197440, train_perplexity=18.656216, train_loss=2.9261794

Batch 197450, train_perplexity=18.656216, train_loss=2.9261794

Batch 197460, train_perplexity=18.656216, train_loss=2.9261794

Batch 197470, train_perplexity=18.656216, train_loss=2.9261794

Batch 197480, train_perplexity=18.656216, train_loss=2.9261794

Batch 197490, train_perplexity=18.656216, train_loss=2.9261794

Batch 197500, train_perplexity=18.656216, train_loss=2.9261794

Batch 197510, train_perplexity=18.656216, train_loss=2.9261794

Batch 197520, train_perplexity=18.656216, train_loss=2.9261794

Batch 197530, train_perplexity=18.656221, train_loss=2.9261796

Batch 197540, train_perplexity=18.656216, train_loss=2.9261794

Batch 197550, train_perplexity=18.656216, train_loss=2.9261794

Batch 197560, train_perplexity=18.656212, train_loss=2.9261792

Batch 197570, train_perplexity=18.656212, train_loss=2.9261792

Batch 197580, train_perplexity=18.656216, train_loss=2.9261794

Batch 197590, train_perplexity=18.656216, train_loss=2.9261794

Batch 197600, train_perplexity=18.656216, train_loss=2.9261794

Batch 197610, train_perplexity=18.656216, train_loss=2.9261794

Batch 197620, train_perplexity=18.656212, train_loss=2.9261792

Batch 197630, train_perplexity=18.656216, train_loss=2.9261794

Batch 197640, train_perplexity=18.656208, train_loss=2.926179

Batch 197650, train_perplexity=18.656212, train_loss=2.9261792

Batch 197660, train_perplexity=18.656208, train_loss=2.926179

Batch 197670, train_perplexity=18.656208, train_loss=2.926179

Batch 197680, train_perplexity=18.656208, train_loss=2.926179

Batch 197690, train_perplexity=18.656208, train_loss=2.926179

Batch 197700, train_perplexity=18.656208, train_loss=2.926179

Batch 197710, train_perplexity=18.656208, train_loss=2.926179

Batch 197720, train_perplexity=18.656212, train_loss=2.9261792

Batch 197730, train_perplexity=18.656208, train_loss=2.926179

Batch 197740, train_perplexity=18.656208, train_loss=2.926179
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 197750, train_perplexity=18.656208, train_loss=2.926179

Batch 197760, train_perplexity=18.656208, train_loss=2.926179

Batch 197770, train_perplexity=18.656208, train_loss=2.926179

Batch 197780, train_perplexity=18.656202, train_loss=2.9261787

Batch 197790, train_perplexity=18.656208, train_loss=2.926179

Batch 197800, train_perplexity=18.656208, train_loss=2.926179

Batch 197810, train_perplexity=18.656208, train_loss=2.926179

Batch 197820, train_perplexity=18.656208, train_loss=2.926179

Batch 197830, train_perplexity=18.656208, train_loss=2.926179

Batch 197840, train_perplexity=18.656208, train_loss=2.926179

Batch 197850, train_perplexity=18.656202, train_loss=2.9261787

Batch 197860, train_perplexity=18.656208, train_loss=2.926179

Batch 197870, train_perplexity=18.656208, train_loss=2.926179

Batch 197880, train_perplexity=18.656199, train_loss=2.9261785

Batch 197890, train_perplexity=18.656202, train_loss=2.9261787

Batch 197900, train_perplexity=18.656199, train_loss=2.9261785

Batch 197910, train_perplexity=18.656202, train_loss=2.9261787

Batch 197920, train_perplexity=18.656208, train_loss=2.926179

Batch 197930, train_perplexity=18.656202, train_loss=2.9261787

Batch 197940, train_perplexity=18.656202, train_loss=2.9261787

Batch 197950, train_perplexity=18.656202, train_loss=2.9261787

Batch 197960, train_perplexity=18.656202, train_loss=2.9261787

Batch 197970, train_perplexity=18.656199, train_loss=2.9261785

Batch 197980, train_perplexity=18.656199, train_loss=2.9261785

Batch 197990, train_perplexity=18.656199, train_loss=2.9261785

Batch 198000, train_perplexity=18.656202, train_loss=2.9261787

Batch 198010, train_perplexity=18.656199, train_loss=2.9261785

Batch 198020, train_perplexity=18.656199, train_loss=2.9261785

Batch 198030, train_perplexity=18.656199, train_loss=2.9261785

Batch 198040, train_perplexity=18.656199, train_loss=2.9261785

Batch 198050, train_perplexity=18.656199, train_loss=2.9261785

Batch 198060, train_perplexity=18.656199, train_loss=2.9261785

Batch 198070, train_perplexity=18.656199, train_loss=2.9261785

Batch 198080, train_perplexity=18.656199, train_loss=2.9261785

Batch 198090, train_perplexity=18.656202, train_loss=2.9261787

Batch 198100, train_perplexity=18.656199, train_loss=2.9261785

Batch 198110, train_perplexity=18.656199, train_loss=2.9261785

Batch 198120, train_perplexity=18.656202, train_loss=2.9261787

Batch 198130, train_perplexity=18.656199, train_loss=2.9261785

Batch 198140, train_perplexity=18.656199, train_loss=2.9261785

Batch 198150, train_perplexity=18.656199, train_loss=2.9261785

Batch 198160, train_perplexity=18.656199, train_loss=2.9261785

Batch 198170, train_perplexity=18.656199, train_loss=2.9261785

Batch 198180, train_perplexity=18.656189, train_loss=2.926178

Batch 198190, train_perplexity=18.656199, train_loss=2.9261785

Batch 198200, train_perplexity=18.656189, train_loss=2.926178

Batch 198210, train_perplexity=18.656195, train_loss=2.9261782

Batch 198220, train_perplexity=18.656199, train_loss=2.9261785

Batch 198230, train_perplexity=18.656199, train_loss=2.9261785

Batch 198240, train_perplexity=18.656199, train_loss=2.9261785

Batch 198250, train_perplexity=18.656199, train_loss=2.9261785

Batch 198260, train_perplexity=18.656199, train_loss=2.9261785

Batch 198270, train_perplexity=18.656199, train_loss=2.9261785

Batch 198280, train_perplexity=18.656189, train_loss=2.926178

Batch 198290, train_perplexity=18.656189, train_loss=2.926178

Batch 198300, train_perplexity=18.656189, train_loss=2.926178

Batch 198310, train_perplexity=18.656195, train_loss=2.9261782

Batch 198320, train_perplexity=18.656189, train_loss=2.926178

Batch 198330, train_perplexity=18.656189, train_loss=2.926178

Batch 198340, train_perplexity=18.656189, train_loss=2.926178

Batch 198350, train_perplexity=18.656189, train_loss=2.926178

Batch 198360, train_perplexity=18.656189, train_loss=2.926178

Batch 198370, train_perplexity=18.656189, train_loss=2.926178

Batch 198380, train_perplexity=18.656189, train_loss=2.926178

Batch 198390, train_perplexity=18.656189, train_loss=2.926178

Batch 198400, train_perplexity=18.656189, train_loss=2.926178

Batch 198410, train_perplexity=18.656189, train_loss=2.926178

Batch 198420, train_perplexity=18.656189, train_loss=2.926178

Batch 198430, train_perplexity=18.656189, train_loss=2.926178

Batch 198440, train_perplexity=18.656189, train_loss=2.926178

Batch 198450, train_perplexity=18.656189, train_loss=2.926178

Batch 198460, train_perplexity=18.656189, train_loss=2.926178

Batch 198470, train_perplexity=18.656189, train_loss=2.926178

Batch 198480, train_perplexity=18.656185, train_loss=2.9261777

Batch 198490, train_perplexity=18.656185, train_loss=2.9261777

Batch 198500, train_perplexity=18.656189, train_loss=2.926178

Batch 198510, train_perplexity=18.656195, train_loss=2.9261782

Batch 198520, train_perplexity=18.656185, train_loss=2.9261777

Batch 198530, train_perplexity=18.656181, train_loss=2.9261775

Batch 198540, train_perplexity=18.656181, train_loss=2.9261775

Batch 198550, train_perplexity=18.656181, train_loss=2.9261775

Batch 198560, train_perplexity=18.656189, train_loss=2.926178

Batch 198570, train_perplexity=18.656189, train_loss=2.926178

Batch 198580, train_perplexity=18.656185, train_loss=2.9261777

Batch 198590, train_perplexity=18.656185, train_loss=2.9261777

Batch 198600, train_perplexity=18.656181, train_loss=2.9261775

Batch 198610, train_perplexity=18.656181, train_loss=2.9261775

Batch 198620, train_perplexity=18.656181, train_loss=2.9261775

Batch 198630, train_perplexity=18.656181, train_loss=2.9261775

Batch 198640, train_perplexity=18.656181, train_loss=2.9261775

Batch 198650, train_perplexity=18.656181, train_loss=2.9261775

Batch 198660, train_perplexity=18.656185, train_loss=2.9261777

Batch 198670, train_perplexity=18.656185, train_loss=2.9261777

Batch 198680, train_perplexity=18.656181, train_loss=2.9261775

Batch 198690, train_perplexity=18.656181, train_loss=2.9261775

Batch 198700, train_perplexity=18.656181, train_loss=2.9261775

Batch 198710, train_perplexity=18.656181, train_loss=2.9261775

Batch 198720, train_perplexity=18.656176, train_loss=2.9261773

Batch 198730, train_perplexity=18.656181, train_loss=2.9261775

Batch 198740, train_perplexity=18.656176, train_loss=2.9261773

Batch 198750, train_perplexity=18.656189, train_loss=2.926178

Batch 198760, train_perplexity=18.656181, train_loss=2.9261775

Batch 198770, train_perplexity=18.656181, train_loss=2.9261775

Batch 198780, train_perplexity=18.656176, train_loss=2.9261773

Batch 198790, train_perplexity=18.656181, train_loss=2.9261775

Batch 198800, train_perplexity=18.656176, train_loss=2.9261773

Batch 198810, train_perplexity=18.656176, train_loss=2.9261773

Batch 198820, train_perplexity=18.656181, train_loss=2.9261775

Batch 198830, train_perplexity=18.656181, train_loss=2.9261775

Batch 198840, train_perplexity=18.656172, train_loss=2.926177

Batch 198850, train_perplexity=18.656181, train_loss=2.9261775

Batch 198860, train_perplexity=18.656181, train_loss=2.9261775

Batch 198870, train_perplexity=18.656181, train_loss=2.9261775

Batch 198880, train_perplexity=18.656172, train_loss=2.926177

Batch 198890, train_perplexity=18.656176, train_loss=2.9261773

Batch 198900, train_perplexity=18.656181, train_loss=2.9261775

Batch 198910, train_perplexity=18.656172, train_loss=2.926177

Batch 198920, train_perplexity=18.656172, train_loss=2.926177

Batch 198930, train_perplexity=18.656172, train_loss=2.926177

Batch 198940, train_perplexity=18.656172, train_loss=2.926177

Batch 198950, train_perplexity=18.656172, train_loss=2.926177

Batch 198960, train_perplexity=18.656176, train_loss=2.9261773

Batch 198970, train_perplexity=18.656172, train_loss=2.926177

Batch 198980, train_perplexity=18.656172, train_loss=2.926177

Batch 198990, train_perplexity=18.656176, train_loss=2.9261773

Batch 199000, train_perplexity=18.656172, train_loss=2.926177

Batch 199010, train_perplexity=18.656172, train_loss=2.926177

Batch 199020, train_perplexity=18.656172, train_loss=2.926177

Batch 199030, train_perplexity=18.656172, train_loss=2.926177
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 199040, train_perplexity=18.656172, train_loss=2.926177

Batch 199050, train_perplexity=18.656168, train_loss=2.9261768

Batch 199060, train_perplexity=18.656172, train_loss=2.926177

Batch 199070, train_perplexity=18.656168, train_loss=2.9261768

Batch 199080, train_perplexity=18.656172, train_loss=2.926177

Batch 199090, train_perplexity=18.656172, train_loss=2.926177

Batch 199100, train_perplexity=18.656172, train_loss=2.926177

Batch 199110, train_perplexity=18.656172, train_loss=2.926177

Batch 199120, train_perplexity=18.656162, train_loss=2.9261765

Batch 199130, train_perplexity=18.656168, train_loss=2.9261768

Batch 199140, train_perplexity=18.656168, train_loss=2.9261768

Batch 199150, train_perplexity=18.656172, train_loss=2.926177

Batch 199160, train_perplexity=18.656162, train_loss=2.9261765

Batch 199170, train_perplexity=18.656162, train_loss=2.9261765

Batch 199180, train_perplexity=18.656162, train_loss=2.9261765

Batch 199190, train_perplexity=18.656162, train_loss=2.9261765

Batch 199200, train_perplexity=18.656162, train_loss=2.9261765

Batch 199210, train_perplexity=18.656162, train_loss=2.9261765

Batch 199220, train_perplexity=18.656168, train_loss=2.9261768

Batch 199230, train_perplexity=18.656168, train_loss=2.9261768

Batch 199240, train_perplexity=18.656162, train_loss=2.9261765

Batch 199250, train_perplexity=18.656162, train_loss=2.9261765

Batch 199260, train_perplexity=18.656162, train_loss=2.9261765

Batch 199270, train_perplexity=18.656162, train_loss=2.9261765

Batch 199280, train_perplexity=18.656162, train_loss=2.9261765

Batch 199290, train_perplexity=18.656172, train_loss=2.926177

Batch 199300, train_perplexity=18.656162, train_loss=2.9261765

Batch 199310, train_perplexity=18.656162, train_loss=2.9261765

Batch 199320, train_perplexity=18.656162, train_loss=2.9261765

Batch 199330, train_perplexity=18.656162, train_loss=2.9261765

Batch 199340, train_perplexity=18.656162, train_loss=2.9261765

Batch 199350, train_perplexity=18.656162, train_loss=2.9261765

Batch 199360, train_perplexity=18.656162, train_loss=2.9261765

Batch 199370, train_perplexity=18.656162, train_loss=2.9261765

Batch 199380, train_perplexity=18.656158, train_loss=2.9261763

Batch 199390, train_perplexity=18.656162, train_loss=2.9261765

Batch 199400, train_perplexity=18.656162, train_loss=2.9261765

Batch 199410, train_perplexity=18.656162, train_loss=2.9261765

Batch 199420, train_perplexity=18.656158, train_loss=2.9261763

Batch 199430, train_perplexity=18.656162, train_loss=2.9261765

Batch 199440, train_perplexity=18.656158, train_loss=2.9261763

Batch 199450, train_perplexity=18.656162, train_loss=2.9261765

Batch 199460, train_perplexity=18.656158, train_loss=2.9261763

Batch 199470, train_perplexity=18.656155, train_loss=2.926176

Batch 199480, train_perplexity=18.656155, train_loss=2.926176

Batch 199490, train_perplexity=18.656162, train_loss=2.9261765

Batch 199500, train_perplexity=18.656162, train_loss=2.9261765

Batch 199510, train_perplexity=18.656155, train_loss=2.926176

Batch 199520, train_perplexity=18.656158, train_loss=2.9261763

Batch 199530, train_perplexity=18.656155, train_loss=2.926176

Batch 199540, train_perplexity=18.656162, train_loss=2.9261765

Batch 199550, train_perplexity=18.656155, train_loss=2.926176

Batch 199560, train_perplexity=18.656155, train_loss=2.926176

Batch 199570, train_perplexity=18.656155, train_loss=2.926176

Batch 199580, train_perplexity=18.656155, train_loss=2.926176

Batch 199590, train_perplexity=18.656155, train_loss=2.926176

Batch 199600, train_perplexity=18.656158, train_loss=2.9261763

Batch 199610, train_perplexity=18.656155, train_loss=2.926176

Batch 199620, train_perplexity=18.656155, train_loss=2.926176

Batch 199630, train_perplexity=18.656155, train_loss=2.926176

Batch 199640, train_perplexity=18.656155, train_loss=2.926176

Batch 199650, train_perplexity=18.656155, train_loss=2.926176

Batch 199660, train_perplexity=18.656155, train_loss=2.926176

Batch 199670, train_perplexity=18.656155, train_loss=2.926176

Batch 199680, train_perplexity=18.656155, train_loss=2.926176

Batch 199690, train_perplexity=18.656155, train_loss=2.926176

Batch 199700, train_perplexity=18.656149, train_loss=2.9261758

Batch 199710, train_perplexity=18.656155, train_loss=2.926176

Batch 199720, train_perplexity=18.656149, train_loss=2.9261758

Batch 199730, train_perplexity=18.656149, train_loss=2.9261758

Batch 199740, train_perplexity=18.656155, train_loss=2.926176

Batch 199750, train_perplexity=18.656149, train_loss=2.9261758

Batch 199760, train_perplexity=18.656149, train_loss=2.9261758

Batch 199770, train_perplexity=18.656149, train_loss=2.9261758

Batch 199780, train_perplexity=18.656149, train_loss=2.9261758

Batch 199790, train_perplexity=18.656149, train_loss=2.9261758

Batch 199800, train_perplexity=18.656149, train_loss=2.9261758

Batch 199810, train_perplexity=18.656149, train_loss=2.9261758

Batch 199820, train_perplexity=18.656145, train_loss=2.9261756

Batch 199830, train_perplexity=18.656155, train_loss=2.926176

Batch 199840, train_perplexity=18.656149, train_loss=2.9261758

Batch 199850, train_perplexity=18.656145, train_loss=2.9261756

Batch 199860, train_perplexity=18.656149, train_loss=2.9261758

Batch 199870, train_perplexity=18.656145, train_loss=2.9261756

Batch 199880, train_perplexity=18.656149, train_loss=2.9261758

Batch 199890, train_perplexity=18.656145, train_loss=2.9261756

Batch 199900, train_perplexity=18.656145, train_loss=2.9261756

Batch 199910, train_perplexity=18.656145, train_loss=2.9261756

Batch 199920, train_perplexity=18.656145, train_loss=2.9261756

Batch 199930, train_perplexity=18.656145, train_loss=2.9261756

Batch 199940, train_perplexity=18.656145, train_loss=2.9261756

Batch 199950, train_perplexity=18.656145, train_loss=2.9261756

Batch 199960, train_perplexity=18.656145, train_loss=2.9261756

Batch 199970, train_perplexity=18.656145, train_loss=2.9261756

Batch 199980, train_perplexity=18.656145, train_loss=2.9261756

Batch 199990, train_perplexity=18.656145, train_loss=2.9261756

Batch 200000, train_perplexity=18.656145, train_loss=2.9261756

Batch 200010, train_perplexity=18.656141, train_loss=2.9261754

Batch 200020, train_perplexity=18.656141, train_loss=2.9261754

Batch 200030, train_perplexity=18.656145, train_loss=2.9261756

Batch 200040, train_perplexity=18.656136, train_loss=2.926175

Batch 200050, train_perplexity=18.656145, train_loss=2.9261756

Batch 200060, train_perplexity=18.656145, train_loss=2.9261756

Batch 200070, train_perplexity=18.656141, train_loss=2.9261754

Batch 200080, train_perplexity=18.656141, train_loss=2.9261754

Batch 200090, train_perplexity=18.656141, train_loss=2.9261754

Batch 200100, train_perplexity=18.656141, train_loss=2.9261754

Batch 200110, train_perplexity=18.656145, train_loss=2.9261756

Batch 200120, train_perplexity=18.656136, train_loss=2.926175

Batch 200130, train_perplexity=18.656136, train_loss=2.926175

Batch 200140, train_perplexity=18.656136, train_loss=2.926175

Batch 200150, train_perplexity=18.656145, train_loss=2.9261756

Batch 200160, train_perplexity=18.656136, train_loss=2.926175

Batch 200170, train_perplexity=18.656136, train_loss=2.926175

Batch 200180, train_perplexity=18.656145, train_loss=2.9261756

Batch 200190, train_perplexity=18.656136, train_loss=2.926175

Batch 200200, train_perplexity=18.656136, train_loss=2.926175

Batch 200210, train_perplexity=18.656136, train_loss=2.926175

Batch 200220, train_perplexity=18.656136, train_loss=2.926175

Batch 200230, train_perplexity=18.656136, train_loss=2.926175

Batch 200240, train_perplexity=18.656136, train_loss=2.926175

Batch 200250, train_perplexity=18.656132, train_loss=2.9261749

Batch 200260, train_perplexity=18.656136, train_loss=2.926175

Batch 200270, train_perplexity=18.656136, train_loss=2.926175

Batch 200280, train_perplexity=18.656136, train_loss=2.926175

Batch 200290, train_perplexity=18.656136, train_loss=2.926175

Batch 200300, train_perplexity=18.656136, train_loss=2.926175

Batch 200310, train_perplexity=18.656136, train_loss=2.926175

Batch 200320, train_perplexity=18.656136, train_loss=2.926175
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 200330, train_perplexity=18.656136, train_loss=2.926175

Batch 200340, train_perplexity=18.656136, train_loss=2.926175

Batch 200350, train_perplexity=18.656136, train_loss=2.926175

Batch 200360, train_perplexity=18.656136, train_loss=2.926175

Batch 200370, train_perplexity=18.656132, train_loss=2.9261749

Batch 200380, train_perplexity=18.656136, train_loss=2.926175

Batch 200390, train_perplexity=18.656136, train_loss=2.926175

Batch 200400, train_perplexity=18.656132, train_loss=2.9261749

Batch 200410, train_perplexity=18.656132, train_loss=2.9261749

Batch 200420, train_perplexity=18.656132, train_loss=2.9261749

Batch 200430, train_perplexity=18.656128, train_loss=2.9261746

Batch 200440, train_perplexity=18.656136, train_loss=2.926175

Batch 200450, train_perplexity=18.656136, train_loss=2.926175

Batch 200460, train_perplexity=18.656132, train_loss=2.9261749

Batch 200470, train_perplexity=18.656136, train_loss=2.926175

Batch 200480, train_perplexity=18.656132, train_loss=2.9261749

Batch 200490, train_perplexity=18.656136, train_loss=2.926175

Batch 200500, train_perplexity=18.656128, train_loss=2.9261746

Batch 200510, train_perplexity=18.656132, train_loss=2.9261749

Batch 200520, train_perplexity=18.656128, train_loss=2.9261746

Batch 200530, train_perplexity=18.656128, train_loss=2.9261746

Batch 200540, train_perplexity=18.656128, train_loss=2.9261746

Batch 200550, train_perplexity=18.656132, train_loss=2.9261749

Batch 200560, train_perplexity=18.656132, train_loss=2.9261749

Batch 200570, train_perplexity=18.656128, train_loss=2.9261746

Batch 200580, train_perplexity=18.656128, train_loss=2.9261746

Batch 200590, train_perplexity=18.656128, train_loss=2.9261746

Batch 200600, train_perplexity=18.656128, train_loss=2.9261746

Batch 200610, train_perplexity=18.656128, train_loss=2.9261746

Batch 200620, train_perplexity=18.656128, train_loss=2.9261746

Batch 200630, train_perplexity=18.656128, train_loss=2.9261746

Batch 200640, train_perplexity=18.656128, train_loss=2.9261746

Batch 200650, train_perplexity=18.656128, train_loss=2.9261746

Batch 200660, train_perplexity=18.656128, train_loss=2.9261746

Batch 200670, train_perplexity=18.656128, train_loss=2.9261746

Batch 200680, train_perplexity=18.656128, train_loss=2.9261746

Batch 200690, train_perplexity=18.656128, train_loss=2.9261746

Batch 200700, train_perplexity=18.656118, train_loss=2.9261742

Batch 200710, train_perplexity=18.656128, train_loss=2.9261746

Batch 200720, train_perplexity=18.656118, train_loss=2.9261742

Batch 200730, train_perplexity=18.656118, train_loss=2.9261742

Batch 200740, train_perplexity=18.656128, train_loss=2.9261746

Batch 200750, train_perplexity=18.656118, train_loss=2.9261742

Batch 200760, train_perplexity=18.656118, train_loss=2.9261742

Batch 200770, train_perplexity=18.656122, train_loss=2.9261744

Batch 200780, train_perplexity=18.656118, train_loss=2.9261742

Batch 200790, train_perplexity=18.656118, train_loss=2.9261742

Batch 200800, train_perplexity=18.656122, train_loss=2.9261744

Batch 200810, train_perplexity=18.656122, train_loss=2.9261744

Batch 200820, train_perplexity=18.656118, train_loss=2.9261742

Batch 200830, train_perplexity=18.656128, train_loss=2.9261746

Batch 200840, train_perplexity=18.656118, train_loss=2.9261742

Batch 200850, train_perplexity=18.656118, train_loss=2.9261742

Batch 200860, train_perplexity=18.656118, train_loss=2.9261742

Batch 200870, train_perplexity=18.656118, train_loss=2.9261742

Batch 200880, train_perplexity=18.656118, train_loss=2.9261742

Batch 200890, train_perplexity=18.656118, train_loss=2.9261742

Batch 200900, train_perplexity=18.656118, train_loss=2.9261742

Batch 200910, train_perplexity=18.656118, train_loss=2.9261742

Batch 200920, train_perplexity=18.656118, train_loss=2.9261742

Batch 200930, train_perplexity=18.656118, train_loss=2.9261742

Batch 200940, train_perplexity=18.656118, train_loss=2.9261742

Batch 200950, train_perplexity=18.656118, train_loss=2.9261742

Batch 200960, train_perplexity=18.656118, train_loss=2.9261742

Batch 200970, train_perplexity=18.656118, train_loss=2.9261742

Batch 200980, train_perplexity=18.656118, train_loss=2.9261742

Batch 200990, train_perplexity=18.656115, train_loss=2.926174

Batch 201000, train_perplexity=18.656118, train_loss=2.9261742

Batch 201010, train_perplexity=18.656109, train_loss=2.9261737

Batch 201020, train_perplexity=18.656115, train_loss=2.926174

Batch 201030, train_perplexity=18.656118, train_loss=2.9261742

Batch 201040, train_perplexity=18.656115, train_loss=2.926174

Batch 201050, train_perplexity=18.656109, train_loss=2.9261737

Batch 201060, train_perplexity=18.656109, train_loss=2.9261737

Batch 201070, train_perplexity=18.656115, train_loss=2.926174

Batch 201080, train_perplexity=18.656115, train_loss=2.926174

Batch 201090, train_perplexity=18.656115, train_loss=2.926174

Batch 201100, train_perplexity=18.656118, train_loss=2.9261742

Batch 201110, train_perplexity=18.656109, train_loss=2.9261737

Batch 201120, train_perplexity=18.656109, train_loss=2.9261737

Batch 201130, train_perplexity=18.656115, train_loss=2.926174

Batch 201140, train_perplexity=18.656109, train_loss=2.9261737

Batch 201150, train_perplexity=18.656109, train_loss=2.9261737

Batch 201160, train_perplexity=18.656115, train_loss=2.926174

Batch 201170, train_perplexity=18.656109, train_loss=2.9261737

Batch 201180, train_perplexity=18.656109, train_loss=2.9261737

Batch 201190, train_perplexity=18.656109, train_loss=2.9261737

Batch 201200, train_perplexity=18.656115, train_loss=2.926174

Batch 201210, train_perplexity=18.656109, train_loss=2.9261737

Batch 201220, train_perplexity=18.656109, train_loss=2.9261737

Batch 201230, train_perplexity=18.656109, train_loss=2.9261737

Batch 201240, train_perplexity=18.656109, train_loss=2.9261737

Batch 201250, train_perplexity=18.656109, train_loss=2.9261737

Batch 201260, train_perplexity=18.656109, train_loss=2.9261737

Batch 201270, train_perplexity=18.656109, train_loss=2.9261737

Batch 201280, train_perplexity=18.656109, train_loss=2.9261737

Batch 201290, train_perplexity=18.656109, train_loss=2.9261737

Batch 201300, train_perplexity=18.656101, train_loss=2.9261732

Batch 201310, train_perplexity=18.656109, train_loss=2.9261737

Batch 201320, train_perplexity=18.656109, train_loss=2.9261737

Batch 201330, train_perplexity=18.656105, train_loss=2.9261734

Batch 201340, train_perplexity=18.656109, train_loss=2.9261737

Batch 201350, train_perplexity=18.656105, train_loss=2.9261734

Batch 201360, train_perplexity=18.656101, train_loss=2.9261732

Batch 201370, train_perplexity=18.656109, train_loss=2.9261737

Batch 201380, train_perplexity=18.656105, train_loss=2.9261734

Batch 201390, train_perplexity=18.656109, train_loss=2.9261737

Batch 201400, train_perplexity=18.656109, train_loss=2.9261737

Batch 201410, train_perplexity=18.656101, train_loss=2.9261732

Batch 201420, train_perplexity=18.656101, train_loss=2.9261732

Batch 201430, train_perplexity=18.656105, train_loss=2.9261734

Batch 201440, train_perplexity=18.656101, train_loss=2.9261732

Batch 201450, train_perplexity=18.656101, train_loss=2.9261732

Batch 201460, train_perplexity=18.656101, train_loss=2.9261732

Batch 201470, train_perplexity=18.656101, train_loss=2.9261732

Batch 201480, train_perplexity=18.656101, train_loss=2.9261732

Batch 201490, train_perplexity=18.656101, train_loss=2.9261732

Batch 201500, train_perplexity=18.656101, train_loss=2.9261732

Batch 201510, train_perplexity=18.656101, train_loss=2.9261732

Batch 201520, train_perplexity=18.656101, train_loss=2.9261732

Batch 201530, train_perplexity=18.656101, train_loss=2.9261732

Batch 201540, train_perplexity=18.656101, train_loss=2.9261732

Batch 201550, train_perplexity=18.656101, train_loss=2.9261732

Batch 201560, train_perplexity=18.656101, train_loss=2.9261732

Batch 201570, train_perplexity=18.656101, train_loss=2.9261732

Batch 201580, train_perplexity=18.656101, train_loss=2.9261732

Batch 201590, train_perplexity=18.656101, train_loss=2.9261732

Batch 201600, train_perplexity=18.656096, train_loss=2.926173

Batch 201610, train_perplexity=18.656096, train_loss=2.926173
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 201620, train_perplexity=18.656096, train_loss=2.926173

Batch 201630, train_perplexity=18.656096, train_loss=2.926173

Batch 201640, train_perplexity=18.656096, train_loss=2.926173

Batch 201650, train_perplexity=18.656092, train_loss=2.9261727

Batch 201660, train_perplexity=18.656096, train_loss=2.926173

Batch 201670, train_perplexity=18.656101, train_loss=2.9261732

Batch 201680, train_perplexity=18.656092, train_loss=2.9261727

Batch 201690, train_perplexity=18.656092, train_loss=2.9261727

Batch 201700, train_perplexity=18.656092, train_loss=2.9261727

Batch 201710, train_perplexity=18.656096, train_loss=2.926173

Batch 201720, train_perplexity=18.656092, train_loss=2.9261727

Batch 201730, train_perplexity=18.656092, train_loss=2.9261727

Batch 201740, train_perplexity=18.656092, train_loss=2.9261727

Batch 201750, train_perplexity=18.656096, train_loss=2.926173

Batch 201760, train_perplexity=18.656092, train_loss=2.9261727

Batch 201770, train_perplexity=18.656092, train_loss=2.9261727

Batch 201780, train_perplexity=18.656092, train_loss=2.9261727

Batch 201790, train_perplexity=18.656092, train_loss=2.9261727

Batch 201800, train_perplexity=18.656096, train_loss=2.926173

Batch 201810, train_perplexity=18.656092, train_loss=2.9261727

Batch 201820, train_perplexity=18.656092, train_loss=2.9261727

Batch 201830, train_perplexity=18.656092, train_loss=2.9261727

Batch 201840, train_perplexity=18.656096, train_loss=2.926173

Batch 201850, train_perplexity=18.656092, train_loss=2.9261727

Batch 201860, train_perplexity=18.656092, train_loss=2.9261727

Batch 201870, train_perplexity=18.656092, train_loss=2.9261727

Batch 201880, train_perplexity=18.656092, train_loss=2.9261727

Batch 201890, train_perplexity=18.656092, train_loss=2.9261727

Batch 201900, train_perplexity=18.656092, train_loss=2.9261727

Batch 201910, train_perplexity=18.656092, train_loss=2.9261727

Batch 201920, train_perplexity=18.656092, train_loss=2.9261727

Batch 201930, train_perplexity=18.656092, train_loss=2.9261727

Batch 201940, train_perplexity=18.656092, train_loss=2.9261727

Batch 201950, train_perplexity=18.656092, train_loss=2.9261727

Batch 201960, train_perplexity=18.656092, train_loss=2.9261727

Batch 201970, train_perplexity=18.656092, train_loss=2.9261727

Batch 201980, train_perplexity=18.656088, train_loss=2.9261725

Batch 201990, train_perplexity=18.656082, train_loss=2.9261723

Batch 202000, train_perplexity=18.656092, train_loss=2.9261727

Batch 202010, train_perplexity=18.656082, train_loss=2.9261723

Batch 202020, train_perplexity=18.656092, train_loss=2.9261727

Batch 202030, train_perplexity=18.656092, train_loss=2.9261727

Batch 202040, train_perplexity=18.656082, train_loss=2.9261723

Batch 202050, train_perplexity=18.656082, train_loss=2.9261723

Batch 202060, train_perplexity=18.656088, train_loss=2.9261725

Batch 202070, train_perplexity=18.656088, train_loss=2.9261725

Batch 202080, train_perplexity=18.656082, train_loss=2.9261723

Batch 202090, train_perplexity=18.656082, train_loss=2.9261723

Batch 202100, train_perplexity=18.656082, train_loss=2.9261723

Batch 202110, train_perplexity=18.656082, train_loss=2.9261723

Batch 202120, train_perplexity=18.656082, train_loss=2.9261723

Batch 202130, train_perplexity=18.656082, train_loss=2.9261723

Batch 202140, train_perplexity=18.656082, train_loss=2.9261723

Batch 202150, train_perplexity=18.656082, train_loss=2.9261723

Batch 202160, train_perplexity=18.656082, train_loss=2.9261723

Batch 202170, train_perplexity=18.656075, train_loss=2.9261718

Batch 202180, train_perplexity=18.656075, train_loss=2.9261718

Batch 202190, train_perplexity=18.656082, train_loss=2.9261723

Batch 202200, train_perplexity=18.656082, train_loss=2.9261723

Batch 202210, train_perplexity=18.656078, train_loss=2.926172

Batch 202220, train_perplexity=18.656082, train_loss=2.9261723

Batch 202230, train_perplexity=18.656082, train_loss=2.9261723

Batch 202240, train_perplexity=18.656082, train_loss=2.9261723

Batch 202250, train_perplexity=18.656082, train_loss=2.9261723

Batch 202260, train_perplexity=18.656075, train_loss=2.9261718

Batch 202270, train_perplexity=18.656082, train_loss=2.9261723

Batch 202280, train_perplexity=18.656082, train_loss=2.9261723

Batch 202290, train_perplexity=18.656078, train_loss=2.926172

Batch 202300, train_perplexity=18.656078, train_loss=2.926172

Batch 202310, train_perplexity=18.656078, train_loss=2.926172

Batch 202320, train_perplexity=18.656082, train_loss=2.9261723

Batch 202330, train_perplexity=18.656078, train_loss=2.926172

Batch 202340, train_perplexity=18.656078, train_loss=2.926172

Batch 202350, train_perplexity=18.656078, train_loss=2.926172

Batch 202360, train_perplexity=18.656075, train_loss=2.9261718

Batch 202370, train_perplexity=18.656075, train_loss=2.9261718

Batch 202380, train_perplexity=18.656078, train_loss=2.926172

Batch 202390, train_perplexity=18.656075, train_loss=2.9261718

Batch 202400, train_perplexity=18.656075, train_loss=2.9261718

Batch 202410, train_perplexity=18.656075, train_loss=2.9261718

Batch 202420, train_perplexity=18.656075, train_loss=2.9261718

Batch 202430, train_perplexity=18.656078, train_loss=2.926172

Batch 202440, train_perplexity=18.656075, train_loss=2.9261718

Batch 202450, train_perplexity=18.656075, train_loss=2.9261718

Batch 202460, train_perplexity=18.656075, train_loss=2.9261718

Batch 202470, train_perplexity=18.656075, train_loss=2.9261718

Batch 202480, train_perplexity=18.656075, train_loss=2.9261718

Batch 202490, train_perplexity=18.656075, train_loss=2.9261718

Batch 202500, train_perplexity=18.656075, train_loss=2.9261718

Batch 202510, train_perplexity=18.656075, train_loss=2.9261718

Batch 202520, train_perplexity=18.656075, train_loss=2.9261718

Batch 202530, train_perplexity=18.656075, train_loss=2.9261718

Batch 202540, train_perplexity=18.656069, train_loss=2.9261715

Batch 202550, train_perplexity=18.656065, train_loss=2.9261713

Batch 202560, train_perplexity=18.656065, train_loss=2.9261713

Batch 202570, train_perplexity=18.656069, train_loss=2.9261715

Batch 202580, train_perplexity=18.656069, train_loss=2.9261715

Batch 202590, train_perplexity=18.656069, train_loss=2.9261715

Batch 202600, train_perplexity=18.656075, train_loss=2.9261718

Batch 202610, train_perplexity=18.656065, train_loss=2.9261713

Batch 202620, train_perplexity=18.656075, train_loss=2.9261718

Batch 202630, train_perplexity=18.656065, train_loss=2.9261713

Batch 202640, train_perplexity=18.656069, train_loss=2.9261715

Batch 202650, train_perplexity=18.656065, train_loss=2.9261713

Batch 202660, train_perplexity=18.656065, train_loss=2.9261713

Batch 202670, train_perplexity=18.656065, train_loss=2.9261713

Batch 202680, train_perplexity=18.656065, train_loss=2.9261713

Batch 202690, train_perplexity=18.656065, train_loss=2.9261713

Batch 202700, train_perplexity=18.656065, train_loss=2.9261713

Batch 202710, train_perplexity=18.656069, train_loss=2.9261715

Batch 202720, train_perplexity=18.656065, train_loss=2.9261713

Batch 202730, train_perplexity=18.656065, train_loss=2.9261713

Batch 202740, train_perplexity=18.656065, train_loss=2.9261713

Batch 202750, train_perplexity=18.656065, train_loss=2.9261713

Batch 202760, train_perplexity=18.656065, train_loss=2.9261713

Batch 202770, train_perplexity=18.656061, train_loss=2.926171

Batch 202780, train_perplexity=18.656065, train_loss=2.9261713

Batch 202790, train_perplexity=18.656065, train_loss=2.9261713

Batch 202800, train_perplexity=18.656065, train_loss=2.9261713

Batch 202810, train_perplexity=18.656065, train_loss=2.9261713

Batch 202820, train_perplexity=18.656065, train_loss=2.9261713

Batch 202830, train_perplexity=18.656065, train_loss=2.9261713

Batch 202840, train_perplexity=18.656065, train_loss=2.9261713

Batch 202850, train_perplexity=18.656065, train_loss=2.9261713

Batch 202860, train_perplexity=18.656061, train_loss=2.926171

Batch 202870, train_perplexity=18.656065, train_loss=2.9261713

Batch 202880, train_perplexity=18.656055, train_loss=2.9261708

Batch 202890, train_perplexity=18.656065, train_loss=2.9261713

Batch 202900, train_perplexity=18.656055, train_loss=2.9261708
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 202910, train_perplexity=18.656061, train_loss=2.926171

Batch 202920, train_perplexity=18.656065, train_loss=2.9261713

Batch 202930, train_perplexity=18.656061, train_loss=2.926171

Batch 202940, train_perplexity=18.656061, train_loss=2.926171

Batch 202950, train_perplexity=18.656061, train_loss=2.926171

Batch 202960, train_perplexity=18.656055, train_loss=2.9261708

Batch 202970, train_perplexity=18.656055, train_loss=2.9261708

Batch 202980, train_perplexity=18.656055, train_loss=2.9261708

Batch 202990, train_perplexity=18.656061, train_loss=2.926171

Batch 203000, train_perplexity=18.656055, train_loss=2.9261708

Batch 203010, train_perplexity=18.656055, train_loss=2.9261708

Batch 203020, train_perplexity=18.656055, train_loss=2.9261708

Batch 203030, train_perplexity=18.656055, train_loss=2.9261708

Batch 203040, train_perplexity=18.656055, train_loss=2.9261708

Batch 203050, train_perplexity=18.656055, train_loss=2.9261708

Batch 203060, train_perplexity=18.656055, train_loss=2.9261708

Batch 203070, train_perplexity=18.656055, train_loss=2.9261708

Batch 203080, train_perplexity=18.656055, train_loss=2.9261708

Batch 203090, train_perplexity=18.656055, train_loss=2.9261708

Batch 203100, train_perplexity=18.656055, train_loss=2.9261708

Batch 203110, train_perplexity=18.656061, train_loss=2.926171

Batch 203120, train_perplexity=18.656055, train_loss=2.9261708

Batch 203130, train_perplexity=18.656055, train_loss=2.9261708

Batch 203140, train_perplexity=18.656055, train_loss=2.9261708

Batch 203150, train_perplexity=18.656055, train_loss=2.9261708

Batch 203160, train_perplexity=18.656052, train_loss=2.9261706

Batch 203170, train_perplexity=18.656055, train_loss=2.9261708

Batch 203180, train_perplexity=18.656055, train_loss=2.9261708

Batch 203190, train_perplexity=18.656052, train_loss=2.9261706

Batch 203200, train_perplexity=18.656055, train_loss=2.9261708

Batch 203210, train_perplexity=18.656052, train_loss=2.9261706

Batch 203220, train_perplexity=18.656055, train_loss=2.9261708

Batch 203230, train_perplexity=18.656055, train_loss=2.9261708

Batch 203240, train_perplexity=18.656052, train_loss=2.9261706

Batch 203250, train_perplexity=18.656048, train_loss=2.9261703

Batch 203260, train_perplexity=18.656048, train_loss=2.9261703

Batch 203270, train_perplexity=18.656055, train_loss=2.9261708

Batch 203280, train_perplexity=18.656048, train_loss=2.9261703

Batch 203290, train_perplexity=18.656048, train_loss=2.9261703

Batch 203300, train_perplexity=18.656048, train_loss=2.9261703

Batch 203310, train_perplexity=18.656052, train_loss=2.9261706

Batch 203320, train_perplexity=18.656048, train_loss=2.9261703

Batch 203330, train_perplexity=18.656048, train_loss=2.9261703

Batch 203340, train_perplexity=18.656048, train_loss=2.9261703

Batch 203350, train_perplexity=18.656048, train_loss=2.9261703

Batch 203360, train_perplexity=18.656048, train_loss=2.9261703

Batch 203370, train_perplexity=18.656048, train_loss=2.9261703

Batch 203380, train_perplexity=18.656048, train_loss=2.9261703

Batch 203390, train_perplexity=18.656048, train_loss=2.9261703

Batch 203400, train_perplexity=18.656048, train_loss=2.9261703

Batch 203410, train_perplexity=18.656048, train_loss=2.9261703

Batch 203420, train_perplexity=18.656048, train_loss=2.9261703

Batch 203430, train_perplexity=18.656048, train_loss=2.9261703

Batch 203440, train_perplexity=18.656048, train_loss=2.9261703

Batch 203450, train_perplexity=18.656048, train_loss=2.9261703

Batch 203460, train_perplexity=18.656042, train_loss=2.92617

Batch 203470, train_perplexity=18.656048, train_loss=2.9261703

Batch 203480, train_perplexity=18.656048, train_loss=2.9261703

Batch 203490, train_perplexity=18.656048, train_loss=2.9261703

Batch 203500, train_perplexity=18.656038, train_loss=2.9261699

Batch 203510, train_perplexity=18.656048, train_loss=2.9261703

Batch 203520, train_perplexity=18.656042, train_loss=2.92617

Batch 203530, train_perplexity=18.656048, train_loss=2.9261703

Batch 203540, train_perplexity=18.656038, train_loss=2.9261699

Batch 203550, train_perplexity=18.656048, train_loss=2.9261703

Batch 203560, train_perplexity=18.656048, train_loss=2.9261703

Batch 203570, train_perplexity=18.656038, train_loss=2.9261699

Batch 203580, train_perplexity=18.656042, train_loss=2.92617

Batch 203590, train_perplexity=18.656038, train_loss=2.9261699

Batch 203600, train_perplexity=18.656038, train_loss=2.9261699

Batch 203610, train_perplexity=18.656048, train_loss=2.9261703

Batch 203620, train_perplexity=18.656038, train_loss=2.9261699

Batch 203630, train_perplexity=18.656038, train_loss=2.9261699

Batch 203640, train_perplexity=18.656038, train_loss=2.9261699

Batch 203650, train_perplexity=18.656042, train_loss=2.92617

Batch 203660, train_perplexity=18.656042, train_loss=2.92617

Batch 203670, train_perplexity=18.656038, train_loss=2.9261699

Batch 203680, train_perplexity=18.656038, train_loss=2.9261699

Batch 203690, train_perplexity=18.656038, train_loss=2.9261699

Batch 203700, train_perplexity=18.656038, train_loss=2.9261699

Batch 203710, train_perplexity=18.656038, train_loss=2.9261699

Batch 203720, train_perplexity=18.656038, train_loss=2.9261699

Batch 203730, train_perplexity=18.656034, train_loss=2.9261696

Batch 203740, train_perplexity=18.656029, train_loss=2.9261694

Batch 203750, train_perplexity=18.656038, train_loss=2.9261699

Batch 203760, train_perplexity=18.656038, train_loss=2.9261699

Batch 203770, train_perplexity=18.656038, train_loss=2.9261699

Batch 203780, train_perplexity=18.656038, train_loss=2.9261699

Batch 203790, train_perplexity=18.656038, train_loss=2.9261699

Batch 203800, train_perplexity=18.656038, train_loss=2.9261699

Batch 203810, train_perplexity=18.656034, train_loss=2.9261696

Batch 203820, train_perplexity=18.656038, train_loss=2.9261699

Batch 203830, train_perplexity=18.656029, train_loss=2.9261694

Batch 203840, train_perplexity=18.656038, train_loss=2.9261699

Batch 203850, train_perplexity=18.656034, train_loss=2.9261696

Batch 203860, train_perplexity=18.656034, train_loss=2.9261696

Batch 203870, train_perplexity=18.656034, train_loss=2.9261696

Batch 203880, train_perplexity=18.656029, train_loss=2.9261694

Batch 203890, train_perplexity=18.656034, train_loss=2.9261696

Batch 203900, train_perplexity=18.656029, train_loss=2.9261694

Batch 203910, train_perplexity=18.656029, train_loss=2.9261694

Batch 203920, train_perplexity=18.656034, train_loss=2.9261696

Batch 203930, train_perplexity=18.656034, train_loss=2.9261696

Batch 203940, train_perplexity=18.656029, train_loss=2.9261694

Batch 203950, train_perplexity=18.656029, train_loss=2.9261694

Batch 203960, train_perplexity=18.656038, train_loss=2.9261699

Batch 203970, train_perplexity=18.656029, train_loss=2.9261694

Batch 203980, train_perplexity=18.656029, train_loss=2.9261694

Batch 203990, train_perplexity=18.656029, train_loss=2.9261694

Batch 204000, train_perplexity=18.656029, train_loss=2.9261694

Batch 204010, train_perplexity=18.656029, train_loss=2.9261694

Batch 204020, train_perplexity=18.656029, train_loss=2.9261694

Batch 204030, train_perplexity=18.656029, train_loss=2.9261694

Batch 204040, train_perplexity=18.656029, train_loss=2.9261694

Batch 204050, train_perplexity=18.656021, train_loss=2.926169

Batch 204060, train_perplexity=18.656029, train_loss=2.9261694

Batch 204070, train_perplexity=18.656029, train_loss=2.9261694

Batch 204080, train_perplexity=18.656029, train_loss=2.9261694

Batch 204090, train_perplexity=18.656029, train_loss=2.9261694

Batch 204100, train_perplexity=18.656029, train_loss=2.9261694

Batch 204110, train_perplexity=18.656029, train_loss=2.9261694

Batch 204120, train_perplexity=18.656025, train_loss=2.9261692

Batch 204130, train_perplexity=18.656021, train_loss=2.926169

Batch 204140, train_perplexity=18.656029, train_loss=2.9261694

Batch 204150, train_perplexity=18.656029, train_loss=2.9261694

Batch 204160, train_perplexity=18.656025, train_loss=2.9261692

Batch 204170, train_perplexity=18.656025, train_loss=2.9261692

Batch 204180, train_perplexity=18.656025, train_loss=2.9261692

Batch 204190, train_perplexity=18.656025, train_loss=2.9261692
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 204200, train_perplexity=18.656021, train_loss=2.926169

Batch 204210, train_perplexity=18.656021, train_loss=2.926169

Batch 204220, train_perplexity=18.656025, train_loss=2.9261692

Batch 204230, train_perplexity=18.656021, train_loss=2.926169

Batch 204240, train_perplexity=18.656021, train_loss=2.926169

Batch 204250, train_perplexity=18.656021, train_loss=2.926169

Batch 204260, train_perplexity=18.656025, train_loss=2.9261692

Batch 204270, train_perplexity=18.656021, train_loss=2.926169

Batch 204280, train_perplexity=18.656021, train_loss=2.926169

Batch 204290, train_perplexity=18.656021, train_loss=2.926169

Batch 204300, train_perplexity=18.656021, train_loss=2.926169

Batch 204310, train_perplexity=18.656021, train_loss=2.926169

Batch 204320, train_perplexity=18.656021, train_loss=2.926169

Batch 204330, train_perplexity=18.656021, train_loss=2.926169

Batch 204340, train_perplexity=18.656021, train_loss=2.926169

Batch 204350, train_perplexity=18.656021, train_loss=2.926169

Batch 204360, train_perplexity=18.656021, train_loss=2.926169

Batch 204370, train_perplexity=18.656021, train_loss=2.926169

Batch 204380, train_perplexity=18.656015, train_loss=2.9261687

Batch 204390, train_perplexity=18.656021, train_loss=2.926169

Batch 204400, train_perplexity=18.656015, train_loss=2.9261687

Batch 204410, train_perplexity=18.656021, train_loss=2.926169

Batch 204420, train_perplexity=18.656012, train_loss=2.9261684

Batch 204430, train_perplexity=18.656021, train_loss=2.926169

Batch 204440, train_perplexity=18.656021, train_loss=2.926169

Batch 204450, train_perplexity=18.656015, train_loss=2.9261687

Batch 204460, train_perplexity=18.656015, train_loss=2.9261687

Batch 204470, train_perplexity=18.656012, train_loss=2.9261684

Batch 204480, train_perplexity=18.656015, train_loss=2.9261687

Batch 204490, train_perplexity=18.656021, train_loss=2.926169

Batch 204500, train_perplexity=18.656012, train_loss=2.9261684

Batch 204510, train_perplexity=18.656021, train_loss=2.926169

Batch 204520, train_perplexity=18.656012, train_loss=2.9261684

Batch 204530, train_perplexity=18.656012, train_loss=2.9261684

Batch 204540, train_perplexity=18.656012, train_loss=2.9261684

Batch 204550, train_perplexity=18.656012, train_loss=2.9261684

Batch 204560, train_perplexity=18.656012, train_loss=2.9261684

Batch 204570, train_perplexity=18.656012, train_loss=2.9261684

Batch 204580, train_perplexity=18.656012, train_loss=2.9261684

Batch 204590, train_perplexity=18.656012, train_loss=2.9261684

Batch 204600, train_perplexity=18.656012, train_loss=2.9261684

Batch 204610, train_perplexity=18.656012, train_loss=2.9261684

Batch 204620, train_perplexity=18.656012, train_loss=2.9261684

Batch 204630, train_perplexity=18.656012, train_loss=2.9261684

Batch 204640, train_perplexity=18.656012, train_loss=2.9261684

Batch 204650, train_perplexity=18.656012, train_loss=2.9261684

Batch 204660, train_perplexity=18.656012, train_loss=2.9261684

Batch 204670, train_perplexity=18.656012, train_loss=2.9261684

Batch 204680, train_perplexity=18.656012, train_loss=2.9261684

Batch 204690, train_perplexity=18.656012, train_loss=2.9261684

Batch 204700, train_perplexity=18.656012, train_loss=2.9261684

Batch 204710, train_perplexity=18.656012, train_loss=2.9261684

Batch 204720, train_perplexity=18.656002, train_loss=2.926168

Batch 204730, train_perplexity=18.656012, train_loss=2.9261684

Batch 204740, train_perplexity=18.656008, train_loss=2.9261682

Batch 204750, train_perplexity=18.656002, train_loss=2.926168

Batch 204760, train_perplexity=18.656008, train_loss=2.9261682

Batch 204770, train_perplexity=18.656012, train_loss=2.9261684

Batch 204780, train_perplexity=18.656012, train_loss=2.9261684

Batch 204790, train_perplexity=18.656012, train_loss=2.9261684

Batch 204800, train_perplexity=18.656002, train_loss=2.926168

Batch 204810, train_perplexity=18.656008, train_loss=2.9261682

Batch 204820, train_perplexity=18.656008, train_loss=2.9261682

Batch 204830, train_perplexity=18.656002, train_loss=2.926168

Batch 204840, train_perplexity=18.656012, train_loss=2.9261684

Batch 204850, train_perplexity=18.656008, train_loss=2.9261682

Batch 204860, train_perplexity=18.656002, train_loss=2.926168

Batch 204870, train_perplexity=18.656008, train_loss=2.9261682

Batch 204880, train_perplexity=18.656002, train_loss=2.926168

Batch 204890, train_perplexity=18.656002, train_loss=2.926168

Batch 204900, train_perplexity=18.656008, train_loss=2.9261682

Batch 204910, train_perplexity=18.656002, train_loss=2.926168

Batch 204920, train_perplexity=18.656002, train_loss=2.926168

Batch 204930, train_perplexity=18.656002, train_loss=2.926168

Batch 204940, train_perplexity=18.656002, train_loss=2.926168

Batch 204950, train_perplexity=18.656002, train_loss=2.926168

Batch 204960, train_perplexity=18.656002, train_loss=2.926168

Batch 204970, train_perplexity=18.655998, train_loss=2.9261677

Batch 204980, train_perplexity=18.656002, train_loss=2.926168

Batch 204990, train_perplexity=18.656002, train_loss=2.926168

Batch 205000, train_perplexity=18.656008, train_loss=2.9261682

Batch 205010, train_perplexity=18.655998, train_loss=2.9261677

Batch 205020, train_perplexity=18.656002, train_loss=2.926168

Batch 205030, train_perplexity=18.655994, train_loss=2.9261675

Batch 205040, train_perplexity=18.656002, train_loss=2.926168

Batch 205050, train_perplexity=18.655998, train_loss=2.9261677

Batch 205060, train_perplexity=18.655998, train_loss=2.9261677

Batch 205070, train_perplexity=18.656002, train_loss=2.926168

Batch 205080, train_perplexity=18.656002, train_loss=2.926168

Batch 205090, train_perplexity=18.655998, train_loss=2.9261677

Batch 205100, train_perplexity=18.655994, train_loss=2.9261675

Batch 205110, train_perplexity=18.655998, train_loss=2.9261677

Batch 205120, train_perplexity=18.655998, train_loss=2.9261677

Batch 205130, train_perplexity=18.655994, train_loss=2.9261675

Batch 205140, train_perplexity=18.655994, train_loss=2.9261675

Batch 205150, train_perplexity=18.655994, train_loss=2.9261675

Batch 205160, train_perplexity=18.655998, train_loss=2.9261677

Batch 205170, train_perplexity=18.655994, train_loss=2.9261675

Batch 205180, train_perplexity=18.655994, train_loss=2.9261675

Batch 205190, train_perplexity=18.655994, train_loss=2.9261675

Batch 205200, train_perplexity=18.655994, train_loss=2.9261675

Batch 205210, train_perplexity=18.655994, train_loss=2.9261675

Batch 205220, train_perplexity=18.655994, train_loss=2.9261675

Batch 205230, train_perplexity=18.655994, train_loss=2.9261675

Batch 205240, train_perplexity=18.655994, train_loss=2.9261675

Batch 205250, train_perplexity=18.655994, train_loss=2.9261675

Batch 205260, train_perplexity=18.655994, train_loss=2.9261675

Batch 205270, train_perplexity=18.655994, train_loss=2.9261675

Batch 205280, train_perplexity=18.655994, train_loss=2.9261675

Batch 205290, train_perplexity=18.655994, train_loss=2.9261675

Batch 205300, train_perplexity=18.655994, train_loss=2.9261675

Batch 205310, train_perplexity=18.655994, train_loss=2.9261675

Batch 205320, train_perplexity=18.655985, train_loss=2.926167

Batch 205330, train_perplexity=18.655994, train_loss=2.9261675

Batch 205340, train_perplexity=18.655994, train_loss=2.9261675

Batch 205350, train_perplexity=18.655989, train_loss=2.9261672

Batch 205360, train_perplexity=18.655994, train_loss=2.9261675

Batch 205370, train_perplexity=18.655994, train_loss=2.9261675

Batch 205380, train_perplexity=18.655985, train_loss=2.926167

Batch 205390, train_perplexity=18.655985, train_loss=2.926167

Batch 205400, train_perplexity=18.655989, train_loss=2.9261672

Batch 205410, train_perplexity=18.655989, train_loss=2.9261672

Batch 205420, train_perplexity=18.655989, train_loss=2.9261672

Batch 205430, train_perplexity=18.655989, train_loss=2.9261672

Batch 205440, train_perplexity=18.655989, train_loss=2.9261672

Batch 205450, train_perplexity=18.655985, train_loss=2.926167

Batch 205460, train_perplexity=18.655989, train_loss=2.9261672

Batch 205470, train_perplexity=18.655985, train_loss=2.926167

Batch 205480, train_perplexity=18.655989, train_loss=2.9261672
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 205490, train_perplexity=18.655989, train_loss=2.9261672

Batch 205500, train_perplexity=18.655989, train_loss=2.9261672

Batch 205510, train_perplexity=18.655985, train_loss=2.926167

Batch 205520, train_perplexity=18.655985, train_loss=2.926167

Batch 205530, train_perplexity=18.655985, train_loss=2.926167

Batch 205540, train_perplexity=18.655985, train_loss=2.926167

Batch 205550, train_perplexity=18.655985, train_loss=2.926167

Batch 205560, train_perplexity=18.655985, train_loss=2.926167

Batch 205570, train_perplexity=18.655985, train_loss=2.926167

Batch 205580, train_perplexity=18.655985, train_loss=2.926167

Batch 205590, train_perplexity=18.655985, train_loss=2.926167

Batch 205600, train_perplexity=18.655985, train_loss=2.926167

Batch 205610, train_perplexity=18.655985, train_loss=2.926167

Batch 205620, train_perplexity=18.655985, train_loss=2.926167

Batch 205630, train_perplexity=18.655985, train_loss=2.926167

Batch 205640, train_perplexity=18.655985, train_loss=2.926167

Batch 205650, train_perplexity=18.655981, train_loss=2.9261668

Batch 205660, train_perplexity=18.655985, train_loss=2.926167

Batch 205670, train_perplexity=18.655981, train_loss=2.9261668

Batch 205680, train_perplexity=18.655981, train_loss=2.9261668

Batch 205690, train_perplexity=18.655975, train_loss=2.9261665

Batch 205700, train_perplexity=18.655981, train_loss=2.9261668

Batch 205710, train_perplexity=18.655985, train_loss=2.926167

Batch 205720, train_perplexity=18.655975, train_loss=2.9261665

Batch 205730, train_perplexity=18.655985, train_loss=2.926167

Batch 205740, train_perplexity=18.655975, train_loss=2.9261665

Batch 205750, train_perplexity=18.655981, train_loss=2.9261668

Batch 205760, train_perplexity=18.655975, train_loss=2.9261665

Batch 205770, train_perplexity=18.655981, train_loss=2.9261668

Batch 205780, train_perplexity=18.655975, train_loss=2.9261665

Batch 205790, train_perplexity=18.655975, train_loss=2.9261665

Batch 205800, train_perplexity=18.655975, train_loss=2.9261665

Batch 205810, train_perplexity=18.655975, train_loss=2.9261665

Batch 205820, train_perplexity=18.655985, train_loss=2.926167

Batch 205830, train_perplexity=18.655981, train_loss=2.9261668

Batch 205840, train_perplexity=18.655975, train_loss=2.9261665

Batch 205850, train_perplexity=18.655975, train_loss=2.9261665

Batch 205860, train_perplexity=18.655975, train_loss=2.9261665

Batch 205870, train_perplexity=18.655975, train_loss=2.9261665

Batch 205880, train_perplexity=18.655975, train_loss=2.9261665

Batch 205890, train_perplexity=18.655975, train_loss=2.9261665

Batch 205900, train_perplexity=18.655975, train_loss=2.9261665

Batch 205910, train_perplexity=18.655975, train_loss=2.9261665

Batch 205920, train_perplexity=18.655975, train_loss=2.9261665

Batch 205930, train_perplexity=18.655975, train_loss=2.9261665

Batch 205940, train_perplexity=18.655975, train_loss=2.9261665

Batch 205950, train_perplexity=18.655975, train_loss=2.9261665

Batch 205960, train_perplexity=18.655975, train_loss=2.9261665

Batch 205970, train_perplexity=18.655975, train_loss=2.9261665

Batch 205980, train_perplexity=18.655975, train_loss=2.9261665

Batch 205990, train_perplexity=18.655975, train_loss=2.9261665

Batch 206000, train_perplexity=18.655975, train_loss=2.9261665

Batch 206010, train_perplexity=18.655972, train_loss=2.9261663

Batch 206020, train_perplexity=18.655968, train_loss=2.926166

Batch 206030, train_perplexity=18.655975, train_loss=2.9261665

Batch 206040, train_perplexity=18.655975, train_loss=2.9261665

Batch 206050, train_perplexity=18.655975, train_loss=2.9261665

Batch 206060, train_perplexity=18.655975, train_loss=2.9261665

Batch 206070, train_perplexity=18.655968, train_loss=2.926166

Batch 206080, train_perplexity=18.655972, train_loss=2.9261663

Batch 206090, train_perplexity=18.655968, train_loss=2.926166

Batch 206100, train_perplexity=18.655968, train_loss=2.926166

Batch 206110, train_perplexity=18.655968, train_loss=2.926166

Batch 206120, train_perplexity=18.655968, train_loss=2.926166

Batch 206130, train_perplexity=18.655968, train_loss=2.926166

Batch 206140, train_perplexity=18.655968, train_loss=2.926166

Batch 206150, train_perplexity=18.655968, train_loss=2.926166

Batch 206160, train_perplexity=18.655972, train_loss=2.9261663

Batch 206170, train_perplexity=18.655968, train_loss=2.926166

Batch 206180, train_perplexity=18.655968, train_loss=2.926166

Batch 206190, train_perplexity=18.655968, train_loss=2.926166

Batch 206200, train_perplexity=18.655968, train_loss=2.926166

Batch 206210, train_perplexity=18.655968, train_loss=2.926166

Batch 206220, train_perplexity=18.655968, train_loss=2.926166

Batch 206230, train_perplexity=18.655968, train_loss=2.926166

Batch 206240, train_perplexity=18.655968, train_loss=2.926166

Batch 206250, train_perplexity=18.655968, train_loss=2.926166

Batch 206260, train_perplexity=18.655962, train_loss=2.9261658

Batch 206270, train_perplexity=18.655958, train_loss=2.9261656

Batch 206280, train_perplexity=18.655962, train_loss=2.9261658

Batch 206290, train_perplexity=18.655968, train_loss=2.926166

Batch 206300, train_perplexity=18.655962, train_loss=2.9261658

Batch 206310, train_perplexity=18.655968, train_loss=2.926166

Batch 206320, train_perplexity=18.655968, train_loss=2.926166

Batch 206330, train_perplexity=18.655962, train_loss=2.9261658

Batch 206340, train_perplexity=18.655968, train_loss=2.926166

Batch 206350, train_perplexity=18.655962, train_loss=2.9261658

Batch 206360, train_perplexity=18.655968, train_loss=2.926166

Batch 206370, train_perplexity=18.655968, train_loss=2.926166

Batch 206380, train_perplexity=18.655958, train_loss=2.9261656

Batch 206390, train_perplexity=18.655958, train_loss=2.9261656

Batch 206400, train_perplexity=18.655962, train_loss=2.9261658

Batch 206410, train_perplexity=18.655958, train_loss=2.9261656

Batch 206420, train_perplexity=18.655958, train_loss=2.9261656

Batch 206430, train_perplexity=18.655958, train_loss=2.9261656

Batch 206440, train_perplexity=18.655958, train_loss=2.9261656

Batch 206450, train_perplexity=18.655962, train_loss=2.9261658

Batch 206460, train_perplexity=18.655962, train_loss=2.9261658

Batch 206470, train_perplexity=18.655962, train_loss=2.9261658

Batch 206480, train_perplexity=18.655958, train_loss=2.9261656

Batch 206490, train_perplexity=18.655958, train_loss=2.9261656

Batch 206500, train_perplexity=18.655958, train_loss=2.9261656

Batch 206510, train_perplexity=18.655958, train_loss=2.9261656

Batch 206520, train_perplexity=18.655958, train_loss=2.9261656

Batch 206530, train_perplexity=18.655958, train_loss=2.9261656

Batch 206540, train_perplexity=18.655958, train_loss=2.9261656

Batch 206550, train_perplexity=18.655958, train_loss=2.9261656

Batch 206560, train_perplexity=18.655958, train_loss=2.9261656

Batch 206570, train_perplexity=18.655958, train_loss=2.9261656

Batch 206580, train_perplexity=18.655958, train_loss=2.9261656

Batch 206590, train_perplexity=18.655958, train_loss=2.9261656

Batch 206600, train_perplexity=18.655958, train_loss=2.9261656

Batch 206610, train_perplexity=18.655949, train_loss=2.926165

Batch 206620, train_perplexity=18.655949, train_loss=2.926165

Batch 206630, train_perplexity=18.655954, train_loss=2.9261653

Batch 206640, train_perplexity=18.655949, train_loss=2.926165

Batch 206650, train_perplexity=18.655954, train_loss=2.9261653

Batch 206660, train_perplexity=18.655949, train_loss=2.926165

Batch 206670, train_perplexity=18.655958, train_loss=2.9261656

Batch 206680, train_perplexity=18.655949, train_loss=2.926165

Batch 206690, train_perplexity=18.655954, train_loss=2.9261653

Batch 206700, train_perplexity=18.655949, train_loss=2.926165

Batch 206710, train_perplexity=18.655958, train_loss=2.9261656

Batch 206720, train_perplexity=18.655949, train_loss=2.926165

Batch 206730, train_perplexity=18.655949, train_loss=2.926165

Batch 206740, train_perplexity=18.655949, train_loss=2.926165

Batch 206750, train_perplexity=18.655949, train_loss=2.926165

Batch 206760, train_perplexity=18.655949, train_loss=2.926165

Batch 206770, train_perplexity=18.655949, train_loss=2.926165
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 206780, train_perplexity=18.655949, train_loss=2.926165

Batch 206790, train_perplexity=18.655949, train_loss=2.926165

Batch 206800, train_perplexity=18.655949, train_loss=2.926165

Batch 206810, train_perplexity=18.655949, train_loss=2.926165

Batch 206820, train_perplexity=18.655949, train_loss=2.926165

Batch 206830, train_perplexity=18.655949, train_loss=2.926165

Batch 206840, train_perplexity=18.655949, train_loss=2.926165

Batch 206850, train_perplexity=18.655949, train_loss=2.926165

Batch 206860, train_perplexity=18.655945, train_loss=2.9261649

Batch 206870, train_perplexity=18.655949, train_loss=2.926165

Batch 206880, train_perplexity=18.655949, train_loss=2.926165

Batch 206890, train_perplexity=18.655949, train_loss=2.926165

Batch 206900, train_perplexity=18.655949, train_loss=2.926165

Batch 206910, train_perplexity=18.655949, train_loss=2.926165

Batch 206920, train_perplexity=18.655941, train_loss=2.9261646

Batch 206930, train_perplexity=18.655949, train_loss=2.926165

Batch 206940, train_perplexity=18.655949, train_loss=2.926165

Batch 206950, train_perplexity=18.655949, train_loss=2.926165

Batch 206960, train_perplexity=18.655945, train_loss=2.9261649

Batch 206970, train_perplexity=18.655949, train_loss=2.926165

Batch 206980, train_perplexity=18.655949, train_loss=2.926165

Batch 206990, train_perplexity=18.655949, train_loss=2.926165

Batch 207000, train_perplexity=18.655941, train_loss=2.9261646

Batch 207010, train_perplexity=18.655949, train_loss=2.926165

Batch 207020, train_perplexity=18.655941, train_loss=2.9261646

Batch 207030, train_perplexity=18.655941, train_loss=2.9261646

Batch 207040, train_perplexity=18.655949, train_loss=2.926165

Batch 207050, train_perplexity=18.655941, train_loss=2.9261646

Batch 207060, train_perplexity=18.655941, train_loss=2.9261646

Batch 207070, train_perplexity=18.655941, train_loss=2.9261646

Batch 207080, train_perplexity=18.655941, train_loss=2.9261646

Batch 207090, train_perplexity=18.655941, train_loss=2.9261646

Batch 207100, train_perplexity=18.655941, train_loss=2.9261646

Batch 207110, train_perplexity=18.655941, train_loss=2.9261646

Batch 207120, train_perplexity=18.655941, train_loss=2.9261646

Batch 207130, train_perplexity=18.655941, train_loss=2.9261646

Batch 207140, train_perplexity=18.655941, train_loss=2.9261646

Batch 207150, train_perplexity=18.655941, train_loss=2.9261646

Batch 207160, train_perplexity=18.655941, train_loss=2.9261646

Batch 207170, train_perplexity=18.655941, train_loss=2.9261646

Batch 207180, train_perplexity=18.655941, train_loss=2.9261646

Batch 207190, train_perplexity=18.655941, train_loss=2.9261646

Batch 207200, train_perplexity=18.655941, train_loss=2.9261646

Batch 207210, train_perplexity=18.655941, train_loss=2.9261646

Batch 207220, train_perplexity=18.655941, train_loss=2.9261646

Batch 207230, train_perplexity=18.655935, train_loss=2.9261644

Batch 207240, train_perplexity=18.655935, train_loss=2.9261644

Batch 207250, train_perplexity=18.655935, train_loss=2.9261644

Batch 207260, train_perplexity=18.655935, train_loss=2.9261644

Batch 207270, train_perplexity=18.655931, train_loss=2.9261642

Batch 207280, train_perplexity=18.655935, train_loss=2.9261644

Batch 207290, train_perplexity=18.655931, train_loss=2.9261642

Batch 207300, train_perplexity=18.655931, train_loss=2.9261642

Batch 207310, train_perplexity=18.655931, train_loss=2.9261642

Batch 207320, train_perplexity=18.655935, train_loss=2.9261644

Batch 207330, train_perplexity=18.655941, train_loss=2.9261646

Batch 207340, train_perplexity=18.655931, train_loss=2.9261642

Batch 207350, train_perplexity=18.655931, train_loss=2.9261642

Batch 207360, train_perplexity=18.655931, train_loss=2.9261642

Batch 207370, train_perplexity=18.655931, train_loss=2.9261642

Batch 207380, train_perplexity=18.655931, train_loss=2.9261642

Batch 207390, train_perplexity=18.655931, train_loss=2.9261642

Batch 207400, train_perplexity=18.655931, train_loss=2.9261642

Batch 207410, train_perplexity=18.655931, train_loss=2.9261642

Batch 207420, train_perplexity=18.655931, train_loss=2.9261642

Batch 207430, train_perplexity=18.655931, train_loss=2.9261642

Batch 207440, train_perplexity=18.655931, train_loss=2.9261642

Batch 207450, train_perplexity=18.655931, train_loss=2.9261642

Batch 207460, train_perplexity=18.655931, train_loss=2.9261642

Batch 207470, train_perplexity=18.655931, train_loss=2.9261642

Batch 207480, train_perplexity=18.655931, train_loss=2.9261642

Batch 207490, train_perplexity=18.655931, train_loss=2.9261642

Batch 207500, train_perplexity=18.655931, train_loss=2.9261642

Batch 207510, train_perplexity=18.655928, train_loss=2.926164

Batch 207520, train_perplexity=18.655931, train_loss=2.9261642

Batch 207530, train_perplexity=18.655928, train_loss=2.926164

Batch 207540, train_perplexity=18.655931, train_loss=2.9261642

Batch 207550, train_perplexity=18.655931, train_loss=2.9261642

Batch 207560, train_perplexity=18.655922, train_loss=2.9261637

Batch 207570, train_perplexity=18.655928, train_loss=2.926164

Batch 207580, train_perplexity=18.655931, train_loss=2.9261642

Batch 207590, train_perplexity=18.655928, train_loss=2.926164

Batch 207600, train_perplexity=18.655931, train_loss=2.9261642

Batch 207610, train_perplexity=18.655931, train_loss=2.9261642

Batch 207620, train_perplexity=18.655922, train_loss=2.9261637

Batch 207630, train_perplexity=18.655922, train_loss=2.9261637

Batch 207640, train_perplexity=18.655922, train_loss=2.9261637

Batch 207650, train_perplexity=18.655922, train_loss=2.9261637

Batch 207660, train_perplexity=18.655922, train_loss=2.9261637

Batch 207670, train_perplexity=18.655922, train_loss=2.9261637

Batch 207680, train_perplexity=18.655922, train_loss=2.9261637

Batch 207690, train_perplexity=18.655922, train_loss=2.9261637

Batch 207700, train_perplexity=18.655922, train_loss=2.9261637

Batch 207710, train_perplexity=18.655922, train_loss=2.9261637

Batch 207720, train_perplexity=18.655922, train_loss=2.9261637

Batch 207730, train_perplexity=18.655922, train_loss=2.9261637

Batch 207740, train_perplexity=18.655922, train_loss=2.9261637

Batch 207750, train_perplexity=18.655922, train_loss=2.9261637

Batch 207760, train_perplexity=18.655922, train_loss=2.9261637

Batch 207770, train_perplexity=18.655922, train_loss=2.9261637

Batch 207780, train_perplexity=18.655922, train_loss=2.9261637

Batch 207790, train_perplexity=18.655922, train_loss=2.9261637

Batch 207800, train_perplexity=18.655922, train_loss=2.9261637

Batch 207810, train_perplexity=18.655922, train_loss=2.9261637

Batch 207820, train_perplexity=18.655922, train_loss=2.9261637

Batch 207830, train_perplexity=18.655922, train_loss=2.9261637

Batch 207840, train_perplexity=18.655922, train_loss=2.9261637

Batch 207850, train_perplexity=18.655922, train_loss=2.9261637

Batch 207860, train_perplexity=18.655914, train_loss=2.9261632

Batch 207870, train_perplexity=18.655922, train_loss=2.9261637

Batch 207880, train_perplexity=18.655918, train_loss=2.9261634

Batch 207890, train_perplexity=18.655922, train_loss=2.9261637

Batch 207900, train_perplexity=18.655918, train_loss=2.9261634

Batch 207910, train_perplexity=18.655918, train_loss=2.9261634

Batch 207920, train_perplexity=18.655914, train_loss=2.9261632

Batch 207930, train_perplexity=18.655918, train_loss=2.9261634

Batch 207940, train_perplexity=18.655914, train_loss=2.9261632

Batch 207950, train_perplexity=18.655914, train_loss=2.9261632

Batch 207960, train_perplexity=18.655914, train_loss=2.9261632

Batch 207970, train_perplexity=18.655918, train_loss=2.9261634

Batch 207980, train_perplexity=18.655918, train_loss=2.9261634

Batch 207990, train_perplexity=18.655914, train_loss=2.9261632

Batch 208000, train_perplexity=18.655914, train_loss=2.9261632

Batch 208010, train_perplexity=18.655914, train_loss=2.9261632

Batch 208020, train_perplexity=18.655914, train_loss=2.9261632

Batch 208030, train_perplexity=18.655918, train_loss=2.9261634

Batch 208040, train_perplexity=18.655914, train_loss=2.9261632

Batch 208050, train_perplexity=18.655914, train_loss=2.9261632

Batch 208060, train_perplexity=18.655914, train_loss=2.9261632
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 208070, train_perplexity=18.655914, train_loss=2.9261632

Batch 208080, train_perplexity=18.655914, train_loss=2.9261632

Batch 208090, train_perplexity=18.655914, train_loss=2.9261632

Batch 208100, train_perplexity=18.655914, train_loss=2.9261632

Batch 208110, train_perplexity=18.655914, train_loss=2.9261632

Batch 208120, train_perplexity=18.655914, train_loss=2.9261632

Batch 208130, train_perplexity=18.655914, train_loss=2.9261632

Batch 208140, train_perplexity=18.655905, train_loss=2.9261627

Batch 208150, train_perplexity=18.655914, train_loss=2.9261632

Batch 208160, train_perplexity=18.655914, train_loss=2.9261632

Batch 208170, train_perplexity=18.655909, train_loss=2.926163

Batch 208180, train_perplexity=18.655914, train_loss=2.9261632

Batch 208190, train_perplexity=18.655914, train_loss=2.9261632

Batch 208200, train_perplexity=18.655914, train_loss=2.9261632

Batch 208210, train_perplexity=18.655914, train_loss=2.9261632

Batch 208220, train_perplexity=18.655914, train_loss=2.9261632

Batch 208230, train_perplexity=18.655909, train_loss=2.926163

Batch 208240, train_perplexity=18.655914, train_loss=2.9261632

Batch 208250, train_perplexity=18.655905, train_loss=2.9261627

Batch 208260, train_perplexity=18.655905, train_loss=2.9261627

Batch 208270, train_perplexity=18.655905, train_loss=2.9261627

Batch 208280, train_perplexity=18.655905, train_loss=2.9261627

Batch 208290, train_perplexity=18.655914, train_loss=2.9261632

Batch 208300, train_perplexity=18.655905, train_loss=2.9261627

Batch 208310, train_perplexity=18.655905, train_loss=2.9261627

Batch 208320, train_perplexity=18.655905, train_loss=2.9261627

Batch 208330, train_perplexity=18.655905, train_loss=2.9261627

Batch 208340, train_perplexity=18.655905, train_loss=2.9261627

Batch 208350, train_perplexity=18.655905, train_loss=2.9261627

Batch 208360, train_perplexity=18.655905, train_loss=2.9261627

Batch 208370, train_perplexity=18.655905, train_loss=2.9261627

Batch 208380, train_perplexity=18.655905, train_loss=2.9261627

Batch 208390, train_perplexity=18.655905, train_loss=2.9261627

Batch 208400, train_perplexity=18.655905, train_loss=2.9261627

Batch 208410, train_perplexity=18.655905, train_loss=2.9261627

Batch 208420, train_perplexity=18.655905, train_loss=2.9261627

Batch 208430, train_perplexity=18.655901, train_loss=2.9261625

Batch 208440, train_perplexity=18.655905, train_loss=2.9261627

Batch 208450, train_perplexity=18.655905, train_loss=2.9261627

Batch 208460, train_perplexity=18.655905, train_loss=2.9261627

Batch 208470, train_perplexity=18.655905, train_loss=2.9261627

Batch 208480, train_perplexity=18.655905, train_loss=2.9261627

Batch 208490, train_perplexity=18.655901, train_loss=2.9261625

Batch 208500, train_perplexity=18.655905, train_loss=2.9261627

Batch 208510, train_perplexity=18.655901, train_loss=2.9261625

Batch 208520, train_perplexity=18.655905, train_loss=2.9261627

Batch 208530, train_perplexity=18.655895, train_loss=2.9261622

Batch 208540, train_perplexity=18.655905, train_loss=2.9261627

Batch 208550, train_perplexity=18.655895, train_loss=2.9261622

Batch 208560, train_perplexity=18.655895, train_loss=2.9261622

Batch 208570, train_perplexity=18.655895, train_loss=2.9261622

Batch 208580, train_perplexity=18.655901, train_loss=2.9261625

Batch 208590, train_perplexity=18.655895, train_loss=2.9261622

Batch 208600, train_perplexity=18.655901, train_loss=2.9261625

Batch 208610, train_perplexity=18.655895, train_loss=2.9261622

Batch 208620, train_perplexity=18.655895, train_loss=2.9261622

Batch 208630, train_perplexity=18.655895, train_loss=2.9261622

Batch 208640, train_perplexity=18.655895, train_loss=2.9261622

Batch 208650, train_perplexity=18.655895, train_loss=2.9261622

Batch 208660, train_perplexity=18.655895, train_loss=2.9261622

Batch 208670, train_perplexity=18.655901, train_loss=2.9261625

Batch 208680, train_perplexity=18.655895, train_loss=2.9261622

Batch 208690, train_perplexity=18.655895, train_loss=2.9261622

Batch 208700, train_perplexity=18.655895, train_loss=2.9261622

Batch 208710, train_perplexity=18.655891, train_loss=2.926162

Batch 208720, train_perplexity=18.655895, train_loss=2.9261622

Batch 208730, train_perplexity=18.655895, train_loss=2.9261622

Batch 208740, train_perplexity=18.655895, train_loss=2.9261622

Batch 208750, train_perplexity=18.655891, train_loss=2.926162

Batch 208760, train_perplexity=18.655895, train_loss=2.9261622

Batch 208770, train_perplexity=18.655895, train_loss=2.9261622

Batch 208780, train_perplexity=18.655895, train_loss=2.9261622

Batch 208790, train_perplexity=18.655888, train_loss=2.9261618

Batch 208800, train_perplexity=18.655895, train_loss=2.9261622

Batch 208810, train_perplexity=18.655895, train_loss=2.9261622

Batch 208820, train_perplexity=18.655895, train_loss=2.9261622

Batch 208830, train_perplexity=18.655895, train_loss=2.9261622

Batch 208840, train_perplexity=18.655895, train_loss=2.9261622

Batch 208850, train_perplexity=18.655891, train_loss=2.926162

Batch 208860, train_perplexity=18.655895, train_loss=2.9261622

Batch 208870, train_perplexity=18.655891, train_loss=2.926162

Batch 208880, train_perplexity=18.655891, train_loss=2.926162

Batch 208890, train_perplexity=18.655888, train_loss=2.9261618

Batch 208900, train_perplexity=18.655888, train_loss=2.9261618

Batch 208910, train_perplexity=18.655888, train_loss=2.9261618

Batch 208920, train_perplexity=18.655888, train_loss=2.9261618

Batch 208930, train_perplexity=18.655888, train_loss=2.9261618

Batch 208940, train_perplexity=18.655888, train_loss=2.9261618

Batch 208950, train_perplexity=18.655888, train_loss=2.9261618

Batch 208960, train_perplexity=18.655888, train_loss=2.9261618

Batch 208970, train_perplexity=18.655888, train_loss=2.9261618

Batch 208980, train_perplexity=18.655888, train_loss=2.9261618

Batch 208990, train_perplexity=18.655888, train_loss=2.9261618

Batch 209000, train_perplexity=18.655888, train_loss=2.9261618

Batch 209010, train_perplexity=18.655888, train_loss=2.9261618

Batch 209020, train_perplexity=18.655888, train_loss=2.9261618

Batch 209030, train_perplexity=18.655888, train_loss=2.9261618

Batch 209040, train_perplexity=18.655888, train_loss=2.9261618

Batch 209050, train_perplexity=18.655888, train_loss=2.9261618

Batch 209060, train_perplexity=18.655888, train_loss=2.9261618

Batch 209070, train_perplexity=18.655888, train_loss=2.9261618

Batch 209080, train_perplexity=18.655882, train_loss=2.9261615

Batch 209090, train_perplexity=18.655888, train_loss=2.9261618

Batch 209100, train_perplexity=18.655882, train_loss=2.9261615

Batch 209110, train_perplexity=18.655878, train_loss=2.9261613

Batch 209120, train_perplexity=18.655888, train_loss=2.9261618

Batch 209130, train_perplexity=18.655888, train_loss=2.9261618

Batch 209140, train_perplexity=18.655878, train_loss=2.9261613

Batch 209150, train_perplexity=18.655878, train_loss=2.9261613

Batch 209160, train_perplexity=18.655888, train_loss=2.9261618

Batch 209170, train_perplexity=18.655882, train_loss=2.9261615

Batch 209180, train_perplexity=18.655888, train_loss=2.9261618

Batch 209190, train_perplexity=18.655882, train_loss=2.9261615

Batch 209200, train_perplexity=18.655882, train_loss=2.9261615

Batch 209210, train_perplexity=18.655882, train_loss=2.9261615

Batch 209220, train_perplexity=18.655878, train_loss=2.9261613

Batch 209230, train_perplexity=18.655878, train_loss=2.9261613

Batch 209240, train_perplexity=18.655878, train_loss=2.9261613

Batch 209250, train_perplexity=18.655882, train_loss=2.9261615

Batch 209260, train_perplexity=18.655878, train_loss=2.9261613

Batch 209270, train_perplexity=18.655878, train_loss=2.9261613

Batch 209280, train_perplexity=18.655878, train_loss=2.9261613

Batch 209290, train_perplexity=18.655878, train_loss=2.9261613

Batch 209300, train_perplexity=18.655882, train_loss=2.9261615

Batch 209310, train_perplexity=18.655874, train_loss=2.926161

Batch 209320, train_perplexity=18.655878, train_loss=2.9261613

Batch 209330, train_perplexity=18.655878, train_loss=2.9261613

Batch 209340, train_perplexity=18.655878, train_loss=2.9261613

Batch 209350, train_perplexity=18.655878, train_loss=2.9261613
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 209360, train_perplexity=18.655878, train_loss=2.9261613

Batch 209370, train_perplexity=18.655878, train_loss=2.9261613

Batch 209380, train_perplexity=18.655878, train_loss=2.9261613

Batch 209390, train_perplexity=18.655878, train_loss=2.9261613

Batch 209400, train_perplexity=18.655878, train_loss=2.9261613

Batch 209410, train_perplexity=18.655878, train_loss=2.9261613

Batch 209420, train_perplexity=18.655874, train_loss=2.926161

Batch 209430, train_perplexity=18.655878, train_loss=2.9261613

Batch 209440, train_perplexity=18.655878, train_loss=2.9261613

Batch 209450, train_perplexity=18.655878, train_loss=2.9261613

Batch 209460, train_perplexity=18.655869, train_loss=2.9261608

Batch 209470, train_perplexity=18.655874, train_loss=2.926161

Batch 209480, train_perplexity=18.655869, train_loss=2.9261608

Batch 209490, train_perplexity=18.655874, train_loss=2.926161

Batch 209500, train_perplexity=18.655869, train_loss=2.9261608

Batch 209510, train_perplexity=18.655869, train_loss=2.9261608

Batch 209520, train_perplexity=18.655874, train_loss=2.926161

Batch 209530, train_perplexity=18.655869, train_loss=2.9261608

Batch 209540, train_perplexity=18.655869, train_loss=2.9261608

Batch 209550, train_perplexity=18.655869, train_loss=2.9261608

Batch 209560, train_perplexity=18.655869, train_loss=2.9261608

Batch 209570, train_perplexity=18.655869, train_loss=2.9261608

Batch 209580, train_perplexity=18.655869, train_loss=2.9261608

Batch 209590, train_perplexity=18.655869, train_loss=2.9261608

Batch 209600, train_perplexity=18.655869, train_loss=2.9261608

Batch 209610, train_perplexity=18.655874, train_loss=2.926161

Batch 209620, train_perplexity=18.655869, train_loss=2.9261608

Batch 209630, train_perplexity=18.655869, train_loss=2.9261608

Batch 209640, train_perplexity=18.655869, train_loss=2.9261608

Batch 209650, train_perplexity=18.655869, train_loss=2.9261608

Batch 209660, train_perplexity=18.655869, train_loss=2.9261608

Batch 209670, train_perplexity=18.655869, train_loss=2.9261608

Batch 209680, train_perplexity=18.65586, train_loss=2.9261603

Batch 209690, train_perplexity=18.655865, train_loss=2.9261606

Batch 209700, train_perplexity=18.655869, train_loss=2.9261608

Batch 209710, train_perplexity=18.655869, train_loss=2.9261608

Batch 209720, train_perplexity=18.655869, train_loss=2.9261608

Batch 209730, train_perplexity=18.655865, train_loss=2.9261606

Batch 209740, train_perplexity=18.655869, train_loss=2.9261608

Batch 209750, train_perplexity=18.655869, train_loss=2.9261608

Batch 209760, train_perplexity=18.655869, train_loss=2.9261608

Batch 209770, train_perplexity=18.655869, train_loss=2.9261608

Batch 209780, train_perplexity=18.65586, train_loss=2.9261603

Batch 209790, train_perplexity=18.65586, train_loss=2.9261603

Batch 209800, train_perplexity=18.655869, train_loss=2.9261608

Batch 209810, train_perplexity=18.655865, train_loss=2.9261606

Batch 209820, train_perplexity=18.65586, train_loss=2.9261603

Batch 209830, train_perplexity=18.655865, train_loss=2.9261606

Batch 209840, train_perplexity=18.65586, train_loss=2.9261603

Batch 209850, train_perplexity=18.65586, train_loss=2.9261603

Batch 209860, train_perplexity=18.65586, train_loss=2.9261603

Batch 209870, train_perplexity=18.65586, train_loss=2.9261603

Batch 209880, train_perplexity=18.65586, train_loss=2.9261603

Batch 209890, train_perplexity=18.65586, train_loss=2.9261603

Batch 209900, train_perplexity=18.65586, train_loss=2.9261603

Batch 209910, train_perplexity=18.65586, train_loss=2.9261603

Batch 209920, train_perplexity=18.65586, train_loss=2.9261603

Batch 209930, train_perplexity=18.655865, train_loss=2.9261606

Batch 209940, train_perplexity=18.65586, train_loss=2.9261603

Batch 209950, train_perplexity=18.65586, train_loss=2.9261603

Batch 209960, train_perplexity=18.65586, train_loss=2.9261603

Batch 209970, train_perplexity=18.65586, train_loss=2.9261603

Batch 209980, train_perplexity=18.655855, train_loss=2.92616

Batch 209990, train_perplexity=18.65586, train_loss=2.9261603

Batch 210000, train_perplexity=18.65586, train_loss=2.9261603

Batch 210010, train_perplexity=18.655851, train_loss=2.9261599

Batch 210020, train_perplexity=18.655855, train_loss=2.92616

Batch 210030, train_perplexity=18.65586, train_loss=2.9261603

Batch 210040, train_perplexity=18.65586, train_loss=2.9261603

Batch 210050, train_perplexity=18.65586, train_loss=2.9261603

Batch 210060, train_perplexity=18.65586, train_loss=2.9261603

Batch 210070, train_perplexity=18.655851, train_loss=2.9261599

Batch 210080, train_perplexity=18.655855, train_loss=2.92616

Batch 210090, train_perplexity=18.655855, train_loss=2.92616

Batch 210100, train_perplexity=18.655851, train_loss=2.9261599

Batch 210110, train_perplexity=18.655855, train_loss=2.92616

Batch 210120, train_perplexity=18.65586, train_loss=2.9261603

Batch 210130, train_perplexity=18.65586, train_loss=2.9261603

Batch 210140, train_perplexity=18.655851, train_loss=2.9261599

Batch 210150, train_perplexity=18.655855, train_loss=2.92616

Batch 210160, train_perplexity=18.655855, train_loss=2.92616

Batch 210170, train_perplexity=18.655851, train_loss=2.9261599

Batch 210180, train_perplexity=18.655851, train_loss=2.9261599

Batch 210190, train_perplexity=18.655851, train_loss=2.9261599

Batch 210200, train_perplexity=18.655855, train_loss=2.92616

Batch 210210, train_perplexity=18.655851, train_loss=2.9261599

Batch 210220, train_perplexity=18.655851, train_loss=2.9261599

Batch 210230, train_perplexity=18.655851, train_loss=2.9261599

Batch 210240, train_perplexity=18.655851, train_loss=2.9261599

Batch 210250, train_perplexity=18.655851, train_loss=2.9261599

Batch 210260, train_perplexity=18.655851, train_loss=2.9261599

Batch 210270, train_perplexity=18.655848, train_loss=2.9261596

Batch 210280, train_perplexity=18.655851, train_loss=2.9261599

Batch 210290, train_perplexity=18.655851, train_loss=2.9261599

Batch 210300, train_perplexity=18.655851, train_loss=2.9261599

Batch 210310, train_perplexity=18.655851, train_loss=2.9261599

Batch 210320, train_perplexity=18.655851, train_loss=2.9261599

Batch 210330, train_perplexity=18.655851, train_loss=2.9261599

Batch 210340, train_perplexity=18.655851, train_loss=2.9261599

Batch 210350, train_perplexity=18.655842, train_loss=2.9261594

Batch 210360, train_perplexity=18.655851, train_loss=2.9261599

Batch 210370, train_perplexity=18.655842, train_loss=2.9261594

Batch 210380, train_perplexity=18.655848, train_loss=2.9261596

Batch 210390, train_perplexity=18.655848, train_loss=2.9261596

Batch 210400, train_perplexity=18.655848, train_loss=2.9261596

Batch 210410, train_perplexity=18.655848, train_loss=2.9261596

Batch 210420, train_perplexity=18.655848, train_loss=2.9261596

Batch 210430, train_perplexity=18.655842, train_loss=2.9261594

Batch 210440, train_perplexity=18.655842, train_loss=2.9261594

Batch 210450, train_perplexity=18.655842, train_loss=2.9261594

Batch 210460, train_perplexity=18.655842, train_loss=2.9261594

Batch 210470, train_perplexity=18.655842, train_loss=2.9261594

Batch 210480, train_perplexity=18.655842, train_loss=2.9261594

Batch 210490, train_perplexity=18.655842, train_loss=2.9261594

Batch 210500, train_perplexity=18.655848, train_loss=2.9261596

Batch 210510, train_perplexity=18.655848, train_loss=2.9261596

Batch 210520, train_perplexity=18.655848, train_loss=2.9261596

Batch 210530, train_perplexity=18.655848, train_loss=2.9261596

Batch 210540, train_perplexity=18.655842, train_loss=2.9261594

Batch 210550, train_perplexity=18.655842, train_loss=2.9261594

Batch 210560, train_perplexity=18.655842, train_loss=2.9261594

Batch 210570, train_perplexity=18.655842, train_loss=2.9261594

Batch 210580, train_perplexity=18.655842, train_loss=2.9261594

Batch 210590, train_perplexity=18.655842, train_loss=2.9261594

Batch 210600, train_perplexity=18.655842, train_loss=2.9261594

Batch 210610, train_perplexity=18.655842, train_loss=2.9261594

Batch 210620, train_perplexity=18.655842, train_loss=2.9261594

Batch 210630, train_perplexity=18.655842, train_loss=2.9261594

Batch 210640, train_perplexity=18.655842, train_loss=2.9261594
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 210650, train_perplexity=18.655842, train_loss=2.9261594

Batch 210660, train_perplexity=18.655838, train_loss=2.9261591

Batch 210670, train_perplexity=18.655838, train_loss=2.9261591

Batch 210680, train_perplexity=18.655842, train_loss=2.9261594

Batch 210690, train_perplexity=18.655842, train_loss=2.9261594

Batch 210700, train_perplexity=18.655838, train_loss=2.9261591

Batch 210710, train_perplexity=18.655842, train_loss=2.9261594

Batch 210720, train_perplexity=18.655842, train_loss=2.9261594

Batch 210730, train_perplexity=18.655834, train_loss=2.926159

Batch 210740, train_perplexity=18.655834, train_loss=2.926159

Batch 210750, train_perplexity=18.655834, train_loss=2.926159

Batch 210760, train_perplexity=18.655838, train_loss=2.9261591

Batch 210770, train_perplexity=18.655834, train_loss=2.926159

Batch 210780, train_perplexity=18.655842, train_loss=2.9261594

Batch 210790, train_perplexity=18.655834, train_loss=2.926159

Batch 210800, train_perplexity=18.655834, train_loss=2.926159

Batch 210810, train_perplexity=18.655834, train_loss=2.926159

Batch 210820, train_perplexity=18.655842, train_loss=2.9261594

Batch 210830, train_perplexity=18.655842, train_loss=2.9261594

Batch 210840, train_perplexity=18.655834, train_loss=2.926159

Batch 210850, train_perplexity=18.655834, train_loss=2.926159

Batch 210860, train_perplexity=18.655834, train_loss=2.926159

Batch 210870, train_perplexity=18.655834, train_loss=2.926159

Batch 210880, train_perplexity=18.655834, train_loss=2.926159

Batch 210890, train_perplexity=18.655834, train_loss=2.926159

Batch 210900, train_perplexity=18.655834, train_loss=2.926159

Batch 210910, train_perplexity=18.655834, train_loss=2.926159

Batch 210920, train_perplexity=18.655834, train_loss=2.926159

Batch 210930, train_perplexity=18.655834, train_loss=2.926159

Batch 210940, train_perplexity=18.655834, train_loss=2.926159

Batch 210950, train_perplexity=18.655834, train_loss=2.926159

Batch 210960, train_perplexity=18.655834, train_loss=2.926159

Batch 210970, train_perplexity=18.655828, train_loss=2.9261587

Batch 210980, train_perplexity=18.655834, train_loss=2.926159

Batch 210990, train_perplexity=18.655828, train_loss=2.9261587

Batch 211000, train_perplexity=18.655828, train_loss=2.9261587

Batch 211010, train_perplexity=18.655834, train_loss=2.926159

Batch 211020, train_perplexity=18.655828, train_loss=2.9261587

Batch 211030, train_perplexity=18.655834, train_loss=2.926159

Batch 211040, train_perplexity=18.655825, train_loss=2.9261584

Batch 211050, train_perplexity=18.655825, train_loss=2.9261584

Batch 211060, train_perplexity=18.655825, train_loss=2.9261584

Batch 211070, train_perplexity=18.655828, train_loss=2.9261587

Batch 211080, train_perplexity=18.655828, train_loss=2.9261587

Batch 211090, train_perplexity=18.655828, train_loss=2.9261587

Batch 211100, train_perplexity=18.655825, train_loss=2.9261584

Batch 211110, train_perplexity=18.655825, train_loss=2.9261584

Batch 211120, train_perplexity=18.655825, train_loss=2.9261584

Batch 211130, train_perplexity=18.655825, train_loss=2.9261584

Batch 211140, train_perplexity=18.655825, train_loss=2.9261584

Batch 211150, train_perplexity=18.655825, train_loss=2.9261584

Batch 211160, train_perplexity=18.655825, train_loss=2.9261584

Batch 211170, train_perplexity=18.655825, train_loss=2.9261584

Batch 211180, train_perplexity=18.655825, train_loss=2.9261584

Batch 211190, train_perplexity=18.655828, train_loss=2.9261587

Batch 211200, train_perplexity=18.655825, train_loss=2.9261584

Batch 211210, train_perplexity=18.655825, train_loss=2.9261584

Batch 211220, train_perplexity=18.655825, train_loss=2.9261584

Batch 211230, train_perplexity=18.655825, train_loss=2.9261584

Batch 211240, train_perplexity=18.65582, train_loss=2.9261582

Batch 211250, train_perplexity=18.655825, train_loss=2.9261584

Batch 211260, train_perplexity=18.655825, train_loss=2.9261584

Batch 211270, train_perplexity=18.655825, train_loss=2.9261584

Batch 211280, train_perplexity=18.655825, train_loss=2.9261584

Batch 211290, train_perplexity=18.655825, train_loss=2.9261584

Batch 211300, train_perplexity=18.655825, train_loss=2.9261584

Batch 211310, train_perplexity=18.655815, train_loss=2.926158

Batch 211320, train_perplexity=18.655825, train_loss=2.9261584

Batch 211330, train_perplexity=18.65582, train_loss=2.9261582

Batch 211340, train_perplexity=18.65582, train_loss=2.9261582

Batch 211350, train_perplexity=18.655825, train_loss=2.9261584

Batch 211360, train_perplexity=18.65582, train_loss=2.9261582

Batch 211370, train_perplexity=18.655815, train_loss=2.926158

Batch 211380, train_perplexity=18.655825, train_loss=2.9261584

Batch 211390, train_perplexity=18.655815, train_loss=2.926158

Batch 211400, train_perplexity=18.655815, train_loss=2.926158

Batch 211410, train_perplexity=18.65582, train_loss=2.9261582

Batch 211420, train_perplexity=18.655815, train_loss=2.926158

Batch 211430, train_perplexity=18.655825, train_loss=2.9261584

Batch 211440, train_perplexity=18.655815, train_loss=2.926158

Batch 211450, train_perplexity=18.65582, train_loss=2.9261582

Batch 211460, train_perplexity=18.655815, train_loss=2.926158

Batch 211470, train_perplexity=18.655815, train_loss=2.926158

Batch 211480, train_perplexity=18.655815, train_loss=2.926158

Batch 211490, train_perplexity=18.655815, train_loss=2.926158

Batch 211500, train_perplexity=18.655815, train_loss=2.926158

Batch 211510, train_perplexity=18.655815, train_loss=2.926158

Batch 211520, train_perplexity=18.655815, train_loss=2.926158

Batch 211530, train_perplexity=18.655815, train_loss=2.926158

Batch 211540, train_perplexity=18.655815, train_loss=2.926158

Batch 211550, train_perplexity=18.655815, train_loss=2.926158

Batch 211560, train_perplexity=18.655815, train_loss=2.926158

Batch 211570, train_perplexity=18.655815, train_loss=2.926158

Batch 211580, train_perplexity=18.655815, train_loss=2.926158

Batch 211590, train_perplexity=18.655815, train_loss=2.926158

Batch 211600, train_perplexity=18.655815, train_loss=2.926158

Batch 211610, train_perplexity=18.655815, train_loss=2.926158

Batch 211620, train_perplexity=18.655815, train_loss=2.926158

Batch 211630, train_perplexity=18.655811, train_loss=2.9261577

Batch 211640, train_perplexity=18.655807, train_loss=2.9261575

Batch 211650, train_perplexity=18.655815, train_loss=2.926158

Batch 211660, train_perplexity=18.655815, train_loss=2.926158

Batch 211670, train_perplexity=18.655811, train_loss=2.9261577

Batch 211680, train_perplexity=18.655807, train_loss=2.9261575

Batch 211690, train_perplexity=18.655807, train_loss=2.9261575

Batch 211700, train_perplexity=18.655807, train_loss=2.9261575

Batch 211710, train_perplexity=18.655807, train_loss=2.9261575

Batch 211720, train_perplexity=18.655807, train_loss=2.9261575

Batch 211730, train_perplexity=18.655807, train_loss=2.9261575

Batch 211740, train_perplexity=18.655807, train_loss=2.9261575

Batch 211750, train_perplexity=18.655807, train_loss=2.9261575

Batch 211760, train_perplexity=18.655807, train_loss=2.9261575

Batch 211770, train_perplexity=18.655807, train_loss=2.9261575

Batch 211780, train_perplexity=18.655807, train_loss=2.9261575

Batch 211790, train_perplexity=18.655807, train_loss=2.9261575

Batch 211800, train_perplexity=18.655811, train_loss=2.9261577

Batch 211810, train_perplexity=18.655807, train_loss=2.9261575

Batch 211820, train_perplexity=18.655807, train_loss=2.9261575

Batch 211830, train_perplexity=18.655807, train_loss=2.9261575

Batch 211840, train_perplexity=18.655804, train_loss=2.9261572

Batch 211850, train_perplexity=18.655807, train_loss=2.9261575

Batch 211860, train_perplexity=18.655807, train_loss=2.9261575

Batch 211870, train_perplexity=18.655807, train_loss=2.9261575

Batch 211880, train_perplexity=18.655807, train_loss=2.9261575

Batch 211890, train_perplexity=18.655807, train_loss=2.9261575

Batch 211900, train_perplexity=18.655807, train_loss=2.9261575

Batch 211910, train_perplexity=18.655807, train_loss=2.9261575

Batch 211920, train_perplexity=18.655807, train_loss=2.9261575

Batch 211930, train_perplexity=18.655807, train_loss=2.9261575
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 211940, train_perplexity=18.655798, train_loss=2.926157

Batch 211950, train_perplexity=18.655804, train_loss=2.9261572

Batch 211960, train_perplexity=18.655804, train_loss=2.9261572

Batch 211970, train_perplexity=18.655798, train_loss=2.926157

Batch 211980, train_perplexity=18.655807, train_loss=2.9261575

Batch 211990, train_perplexity=18.655798, train_loss=2.926157

Batch 212000, train_perplexity=18.655804, train_loss=2.9261572

Batch 212010, train_perplexity=18.655807, train_loss=2.9261575

Batch 212020, train_perplexity=18.655804, train_loss=2.9261572

Batch 212030, train_perplexity=18.655807, train_loss=2.9261575

Batch 212040, train_perplexity=18.655798, train_loss=2.926157

Batch 212050, train_perplexity=18.655798, train_loss=2.926157

Batch 212060, train_perplexity=18.655798, train_loss=2.926157

Batch 212070, train_perplexity=18.655804, train_loss=2.9261572

Batch 212080, train_perplexity=18.655798, train_loss=2.926157

Batch 212090, train_perplexity=18.655798, train_loss=2.926157

Batch 212100, train_perplexity=18.655798, train_loss=2.926157

Batch 212110, train_perplexity=18.655798, train_loss=2.926157

Batch 212120, train_perplexity=18.655798, train_loss=2.926157

Batch 212130, train_perplexity=18.655798, train_loss=2.926157

Batch 212140, train_perplexity=18.655798, train_loss=2.926157

Batch 212150, train_perplexity=18.655798, train_loss=2.926157

Batch 212160, train_perplexity=18.655798, train_loss=2.926157

Batch 212170, train_perplexity=18.655798, train_loss=2.926157

Batch 212180, train_perplexity=18.655798, train_loss=2.926157

Batch 212190, train_perplexity=18.655798, train_loss=2.926157

Batch 212200, train_perplexity=18.655798, train_loss=2.926157

Batch 212210, train_perplexity=18.655798, train_loss=2.926157

Batch 212220, train_perplexity=18.655798, train_loss=2.926157

Batch 212230, train_perplexity=18.655798, train_loss=2.926157

Batch 212240, train_perplexity=18.655798, train_loss=2.926157

Batch 212250, train_perplexity=18.65579, train_loss=2.9261565

Batch 212260, train_perplexity=18.655794, train_loss=2.9261568

Batch 212270, train_perplexity=18.655794, train_loss=2.9261568

Batch 212280, train_perplexity=18.655794, train_loss=2.9261568

Batch 212290, train_perplexity=18.65579, train_loss=2.9261565

Batch 212300, train_perplexity=18.65579, train_loss=2.9261565

Batch 212310, train_perplexity=18.65579, train_loss=2.9261565

Batch 212320, train_perplexity=18.655794, train_loss=2.9261568

Batch 212330, train_perplexity=18.65579, train_loss=2.9261565

Batch 212340, train_perplexity=18.65579, train_loss=2.9261565

Batch 212350, train_perplexity=18.65579, train_loss=2.9261565

Batch 212360, train_perplexity=18.65579, train_loss=2.9261565

Batch 212370, train_perplexity=18.65579, train_loss=2.9261565

Batch 212380, train_perplexity=18.65579, train_loss=2.9261565

Batch 212390, train_perplexity=18.65579, train_loss=2.9261565

Batch 212400, train_perplexity=18.65579, train_loss=2.9261565

Batch 212410, train_perplexity=18.65579, train_loss=2.9261565

Batch 212420, train_perplexity=18.65579, train_loss=2.9261565

Batch 212430, train_perplexity=18.65579, train_loss=2.9261565

Batch 212440, train_perplexity=18.65579, train_loss=2.9261565

Batch 212450, train_perplexity=18.65579, train_loss=2.9261565

Batch 212460, train_perplexity=18.65579, train_loss=2.9261565

Batch 212470, train_perplexity=18.65579, train_loss=2.9261565

Batch 212480, train_perplexity=18.65579, train_loss=2.9261565

Batch 212490, train_perplexity=18.65579, train_loss=2.9261565

Batch 212500, train_perplexity=18.65579, train_loss=2.9261565

Batch 212510, train_perplexity=18.65579, train_loss=2.9261565

Batch 212520, train_perplexity=18.655785, train_loss=2.9261563

Batch 212530, train_perplexity=18.65579, train_loss=2.9261565

Batch 212540, train_perplexity=18.655785, train_loss=2.9261563

Batch 212550, train_perplexity=18.65578, train_loss=2.926156

Batch 212560, train_perplexity=18.65579, train_loss=2.9261565

Batch 212570, train_perplexity=18.65579, train_loss=2.9261565

Batch 212580, train_perplexity=18.65578, train_loss=2.926156

Batch 212590, train_perplexity=18.65579, train_loss=2.9261565

Batch 212600, train_perplexity=18.65579, train_loss=2.9261565

Batch 212610, train_perplexity=18.655785, train_loss=2.9261563

Batch 212620, train_perplexity=18.655785, train_loss=2.9261563

Batch 212630, train_perplexity=18.65578, train_loss=2.926156

Batch 212640, train_perplexity=18.655785, train_loss=2.9261563

Batch 212650, train_perplexity=18.65578, train_loss=2.926156

Batch 212660, train_perplexity=18.65578, train_loss=2.926156

Batch 212670, train_perplexity=18.65578, train_loss=2.926156

Batch 212680, train_perplexity=18.65578, train_loss=2.926156

Batch 212690, train_perplexity=18.65578, train_loss=2.926156

Batch 212700, train_perplexity=18.65578, train_loss=2.926156

Batch 212710, train_perplexity=18.65578, train_loss=2.926156

Batch 212720, train_perplexity=18.65578, train_loss=2.926156

Batch 212730, train_perplexity=18.65578, train_loss=2.926156

Batch 212740, train_perplexity=18.65578, train_loss=2.926156

Batch 212750, train_perplexity=18.65578, train_loss=2.926156

Batch 212760, train_perplexity=18.65578, train_loss=2.926156

Batch 212770, train_perplexity=18.65578, train_loss=2.926156

Batch 212780, train_perplexity=18.65578, train_loss=2.926156

Batch 212790, train_perplexity=18.65578, train_loss=2.926156

Batch 212800, train_perplexity=18.65578, train_loss=2.926156

Batch 212810, train_perplexity=18.65578, train_loss=2.926156

Batch 212820, train_perplexity=18.65578, train_loss=2.926156

Batch 212830, train_perplexity=18.65578, train_loss=2.926156

Batch 212840, train_perplexity=18.65578, train_loss=2.926156

Batch 212850, train_perplexity=18.655771, train_loss=2.9261556

Batch 212860, train_perplexity=18.65578, train_loss=2.926156

Batch 212870, train_perplexity=18.65578, train_loss=2.926156

Batch 212880, train_perplexity=18.65578, train_loss=2.926156

Batch 212890, train_perplexity=18.65578, train_loss=2.926156

Batch 212900, train_perplexity=18.65578, train_loss=2.926156

Batch 212910, train_perplexity=18.655777, train_loss=2.9261558

Batch 212920, train_perplexity=18.655777, train_loss=2.9261558

Batch 212930, train_perplexity=18.655777, train_loss=2.9261558

Batch 212940, train_perplexity=18.655771, train_loss=2.9261556

Batch 212950, train_perplexity=18.655771, train_loss=2.9261556

Batch 212960, train_perplexity=18.655771, train_loss=2.9261556

Batch 212970, train_perplexity=18.655777, train_loss=2.9261558

Batch 212980, train_perplexity=18.655777, train_loss=2.9261558

Batch 212990, train_perplexity=18.655771, train_loss=2.9261556

Batch 213000, train_perplexity=18.655771, train_loss=2.9261556

Batch 213010, train_perplexity=18.655771, train_loss=2.9261556

Batch 213020, train_perplexity=18.655777, train_loss=2.9261558

Batch 213030, train_perplexity=18.655771, train_loss=2.9261556

Batch 213040, train_perplexity=18.655771, train_loss=2.9261556

Batch 213050, train_perplexity=18.655771, train_loss=2.9261556

Batch 213060, train_perplexity=18.655777, train_loss=2.9261558

Batch 213070, train_perplexity=18.655777, train_loss=2.9261558

Batch 213080, train_perplexity=18.655777, train_loss=2.9261558

Batch 213090, train_perplexity=18.655771, train_loss=2.9261556

Batch 213100, train_perplexity=18.655771, train_loss=2.9261556

Batch 213110, train_perplexity=18.655771, train_loss=2.9261556

Batch 213120, train_perplexity=18.655771, train_loss=2.9261556

Batch 213130, train_perplexity=18.655767, train_loss=2.9261553

Batch 213140, train_perplexity=18.655771, train_loss=2.9261556

Batch 213150, train_perplexity=18.655771, train_loss=2.9261556

Batch 213160, train_perplexity=18.655767, train_loss=2.9261553

Batch 213170, train_perplexity=18.655767, train_loss=2.9261553

Batch 213180, train_perplexity=18.655771, train_loss=2.9261556

Batch 213190, train_perplexity=18.655771, train_loss=2.9261556

Batch 213200, train_perplexity=18.655767, train_loss=2.9261553

Batch 213210, train_perplexity=18.655767, train_loss=2.9261553

Batch 213220, train_perplexity=18.655767, train_loss=2.9261553

Batch 213230, train_perplexity=18.655767, train_loss=2.9261553
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 213240, train_perplexity=18.655771, train_loss=2.9261556

Batch 213250, train_perplexity=18.655767, train_loss=2.9261553

Batch 213260, train_perplexity=18.655764, train_loss=2.926155

Batch 213270, train_perplexity=18.655764, train_loss=2.926155

Batch 213280, train_perplexity=18.655764, train_loss=2.926155

Batch 213290, train_perplexity=18.655767, train_loss=2.9261553

Batch 213300, train_perplexity=18.655771, train_loss=2.9261556

Batch 213310, train_perplexity=18.655764, train_loss=2.926155

Batch 213320, train_perplexity=18.655764, train_loss=2.926155

Batch 213330, train_perplexity=18.655764, train_loss=2.926155

Batch 213340, train_perplexity=18.655764, train_loss=2.926155

Batch 213350, train_perplexity=18.655764, train_loss=2.926155

Batch 213360, train_perplexity=18.655764, train_loss=2.926155

Batch 213370, train_perplexity=18.655764, train_loss=2.926155

Batch 213380, train_perplexity=18.655764, train_loss=2.926155

Batch 213390, train_perplexity=18.655764, train_loss=2.926155

Batch 213400, train_perplexity=18.655764, train_loss=2.926155

Batch 213410, train_perplexity=18.655754, train_loss=2.9261546

Batch 213420, train_perplexity=18.655754, train_loss=2.9261546

Batch 213430, train_perplexity=18.655764, train_loss=2.926155

Batch 213440, train_perplexity=18.655764, train_loss=2.926155

Batch 213450, train_perplexity=18.655758, train_loss=2.9261549

Batch 213460, train_perplexity=18.655764, train_loss=2.926155

Batch 213470, train_perplexity=18.655758, train_loss=2.9261549

Batch 213480, train_perplexity=18.655764, train_loss=2.926155

Batch 213490, train_perplexity=18.655754, train_loss=2.9261546

Batch 213500, train_perplexity=18.655764, train_loss=2.926155

Batch 213510, train_perplexity=18.655764, train_loss=2.926155

Batch 213520, train_perplexity=18.655764, train_loss=2.926155

Batch 213530, train_perplexity=18.655764, train_loss=2.926155

Batch 213540, train_perplexity=18.655758, train_loss=2.9261549

Batch 213550, train_perplexity=18.655754, train_loss=2.9261546

Batch 213560, train_perplexity=18.655764, train_loss=2.926155

Batch 213570, train_perplexity=18.655764, train_loss=2.926155

Batch 213580, train_perplexity=18.655764, train_loss=2.926155

Batch 213590, train_perplexity=18.655754, train_loss=2.9261546

Batch 213600, train_perplexity=18.655754, train_loss=2.9261546

Batch 213610, train_perplexity=18.655758, train_loss=2.9261549

Batch 213620, train_perplexity=18.655754, train_loss=2.9261546

Batch 213630, train_perplexity=18.655754, train_loss=2.9261546

Batch 213640, train_perplexity=18.655754, train_loss=2.9261546

Batch 213650, train_perplexity=18.655754, train_loss=2.9261546

Batch 213660, train_perplexity=18.655754, train_loss=2.9261546

Batch 213670, train_perplexity=18.655754, train_loss=2.9261546

Batch 213680, train_perplexity=18.655754, train_loss=2.9261546

Batch 213690, train_perplexity=18.655754, train_loss=2.9261546

Batch 213700, train_perplexity=18.655754, train_loss=2.9261546

Batch 213710, train_perplexity=18.655754, train_loss=2.9261546

Batch 213720, train_perplexity=18.655754, train_loss=2.9261546

Batch 213730, train_perplexity=18.655754, train_loss=2.9261546

Batch 213740, train_perplexity=18.655754, train_loss=2.9261546

Batch 213750, train_perplexity=18.65575, train_loss=2.9261544

Batch 213760, train_perplexity=18.65575, train_loss=2.9261544

Batch 213770, train_perplexity=18.655754, train_loss=2.9261546

Batch 213780, train_perplexity=18.655754, train_loss=2.9261546

Batch 213790, train_perplexity=18.655754, train_loss=2.9261546

Batch 213800, train_perplexity=18.65575, train_loss=2.9261544

Batch 213810, train_perplexity=18.655745, train_loss=2.9261541

Batch 213820, train_perplexity=18.655745, train_loss=2.9261541

Batch 213830, train_perplexity=18.655754, train_loss=2.9261546

Batch 213840, train_perplexity=18.65575, train_loss=2.9261544

Batch 213850, train_perplexity=18.655754, train_loss=2.9261546

Batch 213860, train_perplexity=18.65575, train_loss=2.9261544

Batch 213870, train_perplexity=18.65575, train_loss=2.9261544

Batch 213880, train_perplexity=18.655754, train_loss=2.9261546

Batch 213890, train_perplexity=18.65575, train_loss=2.9261544

Batch 213900, train_perplexity=18.655745, train_loss=2.9261541

Batch 213910, train_perplexity=18.655745, train_loss=2.9261541

Batch 213920, train_perplexity=18.65575, train_loss=2.9261544

Batch 213930, train_perplexity=18.65575, train_loss=2.9261544

Batch 213940, train_perplexity=18.655745, train_loss=2.9261541

Batch 213950, train_perplexity=18.655745, train_loss=2.9261541

Batch 213960, train_perplexity=18.655745, train_loss=2.9261541

Batch 213970, train_perplexity=18.655745, train_loss=2.9261541

Batch 213980, train_perplexity=18.655745, train_loss=2.9261541

Batch 213990, train_perplexity=18.655745, train_loss=2.9261541

Batch 214000, train_perplexity=18.655745, train_loss=2.9261541

Batch 214010, train_perplexity=18.655745, train_loss=2.9261541

Batch 214020, train_perplexity=18.655745, train_loss=2.9261541

Batch 214030, train_perplexity=18.655745, train_loss=2.9261541

Batch 214040, train_perplexity=18.655745, train_loss=2.9261541

Batch 214050, train_perplexity=18.655745, train_loss=2.9261541

Batch 214060, train_perplexity=18.65574, train_loss=2.926154

Batch 214070, train_perplexity=18.655745, train_loss=2.9261541

Batch 214080, train_perplexity=18.655745, train_loss=2.9261541

Batch 214090, train_perplexity=18.655745, train_loss=2.9261541

Batch 214100, train_perplexity=18.655745, train_loss=2.9261541

Batch 214110, train_perplexity=18.65574, train_loss=2.926154

Batch 214120, train_perplexity=18.655745, train_loss=2.9261541

Batch 214130, train_perplexity=18.655737, train_loss=2.9261537

Batch 214140, train_perplexity=18.655737, train_loss=2.9261537

Batch 214150, train_perplexity=18.655745, train_loss=2.9261541

Batch 214160, train_perplexity=18.655737, train_loss=2.9261537

Batch 214170, train_perplexity=18.655745, train_loss=2.9261541

Batch 214180, train_perplexity=18.655745, train_loss=2.9261541

Batch 214190, train_perplexity=18.655737, train_loss=2.9261537

Batch 214200, train_perplexity=18.65574, train_loss=2.926154

Batch 214210, train_perplexity=18.655737, train_loss=2.9261537

Batch 214220, train_perplexity=18.655737, train_loss=2.9261537

Batch 214230, train_perplexity=18.655737, train_loss=2.9261537

Batch 214240, train_perplexity=18.655737, train_loss=2.9261537

Batch 214250, train_perplexity=18.655737, train_loss=2.9261537

Batch 214260, train_perplexity=18.655737, train_loss=2.9261537

Batch 214270, train_perplexity=18.655737, train_loss=2.9261537

Batch 214280, train_perplexity=18.655737, train_loss=2.9261537

Batch 214290, train_perplexity=18.655737, train_loss=2.9261537

Batch 214300, train_perplexity=18.655737, train_loss=2.9261537

Batch 214310, train_perplexity=18.655737, train_loss=2.9261537

Batch 214320, train_perplexity=18.655737, train_loss=2.9261537

Batch 214330, train_perplexity=18.655737, train_loss=2.9261537

Batch 214340, train_perplexity=18.655737, train_loss=2.9261537

Batch 214350, train_perplexity=18.655731, train_loss=2.9261534

Batch 214360, train_perplexity=18.655737, train_loss=2.9261537

Batch 214370, train_perplexity=18.655737, train_loss=2.9261537

Batch 214380, train_perplexity=18.655737, train_loss=2.9261537

Batch 214390, train_perplexity=18.655731, train_loss=2.9261534

Batch 214400, train_perplexity=18.655731, train_loss=2.9261534

Batch 214410, train_perplexity=18.655731, train_loss=2.9261534

Batch 214420, train_perplexity=18.655737, train_loss=2.9261537

Batch 214430, train_perplexity=18.655737, train_loss=2.9261537

Batch 214440, train_perplexity=18.655737, train_loss=2.9261537

Batch 214450, train_perplexity=18.655731, train_loss=2.9261534

Batch 214460, train_perplexity=18.655731, train_loss=2.9261534

Batch 214470, train_perplexity=18.655737, train_loss=2.9261537

Batch 214480, train_perplexity=18.655731, train_loss=2.9261534

Batch 214490, train_perplexity=18.655727, train_loss=2.9261532

Batch 214500, train_perplexity=18.655737, train_loss=2.9261537

Batch 214510, train_perplexity=18.655727, train_loss=2.9261532

Batch 214520, train_perplexity=18.655727, train_loss=2.9261532
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 214530, train_perplexity=18.655727, train_loss=2.9261532

Batch 214540, train_perplexity=18.655727, train_loss=2.9261532

Batch 214550, train_perplexity=18.655727, train_loss=2.9261532

Batch 214560, train_perplexity=18.655727, train_loss=2.9261532

Batch 214570, train_perplexity=18.655727, train_loss=2.9261532

Batch 214580, train_perplexity=18.655727, train_loss=2.9261532

Batch 214590, train_perplexity=18.655727, train_loss=2.9261532

Batch 214600, train_perplexity=18.655727, train_loss=2.9261532

Batch 214610, train_perplexity=18.655727, train_loss=2.9261532

Batch 214620, train_perplexity=18.655727, train_loss=2.9261532

Batch 214630, train_perplexity=18.655727, train_loss=2.9261532

Batch 214640, train_perplexity=18.655727, train_loss=2.9261532

Batch 214650, train_perplexity=18.655727, train_loss=2.9261532

Batch 214660, train_perplexity=18.655727, train_loss=2.9261532

Batch 214670, train_perplexity=18.655727, train_loss=2.9261532

Batch 214680, train_perplexity=18.655718, train_loss=2.9261527

Batch 214690, train_perplexity=18.655727, train_loss=2.9261532

Batch 214700, train_perplexity=18.655718, train_loss=2.9261527

Batch 214710, train_perplexity=18.655727, train_loss=2.9261532

Batch 214720, train_perplexity=18.655727, train_loss=2.9261532

Batch 214730, train_perplexity=18.655718, train_loss=2.9261527

Batch 214740, train_perplexity=18.655718, train_loss=2.9261527

Batch 214750, train_perplexity=18.655724, train_loss=2.926153

Batch 214760, train_perplexity=18.655727, train_loss=2.9261532

Batch 214770, train_perplexity=18.655724, train_loss=2.926153

Batch 214780, train_perplexity=18.655727, train_loss=2.9261532

Batch 214790, train_perplexity=18.655718, train_loss=2.9261527

Batch 214800, train_perplexity=18.655718, train_loss=2.9261527

Batch 214810, train_perplexity=18.655718, train_loss=2.9261527

Batch 214820, train_perplexity=18.655718, train_loss=2.9261527

Batch 214830, train_perplexity=18.655724, train_loss=2.926153

Batch 214840, train_perplexity=18.655718, train_loss=2.9261527

Batch 214850, train_perplexity=18.655718, train_loss=2.9261527

Batch 214860, train_perplexity=18.655718, train_loss=2.9261527

Batch 214870, train_perplexity=18.655718, train_loss=2.9261527

Batch 214880, train_perplexity=18.655724, train_loss=2.926153

Batch 214890, train_perplexity=18.655718, train_loss=2.9261527

Batch 214900, train_perplexity=18.655718, train_loss=2.9261527

Batch 214910, train_perplexity=18.655718, train_loss=2.9261527

Batch 214920, train_perplexity=18.655718, train_loss=2.9261527

Batch 214930, train_perplexity=18.655718, train_loss=2.9261527

Batch 214940, train_perplexity=18.655718, train_loss=2.9261527

Batch 214950, train_perplexity=18.655718, train_loss=2.9261527

Batch 214960, train_perplexity=18.655718, train_loss=2.9261527

Batch 214970, train_perplexity=18.655718, train_loss=2.9261527

Batch 214980, train_perplexity=18.655718, train_loss=2.9261527

Batch 214990, train_perplexity=18.655718, train_loss=2.9261527

Batch 215000, train_perplexity=18.655718, train_loss=2.9261527

Batch 215010, train_perplexity=18.655714, train_loss=2.9261525

Batch 215020, train_perplexity=18.655714, train_loss=2.9261525

Batch 215030, train_perplexity=18.655714, train_loss=2.9261525

Batch 215040, train_perplexity=18.655714, train_loss=2.9261525

Batch 215050, train_perplexity=18.655718, train_loss=2.9261527

Batch 215060, train_perplexity=18.655718, train_loss=2.9261527

Batch 215070, train_perplexity=18.655718, train_loss=2.9261527

Batch 215080, train_perplexity=18.655714, train_loss=2.9261525

Batch 215090, train_perplexity=18.65571, train_loss=2.9261522

Batch 215100, train_perplexity=18.655718, train_loss=2.9261527

Batch 215110, train_perplexity=18.65571, train_loss=2.9261522

Batch 215120, train_perplexity=18.655714, train_loss=2.9261525

Batch 215130, train_perplexity=18.65571, train_loss=2.9261522

Batch 215140, train_perplexity=18.65571, train_loss=2.9261522

Batch 215150, train_perplexity=18.65571, train_loss=2.9261522

Batch 215160, train_perplexity=18.655714, train_loss=2.9261525

Batch 215170, train_perplexity=18.65571, train_loss=2.9261522

Batch 215180, train_perplexity=18.65571, train_loss=2.9261522

Batch 215190, train_perplexity=18.65571, train_loss=2.9261522

Batch 215200, train_perplexity=18.65571, train_loss=2.9261522

Batch 215210, train_perplexity=18.65571, train_loss=2.9261522

Batch 215220, train_perplexity=18.65571, train_loss=2.9261522

Batch 215230, train_perplexity=18.65571, train_loss=2.9261522

Batch 215240, train_perplexity=18.65571, train_loss=2.9261522

Batch 215250, train_perplexity=18.65571, train_loss=2.9261522

Batch 215260, train_perplexity=18.65571, train_loss=2.9261522

Batch 215270, train_perplexity=18.65571, train_loss=2.9261522

Batch 215280, train_perplexity=18.65571, train_loss=2.9261522

Batch 215290, train_perplexity=18.65571, train_loss=2.9261522

Batch 215300, train_perplexity=18.65571, train_loss=2.9261522

Batch 215310, train_perplexity=18.65571, train_loss=2.9261522

Batch 215320, train_perplexity=18.65571, train_loss=2.9261522

Batch 215330, train_perplexity=18.65571, train_loss=2.9261522

Batch 215340, train_perplexity=18.65571, train_loss=2.9261522

Batch 215350, train_perplexity=18.65571, train_loss=2.9261522

Batch 215360, train_perplexity=18.65571, train_loss=2.9261522

Batch 215370, train_perplexity=18.65571, train_loss=2.9261522

Batch 215380, train_perplexity=18.65571, train_loss=2.9261522

Batch 215390, train_perplexity=18.65571, train_loss=2.9261522

Batch 215400, train_perplexity=18.65571, train_loss=2.9261522

Batch 215410, train_perplexity=18.655704, train_loss=2.926152

Batch 215420, train_perplexity=18.6557, train_loss=2.9261518

Batch 215430, train_perplexity=18.655704, train_loss=2.926152

Batch 215440, train_perplexity=18.6557, train_loss=2.9261518

Batch 215450, train_perplexity=18.6557, train_loss=2.9261518

Batch 215460, train_perplexity=18.655704, train_loss=2.926152

Batch 215470, train_perplexity=18.6557, train_loss=2.9261518

Batch 215480, train_perplexity=18.6557, train_loss=2.9261518

Batch 215490, train_perplexity=18.6557, train_loss=2.9261518

Batch 215500, train_perplexity=18.6557, train_loss=2.9261518

Batch 215510, train_perplexity=18.655704, train_loss=2.926152

Batch 215520, train_perplexity=18.6557, train_loss=2.9261518

Batch 215530, train_perplexity=18.6557, train_loss=2.9261518

Batch 215540, train_perplexity=18.6557, train_loss=2.9261518

Batch 215550, train_perplexity=18.6557, train_loss=2.9261518

Batch 215560, train_perplexity=18.6557, train_loss=2.9261518

Batch 215570, train_perplexity=18.6557, train_loss=2.9261518

Batch 215580, train_perplexity=18.6557, train_loss=2.9261518

Batch 215590, train_perplexity=18.6557, train_loss=2.9261518

Batch 215600, train_perplexity=18.6557, train_loss=2.9261518

Batch 215610, train_perplexity=18.6557, train_loss=2.9261518

Batch 215620, train_perplexity=18.6557, train_loss=2.9261518

Batch 215630, train_perplexity=18.6557, train_loss=2.9261518

Batch 215640, train_perplexity=18.6557, train_loss=2.9261518

Batch 215650, train_perplexity=18.6557, train_loss=2.9261518

Batch 215660, train_perplexity=18.6557, train_loss=2.9261518

Batch 215670, train_perplexity=18.655697, train_loss=2.9261515

Batch 215680, train_perplexity=18.655697, train_loss=2.9261515

Batch 215690, train_perplexity=18.655697, train_loss=2.9261515

Batch 215700, train_perplexity=18.655697, train_loss=2.9261515

Batch 215710, train_perplexity=18.6557, train_loss=2.9261518

Batch 215720, train_perplexity=18.655691, train_loss=2.9261513

Batch 215730, train_perplexity=18.655697, train_loss=2.9261515

Batch 215740, train_perplexity=18.655691, train_loss=2.9261513

Batch 215750, train_perplexity=18.655691, train_loss=2.9261513

Batch 215760, train_perplexity=18.6557, train_loss=2.9261518

Batch 215770, train_perplexity=18.6557, train_loss=2.9261518

Batch 215780, train_perplexity=18.655691, train_loss=2.9261513

Batch 215790, train_perplexity=18.655691, train_loss=2.9261513

Batch 215800, train_perplexity=18.655691, train_loss=2.9261513

Batch 215810, train_perplexity=18.655691, train_loss=2.9261513

Batch 215820, train_perplexity=18.655691, train_loss=2.9261513
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 215830, train_perplexity=18.655691, train_loss=2.9261513

Batch 215840, train_perplexity=18.655691, train_loss=2.9261513

Batch 215850, train_perplexity=18.655691, train_loss=2.9261513

Batch 215860, train_perplexity=18.655697, train_loss=2.9261515

Batch 215870, train_perplexity=18.655691, train_loss=2.9261513

Batch 215880, train_perplexity=18.655691, train_loss=2.9261513

Batch 215890, train_perplexity=18.655691, train_loss=2.9261513

Batch 215900, train_perplexity=18.655691, train_loss=2.9261513

Batch 215910, train_perplexity=18.655691, train_loss=2.9261513

Batch 215920, train_perplexity=18.655691, train_loss=2.9261513

Batch 215930, train_perplexity=18.655691, train_loss=2.9261513

Batch 215940, train_perplexity=18.655691, train_loss=2.9261513

Batch 215950, train_perplexity=18.655691, train_loss=2.9261513

Batch 215960, train_perplexity=18.655691, train_loss=2.9261513

Batch 215970, train_perplexity=18.655691, train_loss=2.9261513

Batch 215980, train_perplexity=18.655691, train_loss=2.9261513

Batch 215990, train_perplexity=18.655691, train_loss=2.9261513

Batch 216000, train_perplexity=18.655684, train_loss=2.9261508

Batch 216010, train_perplexity=18.655691, train_loss=2.9261513

Batch 216020, train_perplexity=18.655691, train_loss=2.9261513

Batch 216030, train_perplexity=18.655684, train_loss=2.9261508

Batch 216040, train_perplexity=18.655684, train_loss=2.9261508

Batch 216050, train_perplexity=18.655687, train_loss=2.926151

Batch 216060, train_perplexity=18.655691, train_loss=2.9261513

Batch 216070, train_perplexity=18.655684, train_loss=2.9261508

Batch 216080, train_perplexity=18.655684, train_loss=2.9261508

Batch 216090, train_perplexity=18.655684, train_loss=2.9261508

Batch 216100, train_perplexity=18.655684, train_loss=2.9261508

Batch 216110, train_perplexity=18.655687, train_loss=2.926151

Batch 216120, train_perplexity=18.655691, train_loss=2.9261513

Batch 216130, train_perplexity=18.655684, train_loss=2.9261508

Batch 216140, train_perplexity=18.655684, train_loss=2.9261508

Batch 216150, train_perplexity=18.655684, train_loss=2.9261508

Batch 216160, train_perplexity=18.655684, train_loss=2.9261508

Batch 216170, train_perplexity=18.655684, train_loss=2.9261508

Batch 216180, train_perplexity=18.655684, train_loss=2.9261508

Batch 216190, train_perplexity=18.655687, train_loss=2.926151

Batch 216200, train_perplexity=18.655684, train_loss=2.9261508

Batch 216210, train_perplexity=18.655684, train_loss=2.9261508

Batch 216220, train_perplexity=18.655684, train_loss=2.9261508

Batch 216230, train_perplexity=18.655684, train_loss=2.9261508

Batch 216240, train_perplexity=18.655684, train_loss=2.9261508

Batch 216250, train_perplexity=18.655678, train_loss=2.9261506

Batch 216260, train_perplexity=18.655684, train_loss=2.9261508

Batch 216270, train_perplexity=18.655678, train_loss=2.9261506

Batch 216280, train_perplexity=18.655678, train_loss=2.9261506

Batch 216290, train_perplexity=18.655678, train_loss=2.9261506

Batch 216300, train_perplexity=18.655684, train_loss=2.9261508

Batch 216310, train_perplexity=18.655684, train_loss=2.9261508

Batch 216320, train_perplexity=18.655674, train_loss=2.9261503

Batch 216330, train_perplexity=18.655684, train_loss=2.9261508

Batch 216340, train_perplexity=18.655684, train_loss=2.9261508

Batch 216350, train_perplexity=18.655684, train_loss=2.9261508

Batch 216360, train_perplexity=18.655678, train_loss=2.9261506

Batch 216370, train_perplexity=18.655678, train_loss=2.9261506

Batch 216380, train_perplexity=18.655674, train_loss=2.9261503

Batch 216390, train_perplexity=18.655684, train_loss=2.9261508

Batch 216400, train_perplexity=18.655674, train_loss=2.9261503

Batch 216410, train_perplexity=18.655674, train_loss=2.9261503

Batch 216420, train_perplexity=18.655674, train_loss=2.9261503

Batch 216430, train_perplexity=18.655674, train_loss=2.9261503

Batch 216440, train_perplexity=18.655674, train_loss=2.9261503

Batch 216450, train_perplexity=18.655684, train_loss=2.9261508

Batch 216460, train_perplexity=18.655674, train_loss=2.9261503

Batch 216470, train_perplexity=18.655674, train_loss=2.9261503

Batch 216480, train_perplexity=18.655674, train_loss=2.9261503

Batch 216490, train_perplexity=18.655674, train_loss=2.9261503

Batch 216500, train_perplexity=18.655674, train_loss=2.9261503

Batch 216510, train_perplexity=18.655674, train_loss=2.9261503

Batch 216520, train_perplexity=18.655674, train_loss=2.9261503

Batch 216530, train_perplexity=18.655674, train_loss=2.9261503

Batch 216540, train_perplexity=18.655674, train_loss=2.9261503

Batch 216550, train_perplexity=18.655674, train_loss=2.9261503

Batch 216560, train_perplexity=18.655674, train_loss=2.9261503

Batch 216570, train_perplexity=18.655674, train_loss=2.9261503

Batch 216580, train_perplexity=18.655674, train_loss=2.9261503

Batch 216590, train_perplexity=18.655674, train_loss=2.9261503

Batch 216600, train_perplexity=18.655674, train_loss=2.9261503

Batch 216610, train_perplexity=18.655674, train_loss=2.9261503

Batch 216620, train_perplexity=18.655674, train_loss=2.9261503

Batch 216630, train_perplexity=18.655674, train_loss=2.9261503

Batch 216640, train_perplexity=18.655674, train_loss=2.9261503

Batch 216650, train_perplexity=18.655674, train_loss=2.9261503

Batch 216660, train_perplexity=18.655674, train_loss=2.9261503

Batch 216670, train_perplexity=18.65567, train_loss=2.92615

Batch 216680, train_perplexity=18.655664, train_loss=2.9261498

Batch 216690, train_perplexity=18.655664, train_loss=2.9261498

Batch 216700, train_perplexity=18.655674, train_loss=2.9261503

Batch 216710, train_perplexity=18.655664, train_loss=2.9261498

Batch 216720, train_perplexity=18.655664, train_loss=2.9261498

Batch 216730, train_perplexity=18.655664, train_loss=2.9261498

Batch 216740, train_perplexity=18.655664, train_loss=2.9261498

Batch 216750, train_perplexity=18.655664, train_loss=2.9261498

Batch 216760, train_perplexity=18.655664, train_loss=2.9261498

Batch 216770, train_perplexity=18.655664, train_loss=2.9261498

Batch 216780, train_perplexity=18.655664, train_loss=2.9261498

Batch 216790, train_perplexity=18.655664, train_loss=2.9261498

Batch 216800, train_perplexity=18.655664, train_loss=2.9261498

Batch 216810, train_perplexity=18.655664, train_loss=2.9261498

Batch 216820, train_perplexity=18.655664, train_loss=2.9261498

Batch 216830, train_perplexity=18.655664, train_loss=2.9261498

Batch 216840, train_perplexity=18.655664, train_loss=2.9261498

Batch 216850, train_perplexity=18.655664, train_loss=2.9261498

Batch 216860, train_perplexity=18.655664, train_loss=2.9261498

Batch 216870, train_perplexity=18.655664, train_loss=2.9261498

Batch 216880, train_perplexity=18.655664, train_loss=2.9261498

Batch 216890, train_perplexity=18.65566, train_loss=2.9261496

Batch 216900, train_perplexity=18.655664, train_loss=2.9261498

Batch 216910, train_perplexity=18.65566, train_loss=2.9261496

Batch 216920, train_perplexity=18.65566, train_loss=2.9261496

Batch 216930, train_perplexity=18.655664, train_loss=2.9261498

Batch 216940, train_perplexity=18.65566, train_loss=2.9261496

Batch 216950, train_perplexity=18.65566, train_loss=2.9261496

Batch 216960, train_perplexity=18.655657, train_loss=2.9261494

Batch 216970, train_perplexity=18.655664, train_loss=2.9261498

Batch 216980, train_perplexity=18.655664, train_loss=2.9261498

Batch 216990, train_perplexity=18.655664, train_loss=2.9261498

Batch 217000, train_perplexity=18.655657, train_loss=2.9261494

Batch 217010, train_perplexity=18.655657, train_loss=2.9261494

Batch 217020, train_perplexity=18.65566, train_loss=2.9261496

Batch 217030, train_perplexity=18.655657, train_loss=2.9261494

Batch 217040, train_perplexity=18.655657, train_loss=2.9261494

Batch 217050, train_perplexity=18.655657, train_loss=2.9261494

Batch 217060, train_perplexity=18.655657, train_loss=2.9261494

Batch 217070, train_perplexity=18.655657, train_loss=2.9261494

Batch 217080, train_perplexity=18.655657, train_loss=2.9261494

Batch 217090, train_perplexity=18.655657, train_loss=2.9261494

Batch 217100, train_perplexity=18.655657, train_loss=2.9261494

Batch 217110, train_perplexity=18.655657, train_loss=2.9261494
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 217120, train_perplexity=18.655657, train_loss=2.9261494

Batch 217130, train_perplexity=18.655657, train_loss=2.9261494

Batch 217140, train_perplexity=18.655657, train_loss=2.9261494

Batch 217150, train_perplexity=18.655657, train_loss=2.9261494

Batch 217160, train_perplexity=18.655657, train_loss=2.9261494

Batch 217170, train_perplexity=18.655657, train_loss=2.9261494

Batch 217180, train_perplexity=18.655657, train_loss=2.9261494

Batch 217190, train_perplexity=18.655657, train_loss=2.9261494

Batch 217200, train_perplexity=18.655657, train_loss=2.9261494

Batch 217210, train_perplexity=18.655657, train_loss=2.9261494

Batch 217220, train_perplexity=18.655657, train_loss=2.9261494

Batch 217230, train_perplexity=18.655657, train_loss=2.9261494

Batch 217240, train_perplexity=18.655657, train_loss=2.9261494

Batch 217250, train_perplexity=18.655657, train_loss=2.9261494

Batch 217260, train_perplexity=18.655657, train_loss=2.9261494

Batch 217270, train_perplexity=18.655657, train_loss=2.9261494

Batch 217280, train_perplexity=18.655647, train_loss=2.926149

Batch 217290, train_perplexity=18.655651, train_loss=2.9261491

Batch 217300, train_perplexity=18.655657, train_loss=2.9261494

Batch 217310, train_perplexity=18.655657, train_loss=2.9261494

Batch 217320, train_perplexity=18.655651, train_loss=2.9261491

Batch 217330, train_perplexity=18.655647, train_loss=2.926149

Batch 217340, train_perplexity=18.655647, train_loss=2.926149

Batch 217350, train_perplexity=18.655647, train_loss=2.926149

Batch 217360, train_perplexity=18.655647, train_loss=2.926149

Batch 217370, train_perplexity=18.655651, train_loss=2.9261491

Batch 217380, train_perplexity=18.655647, train_loss=2.926149

Batch 217390, train_perplexity=18.655647, train_loss=2.926149

Batch 217400, train_perplexity=18.655647, train_loss=2.926149

Batch 217410, train_perplexity=18.655647, train_loss=2.926149

Batch 217420, train_perplexity=18.655647, train_loss=2.926149

Batch 217430, train_perplexity=18.655647, train_loss=2.926149

Batch 217440, train_perplexity=18.655647, train_loss=2.926149

Batch 217450, train_perplexity=18.655647, train_loss=2.926149

Batch 217460, train_perplexity=18.655647, train_loss=2.926149

Batch 217470, train_perplexity=18.655643, train_loss=2.9261487

Batch 217480, train_perplexity=18.655647, train_loss=2.926149

Batch 217490, train_perplexity=18.655647, train_loss=2.926149

Batch 217500, train_perplexity=18.655647, train_loss=2.926149

Batch 217510, train_perplexity=18.655647, train_loss=2.926149

Batch 217520, train_perplexity=18.655643, train_loss=2.9261487

Batch 217530, train_perplexity=18.655647, train_loss=2.926149

Batch 217540, train_perplexity=18.655647, train_loss=2.926149

Batch 217550, train_perplexity=18.655647, train_loss=2.926149

Batch 217560, train_perplexity=18.655647, train_loss=2.926149

Batch 217570, train_perplexity=18.655643, train_loss=2.9261487

Batch 217580, train_perplexity=18.655647, train_loss=2.926149

Batch 217590, train_perplexity=18.655647, train_loss=2.926149

Batch 217600, train_perplexity=18.655643, train_loss=2.9261487

Batch 217610, train_perplexity=18.655643, train_loss=2.9261487

Batch 217620, train_perplexity=18.655638, train_loss=2.9261484

Batch 217630, train_perplexity=18.655638, train_loss=2.9261484

Batch 217640, train_perplexity=18.655643, train_loss=2.9261487

Batch 217650, train_perplexity=18.655638, train_loss=2.9261484

Batch 217660, train_perplexity=18.655638, train_loss=2.9261484

Batch 217670, train_perplexity=18.655638, train_loss=2.9261484

Batch 217680, train_perplexity=18.655638, train_loss=2.9261484

Batch 217690, train_perplexity=18.655638, train_loss=2.9261484

Batch 217700, train_perplexity=18.655638, train_loss=2.9261484

Batch 217710, train_perplexity=18.655643, train_loss=2.9261487

Batch 217720, train_perplexity=18.655638, train_loss=2.9261484

Batch 217730, train_perplexity=18.655638, train_loss=2.9261484

Batch 217740, train_perplexity=18.655638, train_loss=2.9261484

Batch 217750, train_perplexity=18.655638, train_loss=2.9261484

Batch 217760, train_perplexity=18.655638, train_loss=2.9261484

Batch 217770, train_perplexity=18.655643, train_loss=2.9261487

Batch 217780, train_perplexity=18.655638, train_loss=2.9261484

Batch 217790, train_perplexity=18.655638, train_loss=2.9261484

Batch 217800, train_perplexity=18.655638, train_loss=2.9261484

Batch 217810, train_perplexity=18.655638, train_loss=2.9261484

Batch 217820, train_perplexity=18.655638, train_loss=2.9261484

Batch 217830, train_perplexity=18.655638, train_loss=2.9261484

Batch 217840, train_perplexity=18.655634, train_loss=2.9261482

Batch 217850, train_perplexity=18.655634, train_loss=2.9261482

Batch 217860, train_perplexity=18.655638, train_loss=2.9261484

Batch 217870, train_perplexity=18.655634, train_loss=2.9261482

Batch 217880, train_perplexity=18.65563, train_loss=2.926148

Batch 217890, train_perplexity=18.65563, train_loss=2.926148

Batch 217900, train_perplexity=18.65563, train_loss=2.926148

Batch 217910, train_perplexity=18.655638, train_loss=2.9261484

Batch 217920, train_perplexity=18.65563, train_loss=2.926148

Batch 217930, train_perplexity=18.65563, train_loss=2.926148

Batch 217940, train_perplexity=18.655634, train_loss=2.9261482

Batch 217950, train_perplexity=18.65563, train_loss=2.926148

Batch 217960, train_perplexity=18.65563, train_loss=2.926148

Batch 217970, train_perplexity=18.65563, train_loss=2.926148

Batch 217980, train_perplexity=18.65563, train_loss=2.926148

Batch 217990, train_perplexity=18.65563, train_loss=2.926148

Batch 218000, train_perplexity=18.655634, train_loss=2.9261482

Batch 218010, train_perplexity=18.65563, train_loss=2.926148

Batch 218020, train_perplexity=18.65563, train_loss=2.926148

Batch 218030, train_perplexity=18.65563, train_loss=2.926148

Batch 218040, train_perplexity=18.65563, train_loss=2.926148

Batch 218050, train_perplexity=18.65563, train_loss=2.926148

Batch 218060, train_perplexity=18.65563, train_loss=2.926148

Batch 218070, train_perplexity=18.65563, train_loss=2.926148

Batch 218080, train_perplexity=18.65563, train_loss=2.926148

Batch 218090, train_perplexity=18.65563, train_loss=2.926148

Batch 218100, train_perplexity=18.65563, train_loss=2.926148

Batch 218110, train_perplexity=18.65563, train_loss=2.926148

Batch 218120, train_perplexity=18.65563, train_loss=2.926148

Batch 218130, train_perplexity=18.65563, train_loss=2.926148

Batch 218140, train_perplexity=18.65563, train_loss=2.926148

Batch 218150, train_perplexity=18.65563, train_loss=2.926148

Batch 218160, train_perplexity=18.65563, train_loss=2.926148

Batch 218170, train_perplexity=18.655624, train_loss=2.9261477

Batch 218180, train_perplexity=18.65563, train_loss=2.926148

Batch 218190, train_perplexity=18.655624, train_loss=2.9261477

Batch 218200, train_perplexity=18.65563, train_loss=2.926148

Batch 218210, train_perplexity=18.65563, train_loss=2.926148

Batch 218220, train_perplexity=18.65563, train_loss=2.926148

Batch 218230, train_perplexity=18.655624, train_loss=2.9261477

Batch 218240, train_perplexity=18.655624, train_loss=2.9261477

Batch 218250, train_perplexity=18.655624, train_loss=2.9261477

Batch 218260, train_perplexity=18.65562, train_loss=2.9261475

Batch 218270, train_perplexity=18.65562, train_loss=2.9261475

Batch 218280, train_perplexity=18.65562, train_loss=2.9261475

Batch 218290, train_perplexity=18.65562, train_loss=2.9261475

Batch 218300, train_perplexity=18.65562, train_loss=2.9261475

Batch 218310, train_perplexity=18.655624, train_loss=2.9261477

Batch 218320, train_perplexity=18.65562, train_loss=2.9261475

Batch 218330, train_perplexity=18.65562, train_loss=2.9261475

Batch 218340, train_perplexity=18.65562, train_loss=2.9261475

Batch 218350, train_perplexity=18.65562, train_loss=2.9261475

Batch 218360, train_perplexity=18.65562, train_loss=2.9261475

Batch 218370, train_perplexity=18.65562, train_loss=2.9261475

Batch 218380, train_perplexity=18.65562, train_loss=2.9261475

Batch 218390, train_perplexity=18.65562, train_loss=2.9261475

Batch 218400, train_perplexity=18.65562, train_loss=2.9261475

Batch 218410, train_perplexity=18.65562, train_loss=2.9261475
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 218420, train_perplexity=18.65562, train_loss=2.9261475

Batch 218430, train_perplexity=18.65562, train_loss=2.9261475

Batch 218440, train_perplexity=18.65562, train_loss=2.9261475

Batch 218450, train_perplexity=18.655617, train_loss=2.9261472

Batch 218460, train_perplexity=18.655617, train_loss=2.9261472

Batch 218470, train_perplexity=18.655611, train_loss=2.926147

Batch 218480, train_perplexity=18.655617, train_loss=2.9261472

Batch 218490, train_perplexity=18.65562, train_loss=2.9261475

Batch 218500, train_perplexity=18.655617, train_loss=2.9261472

Batch 218510, train_perplexity=18.655611, train_loss=2.926147

Batch 218520, train_perplexity=18.655611, train_loss=2.926147

Batch 218530, train_perplexity=18.65562, train_loss=2.9261475

Batch 218540, train_perplexity=18.655611, train_loss=2.926147

Batch 218550, train_perplexity=18.655611, train_loss=2.926147

Batch 218560, train_perplexity=18.655611, train_loss=2.926147

Batch 218570, train_perplexity=18.655611, train_loss=2.926147

Batch 218580, train_perplexity=18.655611, train_loss=2.926147

Batch 218590, train_perplexity=18.655611, train_loss=2.926147

Batch 218600, train_perplexity=18.655617, train_loss=2.9261472

Batch 218610, train_perplexity=18.655611, train_loss=2.926147

Batch 218620, train_perplexity=18.655611, train_loss=2.926147

Batch 218630, train_perplexity=18.655611, train_loss=2.926147

Batch 218640, train_perplexity=18.655611, train_loss=2.926147

Batch 218650, train_perplexity=18.655611, train_loss=2.926147

Batch 218660, train_perplexity=18.655611, train_loss=2.926147

Batch 218670, train_perplexity=18.655611, train_loss=2.926147

Batch 218680, train_perplexity=18.655611, train_loss=2.926147

Batch 218690, train_perplexity=18.655611, train_loss=2.926147

Batch 218700, train_perplexity=18.655611, train_loss=2.926147

Batch 218710, train_perplexity=18.655611, train_loss=2.926147

Batch 218720, train_perplexity=18.655611, train_loss=2.926147

Batch 218730, train_perplexity=18.655611, train_loss=2.926147

Batch 218740, train_perplexity=18.655611, train_loss=2.926147

Batch 218750, train_perplexity=18.655611, train_loss=2.926147

Batch 218760, train_perplexity=18.655611, train_loss=2.926147

Batch 218770, train_perplexity=18.655611, train_loss=2.926147

Batch 218780, train_perplexity=18.655603, train_loss=2.9261465

Batch 218790, train_perplexity=18.655611, train_loss=2.926147

Batch 218800, train_perplexity=18.655607, train_loss=2.9261467

Batch 218810, train_perplexity=18.655607, train_loss=2.9261467

Batch 218820, train_perplexity=18.655611, train_loss=2.926147

Batch 218830, train_perplexity=18.655611, train_loss=2.926147

Batch 218840, train_perplexity=18.655611, train_loss=2.926147

Batch 218850, train_perplexity=18.655603, train_loss=2.9261465

Batch 218860, train_perplexity=18.655607, train_loss=2.9261467

Batch 218870, train_perplexity=18.655611, train_loss=2.926147

Batch 218880, train_perplexity=18.655611, train_loss=2.926147

Batch 218890, train_perplexity=18.655603, train_loss=2.9261465

Batch 218900, train_perplexity=18.655603, train_loss=2.9261465

Batch 218910, train_perplexity=18.655603, train_loss=2.9261465

Batch 218920, train_perplexity=18.655603, train_loss=2.9261465

Batch 218930, train_perplexity=18.655603, train_loss=2.9261465

Batch 218940, train_perplexity=18.655603, train_loss=2.9261465

Batch 218950, train_perplexity=18.655603, train_loss=2.9261465

Batch 218960, train_perplexity=18.655603, train_loss=2.9261465

Batch 218970, train_perplexity=18.655603, train_loss=2.9261465

Batch 218980, train_perplexity=18.655603, train_loss=2.9261465

Batch 218990, train_perplexity=18.655607, train_loss=2.9261467

Batch 219000, train_perplexity=18.655603, train_loss=2.9261465

Batch 219010, train_perplexity=18.655603, train_loss=2.9261465

Batch 219020, train_perplexity=18.655603, train_loss=2.9261465

Batch 219030, train_perplexity=18.655603, train_loss=2.9261465

Batch 219040, train_perplexity=18.655603, train_loss=2.9261465

Batch 219050, train_perplexity=18.655603, train_loss=2.9261465

Batch 219060, train_perplexity=18.655603, train_loss=2.9261465

Batch 219070, train_perplexity=18.655603, train_loss=2.9261465

Batch 219080, train_perplexity=18.655603, train_loss=2.9261465

Batch 219090, train_perplexity=18.655603, train_loss=2.9261465

Batch 219100, train_perplexity=18.655594, train_loss=2.926146

Batch 219110, train_perplexity=18.655594, train_loss=2.926146

Batch 219120, train_perplexity=18.655603, train_loss=2.9261465

Batch 219130, train_perplexity=18.655603, train_loss=2.9261465

Batch 219140, train_perplexity=18.655603, train_loss=2.9261465

Batch 219150, train_perplexity=18.655603, train_loss=2.9261465

Batch 219160, train_perplexity=18.655594, train_loss=2.926146

Batch 219170, train_perplexity=18.655598, train_loss=2.9261463

Batch 219180, train_perplexity=18.655594, train_loss=2.926146

Batch 219190, train_perplexity=18.655594, train_loss=2.926146

Batch 219200, train_perplexity=18.655594, train_loss=2.926146

Batch 219210, train_perplexity=18.655594, train_loss=2.926146

Batch 219220, train_perplexity=18.655594, train_loss=2.926146

Batch 219230, train_perplexity=18.655594, train_loss=2.926146

Batch 219240, train_perplexity=18.655594, train_loss=2.926146

Batch 219250, train_perplexity=18.655594, train_loss=2.926146

Batch 219260, train_perplexity=18.655594, train_loss=2.926146

Batch 219270, train_perplexity=18.655594, train_loss=2.926146

Batch 219280, train_perplexity=18.655594, train_loss=2.926146

Batch 219290, train_perplexity=18.655594, train_loss=2.926146

Batch 219300, train_perplexity=18.655594, train_loss=2.926146

Batch 219310, train_perplexity=18.655594, train_loss=2.926146

Batch 219320, train_perplexity=18.655594, train_loss=2.926146

Batch 219330, train_perplexity=18.655594, train_loss=2.926146

Batch 219340, train_perplexity=18.655594, train_loss=2.926146

Batch 219350, train_perplexity=18.655594, train_loss=2.926146

Batch 219360, train_perplexity=18.655594, train_loss=2.926146

Batch 219370, train_perplexity=18.655594, train_loss=2.926146

Batch 219380, train_perplexity=18.655584, train_loss=2.9261456

Batch 219390, train_perplexity=18.655594, train_loss=2.926146

Batch 219400, train_perplexity=18.655594, train_loss=2.926146

Batch 219410, train_perplexity=18.655594, train_loss=2.926146

Batch 219420, train_perplexity=18.655594, train_loss=2.926146

Batch 219430, train_perplexity=18.65559, train_loss=2.9261458

Batch 219440, train_perplexity=18.65559, train_loss=2.9261458

Batch 219450, train_perplexity=18.655594, train_loss=2.926146

Batch 219460, train_perplexity=18.655594, train_loss=2.926146

Batch 219470, train_perplexity=18.655594, train_loss=2.926146

Batch 219480, train_perplexity=18.655594, train_loss=2.926146

Batch 219490, train_perplexity=18.655594, train_loss=2.926146

Batch 219500, train_perplexity=18.655584, train_loss=2.9261456

Batch 219510, train_perplexity=18.65559, train_loss=2.9261458

Batch 219520, train_perplexity=18.65559, train_loss=2.9261458

Batch 219530, train_perplexity=18.655584, train_loss=2.9261456

Batch 219540, train_perplexity=18.65559, train_loss=2.9261458

Batch 219550, train_perplexity=18.655584, train_loss=2.9261456

Batch 219560, train_perplexity=18.65559, train_loss=2.9261458

Batch 219570, train_perplexity=18.655584, train_loss=2.9261456

Batch 219580, train_perplexity=18.65559, train_loss=2.9261458

Batch 219590, train_perplexity=18.655584, train_loss=2.9261456

Batch 219600, train_perplexity=18.655584, train_loss=2.9261456

Batch 219610, train_perplexity=18.655584, train_loss=2.9261456

Batch 219620, train_perplexity=18.655584, train_loss=2.9261456

Batch 219630, train_perplexity=18.655584, train_loss=2.9261456

Batch 219640, train_perplexity=18.655584, train_loss=2.9261456

Batch 219650, train_perplexity=18.655584, train_loss=2.9261456

Batch 219660, train_perplexity=18.655584, train_loss=2.9261456

Batch 219670, train_perplexity=18.655584, train_loss=2.9261456

Batch 219680, train_perplexity=18.65558, train_loss=2.9261453

Batch 219690, train_perplexity=18.655584, train_loss=2.9261456

Batch 219700, train_perplexity=18.655584, train_loss=2.9261456

Batch 219710, train_perplexity=18.65558, train_loss=2.9261453
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 219720, train_perplexity=18.655584, train_loss=2.9261456

Batch 219730, train_perplexity=18.655584, train_loss=2.9261456

Batch 219740, train_perplexity=18.655584, train_loss=2.9261456

Batch 219750, train_perplexity=18.655584, train_loss=2.9261456

Batch 219760, train_perplexity=18.655584, train_loss=2.9261456

Batch 219770, train_perplexity=18.65558, train_loss=2.9261453

Batch 219780, train_perplexity=18.655577, train_loss=2.926145

Batch 219790, train_perplexity=18.655584, train_loss=2.9261456

Batch 219800, train_perplexity=18.65558, train_loss=2.9261453

Batch 219810, train_perplexity=18.655577, train_loss=2.926145

Batch 219820, train_perplexity=18.655577, train_loss=2.926145

Batch 219830, train_perplexity=18.655577, train_loss=2.926145

Batch 219840, train_perplexity=18.655577, train_loss=2.926145

Batch 219850, train_perplexity=18.655584, train_loss=2.9261456

Batch 219860, train_perplexity=18.655577, train_loss=2.926145

Batch 219870, train_perplexity=18.655577, train_loss=2.926145

Batch 219880, train_perplexity=18.655577, train_loss=2.926145

Batch 219890, train_perplexity=18.655577, train_loss=2.926145

Batch 219900, train_perplexity=18.655577, train_loss=2.926145

Batch 219910, train_perplexity=18.655577, train_loss=2.926145

Batch 219920, train_perplexity=18.655577, train_loss=2.926145

Batch 219930, train_perplexity=18.655577, train_loss=2.926145

Batch 219940, train_perplexity=18.655577, train_loss=2.926145

Batch 219950, train_perplexity=18.655577, train_loss=2.926145

Batch 219960, train_perplexity=18.655577, train_loss=2.926145

Batch 219970, train_perplexity=18.655571, train_loss=2.9261448

Batch 219980, train_perplexity=18.655577, train_loss=2.926145

Batch 219990, train_perplexity=18.655577, train_loss=2.926145

Batch 220000, train_perplexity=18.655571, train_loss=2.9261448

Batch 220010, train_perplexity=18.655571, train_loss=2.9261448

Batch 220020, train_perplexity=18.655571, train_loss=2.9261448

Batch 220030, train_perplexity=18.655577, train_loss=2.926145

Batch 220040, train_perplexity=18.655571, train_loss=2.9261448

Batch 220050, train_perplexity=18.655577, train_loss=2.926145

Batch 220060, train_perplexity=18.655571, train_loss=2.9261448

Batch 220070, train_perplexity=18.655577, train_loss=2.926145

Batch 220080, train_perplexity=18.655571, train_loss=2.9261448

Batch 220090, train_perplexity=18.655571, train_loss=2.9261448

Batch 220100, train_perplexity=18.655571, train_loss=2.9261448

Batch 220110, train_perplexity=18.655567, train_loss=2.9261446

Batch 220120, train_perplexity=18.655571, train_loss=2.9261448

Batch 220130, train_perplexity=18.655567, train_loss=2.9261446

Batch 220140, train_perplexity=18.655577, train_loss=2.926145

Batch 220150, train_perplexity=18.655567, train_loss=2.9261446

Batch 220160, train_perplexity=18.655567, train_loss=2.9261446

Batch 220170, train_perplexity=18.655567, train_loss=2.9261446

Batch 220180, train_perplexity=18.655567, train_loss=2.9261446

Batch 220190, train_perplexity=18.655567, train_loss=2.9261446

Batch 220200, train_perplexity=18.655571, train_loss=2.9261448

Batch 220210, train_perplexity=18.655567, train_loss=2.9261446

Batch 220220, train_perplexity=18.655567, train_loss=2.9261446

Batch 220230, train_perplexity=18.655567, train_loss=2.9261446

Batch 220240, train_perplexity=18.655571, train_loss=2.9261448

Batch 220250, train_perplexity=18.655567, train_loss=2.9261446

Batch 220260, train_perplexity=18.655567, train_loss=2.9261446

Batch 220270, train_perplexity=18.655567, train_loss=2.9261446

Batch 220280, train_perplexity=18.655567, train_loss=2.9261446

Batch 220290, train_perplexity=18.655567, train_loss=2.9261446

Batch 220300, train_perplexity=18.655567, train_loss=2.9261446

Batch 220310, train_perplexity=18.655567, train_loss=2.9261446

Batch 220320, train_perplexity=18.655567, train_loss=2.9261446

Batch 220330, train_perplexity=18.655567, train_loss=2.9261446

Batch 220340, train_perplexity=18.655563, train_loss=2.9261444

Batch 220350, train_perplexity=18.655567, train_loss=2.9261446

Batch 220360, train_perplexity=18.655567, train_loss=2.9261446

Batch 220370, train_perplexity=18.655567, train_loss=2.9261446

Batch 220380, train_perplexity=18.655563, train_loss=2.9261444

Batch 220390, train_perplexity=18.655563, train_loss=2.9261444

Batch 220400, train_perplexity=18.655567, train_loss=2.9261446

Batch 220410, train_perplexity=18.655567, train_loss=2.9261446

Batch 220420, train_perplexity=18.655563, train_loss=2.9261444

Batch 220430, train_perplexity=18.655567, train_loss=2.9261446

Batch 220440, train_perplexity=18.655558, train_loss=2.9261441

Batch 220450, train_perplexity=18.655558, train_loss=2.9261441

Batch 220460, train_perplexity=18.655558, train_loss=2.9261441

Batch 220470, train_perplexity=18.655558, train_loss=2.9261441

Batch 220480, train_perplexity=18.655563, train_loss=2.9261444

Batch 220490, train_perplexity=18.655558, train_loss=2.9261441

Batch 220500, train_perplexity=18.655563, train_loss=2.9261444

Batch 220510, train_perplexity=18.655558, train_loss=2.9261441

Batch 220520, train_perplexity=18.655558, train_loss=2.9261441

Batch 220530, train_perplexity=18.655558, train_loss=2.9261441

Batch 220540, train_perplexity=18.655558, train_loss=2.9261441

Batch 220550, train_perplexity=18.655558, train_loss=2.9261441

Batch 220560, train_perplexity=18.655558, train_loss=2.9261441

Batch 220570, train_perplexity=18.655558, train_loss=2.9261441

Batch 220580, train_perplexity=18.655558, train_loss=2.9261441

Batch 220590, train_perplexity=18.655558, train_loss=2.9261441

Batch 220600, train_perplexity=18.655554, train_loss=2.926144

Batch 220610, train_perplexity=18.655558, train_loss=2.9261441

Batch 220620, train_perplexity=18.655558, train_loss=2.9261441

Batch 220630, train_perplexity=18.655558, train_loss=2.9261441

Batch 220640, train_perplexity=18.655558, train_loss=2.9261441

Batch 220650, train_perplexity=18.655554, train_loss=2.926144

Batch 220660, train_perplexity=18.655558, train_loss=2.9261441

Batch 220670, train_perplexity=18.655554, train_loss=2.926144

Batch 220680, train_perplexity=18.655558, train_loss=2.9261441

Batch 220690, train_perplexity=18.655554, train_loss=2.926144

Batch 220700, train_perplexity=18.655558, train_loss=2.9261441

Batch 220710, train_perplexity=18.65555, train_loss=2.9261436

Batch 220720, train_perplexity=18.655554, train_loss=2.926144

Batch 220730, train_perplexity=18.655554, train_loss=2.926144

Batch 220740, train_perplexity=18.65555, train_loss=2.9261436

Batch 220750, train_perplexity=18.655554, train_loss=2.926144

Batch 220760, train_perplexity=18.655554, train_loss=2.926144

Batch 220770, train_perplexity=18.65555, train_loss=2.9261436

Batch 220780, train_perplexity=18.65555, train_loss=2.9261436

Batch 220790, train_perplexity=18.65555, train_loss=2.9261436

Batch 220800, train_perplexity=18.655554, train_loss=2.926144

Batch 220810, train_perplexity=18.65555, train_loss=2.9261436

Batch 220820, train_perplexity=18.65555, train_loss=2.9261436

Batch 220830, train_perplexity=18.65555, train_loss=2.9261436

Batch 220840, train_perplexity=18.65555, train_loss=2.9261436

Batch 220850, train_perplexity=18.65555, train_loss=2.9261436

Batch 220860, train_perplexity=18.65555, train_loss=2.9261436

Batch 220870, train_perplexity=18.65555, train_loss=2.9261436

Batch 220880, train_perplexity=18.65555, train_loss=2.9261436

Batch 220890, train_perplexity=18.65555, train_loss=2.9261436

Batch 220900, train_perplexity=18.65555, train_loss=2.9261436

Batch 220910, train_perplexity=18.65555, train_loss=2.9261436

Batch 220920, train_perplexity=18.65555, train_loss=2.9261436

Batch 220930, train_perplexity=18.65555, train_loss=2.9261436

Batch 220940, train_perplexity=18.65555, train_loss=2.9261436

Batch 220950, train_perplexity=18.655544, train_loss=2.9261434

Batch 220960, train_perplexity=18.65555, train_loss=2.9261436

Batch 220970, train_perplexity=18.65555, train_loss=2.9261436

Batch 220980, train_perplexity=18.655544, train_loss=2.9261434

Batch 220990, train_perplexity=18.65555, train_loss=2.9261436

Batch 221000, train_perplexity=18.65555, train_loss=2.9261436
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 221010, train_perplexity=18.65555, train_loss=2.9261436

Batch 221020, train_perplexity=18.65554, train_loss=2.9261432

Batch 221030, train_perplexity=18.655544, train_loss=2.9261434

Batch 221040, train_perplexity=18.655544, train_loss=2.9261434

Batch 221050, train_perplexity=18.655544, train_loss=2.9261434

Batch 221060, train_perplexity=18.65554, train_loss=2.9261432

Batch 221070, train_perplexity=18.65554, train_loss=2.9261432

Batch 221080, train_perplexity=18.65555, train_loss=2.9261436

Batch 221090, train_perplexity=18.65555, train_loss=2.9261436

Batch 221100, train_perplexity=18.65554, train_loss=2.9261432

Batch 221110, train_perplexity=18.65554, train_loss=2.9261432

Batch 221120, train_perplexity=18.65554, train_loss=2.9261432

Batch 221130, train_perplexity=18.65554, train_loss=2.9261432

Batch 221140, train_perplexity=18.65554, train_loss=2.9261432

Batch 221150, train_perplexity=18.65554, train_loss=2.9261432

Batch 221160, train_perplexity=18.65554, train_loss=2.9261432

Batch 221170, train_perplexity=18.65554, train_loss=2.9261432

Batch 221180, train_perplexity=18.65554, train_loss=2.9261432

Batch 221190, train_perplexity=18.65554, train_loss=2.9261432

Batch 221200, train_perplexity=18.65554, train_loss=2.9261432

Batch 221210, train_perplexity=18.65554, train_loss=2.9261432

Batch 221220, train_perplexity=18.65554, train_loss=2.9261432

Batch 221230, train_perplexity=18.65554, train_loss=2.9261432

Batch 221240, train_perplexity=18.655537, train_loss=2.926143

Batch 221250, train_perplexity=18.655537, train_loss=2.926143

Batch 221260, train_perplexity=18.65554, train_loss=2.9261432

Batch 221270, train_perplexity=18.65554, train_loss=2.9261432

Batch 221280, train_perplexity=18.65554, train_loss=2.9261432

Batch 221290, train_perplexity=18.655537, train_loss=2.926143

Batch 221300, train_perplexity=18.65553, train_loss=2.9261427

Batch 221310, train_perplexity=18.65554, train_loss=2.9261432

Batch 221320, train_perplexity=18.65554, train_loss=2.9261432

Batch 221330, train_perplexity=18.655537, train_loss=2.926143

Batch 221340, train_perplexity=18.655537, train_loss=2.926143

Batch 221350, train_perplexity=18.655537, train_loss=2.926143

Batch 221360, train_perplexity=18.65553, train_loss=2.9261427

Batch 221370, train_perplexity=18.655537, train_loss=2.926143

Batch 221380, train_perplexity=18.65554, train_loss=2.9261432

Batch 221390, train_perplexity=18.655537, train_loss=2.926143

Batch 221400, train_perplexity=18.655537, train_loss=2.926143

Batch 221410, train_perplexity=18.655537, train_loss=2.926143

Batch 221420, train_perplexity=18.655537, train_loss=2.926143

Batch 221430, train_perplexity=18.65553, train_loss=2.9261427

Batch 221440, train_perplexity=18.65553, train_loss=2.9261427

Batch 221450, train_perplexity=18.65553, train_loss=2.9261427

Batch 221460, train_perplexity=18.65553, train_loss=2.9261427

Batch 221470, train_perplexity=18.65553, train_loss=2.9261427

Batch 221480, train_perplexity=18.65553, train_loss=2.9261427

Batch 221490, train_perplexity=18.65553, train_loss=2.9261427

Batch 221500, train_perplexity=18.65553, train_loss=2.9261427

Batch 221510, train_perplexity=18.65553, train_loss=2.9261427

Batch 221520, train_perplexity=18.65553, train_loss=2.9261427

Batch 221530, train_perplexity=18.65553, train_loss=2.9261427

Batch 221540, train_perplexity=18.65553, train_loss=2.9261427

Batch 221550, train_perplexity=18.65553, train_loss=2.9261427

Batch 221560, train_perplexity=18.65553, train_loss=2.9261427

Batch 221570, train_perplexity=18.655523, train_loss=2.9261422

Batch 221580, train_perplexity=18.65553, train_loss=2.9261427

Batch 221590, train_perplexity=18.65553, train_loss=2.9261427

Batch 221600, train_perplexity=18.65553, train_loss=2.9261427

Batch 221610, train_perplexity=18.655527, train_loss=2.9261425

Batch 221620, train_perplexity=18.655527, train_loss=2.9261425

Batch 221630, train_perplexity=18.655527, train_loss=2.9261425

Batch 221640, train_perplexity=18.655527, train_loss=2.9261425

Batch 221650, train_perplexity=18.655523, train_loss=2.9261422

Batch 221660, train_perplexity=18.655523, train_loss=2.9261422

Batch 221670, train_perplexity=18.655523, train_loss=2.9261422

Batch 221680, train_perplexity=18.65553, train_loss=2.9261427

Batch 221690, train_perplexity=18.655527, train_loss=2.9261425

Batch 221700, train_perplexity=18.655523, train_loss=2.9261422

Batch 221710, train_perplexity=18.655523, train_loss=2.9261422

Batch 221720, train_perplexity=18.655523, train_loss=2.9261422

Batch 221730, train_perplexity=18.655523, train_loss=2.9261422

Batch 221740, train_perplexity=18.655527, train_loss=2.9261425

Batch 221750, train_perplexity=18.655523, train_loss=2.9261422

Batch 221760, train_perplexity=18.655523, train_loss=2.9261422

Batch 221770, train_perplexity=18.65553, train_loss=2.9261427

Batch 221780, train_perplexity=18.655523, train_loss=2.9261422

Batch 221790, train_perplexity=18.655523, train_loss=2.9261422

Batch 221800, train_perplexity=18.655523, train_loss=2.9261422

Batch 221810, train_perplexity=18.655523, train_loss=2.9261422

Batch 221820, train_perplexity=18.655523, train_loss=2.9261422

Batch 221830, train_perplexity=18.655523, train_loss=2.9261422

Batch 221840, train_perplexity=18.655523, train_loss=2.9261422

Batch 221850, train_perplexity=18.655523, train_loss=2.9261422

Batch 221860, train_perplexity=18.655523, train_loss=2.9261422

Batch 221870, train_perplexity=18.655523, train_loss=2.9261422

Batch 221880, train_perplexity=18.655523, train_loss=2.9261422

Batch 221890, train_perplexity=18.655518, train_loss=2.926142

Batch 221900, train_perplexity=18.655518, train_loss=2.926142

Batch 221910, train_perplexity=18.655523, train_loss=2.9261422

Batch 221920, train_perplexity=18.655523, train_loss=2.9261422

Batch 221930, train_perplexity=18.655518, train_loss=2.926142

Batch 221940, train_perplexity=18.655518, train_loss=2.926142

Batch 221950, train_perplexity=18.655518, train_loss=2.926142

Batch 221960, train_perplexity=18.655518, train_loss=2.926142

Batch 221970, train_perplexity=18.655523, train_loss=2.9261422

Batch 221980, train_perplexity=18.655514, train_loss=2.9261417

Batch 221990, train_perplexity=18.655514, train_loss=2.9261417

Batch 222000, train_perplexity=18.655514, train_loss=2.9261417

Batch 222010, train_perplexity=18.655523, train_loss=2.9261422

Batch 222020, train_perplexity=18.655514, train_loss=2.9261417

Batch 222030, train_perplexity=18.655518, train_loss=2.926142

Batch 222040, train_perplexity=18.655518, train_loss=2.926142

Batch 222050, train_perplexity=18.655514, train_loss=2.9261417

Batch 222060, train_perplexity=18.655514, train_loss=2.9261417

Batch 222070, train_perplexity=18.655514, train_loss=2.9261417

Batch 222080, train_perplexity=18.655514, train_loss=2.9261417

Batch 222090, train_perplexity=18.655514, train_loss=2.9261417

Batch 222100, train_perplexity=18.655514, train_loss=2.9261417

Batch 222110, train_perplexity=18.655514, train_loss=2.9261417

Batch 222120, train_perplexity=18.655514, train_loss=2.9261417

Batch 222130, train_perplexity=18.655514, train_loss=2.9261417

Batch 222140, train_perplexity=18.655514, train_loss=2.9261417

Batch 222150, train_perplexity=18.655514, train_loss=2.9261417

Batch 222160, train_perplexity=18.655514, train_loss=2.9261417

Batch 222170, train_perplexity=18.655514, train_loss=2.9261417

Batch 222180, train_perplexity=18.65551, train_loss=2.9261415

Batch 222190, train_perplexity=18.655514, train_loss=2.9261417

Batch 222200, train_perplexity=18.655514, train_loss=2.9261417

Batch 222210, train_perplexity=18.655514, train_loss=2.9261417

Batch 222220, train_perplexity=18.65551, train_loss=2.9261415

Batch 222230, train_perplexity=18.655514, train_loss=2.9261417

Batch 222240, train_perplexity=18.65551, train_loss=2.9261415

Batch 222250, train_perplexity=18.655514, train_loss=2.9261417

Batch 222260, train_perplexity=18.65551, train_loss=2.9261415

Batch 222270, train_perplexity=18.655504, train_loss=2.9261413

Batch 222280, train_perplexity=18.655514, train_loss=2.9261417

Batch 222290, train_perplexity=18.65551, train_loss=2.9261415

Batch 222300, train_perplexity=18.65551, train_loss=2.9261415
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 222310, train_perplexity=18.65551, train_loss=2.9261415

Batch 222320, train_perplexity=18.655504, train_loss=2.9261413

Batch 222330, train_perplexity=18.655504, train_loss=2.9261413

Batch 222340, train_perplexity=18.65551, train_loss=2.9261415

Batch 222350, train_perplexity=18.655504, train_loss=2.9261413

Batch 222360, train_perplexity=18.655504, train_loss=2.9261413

Batch 222370, train_perplexity=18.655504, train_loss=2.9261413

Batch 222380, train_perplexity=18.655504, train_loss=2.9261413

Batch 222390, train_perplexity=18.655504, train_loss=2.9261413

Batch 222400, train_perplexity=18.655504, train_loss=2.9261413

Batch 222410, train_perplexity=18.655504, train_loss=2.9261413

Batch 222420, train_perplexity=18.655504, train_loss=2.9261413

Batch 222430, train_perplexity=18.655504, train_loss=2.9261413

Batch 222440, train_perplexity=18.655504, train_loss=2.9261413

Batch 222450, train_perplexity=18.655504, train_loss=2.9261413

Batch 222460, train_perplexity=18.655504, train_loss=2.9261413

Batch 222470, train_perplexity=18.655504, train_loss=2.9261413

Batch 222480, train_perplexity=18.655504, train_loss=2.9261413

Batch 222490, train_perplexity=18.655504, train_loss=2.9261413

Batch 222500, train_perplexity=18.655504, train_loss=2.9261413

Batch 222510, train_perplexity=18.655504, train_loss=2.9261413

Batch 222520, train_perplexity=18.6555, train_loss=2.926141

Batch 222530, train_perplexity=18.655504, train_loss=2.9261413

Batch 222540, train_perplexity=18.655504, train_loss=2.9261413

Batch 222550, train_perplexity=18.655504, train_loss=2.9261413

Batch 222560, train_perplexity=18.6555, train_loss=2.926141

Batch 222570, train_perplexity=18.6555, train_loss=2.926141

Batch 222580, train_perplexity=18.6555, train_loss=2.926141

Batch 222590, train_perplexity=18.6555, train_loss=2.926141

Batch 222600, train_perplexity=18.6555, train_loss=2.926141

Batch 222610, train_perplexity=18.6555, train_loss=2.926141

Batch 222620, train_perplexity=18.655497, train_loss=2.9261408

Batch 222630, train_perplexity=18.6555, train_loss=2.926141

Batch 222640, train_perplexity=18.6555, train_loss=2.926141

Batch 222650, train_perplexity=18.655497, train_loss=2.9261408

Batch 222660, train_perplexity=18.655504, train_loss=2.9261413

Batch 222670, train_perplexity=18.655497, train_loss=2.9261408

Batch 222680, train_perplexity=18.655497, train_loss=2.9261408

Batch 222690, train_perplexity=18.6555, train_loss=2.926141

Batch 222700, train_perplexity=18.655497, train_loss=2.9261408

Batch 222710, train_perplexity=18.655497, train_loss=2.9261408

Batch 222720, train_perplexity=18.655497, train_loss=2.9261408

Batch 222730, train_perplexity=18.6555, train_loss=2.926141

Batch 222740, train_perplexity=18.655497, train_loss=2.9261408

Batch 222750, train_perplexity=18.655497, train_loss=2.9261408

Batch 222760, train_perplexity=18.655497, train_loss=2.9261408

Batch 222770, train_perplexity=18.655497, train_loss=2.9261408

Batch 222780, train_perplexity=18.655497, train_loss=2.9261408

Batch 222790, train_perplexity=18.655497, train_loss=2.9261408

Batch 222800, train_perplexity=18.65549, train_loss=2.9261405

Batch 222810, train_perplexity=18.655497, train_loss=2.9261408

Batch 222820, train_perplexity=18.655497, train_loss=2.9261408

Batch 222830, train_perplexity=18.655497, train_loss=2.9261408

Batch 222840, train_perplexity=18.655497, train_loss=2.9261408

Batch 222850, train_perplexity=18.655497, train_loss=2.9261408

Batch 222860, train_perplexity=18.655497, train_loss=2.9261408

Batch 222870, train_perplexity=18.65549, train_loss=2.9261405

Batch 222880, train_perplexity=18.655487, train_loss=2.9261403

Batch 222890, train_perplexity=18.655487, train_loss=2.9261403

Batch 222900, train_perplexity=18.65549, train_loss=2.9261405

Batch 222910, train_perplexity=18.65549, train_loss=2.9261405

Batch 222920, train_perplexity=18.655497, train_loss=2.9261408

Batch 222930, train_perplexity=18.65549, train_loss=2.9261405

Batch 222940, train_perplexity=18.655487, train_loss=2.9261403

Batch 222950, train_perplexity=18.655497, train_loss=2.9261408

Batch 222960, train_perplexity=18.655497, train_loss=2.9261408

Batch 222970, train_perplexity=18.655487, train_loss=2.9261403

Batch 222980, train_perplexity=18.655487, train_loss=2.9261403

Batch 222990, train_perplexity=18.655487, train_loss=2.9261403

Batch 223000, train_perplexity=18.655487, train_loss=2.9261403

Batch 223010, train_perplexity=18.655487, train_loss=2.9261403

Batch 223020, train_perplexity=18.655487, train_loss=2.9261403

Batch 223030, train_perplexity=18.655487, train_loss=2.9261403

Batch 223040, train_perplexity=18.655487, train_loss=2.9261403

Batch 223050, train_perplexity=18.655487, train_loss=2.9261403

Batch 223060, train_perplexity=18.655487, train_loss=2.9261403

Batch 223070, train_perplexity=18.655487, train_loss=2.9261403

Batch 223080, train_perplexity=18.655487, train_loss=2.9261403

Batch 223090, train_perplexity=18.655487, train_loss=2.9261403

Batch 223100, train_perplexity=18.655487, train_loss=2.9261403

Batch 223110, train_perplexity=18.655487, train_loss=2.9261403

Batch 223120, train_perplexity=18.655487, train_loss=2.9261403

Batch 223130, train_perplexity=18.655487, train_loss=2.9261403

Batch 223140, train_perplexity=18.655487, train_loss=2.9261403

Batch 223150, train_perplexity=18.655483, train_loss=2.92614

Batch 223160, train_perplexity=18.655487, train_loss=2.9261403

Batch 223170, train_perplexity=18.655483, train_loss=2.92614

Batch 223180, train_perplexity=18.655487, train_loss=2.9261403

Batch 223190, train_perplexity=18.655483, train_loss=2.92614

Batch 223200, train_perplexity=18.655483, train_loss=2.92614

Batch 223210, train_perplexity=18.655483, train_loss=2.92614

Batch 223220, train_perplexity=18.655483, train_loss=2.92614

Batch 223230, train_perplexity=18.655478, train_loss=2.9261398

Batch 223240, train_perplexity=18.655483, train_loss=2.92614

Batch 223250, train_perplexity=18.655487, train_loss=2.9261403

Batch 223260, train_perplexity=18.655483, train_loss=2.92614

Batch 223270, train_perplexity=18.655483, train_loss=2.92614

Batch 223280, train_perplexity=18.655478, train_loss=2.9261398

Batch 223290, train_perplexity=18.655478, train_loss=2.9261398

Batch 223300, train_perplexity=18.655478, train_loss=2.9261398

Batch 223310, train_perplexity=18.655478, train_loss=2.9261398

Batch 223320, train_perplexity=18.655478, train_loss=2.9261398

Batch 223330, train_perplexity=18.655478, train_loss=2.9261398

Batch 223340, train_perplexity=18.655478, train_loss=2.9261398

Batch 223350, train_perplexity=18.655478, train_loss=2.9261398

Batch 223360, train_perplexity=18.655478, train_loss=2.9261398

Batch 223370, train_perplexity=18.655478, train_loss=2.9261398

Batch 223380, train_perplexity=18.655478, train_loss=2.9261398

Batch 223390, train_perplexity=18.655478, train_loss=2.9261398

Batch 223400, train_perplexity=18.655478, train_loss=2.9261398

Batch 223410, train_perplexity=18.655478, train_loss=2.9261398

Batch 223420, train_perplexity=18.655478, train_loss=2.9261398

Batch 223430, train_perplexity=18.655478, train_loss=2.9261398

Batch 223440, train_perplexity=18.655478, train_loss=2.9261398

Batch 223450, train_perplexity=18.655478, train_loss=2.9261398

Batch 223460, train_perplexity=18.655478, train_loss=2.9261398

Batch 223470, train_perplexity=18.655478, train_loss=2.9261398

Batch 223480, train_perplexity=18.655478, train_loss=2.9261398

Batch 223490, train_perplexity=18.655478, train_loss=2.9261398

Batch 223500, train_perplexity=18.655478, train_loss=2.9261398

Batch 223510, train_perplexity=18.65547, train_loss=2.9261394

Batch 223520, train_perplexity=18.655478, train_loss=2.9261398

Batch 223530, train_perplexity=18.655478, train_loss=2.9261398

Batch 223540, train_perplexity=18.655474, train_loss=2.9261396

Batch 223550, train_perplexity=18.655478, train_loss=2.9261398

Batch 223560, train_perplexity=18.655474, train_loss=2.9261396

Batch 223570, train_perplexity=18.65547, train_loss=2.9261394

Batch 223580, train_perplexity=18.65547, train_loss=2.9261394

Batch 223590, train_perplexity=18.655474, train_loss=2.9261396
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 223600, train_perplexity=18.655474, train_loss=2.9261396

Batch 223610, train_perplexity=18.65547, train_loss=2.9261394

Batch 223620, train_perplexity=18.65547, train_loss=2.9261394

Batch 223630, train_perplexity=18.65547, train_loss=2.9261394

Batch 223640, train_perplexity=18.65547, train_loss=2.9261394

Batch 223650, train_perplexity=18.65547, train_loss=2.9261394

Batch 223660, train_perplexity=18.65547, train_loss=2.9261394

Batch 223670, train_perplexity=18.65547, train_loss=2.9261394

Batch 223680, train_perplexity=18.65547, train_loss=2.9261394

Batch 223690, train_perplexity=18.65547, train_loss=2.9261394

Batch 223700, train_perplexity=18.655464, train_loss=2.926139

Batch 223710, train_perplexity=18.65547, train_loss=2.9261394

Batch 223720, train_perplexity=18.65547, train_loss=2.9261394

Batch 223730, train_perplexity=18.65547, train_loss=2.9261394

Batch 223740, train_perplexity=18.65547, train_loss=2.9261394

Batch 223750, train_perplexity=18.65547, train_loss=2.9261394

Batch 223760, train_perplexity=18.65547, train_loss=2.9261394

Batch 223770, train_perplexity=18.655464, train_loss=2.926139

Batch 223780, train_perplexity=18.655464, train_loss=2.926139

Batch 223790, train_perplexity=18.65547, train_loss=2.9261394

Batch 223800, train_perplexity=18.65547, train_loss=2.9261394

Batch 223810, train_perplexity=18.655464, train_loss=2.926139

Batch 223820, train_perplexity=18.65547, train_loss=2.9261394

Batch 223830, train_perplexity=18.655464, train_loss=2.926139

Batch 223840, train_perplexity=18.65546, train_loss=2.9261389

Batch 223850, train_perplexity=18.65546, train_loss=2.9261389

Batch 223860, train_perplexity=18.65546, train_loss=2.9261389

Batch 223870, train_perplexity=18.65547, train_loss=2.9261394

Batch 223880, train_perplexity=18.655464, train_loss=2.926139

Batch 223890, train_perplexity=18.65546, train_loss=2.9261389

Batch 223900, train_perplexity=18.65546, train_loss=2.9261389

Batch 223910, train_perplexity=18.655464, train_loss=2.926139

Batch 223920, train_perplexity=18.65546, train_loss=2.9261389

Batch 223930, train_perplexity=18.65546, train_loss=2.9261389

Batch 223940, train_perplexity=18.65546, train_loss=2.9261389

Batch 223950, train_perplexity=18.65546, train_loss=2.9261389

Batch 223960, train_perplexity=18.65546, train_loss=2.9261389

Batch 223970, train_perplexity=18.65546, train_loss=2.9261389

Batch 223980, train_perplexity=18.65546, train_loss=2.9261389

Batch 223990, train_perplexity=18.65546, train_loss=2.9261389

Batch 224000, train_perplexity=18.65546, train_loss=2.9261389

Batch 224010, train_perplexity=18.65546, train_loss=2.9261389

Batch 224020, train_perplexity=18.65546, train_loss=2.9261389

Batch 224030, train_perplexity=18.65546, train_loss=2.9261389

Batch 224040, train_perplexity=18.65546, train_loss=2.9261389

Batch 224050, train_perplexity=18.655457, train_loss=2.9261386

Batch 224060, train_perplexity=18.65546, train_loss=2.9261389

Batch 224070, train_perplexity=18.65546, train_loss=2.9261389

Batch 224080, train_perplexity=18.65546, train_loss=2.9261389

Batch 224090, train_perplexity=18.65546, train_loss=2.9261389

Batch 224100, train_perplexity=18.65546, train_loss=2.9261389

Batch 224110, train_perplexity=18.65546, train_loss=2.9261389

Batch 224120, train_perplexity=18.65546, train_loss=2.9261389

Batch 224130, train_perplexity=18.65545, train_loss=2.9261384

Batch 224140, train_perplexity=18.65546, train_loss=2.9261389

Batch 224150, train_perplexity=18.655457, train_loss=2.9261386

Batch 224160, train_perplexity=18.655457, train_loss=2.9261386

Batch 224170, train_perplexity=18.655457, train_loss=2.9261386

Batch 224180, train_perplexity=18.65545, train_loss=2.9261384

Batch 224190, train_perplexity=18.65545, train_loss=2.9261384

Batch 224200, train_perplexity=18.655457, train_loss=2.9261386

Batch 224210, train_perplexity=18.65545, train_loss=2.9261384

Batch 224220, train_perplexity=18.655457, train_loss=2.9261386

Batch 224230, train_perplexity=18.65545, train_loss=2.9261384

Batch 224240, train_perplexity=18.65545, train_loss=2.9261384

Batch 224250, train_perplexity=18.65545, train_loss=2.9261384

Batch 224260, train_perplexity=18.655457, train_loss=2.9261386

Batch 224270, train_perplexity=18.655457, train_loss=2.9261386

Batch 224280, train_perplexity=18.65545, train_loss=2.9261384

Batch 224290, train_perplexity=18.65545, train_loss=2.9261384

Batch 224300, train_perplexity=18.65545, train_loss=2.9261384

Batch 224310, train_perplexity=18.65545, train_loss=2.9261384

Batch 224320, train_perplexity=18.65545, train_loss=2.9261384

Batch 224330, train_perplexity=18.65545, train_loss=2.9261384

Batch 224340, train_perplexity=18.65545, train_loss=2.9261384

Batch 224350, train_perplexity=18.65545, train_loss=2.9261384

Batch 224360, train_perplexity=18.65545, train_loss=2.9261384

Batch 224370, train_perplexity=18.65545, train_loss=2.9261384

Batch 224380, train_perplexity=18.65545, train_loss=2.9261384

Batch 224390, train_perplexity=18.655447, train_loss=2.9261382

Batch 224400, train_perplexity=18.65545, train_loss=2.9261384

Batch 224410, train_perplexity=18.655447, train_loss=2.9261382

Batch 224420, train_perplexity=18.65545, train_loss=2.9261384

Batch 224430, train_perplexity=18.65545, train_loss=2.9261384

Batch 224440, train_perplexity=18.655447, train_loss=2.9261382

Batch 224450, train_perplexity=18.655447, train_loss=2.9261382

Batch 224460, train_perplexity=18.655447, train_loss=2.9261382

Batch 224470, train_perplexity=18.65545, train_loss=2.9261384

Batch 224480, train_perplexity=18.655447, train_loss=2.9261382

Batch 224490, train_perplexity=18.655443, train_loss=2.926138

Batch 224500, train_perplexity=18.655447, train_loss=2.9261382

Batch 224510, train_perplexity=18.655447, train_loss=2.9261382

Batch 224520, train_perplexity=18.655443, train_loss=2.926138

Batch 224530, train_perplexity=18.655443, train_loss=2.926138

Batch 224540, train_perplexity=18.655443, train_loss=2.926138

Batch 224550, train_perplexity=18.655443, train_loss=2.926138

Batch 224560, train_perplexity=18.655443, train_loss=2.926138

Batch 224570, train_perplexity=18.655443, train_loss=2.926138

Batch 224580, train_perplexity=18.655443, train_loss=2.926138

Batch 224590, train_perplexity=18.655443, train_loss=2.926138

Batch 224600, train_perplexity=18.655443, train_loss=2.926138

Batch 224610, train_perplexity=18.655447, train_loss=2.9261382

Batch 224620, train_perplexity=18.655443, train_loss=2.926138

Batch 224630, train_perplexity=18.655443, train_loss=2.926138

Batch 224640, train_perplexity=18.655443, train_loss=2.926138

Batch 224650, train_perplexity=18.655443, train_loss=2.926138

Batch 224660, train_perplexity=18.655443, train_loss=2.926138

Batch 224670, train_perplexity=18.655443, train_loss=2.926138

Batch 224680, train_perplexity=18.655443, train_loss=2.926138

Batch 224690, train_perplexity=18.655443, train_loss=2.926138

Batch 224700, train_perplexity=18.655443, train_loss=2.926138

Batch 224710, train_perplexity=18.655443, train_loss=2.926138

Batch 224720, train_perplexity=18.655443, train_loss=2.926138

Batch 224730, train_perplexity=18.655437, train_loss=2.9261377

Batch 224740, train_perplexity=18.655437, train_loss=2.9261377

Batch 224750, train_perplexity=18.655443, train_loss=2.926138

Batch 224760, train_perplexity=18.655443, train_loss=2.926138

Batch 224770, train_perplexity=18.655443, train_loss=2.926138

Batch 224780, train_perplexity=18.655434, train_loss=2.9261374

Batch 224790, train_perplexity=18.655434, train_loss=2.9261374

Batch 224800, train_perplexity=18.655437, train_loss=2.9261377

Batch 224810, train_perplexity=18.655434, train_loss=2.9261374

Batch 224820, train_perplexity=18.655437, train_loss=2.9261377

Batch 224830, train_perplexity=18.655443, train_loss=2.926138

Batch 224840, train_perplexity=18.655434, train_loss=2.9261374

Batch 224850, train_perplexity=18.655434, train_loss=2.9261374

Batch 224860, train_perplexity=18.655443, train_loss=2.926138

Batch 224870, train_perplexity=18.655434, train_loss=2.9261374

Batch 224880, train_perplexity=18.655434, train_loss=2.9261374

Batch 224890, train_perplexity=18.655434, train_loss=2.9261374
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 224900, train_perplexity=18.655434, train_loss=2.9261374

Batch 224910, train_perplexity=18.655443, train_loss=2.926138

Batch 224920, train_perplexity=18.655434, train_loss=2.9261374

Batch 224930, train_perplexity=18.655434, train_loss=2.9261374

Batch 224940, train_perplexity=18.655434, train_loss=2.9261374

Batch 224950, train_perplexity=18.655434, train_loss=2.9261374

Batch 224960, train_perplexity=18.655434, train_loss=2.9261374

Batch 224970, train_perplexity=18.655434, train_loss=2.9261374

Batch 224980, train_perplexity=18.655434, train_loss=2.9261374

Batch 224990, train_perplexity=18.655434, train_loss=2.9261374

Batch 225000, train_perplexity=18.65543, train_loss=2.9261372

Batch 225010, train_perplexity=18.655434, train_loss=2.9261374

Batch 225020, train_perplexity=18.655434, train_loss=2.9261374

Batch 225030, train_perplexity=18.655434, train_loss=2.9261374

Batch 225040, train_perplexity=18.655434, train_loss=2.9261374

Batch 225050, train_perplexity=18.655434, train_loss=2.9261374

Batch 225060, train_perplexity=18.655434, train_loss=2.9261374

Batch 225070, train_perplexity=18.655424, train_loss=2.926137

Batch 225080, train_perplexity=18.65543, train_loss=2.9261372

Batch 225090, train_perplexity=18.65543, train_loss=2.9261372

Batch 225100, train_perplexity=18.655434, train_loss=2.9261374

Batch 225110, train_perplexity=18.655434, train_loss=2.9261374

Batch 225120, train_perplexity=18.65543, train_loss=2.9261372

Batch 225130, train_perplexity=18.655424, train_loss=2.926137

Batch 225140, train_perplexity=18.65543, train_loss=2.9261372

Batch 225150, train_perplexity=18.65543, train_loss=2.9261372

Batch 225160, train_perplexity=18.655424, train_loss=2.926137

Batch 225170, train_perplexity=18.65543, train_loss=2.9261372

Batch 225180, train_perplexity=18.655424, train_loss=2.926137

Batch 225190, train_perplexity=18.655424, train_loss=2.926137

Batch 225200, train_perplexity=18.655424, train_loss=2.926137

Batch 225210, train_perplexity=18.655424, train_loss=2.926137

Batch 225220, train_perplexity=18.655424, train_loss=2.926137

Batch 225230, train_perplexity=18.655424, train_loss=2.926137

Batch 225240, train_perplexity=18.655424, train_loss=2.926137

Batch 225250, train_perplexity=18.655424, train_loss=2.926137

Batch 225260, train_perplexity=18.655424, train_loss=2.926137

Batch 225270, train_perplexity=18.655424, train_loss=2.926137

Batch 225280, train_perplexity=18.655424, train_loss=2.926137

Batch 225290, train_perplexity=18.655424, train_loss=2.926137

Batch 225300, train_perplexity=18.655424, train_loss=2.926137

Batch 225310, train_perplexity=18.655424, train_loss=2.926137

Batch 225320, train_perplexity=18.655424, train_loss=2.926137

Batch 225330, train_perplexity=18.655424, train_loss=2.926137

Batch 225340, train_perplexity=18.655424, train_loss=2.926137

Batch 225350, train_perplexity=18.655424, train_loss=2.926137

Batch 225360, train_perplexity=18.655424, train_loss=2.926137

Batch 225370, train_perplexity=18.655424, train_loss=2.926137

Batch 225380, train_perplexity=18.655416, train_loss=2.9261365

Batch 225390, train_perplexity=18.655424, train_loss=2.926137

Batch 225400, train_perplexity=18.65542, train_loss=2.9261367

Batch 225410, train_perplexity=18.655424, train_loss=2.926137

Batch 225420, train_perplexity=18.655416, train_loss=2.9261365

Batch 225430, train_perplexity=18.655416, train_loss=2.9261365

Batch 225440, train_perplexity=18.65542, train_loss=2.9261367

Batch 225450, train_perplexity=18.655416, train_loss=2.9261365

Batch 225460, train_perplexity=18.655424, train_loss=2.926137

Batch 225470, train_perplexity=18.655416, train_loss=2.9261365

Batch 225480, train_perplexity=18.655416, train_loss=2.9261365

Batch 225490, train_perplexity=18.65542, train_loss=2.9261367

Batch 225500, train_perplexity=18.655416, train_loss=2.9261365

Batch 225510, train_perplexity=18.655424, train_loss=2.926137

Batch 225520, train_perplexity=18.655416, train_loss=2.9261365

Batch 225530, train_perplexity=18.655416, train_loss=2.9261365

Batch 225540, train_perplexity=18.655416, train_loss=2.9261365

Batch 225550, train_perplexity=18.655416, train_loss=2.9261365

Batch 225560, train_perplexity=18.655416, train_loss=2.9261365

Batch 225570, train_perplexity=18.655416, train_loss=2.9261365

Batch 225580, train_perplexity=18.655416, train_loss=2.9261365

Batch 225590, train_perplexity=18.655416, train_loss=2.9261365

Batch 225600, train_perplexity=18.655416, train_loss=2.9261365

Batch 225610, train_perplexity=18.655416, train_loss=2.9261365

Batch 225620, train_perplexity=18.65541, train_loss=2.9261363

Batch 225630, train_perplexity=18.655416, train_loss=2.9261365

Batch 225640, train_perplexity=18.655407, train_loss=2.926136

Batch 225650, train_perplexity=18.655416, train_loss=2.9261365

Batch 225660, train_perplexity=18.655416, train_loss=2.9261365

Batch 225670, train_perplexity=18.655407, train_loss=2.926136

Batch 225680, train_perplexity=18.65541, train_loss=2.9261363

Batch 225690, train_perplexity=18.655416, train_loss=2.9261365

Batch 225700, train_perplexity=18.65541, train_loss=2.9261363

Batch 225710, train_perplexity=18.655416, train_loss=2.9261365

Batch 225720, train_perplexity=18.65541, train_loss=2.9261363

Batch 225730, train_perplexity=18.655407, train_loss=2.926136

Batch 225740, train_perplexity=18.655407, train_loss=2.926136

Batch 225750, train_perplexity=18.65541, train_loss=2.9261363

Batch 225760, train_perplexity=18.655407, train_loss=2.926136

Batch 225770, train_perplexity=18.655416, train_loss=2.9261365

Batch 225780, train_perplexity=18.655407, train_loss=2.926136

Batch 225790, train_perplexity=18.65541, train_loss=2.9261363

Batch 225800, train_perplexity=18.65541, train_loss=2.9261363

Batch 225810, train_perplexity=18.655403, train_loss=2.9261358

Batch 225820, train_perplexity=18.655407, train_loss=2.926136

Batch 225830, train_perplexity=18.655407, train_loss=2.926136

Batch 225840, train_perplexity=18.655407, train_loss=2.926136

Batch 225850, train_perplexity=18.655407, train_loss=2.926136

Batch 225860, train_perplexity=18.655403, train_loss=2.9261358

Batch 225870, train_perplexity=18.655407, train_loss=2.926136

Batch 225880, train_perplexity=18.65541, train_loss=2.9261363

Batch 225890, train_perplexity=18.655403, train_loss=2.9261358

Batch 225900, train_perplexity=18.655407, train_loss=2.926136

Batch 225910, train_perplexity=18.655407, train_loss=2.926136

Batch 225920, train_perplexity=18.655407, train_loss=2.926136

Batch 225930, train_perplexity=18.655403, train_loss=2.9261358

Batch 225940, train_perplexity=18.655407, train_loss=2.926136

Batch 225950, train_perplexity=18.655403, train_loss=2.9261358

Batch 225960, train_perplexity=18.655407, train_loss=2.926136

Batch 225970, train_perplexity=18.655407, train_loss=2.926136

Batch 225980, train_perplexity=18.655407, train_loss=2.926136

Batch 225990, train_perplexity=18.655407, train_loss=2.926136

Batch 226000, train_perplexity=18.655403, train_loss=2.9261358

Batch 226010, train_perplexity=18.655407, train_loss=2.926136

Batch 226020, train_perplexity=18.655407, train_loss=2.926136

Batch 226030, train_perplexity=18.655397, train_loss=2.9261355

Batch 226040, train_perplexity=18.655403, train_loss=2.9261358

Batch 226050, train_perplexity=18.655397, train_loss=2.9261355

Batch 226060, train_perplexity=18.655403, train_loss=2.9261358

Batch 226070, train_perplexity=18.655397, train_loss=2.9261355

Batch 226080, train_perplexity=18.655403, train_loss=2.9261358

Batch 226090, train_perplexity=18.655397, train_loss=2.9261355

Batch 226100, train_perplexity=18.655397, train_loss=2.9261355

Batch 226110, train_perplexity=18.655397, train_loss=2.9261355

Batch 226120, train_perplexity=18.655397, train_loss=2.9261355

Batch 226130, train_perplexity=18.655397, train_loss=2.9261355

Batch 226140, train_perplexity=18.655397, train_loss=2.9261355

Batch 226150, train_perplexity=18.655397, train_loss=2.9261355

Batch 226160, train_perplexity=18.655397, train_loss=2.9261355

Batch 226170, train_perplexity=18.655403, train_loss=2.9261358

Batch 226180, train_perplexity=18.655397, train_loss=2.9261355

Batch 226190, train_perplexity=18.655394, train_loss=2.9261353
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 226200, train_perplexity=18.655397, train_loss=2.9261355

Batch 226210, train_perplexity=18.655397, train_loss=2.9261355

Batch 226220, train_perplexity=18.655397, train_loss=2.9261355

Batch 226230, train_perplexity=18.655397, train_loss=2.9261355

Batch 226240, train_perplexity=18.655397, train_loss=2.9261355

Batch 226250, train_perplexity=18.655397, train_loss=2.9261355

Batch 226260, train_perplexity=18.655397, train_loss=2.9261355

Batch 226270, train_perplexity=18.655394, train_loss=2.9261353

Batch 226280, train_perplexity=18.655394, train_loss=2.9261353

Batch 226290, train_perplexity=18.655397, train_loss=2.9261355

Batch 226300, train_perplexity=18.655394, train_loss=2.9261353

Batch 226310, train_perplexity=18.65539, train_loss=2.926135

Batch 226320, train_perplexity=18.65539, train_loss=2.926135

Batch 226330, train_perplexity=18.65539, train_loss=2.926135

Batch 226340, train_perplexity=18.655397, train_loss=2.9261355

Batch 226350, train_perplexity=18.655394, train_loss=2.9261353

Batch 226360, train_perplexity=18.65539, train_loss=2.926135

Batch 226370, train_perplexity=18.655397, train_loss=2.9261355

Batch 226380, train_perplexity=18.655394, train_loss=2.9261353

Batch 226390, train_perplexity=18.655397, train_loss=2.9261355

Batch 226400, train_perplexity=18.65539, train_loss=2.926135

Batch 226410, train_perplexity=18.655394, train_loss=2.9261353

Batch 226420, train_perplexity=18.65539, train_loss=2.926135

Batch 226430, train_perplexity=18.65539, train_loss=2.926135

Batch 226440, train_perplexity=18.655394, train_loss=2.9261353

Batch 226450, train_perplexity=18.65539, train_loss=2.926135

Batch 226460, train_perplexity=18.65539, train_loss=2.926135

Batch 226470, train_perplexity=18.65539, train_loss=2.926135

Batch 226480, train_perplexity=18.65539, train_loss=2.926135

Batch 226490, train_perplexity=18.65539, train_loss=2.926135

Batch 226500, train_perplexity=18.65539, train_loss=2.926135

Batch 226510, train_perplexity=18.65539, train_loss=2.926135

Batch 226520, train_perplexity=18.65539, train_loss=2.926135

Batch 226530, train_perplexity=18.65539, train_loss=2.926135

Batch 226540, train_perplexity=18.65539, train_loss=2.926135

Batch 226550, train_perplexity=18.65539, train_loss=2.926135

Batch 226560, train_perplexity=18.65539, train_loss=2.926135

Batch 226570, train_perplexity=18.65539, train_loss=2.926135

Batch 226580, train_perplexity=18.65539, train_loss=2.926135

Batch 226590, train_perplexity=18.65539, train_loss=2.926135

Batch 226600, train_perplexity=18.655384, train_loss=2.9261348

Batch 226610, train_perplexity=18.65538, train_loss=2.9261346

Batch 226620, train_perplexity=18.65539, train_loss=2.926135

Batch 226630, train_perplexity=18.65539, train_loss=2.926135

Batch 226640, train_perplexity=18.655384, train_loss=2.9261348

Batch 226650, train_perplexity=18.655384, train_loss=2.9261348

Batch 226660, train_perplexity=18.65538, train_loss=2.9261346

Batch 226670, train_perplexity=18.65538, train_loss=2.9261346

Batch 226680, train_perplexity=18.65539, train_loss=2.926135

Batch 226690, train_perplexity=18.65538, train_loss=2.9261346

Batch 226700, train_perplexity=18.65538, train_loss=2.9261346

Batch 226710, train_perplexity=18.655384, train_loss=2.9261348

Batch 226720, train_perplexity=18.65538, train_loss=2.9261346

Batch 226730, train_perplexity=18.65538, train_loss=2.9261346

Batch 226740, train_perplexity=18.65538, train_loss=2.9261346

Batch 226750, train_perplexity=18.65538, train_loss=2.9261346

Batch 226760, train_perplexity=18.65538, train_loss=2.9261346

Batch 226770, train_perplexity=18.65538, train_loss=2.9261346

Batch 226780, train_perplexity=18.65538, train_loss=2.9261346

Batch 226790, train_perplexity=18.65537, train_loss=2.926134

Batch 226800, train_perplexity=18.65538, train_loss=2.9261346

Batch 226810, train_perplexity=18.65538, train_loss=2.9261346

Batch 226820, train_perplexity=18.65538, train_loss=2.9261346

Batch 226830, train_perplexity=18.65538, train_loss=2.9261346

Batch 226840, train_perplexity=18.655376, train_loss=2.9261343

Batch 226850, train_perplexity=18.65538, train_loss=2.9261346

Batch 226860, train_perplexity=18.65538, train_loss=2.9261346

Batch 226870, train_perplexity=18.655376, train_loss=2.9261343

Batch 226880, train_perplexity=18.65538, train_loss=2.9261346

Batch 226890, train_perplexity=18.65538, train_loss=2.9261346

Batch 226900, train_perplexity=18.65538, train_loss=2.9261346

Batch 226910, train_perplexity=18.655376, train_loss=2.9261343

Batch 226920, train_perplexity=18.65537, train_loss=2.926134

Batch 226930, train_perplexity=18.65538, train_loss=2.9261346

Batch 226940, train_perplexity=18.655376, train_loss=2.9261343

Batch 226950, train_perplexity=18.65537, train_loss=2.926134

Batch 226960, train_perplexity=18.65537, train_loss=2.926134

Batch 226970, train_perplexity=18.65537, train_loss=2.926134

Batch 226980, train_perplexity=18.655376, train_loss=2.9261343

Batch 226990, train_perplexity=18.655376, train_loss=2.9261343

Batch 227000, train_perplexity=18.65537, train_loss=2.926134

Batch 227010, train_perplexity=18.655367, train_loss=2.9261339

Batch 227020, train_perplexity=18.65537, train_loss=2.926134

Batch 227030, train_perplexity=18.65537, train_loss=2.926134

Batch 227040, train_perplexity=18.65537, train_loss=2.926134

Batch 227050, train_perplexity=18.655376, train_loss=2.9261343

Batch 227060, train_perplexity=18.65537, train_loss=2.926134

Batch 227070, train_perplexity=18.65537, train_loss=2.926134

Batch 227080, train_perplexity=18.65537, train_loss=2.926134

Batch 227090, train_perplexity=18.655376, train_loss=2.9261343

Batch 227100, train_perplexity=18.65537, train_loss=2.926134

Batch 227110, train_perplexity=18.65537, train_loss=2.926134

Batch 227120, train_perplexity=18.65537, train_loss=2.926134

Batch 227130, train_perplexity=18.65537, train_loss=2.926134

Batch 227140, train_perplexity=18.65537, train_loss=2.926134

Batch 227150, train_perplexity=18.65537, train_loss=2.926134

Batch 227160, train_perplexity=18.65537, train_loss=2.926134

Batch 227170, train_perplexity=18.655363, train_loss=2.9261336

Batch 227180, train_perplexity=18.65537, train_loss=2.926134

Batch 227190, train_perplexity=18.65537, train_loss=2.926134

Batch 227200, train_perplexity=18.65537, train_loss=2.926134

Batch 227210, train_perplexity=18.65537, train_loss=2.926134

Batch 227220, train_perplexity=18.65537, train_loss=2.926134

Batch 227230, train_perplexity=18.655367, train_loss=2.9261339

Batch 227240, train_perplexity=18.655363, train_loss=2.9261336

Batch 227250, train_perplexity=18.655363, train_loss=2.9261336

Batch 227260, train_perplexity=18.655363, train_loss=2.9261336

Batch 227270, train_perplexity=18.655367, train_loss=2.9261339

Batch 227280, train_perplexity=18.655367, train_loss=2.9261339

Batch 227290, train_perplexity=18.655363, train_loss=2.9261336

Batch 227300, train_perplexity=18.655363, train_loss=2.9261336

Batch 227310, train_perplexity=18.65537, train_loss=2.926134

Batch 227320, train_perplexity=18.655363, train_loss=2.9261336

Batch 227330, train_perplexity=18.655363, train_loss=2.9261336

Batch 227340, train_perplexity=18.655363, train_loss=2.9261336

Batch 227350, train_perplexity=18.655363, train_loss=2.9261336

Batch 227360, train_perplexity=18.655363, train_loss=2.9261336

Batch 227370, train_perplexity=18.655363, train_loss=2.9261336

Batch 227380, train_perplexity=18.655363, train_loss=2.9261336

Batch 227390, train_perplexity=18.655363, train_loss=2.9261336

Batch 227400, train_perplexity=18.655363, train_loss=2.9261336

Batch 227410, train_perplexity=18.655363, train_loss=2.9261336

Batch 227420, train_perplexity=18.655363, train_loss=2.9261336

Batch 227430, train_perplexity=18.655363, train_loss=2.9261336

Batch 227440, train_perplexity=18.655363, train_loss=2.9261336

Batch 227450, train_perplexity=18.655363, train_loss=2.9261336

Batch 227460, train_perplexity=18.655363, train_loss=2.9261336

Batch 227470, train_perplexity=18.655363, train_loss=2.9261336

Batch 227480, train_perplexity=18.655363, train_loss=2.9261336

Batch 227490, train_perplexity=18.655363, train_loss=2.9261336
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 227500, train_perplexity=18.655363, train_loss=2.9261336

Batch 227510, train_perplexity=18.655357, train_loss=2.9261334

Batch 227520, train_perplexity=18.655363, train_loss=2.9261336

Batch 227530, train_perplexity=18.655354, train_loss=2.9261332

Batch 227540, train_perplexity=18.655357, train_loss=2.9261334

Batch 227550, train_perplexity=18.655363, train_loss=2.9261336

Batch 227560, train_perplexity=18.655363, train_loss=2.9261336

Batch 227570, train_perplexity=18.655357, train_loss=2.9261334

Batch 227580, train_perplexity=18.655363, train_loss=2.9261336

Batch 227590, train_perplexity=18.655357, train_loss=2.9261334

Batch 227600, train_perplexity=18.655354, train_loss=2.9261332

Batch 227610, train_perplexity=18.655357, train_loss=2.9261334

Batch 227620, train_perplexity=18.655354, train_loss=2.9261332

Batch 227630, train_perplexity=18.655354, train_loss=2.9261332

Batch 227640, train_perplexity=18.655354, train_loss=2.9261332

Batch 227650, train_perplexity=18.655357, train_loss=2.9261334

Batch 227660, train_perplexity=18.655363, train_loss=2.9261336

Batch 227670, train_perplexity=18.655354, train_loss=2.9261332

Batch 227680, train_perplexity=18.655357, train_loss=2.9261334

Batch 227690, train_perplexity=18.655354, train_loss=2.9261332

Batch 227700, train_perplexity=18.655354, train_loss=2.9261332

Batch 227710, train_perplexity=18.655354, train_loss=2.9261332

Batch 227720, train_perplexity=18.655354, train_loss=2.9261332

Batch 227730, train_perplexity=18.65535, train_loss=2.926133

Batch 227740, train_perplexity=18.655354, train_loss=2.9261332

Batch 227750, train_perplexity=18.65535, train_loss=2.926133

Batch 227760, train_perplexity=18.655354, train_loss=2.9261332

Batch 227770, train_perplexity=18.655354, train_loss=2.9261332

Batch 227780, train_perplexity=18.655354, train_loss=2.9261332

Batch 227790, train_perplexity=18.655354, train_loss=2.9261332

Batch 227800, train_perplexity=18.655354, train_loss=2.9261332

Batch 227810, train_perplexity=18.655354, train_loss=2.9261332

Batch 227820, train_perplexity=18.65535, train_loss=2.926133

Batch 227830, train_perplexity=18.655354, train_loss=2.9261332

Batch 227840, train_perplexity=18.655354, train_loss=2.9261332

Batch 227850, train_perplexity=18.655354, train_loss=2.9261332

Batch 227860, train_perplexity=18.655354, train_loss=2.9261332

Batch 227870, train_perplexity=18.65535, train_loss=2.926133

Batch 227880, train_perplexity=18.655344, train_loss=2.9261327

Batch 227890, train_perplexity=18.655354, train_loss=2.9261332

Batch 227900, train_perplexity=18.655344, train_loss=2.9261327

Batch 227910, train_perplexity=18.655344, train_loss=2.9261327

Batch 227920, train_perplexity=18.65535, train_loss=2.926133

Batch 227930, train_perplexity=18.655354, train_loss=2.9261332

Batch 227940, train_perplexity=18.655344, train_loss=2.9261327

Batch 227950, train_perplexity=18.655344, train_loss=2.9261327

Batch 227960, train_perplexity=18.65534, train_loss=2.9261324

Batch 227970, train_perplexity=18.65535, train_loss=2.926133

Batch 227980, train_perplexity=18.655344, train_loss=2.9261327

Batch 227990, train_perplexity=18.655344, train_loss=2.9261327

Batch 228000, train_perplexity=18.655344, train_loss=2.9261327

Batch 228010, train_perplexity=18.655344, train_loss=2.9261327

Batch 228020, train_perplexity=18.655354, train_loss=2.9261332

Batch 228030, train_perplexity=18.655344, train_loss=2.9261327

Batch 228040, train_perplexity=18.655344, train_loss=2.9261327

Batch 228050, train_perplexity=18.655344, train_loss=2.9261327

Batch 228060, train_perplexity=18.655344, train_loss=2.9261327

Batch 228070, train_perplexity=18.655344, train_loss=2.9261327

Batch 228080, train_perplexity=18.655344, train_loss=2.9261327

Batch 228090, train_perplexity=18.655344, train_loss=2.9261327

Batch 228100, train_perplexity=18.655344, train_loss=2.9261327

Batch 228110, train_perplexity=18.655344, train_loss=2.9261327

Batch 228120, train_perplexity=18.655336, train_loss=2.9261322

Batch 228130, train_perplexity=18.655344, train_loss=2.9261327

Batch 228140, train_perplexity=18.655344, train_loss=2.9261327

Batch 228150, train_perplexity=18.655344, train_loss=2.9261327

Batch 228160, train_perplexity=18.65534, train_loss=2.9261324

Batch 228170, train_perplexity=18.655336, train_loss=2.9261322

Batch 228180, train_perplexity=18.655336, train_loss=2.9261322

Batch 228190, train_perplexity=18.655336, train_loss=2.9261322

Batch 228200, train_perplexity=18.65534, train_loss=2.9261324

Batch 228210, train_perplexity=18.65534, train_loss=2.9261324

Batch 228220, train_perplexity=18.65534, train_loss=2.9261324

Batch 228230, train_perplexity=18.65534, train_loss=2.9261324

Batch 228240, train_perplexity=18.65534, train_loss=2.9261324

Batch 228250, train_perplexity=18.65534, train_loss=2.9261324

Batch 228260, train_perplexity=18.655336, train_loss=2.9261322

Batch 228270, train_perplexity=18.655336, train_loss=2.9261322

Batch 228280, train_perplexity=18.655336, train_loss=2.9261322

Batch 228290, train_perplexity=18.65534, train_loss=2.9261324

Batch 228300, train_perplexity=18.655336, train_loss=2.9261322

Batch 228310, train_perplexity=18.655336, train_loss=2.9261322

Batch 228320, train_perplexity=18.65534, train_loss=2.9261324

Batch 228330, train_perplexity=18.655336, train_loss=2.9261322

Batch 228340, train_perplexity=18.655336, train_loss=2.9261322

Batch 228350, train_perplexity=18.65533, train_loss=2.926132

Batch 228360, train_perplexity=18.655336, train_loss=2.9261322

Batch 228370, train_perplexity=18.655336, train_loss=2.9261322

Batch 228380, train_perplexity=18.655336, train_loss=2.9261322

Batch 228390, train_perplexity=18.655336, train_loss=2.9261322

Batch 228400, train_perplexity=18.655336, train_loss=2.9261322

Batch 228410, train_perplexity=18.655336, train_loss=2.9261322

Batch 228420, train_perplexity=18.655336, train_loss=2.9261322

Batch 228430, train_perplexity=18.65533, train_loss=2.926132

Batch 228440, train_perplexity=18.655336, train_loss=2.9261322

Batch 228450, train_perplexity=18.655336, train_loss=2.9261322

Batch 228460, train_perplexity=18.655336, train_loss=2.9261322

Batch 228470, train_perplexity=18.655336, train_loss=2.9261322

Batch 228480, train_perplexity=18.655336, train_loss=2.9261322

Batch 228490, train_perplexity=18.655336, train_loss=2.9261322

Batch 228500, train_perplexity=18.65533, train_loss=2.926132

Batch 228510, train_perplexity=18.655336, train_loss=2.9261322

Batch 228520, train_perplexity=18.65533, train_loss=2.926132

Batch 228530, train_perplexity=18.655336, train_loss=2.9261322

Batch 228540, train_perplexity=18.65533, train_loss=2.926132

Batch 228550, train_perplexity=18.65533, train_loss=2.926132

Batch 228560, train_perplexity=18.65533, train_loss=2.926132

Batch 228570, train_perplexity=18.655336, train_loss=2.9261322

Batch 228580, train_perplexity=18.655327, train_loss=2.9261317

Batch 228590, train_perplexity=18.655327, train_loss=2.9261317

Batch 228600, train_perplexity=18.655336, train_loss=2.9261322

Batch 228610, train_perplexity=18.655327, train_loss=2.9261317

Batch 228620, train_perplexity=18.655327, train_loss=2.9261317

Batch 228630, train_perplexity=18.65533, train_loss=2.926132

Batch 228640, train_perplexity=18.655327, train_loss=2.9261317

Batch 228650, train_perplexity=18.65533, train_loss=2.926132

Batch 228660, train_perplexity=18.655327, train_loss=2.9261317

Batch 228670, train_perplexity=18.655327, train_loss=2.9261317

Batch 228680, train_perplexity=18.655323, train_loss=2.9261315

Batch 228690, train_perplexity=18.655327, train_loss=2.9261317

Batch 228700, train_perplexity=18.655327, train_loss=2.9261317

Batch 228710, train_perplexity=18.655327, train_loss=2.9261317

Batch 228720, train_perplexity=18.655327, train_loss=2.9261317

Batch 228730, train_perplexity=18.655327, train_loss=2.9261317

Batch 228740, train_perplexity=18.655323, train_loss=2.9261315

Batch 228750, train_perplexity=18.655327, train_loss=2.9261317

Batch 228760, train_perplexity=18.655327, train_loss=2.9261317

Batch 228770, train_perplexity=18.655327, train_loss=2.9261317

Batch 228780, train_perplexity=18.655327, train_loss=2.9261317
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 228790, train_perplexity=18.655327, train_loss=2.9261317

Batch 228800, train_perplexity=18.655327, train_loss=2.9261317

Batch 228810, train_perplexity=18.655317, train_loss=2.9261312

Batch 228820, train_perplexity=18.655323, train_loss=2.9261315

Batch 228830, train_perplexity=18.655323, train_loss=2.9261315

Batch 228840, train_perplexity=18.655317, train_loss=2.9261312

Batch 228850, train_perplexity=18.655317, train_loss=2.9261312

Batch 228860, train_perplexity=18.655323, train_loss=2.9261315

Batch 228870, train_perplexity=18.655317, train_loss=2.9261312

Batch 228880, train_perplexity=18.655323, train_loss=2.9261315

Batch 228890, train_perplexity=18.655317, train_loss=2.9261312

Batch 228900, train_perplexity=18.655317, train_loss=2.9261312

Batch 228910, train_perplexity=18.655323, train_loss=2.9261315

Batch 228920, train_perplexity=18.655317, train_loss=2.9261312

Batch 228930, train_perplexity=18.655317, train_loss=2.9261312

Batch 228940, train_perplexity=18.655323, train_loss=2.9261315

Batch 228950, train_perplexity=18.655323, train_loss=2.9261315

Batch 228960, train_perplexity=18.655313, train_loss=2.926131

Batch 228970, train_perplexity=18.655317, train_loss=2.9261312

Batch 228980, train_perplexity=18.655317, train_loss=2.9261312

Batch 228990, train_perplexity=18.655317, train_loss=2.9261312

Batch 229000, train_perplexity=18.655317, train_loss=2.9261312

Batch 229010, train_perplexity=18.655317, train_loss=2.9261312

Batch 229020, train_perplexity=18.655317, train_loss=2.9261312

Batch 229030, train_perplexity=18.655317, train_loss=2.9261312

Batch 229040, train_perplexity=18.655317, train_loss=2.9261312

Batch 229050, train_perplexity=18.655317, train_loss=2.9261312

Batch 229060, train_perplexity=18.65531, train_loss=2.9261308

Batch 229070, train_perplexity=18.655313, train_loss=2.926131

Batch 229080, train_perplexity=18.655317, train_loss=2.9261312

Batch 229090, train_perplexity=18.655317, train_loss=2.9261312

Batch 229100, train_perplexity=18.655317, train_loss=2.9261312

Batch 229110, train_perplexity=18.655317, train_loss=2.9261312

Batch 229120, train_perplexity=18.655313, train_loss=2.926131

Batch 229130, train_perplexity=18.65531, train_loss=2.9261308

Batch 229140, train_perplexity=18.655313, train_loss=2.926131

Batch 229150, train_perplexity=18.655317, train_loss=2.9261312

Batch 229160, train_perplexity=18.655317, train_loss=2.9261312

Batch 229170, train_perplexity=18.65531, train_loss=2.9261308

Batch 229180, train_perplexity=18.655317, train_loss=2.9261312

Batch 229190, train_perplexity=18.65531, train_loss=2.9261308

Batch 229200, train_perplexity=18.655317, train_loss=2.9261312

Batch 229210, train_perplexity=18.65531, train_loss=2.9261308

Batch 229220, train_perplexity=18.65531, train_loss=2.9261308

Batch 229230, train_perplexity=18.655313, train_loss=2.926131

Batch 229240, train_perplexity=18.65531, train_loss=2.9261308

Batch 229250, train_perplexity=18.65531, train_loss=2.9261308

Batch 229260, train_perplexity=18.65531, train_loss=2.9261308

Batch 229270, train_perplexity=18.65531, train_loss=2.9261308

Batch 229280, train_perplexity=18.65531, train_loss=2.9261308

Batch 229290, train_perplexity=18.655304, train_loss=2.9261305

Batch 229300, train_perplexity=18.65531, train_loss=2.9261308

Batch 229310, train_perplexity=18.65531, train_loss=2.9261308

Batch 229320, train_perplexity=18.65531, train_loss=2.9261308

Batch 229330, train_perplexity=18.65531, train_loss=2.9261308

Batch 229340, train_perplexity=18.65531, train_loss=2.9261308

Batch 229350, train_perplexity=18.65531, train_loss=2.9261308

Batch 229360, train_perplexity=18.65531, train_loss=2.9261308

Batch 229370, train_perplexity=18.65531, train_loss=2.9261308

Batch 229380, train_perplexity=18.65531, train_loss=2.9261308

Batch 229390, train_perplexity=18.65531, train_loss=2.9261308

Batch 229400, train_perplexity=18.655304, train_loss=2.9261305

Batch 229410, train_perplexity=18.65531, train_loss=2.9261308

Batch 229420, train_perplexity=18.65531, train_loss=2.9261308

Batch 229430, train_perplexity=18.65531, train_loss=2.9261308

Batch 229440, train_perplexity=18.655304, train_loss=2.9261305

Batch 229450, train_perplexity=18.65531, train_loss=2.9261308

Batch 229460, train_perplexity=18.655304, train_loss=2.9261305

Batch 229470, train_perplexity=18.6553, train_loss=2.9261303

Batch 229480, train_perplexity=18.6553, train_loss=2.9261303

Batch 229490, train_perplexity=18.6553, train_loss=2.9261303

Batch 229500, train_perplexity=18.655304, train_loss=2.9261305

Batch 229510, train_perplexity=18.6553, train_loss=2.9261303

Batch 229520, train_perplexity=18.6553, train_loss=2.9261303

Batch 229530, train_perplexity=18.6553, train_loss=2.9261303

Batch 229540, train_perplexity=18.65531, train_loss=2.9261308

Batch 229550, train_perplexity=18.655304, train_loss=2.9261305

Batch 229560, train_perplexity=18.6553, train_loss=2.9261303

Batch 229570, train_perplexity=18.655304, train_loss=2.9261305

Batch 229580, train_perplexity=18.655304, train_loss=2.9261305

Batch 229590, train_perplexity=18.6553, train_loss=2.9261303

Batch 229600, train_perplexity=18.6553, train_loss=2.9261303

Batch 229610, train_perplexity=18.6553, train_loss=2.9261303

Batch 229620, train_perplexity=18.6553, train_loss=2.9261303

Batch 229630, train_perplexity=18.6553, train_loss=2.9261303

Batch 229640, train_perplexity=18.6553, train_loss=2.9261303

Batch 229650, train_perplexity=18.6553, train_loss=2.9261303

Batch 229660, train_perplexity=18.6553, train_loss=2.9261303

Batch 229670, train_perplexity=18.65529, train_loss=2.9261298

Batch 229680, train_perplexity=18.6553, train_loss=2.9261303

Batch 229690, train_perplexity=18.6553, train_loss=2.9261303

Batch 229700, train_perplexity=18.6553, train_loss=2.9261303

Batch 229710, train_perplexity=18.6553, train_loss=2.9261303

Batch 229720, train_perplexity=18.65529, train_loss=2.9261298

Batch 229730, train_perplexity=18.6553, train_loss=2.9261303

Batch 229740, train_perplexity=18.6553, train_loss=2.9261303

Batch 229750, train_perplexity=18.6553, train_loss=2.9261303

Batch 229760, train_perplexity=18.655296, train_loss=2.92613

Batch 229770, train_perplexity=18.6553, train_loss=2.9261303

Batch 229780, train_perplexity=18.655296, train_loss=2.92613

Batch 229790, train_perplexity=18.6553, train_loss=2.9261303

Batch 229800, train_perplexity=18.65529, train_loss=2.9261298

Batch 229810, train_perplexity=18.65529, train_loss=2.9261298

Batch 229820, train_perplexity=18.65529, train_loss=2.9261298

Batch 229830, train_perplexity=18.6553, train_loss=2.9261303

Batch 229840, train_perplexity=18.65529, train_loss=2.9261298

Batch 229850, train_perplexity=18.65529, train_loss=2.9261298

Batch 229860, train_perplexity=18.65529, train_loss=2.9261298

Batch 229870, train_perplexity=18.65529, train_loss=2.9261298

Batch 229880, train_perplexity=18.65529, train_loss=2.9261298

Batch 229890, train_perplexity=18.65529, train_loss=2.9261298

Batch 229900, train_perplexity=18.65529, train_loss=2.9261298

Batch 229910, train_perplexity=18.65529, train_loss=2.9261298

Batch 229920, train_perplexity=18.65529, train_loss=2.9261298

Batch 229930, train_perplexity=18.65529, train_loss=2.9261298

Batch 229940, train_perplexity=18.65529, train_loss=2.9261298

Batch 229950, train_perplexity=18.65529, train_loss=2.9261298

Batch 229960, train_perplexity=18.65529, train_loss=2.9261298

Batch 229970, train_perplexity=18.65529, train_loss=2.9261298

Batch 229980, train_perplexity=18.65529, train_loss=2.9261298

Batch 229990, train_perplexity=18.65529, train_loss=2.9261298

Batch 230000, train_perplexity=18.65529, train_loss=2.9261298

Batch 230010, train_perplexity=18.655287, train_loss=2.9261296

Batch 230020, train_perplexity=18.65529, train_loss=2.9261298

Batch 230030, train_perplexity=18.65529, train_loss=2.9261298

Batch 230040, train_perplexity=18.655287, train_loss=2.9261296

Batch 230050, train_perplexity=18.655287, train_loss=2.9261296

Batch 230060, train_perplexity=18.655287, train_loss=2.9261296

Batch 230070, train_perplexity=18.655283, train_loss=2.9261293

Batch 230080, train_perplexity=18.655287, train_loss=2.9261296
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 230090, train_perplexity=18.655287, train_loss=2.9261296

Batch 230100, train_perplexity=18.655287, train_loss=2.9261296

Batch 230110, train_perplexity=18.655283, train_loss=2.9261293

Batch 230120, train_perplexity=18.65529, train_loss=2.9261298

Batch 230130, train_perplexity=18.655283, train_loss=2.9261293

Batch 230140, train_perplexity=18.655283, train_loss=2.9261293

Batch 230150, train_perplexity=18.655283, train_loss=2.9261293

Batch 230160, train_perplexity=18.655287, train_loss=2.9261296

Batch 230170, train_perplexity=18.655283, train_loss=2.9261293

Batch 230180, train_perplexity=18.655283, train_loss=2.9261293

Batch 230190, train_perplexity=18.655283, train_loss=2.9261293

Batch 230200, train_perplexity=18.655283, train_loss=2.9261293

Batch 230210, train_perplexity=18.655283, train_loss=2.9261293

Batch 230220, train_perplexity=18.655283, train_loss=2.9261293

Batch 230230, train_perplexity=18.655283, train_loss=2.9261293

Batch 230240, train_perplexity=18.655283, train_loss=2.9261293

Batch 230250, train_perplexity=18.655283, train_loss=2.9261293

Batch 230260, train_perplexity=18.655283, train_loss=2.9261293

Batch 230270, train_perplexity=18.655283, train_loss=2.9261293

Batch 230280, train_perplexity=18.655283, train_loss=2.9261293

Batch 230290, train_perplexity=18.655277, train_loss=2.926129

Batch 230300, train_perplexity=18.655283, train_loss=2.9261293

Batch 230310, train_perplexity=18.655283, train_loss=2.9261293

Batch 230320, train_perplexity=18.655273, train_loss=2.9261289

Batch 230330, train_perplexity=18.655273, train_loss=2.9261289

Batch 230340, train_perplexity=18.655273, train_loss=2.9261289

Batch 230350, train_perplexity=18.655283, train_loss=2.9261293

Batch 230360, train_perplexity=18.655277, train_loss=2.926129

Batch 230370, train_perplexity=18.655283, train_loss=2.9261293

Batch 230380, train_perplexity=18.655273, train_loss=2.9261289

Batch 230390, train_perplexity=18.655273, train_loss=2.9261289

Batch 230400, train_perplexity=18.655273, train_loss=2.9261289

Batch 230410, train_perplexity=18.655283, train_loss=2.9261293

Batch 230420, train_perplexity=18.655277, train_loss=2.926129

Batch 230430, train_perplexity=18.655273, train_loss=2.9261289

Batch 230440, train_perplexity=18.655273, train_loss=2.9261289

Batch 230450, train_perplexity=18.655273, train_loss=2.9261289

Batch 230460, train_perplexity=18.655273, train_loss=2.9261289

Batch 230470, train_perplexity=18.655283, train_loss=2.9261293

Batch 230480, train_perplexity=18.655273, train_loss=2.9261289

Batch 230490, train_perplexity=18.655273, train_loss=2.9261289

Batch 230500, train_perplexity=18.655273, train_loss=2.9261289

Batch 230510, train_perplexity=18.655273, train_loss=2.9261289

Batch 230520, train_perplexity=18.655273, train_loss=2.9261289

Batch 230530, train_perplexity=18.655273, train_loss=2.9261289

Batch 230540, train_perplexity=18.655273, train_loss=2.9261289

Batch 230550, train_perplexity=18.655273, train_loss=2.9261289

Batch 230560, train_perplexity=18.655273, train_loss=2.9261289

Batch 230570, train_perplexity=18.655273, train_loss=2.9261289

Batch 230580, train_perplexity=18.655273, train_loss=2.9261289

Batch 230590, train_perplexity=18.655273, train_loss=2.9261289

Batch 230600, train_perplexity=18.655273, train_loss=2.9261289

Batch 230610, train_perplexity=18.655273, train_loss=2.9261289

Batch 230620, train_perplexity=18.65527, train_loss=2.9261286

Batch 230630, train_perplexity=18.65527, train_loss=2.9261286

Batch 230640, train_perplexity=18.655273, train_loss=2.9261289

Batch 230650, train_perplexity=18.655273, train_loss=2.9261289

Batch 230660, train_perplexity=18.65527, train_loss=2.9261286

Batch 230670, train_perplexity=18.65527, train_loss=2.9261286

Batch 230680, train_perplexity=18.655273, train_loss=2.9261289

Batch 230690, train_perplexity=18.655264, train_loss=2.9261284

Batch 230700, train_perplexity=18.655273, train_loss=2.9261289

Batch 230710, train_perplexity=18.655273, train_loss=2.9261289

Batch 230720, train_perplexity=18.655264, train_loss=2.9261284

Batch 230730, train_perplexity=18.655273, train_loss=2.9261289

Batch 230740, train_perplexity=18.65527, train_loss=2.9261286

Batch 230750, train_perplexity=18.655264, train_loss=2.9261284

Batch 230760, train_perplexity=18.655264, train_loss=2.9261284

Batch 230770, train_perplexity=18.655264, train_loss=2.9261284

Batch 230780, train_perplexity=18.655264, train_loss=2.9261284

Batch 230790, train_perplexity=18.655264, train_loss=2.9261284

Batch 230800, train_perplexity=18.655264, train_loss=2.9261284

Batch 230810, train_perplexity=18.655264, train_loss=2.9261284

Batch 230820, train_perplexity=18.655264, train_loss=2.9261284

Batch 230830, train_perplexity=18.655264, train_loss=2.9261284

Batch 230840, train_perplexity=18.655264, train_loss=2.9261284

Batch 230850, train_perplexity=18.655264, train_loss=2.9261284

Batch 230860, train_perplexity=18.655264, train_loss=2.9261284

Batch 230870, train_perplexity=18.655264, train_loss=2.9261284

Batch 230880, train_perplexity=18.655264, train_loss=2.9261284

Batch 230890, train_perplexity=18.655264, train_loss=2.9261284

Batch 230900, train_perplexity=18.655264, train_loss=2.9261284

Batch 230910, train_perplexity=18.655264, train_loss=2.9261284

Batch 230920, train_perplexity=18.655264, train_loss=2.9261284

Batch 230930, train_perplexity=18.655264, train_loss=2.9261284

Batch 230940, train_perplexity=18.655264, train_loss=2.9261284

Batch 230950, train_perplexity=18.655264, train_loss=2.9261284

Batch 230960, train_perplexity=18.655264, train_loss=2.9261284

Batch 230970, train_perplexity=18.65526, train_loss=2.9261281

Batch 230980, train_perplexity=18.655264, train_loss=2.9261284

Batch 230990, train_perplexity=18.655264, train_loss=2.9261284

Batch 231000, train_perplexity=18.655264, train_loss=2.9261284

Batch 231010, train_perplexity=18.65526, train_loss=2.9261281

Batch 231020, train_perplexity=18.65526, train_loss=2.9261281

Batch 231030, train_perplexity=18.655264, train_loss=2.9261284

Batch 231040, train_perplexity=18.65526, train_loss=2.9261281

Batch 231050, train_perplexity=18.655264, train_loss=2.9261284

Batch 231060, train_perplexity=18.655256, train_loss=2.926128

Batch 231070, train_perplexity=18.655256, train_loss=2.926128

Batch 231080, train_perplexity=18.65526, train_loss=2.9261281

Batch 231090, train_perplexity=18.655256, train_loss=2.926128

Batch 231100, train_perplexity=18.655256, train_loss=2.926128

Batch 231110, train_perplexity=18.655256, train_loss=2.926128

Batch 231120, train_perplexity=18.655256, train_loss=2.926128

Batch 231130, train_perplexity=18.655256, train_loss=2.926128

Batch 231140, train_perplexity=18.655256, train_loss=2.926128

Batch 231150, train_perplexity=18.655256, train_loss=2.926128

Batch 231160, train_perplexity=18.655256, train_loss=2.926128

Batch 231170, train_perplexity=18.655256, train_loss=2.926128

Batch 231180, train_perplexity=18.655256, train_loss=2.926128

Batch 231190, train_perplexity=18.655256, train_loss=2.926128

Batch 231200, train_perplexity=18.655256, train_loss=2.926128

Batch 231210, train_perplexity=18.655256, train_loss=2.926128

Batch 231220, train_perplexity=18.655247, train_loss=2.9261274

Batch 231230, train_perplexity=18.65525, train_loss=2.9261277

Batch 231240, train_perplexity=18.655256, train_loss=2.926128

Batch 231250, train_perplexity=18.65525, train_loss=2.9261277

Batch 231260, train_perplexity=18.655247, train_loss=2.9261274

Batch 231270, train_perplexity=18.655256, train_loss=2.926128

Batch 231280, train_perplexity=18.655256, train_loss=2.926128

Batch 231290, train_perplexity=18.655247, train_loss=2.9261274

Batch 231300, train_perplexity=18.655256, train_loss=2.926128

Batch 231310, train_perplexity=18.655256, train_loss=2.926128

Batch 231320, train_perplexity=18.655256, train_loss=2.926128

Batch 231330, train_perplexity=18.65525, train_loss=2.9261277

Batch 231340, train_perplexity=18.65525, train_loss=2.9261277

Batch 231350, train_perplexity=18.65525, train_loss=2.9261277

Batch 231360, train_perplexity=18.655247, train_loss=2.9261274

Batch 231370, train_perplexity=18.655256, train_loss=2.926128
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 231380, train_perplexity=18.655247, train_loss=2.9261274

Batch 231390, train_perplexity=18.65525, train_loss=2.9261277

Batch 231400, train_perplexity=18.655247, train_loss=2.9261274

Batch 231410, train_perplexity=18.65525, train_loss=2.9261277

Batch 231420, train_perplexity=18.655256, train_loss=2.926128

Batch 231430, train_perplexity=18.655247, train_loss=2.9261274

Batch 231440, train_perplexity=18.655256, train_loss=2.926128

Batch 231450, train_perplexity=18.655247, train_loss=2.9261274

Batch 231460, train_perplexity=18.655247, train_loss=2.9261274

Batch 231470, train_perplexity=18.655247, train_loss=2.9261274

Batch 231480, train_perplexity=18.655247, train_loss=2.9261274

Batch 231490, train_perplexity=18.655247, train_loss=2.9261274

Batch 231500, train_perplexity=18.655247, train_loss=2.9261274

Batch 231510, train_perplexity=18.655243, train_loss=2.9261272

Batch 231520, train_perplexity=18.655247, train_loss=2.9261274

Batch 231530, train_perplexity=18.655247, train_loss=2.9261274

Batch 231540, train_perplexity=18.655247, train_loss=2.9261274

Batch 231550, train_perplexity=18.655247, train_loss=2.9261274

Batch 231560, train_perplexity=18.655237, train_loss=2.926127

Batch 231570, train_perplexity=18.655247, train_loss=2.9261274

Batch 231580, train_perplexity=18.655243, train_loss=2.9261272

Batch 231590, train_perplexity=18.655243, train_loss=2.9261272

Batch 231600, train_perplexity=18.655247, train_loss=2.9261274

Batch 231610, train_perplexity=18.655237, train_loss=2.926127

Batch 231620, train_perplexity=18.655237, train_loss=2.926127

Batch 231630, train_perplexity=18.655243, train_loss=2.9261272

Batch 231640, train_perplexity=18.655237, train_loss=2.926127

Batch 231650, train_perplexity=18.655237, train_loss=2.926127

Batch 231660, train_perplexity=18.655243, train_loss=2.9261272

Batch 231670, train_perplexity=18.655247, train_loss=2.9261274

Batch 231680, train_perplexity=18.655247, train_loss=2.9261274

Batch 231690, train_perplexity=18.655237, train_loss=2.926127

Batch 231700, train_perplexity=18.655237, train_loss=2.926127

Batch 231710, train_perplexity=18.655243, train_loss=2.9261272

Batch 231720, train_perplexity=18.655237, train_loss=2.926127

Batch 231730, train_perplexity=18.655237, train_loss=2.926127

Batch 231740, train_perplexity=18.655237, train_loss=2.926127

Batch 231750, train_perplexity=18.655237, train_loss=2.926127

Batch 231760, train_perplexity=18.655237, train_loss=2.926127

Batch 231770, train_perplexity=18.655237, train_loss=2.926127

Batch 231780, train_perplexity=18.655233, train_loss=2.9261267

Batch 231790, train_perplexity=18.655237, train_loss=2.926127

Batch 231800, train_perplexity=18.655237, train_loss=2.926127

Batch 231810, train_perplexity=18.655237, train_loss=2.926127

Batch 231820, train_perplexity=18.655237, train_loss=2.926127

Batch 231830, train_perplexity=18.655243, train_loss=2.9261272

Batch 231840, train_perplexity=18.655237, train_loss=2.926127

Batch 231850, train_perplexity=18.655233, train_loss=2.9261267

Batch 231860, train_perplexity=18.65523, train_loss=2.9261265

Batch 231870, train_perplexity=18.655237, train_loss=2.926127

Batch 231880, train_perplexity=18.655233, train_loss=2.9261267

Batch 231890, train_perplexity=18.655233, train_loss=2.9261267

Batch 231900, train_perplexity=18.655237, train_loss=2.926127

Batch 231910, train_perplexity=18.655237, train_loss=2.926127

Batch 231920, train_perplexity=18.655237, train_loss=2.926127

Batch 231930, train_perplexity=18.655237, train_loss=2.926127

Batch 231940, train_perplexity=18.655237, train_loss=2.926127

Batch 231950, train_perplexity=18.655237, train_loss=2.926127

Batch 231960, train_perplexity=18.655233, train_loss=2.9261267

Batch 231970, train_perplexity=18.655233, train_loss=2.9261267

Batch 231980, train_perplexity=18.655233, train_loss=2.9261267

Batch 231990, train_perplexity=18.65523, train_loss=2.9261265

Batch 232000, train_perplexity=18.65523, train_loss=2.9261265

Batch 232010, train_perplexity=18.65523, train_loss=2.9261265

Batch 232020, train_perplexity=18.655233, train_loss=2.9261267

Batch 232030, train_perplexity=18.65523, train_loss=2.9261265

Batch 232040, train_perplexity=18.65523, train_loss=2.9261265

Batch 232050, train_perplexity=18.655224, train_loss=2.9261262

Batch 232060, train_perplexity=18.65523, train_loss=2.9261265

Batch 232070, train_perplexity=18.65523, train_loss=2.9261265

Batch 232080, train_perplexity=18.65523, train_loss=2.9261265

Batch 232090, train_perplexity=18.655233, train_loss=2.9261267

Batch 232100, train_perplexity=18.655224, train_loss=2.9261262

Batch 232110, train_perplexity=18.65523, train_loss=2.9261265

Batch 232120, train_perplexity=18.65523, train_loss=2.9261265

Batch 232130, train_perplexity=18.65523, train_loss=2.9261265

Batch 232140, train_perplexity=18.65523, train_loss=2.9261265

Batch 232150, train_perplexity=18.655224, train_loss=2.9261262

Batch 232160, train_perplexity=18.65523, train_loss=2.9261265

Batch 232170, train_perplexity=18.65523, train_loss=2.9261265

Batch 232180, train_perplexity=18.65523, train_loss=2.9261265

Batch 232190, train_perplexity=18.65523, train_loss=2.9261265

Batch 232200, train_perplexity=18.655224, train_loss=2.9261262

Batch 232210, train_perplexity=18.655224, train_loss=2.9261262

Batch 232220, train_perplexity=18.65523, train_loss=2.9261265

Batch 232230, train_perplexity=18.655224, train_loss=2.9261262

Batch 232240, train_perplexity=18.65522, train_loss=2.926126

Batch 232250, train_perplexity=18.65523, train_loss=2.9261265

Batch 232260, train_perplexity=18.65522, train_loss=2.926126

Batch 232270, train_perplexity=18.65523, train_loss=2.9261265

Batch 232280, train_perplexity=18.65523, train_loss=2.9261265

Batch 232290, train_perplexity=18.65523, train_loss=2.9261265

Batch 232300, train_perplexity=18.65523, train_loss=2.9261265

Batch 232310, train_perplexity=18.655224, train_loss=2.9261262

Batch 232320, train_perplexity=18.655224, train_loss=2.9261262

Batch 232330, train_perplexity=18.65522, train_loss=2.926126

Batch 232340, train_perplexity=18.65523, train_loss=2.9261265

Batch 232350, train_perplexity=18.655224, train_loss=2.9261262

Batch 232360, train_perplexity=18.65522, train_loss=2.926126

Batch 232370, train_perplexity=18.655224, train_loss=2.9261262

Batch 232380, train_perplexity=18.65522, train_loss=2.926126

Batch 232390, train_perplexity=18.65522, train_loss=2.926126

Batch 232400, train_perplexity=18.65522, train_loss=2.926126

Batch 232410, train_perplexity=18.65522, train_loss=2.926126

Batch 232420, train_perplexity=18.65522, train_loss=2.926126

Batch 232430, train_perplexity=18.65522, train_loss=2.926126

Batch 232440, train_perplexity=18.65522, train_loss=2.926126

Batch 232450, train_perplexity=18.65522, train_loss=2.926126

Batch 232460, train_perplexity=18.65522, train_loss=2.926126

Batch 232470, train_perplexity=18.655216, train_loss=2.9261258

Batch 232480, train_perplexity=18.65522, train_loss=2.926126

Batch 232490, train_perplexity=18.65522, train_loss=2.926126

Batch 232500, train_perplexity=18.65522, train_loss=2.926126

Batch 232510, train_perplexity=18.65522, train_loss=2.926126

Batch 232520, train_perplexity=18.655216, train_loss=2.9261258

Batch 232530, train_perplexity=18.65521, train_loss=2.9261255

Batch 232540, train_perplexity=18.65521, train_loss=2.9261255

Batch 232550, train_perplexity=18.65521, train_loss=2.9261255

Batch 232560, train_perplexity=18.655216, train_loss=2.9261258

Batch 232570, train_perplexity=18.655216, train_loss=2.9261258

Batch 232580, train_perplexity=18.65522, train_loss=2.926126

Batch 232590, train_perplexity=18.65522, train_loss=2.926126

Batch 232600, train_perplexity=18.65521, train_loss=2.9261255

Batch 232610, train_perplexity=18.655216, train_loss=2.9261258

Batch 232620, train_perplexity=18.655216, train_loss=2.9261258

Batch 232630, train_perplexity=18.655216, train_loss=2.9261258

Batch 232640, train_perplexity=18.65521, train_loss=2.9261255

Batch 232650, train_perplexity=18.65521, train_loss=2.9261255

Batch 232660, train_perplexity=18.65521, train_loss=2.9261255

Batch 232670, train_perplexity=18.655216, train_loss=2.9261258
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 232680, train_perplexity=18.655216, train_loss=2.9261258

Batch 232690, train_perplexity=18.65521, train_loss=2.9261255

Batch 232700, train_perplexity=18.655216, train_loss=2.9261258

Batch 232710, train_perplexity=18.65521, train_loss=2.9261255

Batch 232720, train_perplexity=18.65521, train_loss=2.9261255

Batch 232730, train_perplexity=18.65521, train_loss=2.9261255

Batch 232740, train_perplexity=18.655207, train_loss=2.9261253

Batch 232750, train_perplexity=18.655203, train_loss=2.926125

Batch 232760, train_perplexity=18.65521, train_loss=2.9261255

Batch 232770, train_perplexity=18.65521, train_loss=2.9261255

Batch 232780, train_perplexity=18.65521, train_loss=2.9261255

Batch 232790, train_perplexity=18.65521, train_loss=2.9261255

Batch 232800, train_perplexity=18.65521, train_loss=2.9261255

Batch 232810, train_perplexity=18.65521, train_loss=2.9261255

Batch 232820, train_perplexity=18.655207, train_loss=2.9261253

Batch 232830, train_perplexity=18.65521, train_loss=2.9261255

Batch 232840, train_perplexity=18.655207, train_loss=2.9261253

Batch 232850, train_perplexity=18.65521, train_loss=2.9261255

Batch 232860, train_perplexity=18.65521, train_loss=2.9261255

Batch 232870, train_perplexity=18.65521, train_loss=2.9261255

Batch 232880, train_perplexity=18.65521, train_loss=2.9261255

Batch 232890, train_perplexity=18.65521, train_loss=2.9261255

Batch 232900, train_perplexity=18.655207, train_loss=2.9261253

Batch 232910, train_perplexity=18.65521, train_loss=2.9261255

Batch 232920, train_perplexity=18.655203, train_loss=2.926125

Batch 232930, train_perplexity=18.65521, train_loss=2.9261255

Batch 232940, train_perplexity=18.655203, train_loss=2.926125

Batch 232950, train_perplexity=18.655203, train_loss=2.926125

Batch 232960, train_perplexity=18.655203, train_loss=2.926125

Batch 232970, train_perplexity=18.655203, train_loss=2.926125

Batch 232980, train_perplexity=18.655203, train_loss=2.926125

Batch 232990, train_perplexity=18.655203, train_loss=2.926125

Batch 233000, train_perplexity=18.655203, train_loss=2.926125

Batch 233010, train_perplexity=18.655203, train_loss=2.926125

Batch 233020, train_perplexity=18.655203, train_loss=2.926125

Batch 233030, train_perplexity=18.655203, train_loss=2.926125

Batch 233040, train_perplexity=18.655203, train_loss=2.926125

Batch 233050, train_perplexity=18.655203, train_loss=2.926125

Batch 233060, train_perplexity=18.655203, train_loss=2.926125

Batch 233070, train_perplexity=18.655203, train_loss=2.926125

Batch 233080, train_perplexity=18.655203, train_loss=2.926125

Batch 233090, train_perplexity=18.655203, train_loss=2.926125

Batch 233100, train_perplexity=18.655203, train_loss=2.926125

Batch 233110, train_perplexity=18.655203, train_loss=2.926125

Batch 233120, train_perplexity=18.655203, train_loss=2.926125

Batch 233130, train_perplexity=18.655203, train_loss=2.926125

Batch 233140, train_perplexity=18.655197, train_loss=2.9261248

Batch 233150, train_perplexity=18.655193, train_loss=2.9261246

Batch 233160, train_perplexity=18.655193, train_loss=2.9261246

Batch 233170, train_perplexity=18.655203, train_loss=2.926125

Batch 233180, train_perplexity=18.655203, train_loss=2.926125

Batch 233190, train_perplexity=18.655193, train_loss=2.9261246

Batch 233200, train_perplexity=18.655197, train_loss=2.9261248

Batch 233210, train_perplexity=18.655197, train_loss=2.9261248

Batch 233220, train_perplexity=18.655193, train_loss=2.9261246

Batch 233230, train_perplexity=18.655193, train_loss=2.9261246

Batch 233240, train_perplexity=18.655197, train_loss=2.9261248

Batch 233250, train_perplexity=18.655193, train_loss=2.9261246

Batch 233260, train_perplexity=18.655193, train_loss=2.9261246

Batch 233270, train_perplexity=18.655197, train_loss=2.9261248

Batch 233280, train_perplexity=18.655193, train_loss=2.9261246

Batch 233290, train_perplexity=18.655193, train_loss=2.9261246

Batch 233300, train_perplexity=18.655193, train_loss=2.9261246

Batch 233310, train_perplexity=18.655193, train_loss=2.9261246

Batch 233320, train_perplexity=18.655193, train_loss=2.9261246

Batch 233330, train_perplexity=18.655193, train_loss=2.9261246

Batch 233340, train_perplexity=18.655193, train_loss=2.9261246

Batch 233350, train_perplexity=18.655193, train_loss=2.9261246

Batch 233360, train_perplexity=18.655193, train_loss=2.9261246

Batch 233370, train_perplexity=18.655193, train_loss=2.9261246

Batch 233380, train_perplexity=18.655193, train_loss=2.9261246

Batch 233390, train_perplexity=18.655193, train_loss=2.9261246

Batch 233400, train_perplexity=18.655193, train_loss=2.9261246

Batch 233410, train_perplexity=18.655193, train_loss=2.9261246

Batch 233420, train_perplexity=18.655193, train_loss=2.9261246

Batch 233430, train_perplexity=18.655193, train_loss=2.9261246

Batch 233440, train_perplexity=18.65519, train_loss=2.9261243

Batch 233450, train_perplexity=18.655193, train_loss=2.9261246

Batch 233460, train_perplexity=18.655193, train_loss=2.9261246

Batch 233470, train_perplexity=18.65519, train_loss=2.9261243

Batch 233480, train_perplexity=18.655193, train_loss=2.9261246

Batch 233490, train_perplexity=18.655193, train_loss=2.9261246

Batch 233500, train_perplexity=18.65519, train_loss=2.9261243

Batch 233510, train_perplexity=18.655193, train_loss=2.9261246

Batch 233520, train_perplexity=18.655184, train_loss=2.926124

Batch 233530, train_perplexity=18.655184, train_loss=2.926124

Batch 233540, train_perplexity=18.65519, train_loss=2.9261243

Batch 233550, train_perplexity=18.655184, train_loss=2.926124

Batch 233560, train_perplexity=18.655193, train_loss=2.9261246

Batch 233570, train_perplexity=18.65519, train_loss=2.9261243

Batch 233580, train_perplexity=18.655184, train_loss=2.926124

Batch 233590, train_perplexity=18.655184, train_loss=2.926124

Batch 233600, train_perplexity=18.655184, train_loss=2.926124

Batch 233610, train_perplexity=18.65519, train_loss=2.9261243

Batch 233620, train_perplexity=18.655184, train_loss=2.926124

Batch 233630, train_perplexity=18.65519, train_loss=2.9261243

Batch 233640, train_perplexity=18.65518, train_loss=2.9261239

Batch 233650, train_perplexity=18.655184, train_loss=2.926124

Batch 233660, train_perplexity=18.655184, train_loss=2.926124

Batch 233670, train_perplexity=18.65519, train_loss=2.9261243

Batch 233680, train_perplexity=18.655184, train_loss=2.926124

Batch 233690, train_perplexity=18.65518, train_loss=2.9261239

Batch 233700, train_perplexity=18.65518, train_loss=2.9261239

Batch 233710, train_perplexity=18.655184, train_loss=2.926124

Batch 233720, train_perplexity=18.65518, train_loss=2.9261239

Batch 233730, train_perplexity=18.655184, train_loss=2.926124

Batch 233740, train_perplexity=18.655184, train_loss=2.926124

Batch 233750, train_perplexity=18.655184, train_loss=2.926124

Batch 233760, train_perplexity=18.655184, train_loss=2.926124

Batch 233770, train_perplexity=18.65518, train_loss=2.9261239

Batch 233780, train_perplexity=18.655176, train_loss=2.9261236

Batch 233790, train_perplexity=18.65518, train_loss=2.9261239

Batch 233800, train_perplexity=18.65518, train_loss=2.9261239

Batch 233810, train_perplexity=18.65518, train_loss=2.9261239

Batch 233820, train_perplexity=18.655176, train_loss=2.9261236

Batch 233830, train_perplexity=18.655184, train_loss=2.926124

Batch 233840, train_perplexity=18.655184, train_loss=2.926124

Batch 233850, train_perplexity=18.655176, train_loss=2.9261236

Batch 233860, train_perplexity=18.65518, train_loss=2.9261239

Batch 233870, train_perplexity=18.655176, train_loss=2.9261236

Batch 233880, train_perplexity=18.65518, train_loss=2.9261239

Batch 233890, train_perplexity=18.655176, train_loss=2.9261236

Batch 233900, train_perplexity=18.65518, train_loss=2.9261239

Batch 233910, train_perplexity=18.655176, train_loss=2.9261236

Batch 233920, train_perplexity=18.655176, train_loss=2.9261236

Batch 233930, train_perplexity=18.655176, train_loss=2.9261236

Batch 233940, train_perplexity=18.655176, train_loss=2.9261236

Batch 233950, train_perplexity=18.655176, train_loss=2.9261236

Batch 233960, train_perplexity=18.655176, train_loss=2.9261236

Batch 233970, train_perplexity=18.655176, train_loss=2.9261236
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 233980, train_perplexity=18.655176, train_loss=2.9261236

Batch 233990, train_perplexity=18.65517, train_loss=2.9261234

Batch 234000, train_perplexity=18.655176, train_loss=2.9261236

Batch 234010, train_perplexity=18.655176, train_loss=2.9261236

Batch 234020, train_perplexity=18.655176, train_loss=2.9261236

Batch 234030, train_perplexity=18.655176, train_loss=2.9261236

Batch 234040, train_perplexity=18.655176, train_loss=2.9261236

Batch 234050, train_perplexity=18.655176, train_loss=2.9261236

Batch 234060, train_perplexity=18.655176, train_loss=2.9261236

Batch 234070, train_perplexity=18.65517, train_loss=2.9261234

Batch 234080, train_perplexity=18.65517, train_loss=2.9261234

Batch 234090, train_perplexity=18.655176, train_loss=2.9261236

Batch 234100, train_perplexity=18.655167, train_loss=2.9261231

Batch 234110, train_perplexity=18.655176, train_loss=2.9261236

Batch 234120, train_perplexity=18.655176, train_loss=2.9261236

Batch 234130, train_perplexity=18.655176, train_loss=2.9261236

Batch 234140, train_perplexity=18.655176, train_loss=2.9261236

Batch 234150, train_perplexity=18.655167, train_loss=2.9261231

Batch 234160, train_perplexity=18.65517, train_loss=2.9261234

Batch 234170, train_perplexity=18.65517, train_loss=2.9261234

Batch 234180, train_perplexity=18.655167, train_loss=2.9261231

Batch 234190, train_perplexity=18.655167, train_loss=2.9261231

Batch 234200, train_perplexity=18.655167, train_loss=2.9261231

Batch 234210, train_perplexity=18.65517, train_loss=2.9261234

Batch 234220, train_perplexity=18.655167, train_loss=2.9261231

Batch 234230, train_perplexity=18.655167, train_loss=2.9261231

Batch 234240, train_perplexity=18.655167, train_loss=2.9261231

Batch 234250, train_perplexity=18.655167, train_loss=2.9261231

Batch 234260, train_perplexity=18.655167, train_loss=2.9261231

Batch 234270, train_perplexity=18.655167, train_loss=2.9261231

Batch 234280, train_perplexity=18.655167, train_loss=2.9261231

Batch 234290, train_perplexity=18.655167, train_loss=2.9261231

Batch 234300, train_perplexity=18.655167, train_loss=2.9261231

Batch 234310, train_perplexity=18.655167, train_loss=2.9261231

Batch 234320, train_perplexity=18.655167, train_loss=2.9261231

Batch 234330, train_perplexity=18.655167, train_loss=2.9261231

Batch 234340, train_perplexity=18.655167, train_loss=2.9261231

Batch 234350, train_perplexity=18.655167, train_loss=2.9261231

Batch 234360, train_perplexity=18.655167, train_loss=2.9261231

Batch 234370, train_perplexity=18.655167, train_loss=2.9261231

Batch 234380, train_perplexity=18.655167, train_loss=2.9261231

Batch 234390, train_perplexity=18.655157, train_loss=2.9261227

Batch 234400, train_perplexity=18.655167, train_loss=2.9261231

Batch 234410, train_perplexity=18.655157, train_loss=2.9261227

Batch 234420, train_perplexity=18.655157, train_loss=2.9261227

Batch 234430, train_perplexity=18.655163, train_loss=2.926123

Batch 234440, train_perplexity=18.655163, train_loss=2.926123

Batch 234450, train_perplexity=18.655157, train_loss=2.9261227

Batch 234460, train_perplexity=18.655167, train_loss=2.9261231

Batch 234470, train_perplexity=18.655167, train_loss=2.9261231

Batch 234480, train_perplexity=18.655157, train_loss=2.9261227

Batch 234490, train_perplexity=18.655167, train_loss=2.9261231

Batch 234500, train_perplexity=18.655167, train_loss=2.9261231

Batch 234510, train_perplexity=18.655157, train_loss=2.9261227

Batch 234520, train_perplexity=18.655163, train_loss=2.926123

Batch 234530, train_perplexity=18.655157, train_loss=2.9261227

Batch 234540, train_perplexity=18.655157, train_loss=2.9261227

Batch 234550, train_perplexity=18.655153, train_loss=2.9261224

Batch 234560, train_perplexity=18.655157, train_loss=2.9261227

Batch 234570, train_perplexity=18.655157, train_loss=2.9261227

Batch 234580, train_perplexity=18.655157, train_loss=2.9261227

Batch 234590, train_perplexity=18.655157, train_loss=2.9261227

Batch 234600, train_perplexity=18.655157, train_loss=2.9261227

Batch 234610, train_perplexity=18.655157, train_loss=2.9261227

Batch 234620, train_perplexity=18.655157, train_loss=2.9261227

Batch 234630, train_perplexity=18.655157, train_loss=2.9261227

Batch 234640, train_perplexity=18.655157, train_loss=2.9261227

Batch 234650, train_perplexity=18.655157, train_loss=2.9261227

Batch 234660, train_perplexity=18.655157, train_loss=2.9261227

Batch 234670, train_perplexity=18.655157, train_loss=2.9261227

Batch 234680, train_perplexity=18.655157, train_loss=2.9261227

Batch 234690, train_perplexity=18.655153, train_loss=2.9261224

Batch 234700, train_perplexity=18.65515, train_loss=2.9261222

Batch 234710, train_perplexity=18.655153, train_loss=2.9261224

Batch 234720, train_perplexity=18.655153, train_loss=2.9261224

Batch 234730, train_perplexity=18.655153, train_loss=2.9261224

Batch 234740, train_perplexity=18.655157, train_loss=2.9261227

Batch 234750, train_perplexity=18.65515, train_loss=2.9261222

Batch 234760, train_perplexity=18.655157, train_loss=2.9261227

Batch 234770, train_perplexity=18.655157, train_loss=2.9261227

Batch 234780, train_perplexity=18.65515, train_loss=2.9261222

Batch 234790, train_perplexity=18.65515, train_loss=2.9261222

Batch 234800, train_perplexity=18.655157, train_loss=2.9261227

Batch 234810, train_perplexity=18.65515, train_loss=2.9261222

Batch 234820, train_perplexity=18.655153, train_loss=2.9261224

Batch 234830, train_perplexity=18.65515, train_loss=2.9261222

Batch 234840, train_perplexity=18.65515, train_loss=2.9261222

Batch 234850, train_perplexity=18.655153, train_loss=2.9261224

Batch 234860, train_perplexity=18.65515, train_loss=2.9261222

Batch 234870, train_perplexity=18.65515, train_loss=2.9261222

Batch 234880, train_perplexity=18.655144, train_loss=2.926122

Batch 234890, train_perplexity=18.65515, train_loss=2.9261222

Batch 234900, train_perplexity=18.655153, train_loss=2.9261224

Batch 234910, train_perplexity=18.65515, train_loss=2.9261222

Batch 234920, train_perplexity=18.65515, train_loss=2.9261222

Batch 234930, train_perplexity=18.65515, train_loss=2.9261222

Batch 234940, train_perplexity=18.65515, train_loss=2.9261222

Batch 234950, train_perplexity=18.65515, train_loss=2.9261222

Batch 234960, train_perplexity=18.65515, train_loss=2.9261222

Batch 234970, train_perplexity=18.65515, train_loss=2.9261222

Batch 234980, train_perplexity=18.65515, train_loss=2.9261222

Batch 234990, train_perplexity=18.655144, train_loss=2.926122

Batch 235000, train_perplexity=18.65515, train_loss=2.9261222

Batch 235010, train_perplexity=18.65515, train_loss=2.9261222

Batch 235020, train_perplexity=18.65515, train_loss=2.9261222

Batch 235030, train_perplexity=18.65514, train_loss=2.9261217

Batch 235040, train_perplexity=18.65515, train_loss=2.9261222

Batch 235050, train_perplexity=18.655144, train_loss=2.926122

Batch 235060, train_perplexity=18.655144, train_loss=2.926122

Batch 235070, train_perplexity=18.65515, train_loss=2.9261222

Batch 235080, train_perplexity=18.655144, train_loss=2.926122

Batch 235090, train_perplexity=18.65514, train_loss=2.9261217

Batch 235100, train_perplexity=18.65514, train_loss=2.9261217

Batch 235110, train_perplexity=18.65514, train_loss=2.9261217

Batch 235120, train_perplexity=18.65514, train_loss=2.9261217

Batch 235130, train_perplexity=18.65514, train_loss=2.9261217

Batch 235140, train_perplexity=18.65514, train_loss=2.9261217

Batch 235150, train_perplexity=18.65514, train_loss=2.9261217

Batch 235160, train_perplexity=18.65514, train_loss=2.9261217

Batch 235170, train_perplexity=18.655144, train_loss=2.926122

Batch 235180, train_perplexity=18.65514, train_loss=2.9261217

Batch 235190, train_perplexity=18.65514, train_loss=2.9261217

Batch 235200, train_perplexity=18.655144, train_loss=2.926122

Batch 235210, train_perplexity=18.65514, train_loss=2.9261217

Batch 235220, train_perplexity=18.65514, train_loss=2.9261217

Batch 235230, train_perplexity=18.65514, train_loss=2.9261217

Batch 235240, train_perplexity=18.65514, train_loss=2.9261217

Batch 235250, train_perplexity=18.655136, train_loss=2.9261215

Batch 235260, train_perplexity=18.65514, train_loss=2.9261217
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 235270, train_perplexity=18.65514, train_loss=2.9261217

Batch 235280, train_perplexity=18.655136, train_loss=2.9261215

Batch 235290, train_perplexity=18.65514, train_loss=2.9261217

Batch 235300, train_perplexity=18.65513, train_loss=2.9261212

Batch 235310, train_perplexity=18.655136, train_loss=2.9261215

Batch 235320, train_perplexity=18.65514, train_loss=2.9261217

Batch 235330, train_perplexity=18.655136, train_loss=2.9261215

Batch 235340, train_perplexity=18.655136, train_loss=2.9261215

Batch 235350, train_perplexity=18.655136, train_loss=2.9261215

Batch 235360, train_perplexity=18.65514, train_loss=2.9261217

Batch 235370, train_perplexity=18.65513, train_loss=2.9261212

Batch 235380, train_perplexity=18.65514, train_loss=2.9261217

Batch 235390, train_perplexity=18.65514, train_loss=2.9261217

Batch 235400, train_perplexity=18.655136, train_loss=2.9261215

Batch 235410, train_perplexity=18.65513, train_loss=2.9261212

Batch 235420, train_perplexity=18.65513, train_loss=2.9261212

Batch 235430, train_perplexity=18.65513, train_loss=2.9261212

Batch 235440, train_perplexity=18.65513, train_loss=2.9261212

Batch 235450, train_perplexity=18.65514, train_loss=2.9261217

Batch 235460, train_perplexity=18.65513, train_loss=2.9261212

Batch 235470, train_perplexity=18.65513, train_loss=2.9261212

Batch 235480, train_perplexity=18.65513, train_loss=2.9261212

Batch 235490, train_perplexity=18.65513, train_loss=2.9261212

Batch 235500, train_perplexity=18.65513, train_loss=2.9261212

Batch 235510, train_perplexity=18.65513, train_loss=2.9261212

Batch 235520, train_perplexity=18.65513, train_loss=2.9261212

Batch 235530, train_perplexity=18.65513, train_loss=2.9261212

Batch 235540, train_perplexity=18.65513, train_loss=2.9261212

Batch 235550, train_perplexity=18.655127, train_loss=2.926121

Batch 235560, train_perplexity=18.65513, train_loss=2.9261212

Batch 235570, train_perplexity=18.65513, train_loss=2.9261212

Batch 235580, train_perplexity=18.65513, train_loss=2.9261212

Batch 235590, train_perplexity=18.655127, train_loss=2.926121

Batch 235600, train_perplexity=18.65513, train_loss=2.9261212

Batch 235610, train_perplexity=18.65513, train_loss=2.9261212

Batch 235620, train_perplexity=18.65513, train_loss=2.9261212

Batch 235630, train_perplexity=18.65513, train_loss=2.9261212

Batch 235640, train_perplexity=18.65513, train_loss=2.9261212

Batch 235650, train_perplexity=18.65513, train_loss=2.9261212

Batch 235660, train_perplexity=18.655123, train_loss=2.9261208

Batch 235670, train_perplexity=18.65513, train_loss=2.9261212

Batch 235680, train_perplexity=18.65513, train_loss=2.9261212

Batch 235690, train_perplexity=18.655123, train_loss=2.9261208

Batch 235700, train_perplexity=18.655123, train_loss=2.9261208

Batch 235710, train_perplexity=18.65513, train_loss=2.9261212

Batch 235720, train_perplexity=18.655127, train_loss=2.926121

Batch 235730, train_perplexity=18.655123, train_loss=2.9261208

Batch 235740, train_perplexity=18.655127, train_loss=2.926121

Batch 235750, train_perplexity=18.655127, train_loss=2.926121

Batch 235760, train_perplexity=18.655127, train_loss=2.926121

Batch 235770, train_perplexity=18.655123, train_loss=2.9261208

Batch 235780, train_perplexity=18.655123, train_loss=2.9261208

Batch 235790, train_perplexity=18.655123, train_loss=2.9261208

Batch 235800, train_perplexity=18.655123, train_loss=2.9261208

Batch 235810, train_perplexity=18.655123, train_loss=2.9261208

Batch 235820, train_perplexity=18.655123, train_loss=2.9261208

Batch 235830, train_perplexity=18.655123, train_loss=2.9261208

Batch 235840, train_perplexity=18.655123, train_loss=2.9261208

Batch 235850, train_perplexity=18.655123, train_loss=2.9261208

Batch 235860, train_perplexity=18.655123, train_loss=2.9261208

Batch 235870, train_perplexity=18.655123, train_loss=2.9261208

Batch 235880, train_perplexity=18.655123, train_loss=2.9261208

Batch 235890, train_perplexity=18.655123, train_loss=2.9261208

Batch 235900, train_perplexity=18.655123, train_loss=2.9261208

Batch 235910, train_perplexity=18.655123, train_loss=2.9261208

Batch 235920, train_perplexity=18.655123, train_loss=2.9261208

Batch 235930, train_perplexity=18.655123, train_loss=2.9261208

Batch 235940, train_perplexity=18.655117, train_loss=2.9261205

Batch 235950, train_perplexity=18.655123, train_loss=2.9261208

Batch 235960, train_perplexity=18.655117, train_loss=2.9261205

Batch 235970, train_perplexity=18.655123, train_loss=2.9261208

Batch 235980, train_perplexity=18.655113, train_loss=2.9261203

Batch 235990, train_perplexity=18.655123, train_loss=2.9261208

Batch 236000, train_perplexity=18.655113, train_loss=2.9261203

Batch 236010, train_perplexity=18.655123, train_loss=2.9261208

Batch 236020, train_perplexity=18.655113, train_loss=2.9261203

Batch 236030, train_perplexity=18.655113, train_loss=2.9261203

Batch 236040, train_perplexity=18.655113, train_loss=2.9261203

Batch 236050, train_perplexity=18.655117, train_loss=2.9261205

Batch 236060, train_perplexity=18.655113, train_loss=2.9261203

Batch 236070, train_perplexity=18.65511, train_loss=2.92612

Batch 236080, train_perplexity=18.655113, train_loss=2.9261203

Batch 236090, train_perplexity=18.655113, train_loss=2.9261203

Batch 236100, train_perplexity=18.655113, train_loss=2.9261203

Batch 236110, train_perplexity=18.655113, train_loss=2.9261203

Batch 236120, train_perplexity=18.655113, train_loss=2.9261203

Batch 236130, train_perplexity=18.655113, train_loss=2.9261203

Batch 236140, train_perplexity=18.655113, train_loss=2.9261203

Batch 236150, train_perplexity=18.655113, train_loss=2.9261203

Batch 236160, train_perplexity=18.655113, train_loss=2.9261203

Batch 236170, train_perplexity=18.65511, train_loss=2.92612

Batch 236180, train_perplexity=18.655113, train_loss=2.9261203

Batch 236190, train_perplexity=18.655113, train_loss=2.9261203

Batch 236200, train_perplexity=18.655113, train_loss=2.9261203

Batch 236210, train_perplexity=18.655113, train_loss=2.9261203

Batch 236220, train_perplexity=18.655113, train_loss=2.9261203

Batch 236230, train_perplexity=18.655113, train_loss=2.9261203

Batch 236240, train_perplexity=18.655113, train_loss=2.9261203

Batch 236250, train_perplexity=18.655113, train_loss=2.9261203

Batch 236260, train_perplexity=18.655104, train_loss=2.9261198

Batch 236270, train_perplexity=18.655113, train_loss=2.9261203

Batch 236280, train_perplexity=18.65511, train_loss=2.92612

Batch 236290, train_perplexity=18.65511, train_loss=2.92612

Batch 236300, train_perplexity=18.655113, train_loss=2.9261203

Batch 236310, train_perplexity=18.655113, train_loss=2.9261203

Batch 236320, train_perplexity=18.655113, train_loss=2.9261203

Batch 236330, train_perplexity=18.65511, train_loss=2.92612

Batch 236340, train_perplexity=18.65511, train_loss=2.92612

Batch 236350, train_perplexity=18.655104, train_loss=2.9261198

Batch 236360, train_perplexity=18.65511, train_loss=2.92612

Batch 236370, train_perplexity=18.65511, train_loss=2.92612

Batch 236380, train_perplexity=18.655113, train_loss=2.9261203

Batch 236390, train_perplexity=18.655104, train_loss=2.9261198

Batch 236400, train_perplexity=18.655104, train_loss=2.9261198

Batch 236410, train_perplexity=18.655104, train_loss=2.9261198

Batch 236420, train_perplexity=18.655113, train_loss=2.9261203

Batch 236430, train_perplexity=18.655104, train_loss=2.9261198

Batch 236440, train_perplexity=18.65511, train_loss=2.92612

Batch 236450, train_perplexity=18.655104, train_loss=2.9261198

Batch 236460, train_perplexity=18.655104, train_loss=2.9261198

Batch 236470, train_perplexity=18.6551, train_loss=2.9261196

Batch 236480, train_perplexity=18.655104, train_loss=2.9261198

Batch 236490, train_perplexity=18.655104, train_loss=2.9261198

Batch 236500, train_perplexity=18.655104, train_loss=2.9261198

Batch 236510, train_perplexity=18.655104, train_loss=2.9261198

Batch 236520, train_perplexity=18.6551, train_loss=2.9261196

Batch 236530, train_perplexity=18.6551, train_loss=2.9261196

Batch 236540, train_perplexity=18.6551, train_loss=2.9261196

Batch 236550, train_perplexity=18.655104, train_loss=2.9261198

Batch 236560, train_perplexity=18.655096, train_loss=2.9261193
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 236570, train_perplexity=18.655104, train_loss=2.9261198

Batch 236580, train_perplexity=18.655104, train_loss=2.9261198

Batch 236590, train_perplexity=18.655104, train_loss=2.9261198

Batch 236600, train_perplexity=18.6551, train_loss=2.9261196

Batch 236610, train_perplexity=18.655104, train_loss=2.9261198

Batch 236620, train_perplexity=18.655104, train_loss=2.9261198

Batch 236630, train_perplexity=18.655096, train_loss=2.9261193

Batch 236640, train_perplexity=18.6551, train_loss=2.9261196

Batch 236650, train_perplexity=18.655096, train_loss=2.9261193

Batch 236660, train_perplexity=18.655096, train_loss=2.9261193

Batch 236670, train_perplexity=18.655096, train_loss=2.9261193

Batch 236680, train_perplexity=18.6551, train_loss=2.9261196

Batch 236690, train_perplexity=18.655096, train_loss=2.9261193

Batch 236700, train_perplexity=18.6551, train_loss=2.9261196

Batch 236710, train_perplexity=18.655096, train_loss=2.9261193

Batch 236720, train_perplexity=18.6551, train_loss=2.9261196

Batch 236730, train_perplexity=18.655096, train_loss=2.9261193

Batch 236740, train_perplexity=18.655096, train_loss=2.9261193

Batch 236750, train_perplexity=18.6551, train_loss=2.9261196

Batch 236760, train_perplexity=18.655096, train_loss=2.9261193

Batch 236770, train_perplexity=18.655096, train_loss=2.9261193

Batch 236780, train_perplexity=18.655096, train_loss=2.9261193

Batch 236790, train_perplexity=18.65509, train_loss=2.926119

Batch 236800, train_perplexity=18.65509, train_loss=2.926119

Batch 236810, train_perplexity=18.655096, train_loss=2.9261193

Batch 236820, train_perplexity=18.655096, train_loss=2.9261193

Batch 236830, train_perplexity=18.655096, train_loss=2.9261193

Batch 236840, train_perplexity=18.655096, train_loss=2.9261193

Batch 236850, train_perplexity=18.655096, train_loss=2.9261193

Batch 236860, train_perplexity=18.65509, train_loss=2.926119

Batch 236870, train_perplexity=18.655096, train_loss=2.9261193

Batch 236880, train_perplexity=18.655096, train_loss=2.9261193

Batch 236890, train_perplexity=18.655096, train_loss=2.9261193

Batch 236900, train_perplexity=18.655096, train_loss=2.9261193

Batch 236910, train_perplexity=18.655096, train_loss=2.9261193

Batch 236920, train_perplexity=18.655096, train_loss=2.9261193

Batch 236930, train_perplexity=18.655096, train_loss=2.9261193

Batch 236940, train_perplexity=18.655087, train_loss=2.9261189

Batch 236950, train_perplexity=18.655087, train_loss=2.9261189

Batch 236960, train_perplexity=18.655096, train_loss=2.9261193

Batch 236970, train_perplexity=18.655087, train_loss=2.9261189

Batch 236980, train_perplexity=18.65509, train_loss=2.926119

Batch 236990, train_perplexity=18.655083, train_loss=2.9261186

Batch 237000, train_perplexity=18.655087, train_loss=2.9261189

Batch 237010, train_perplexity=18.65509, train_loss=2.926119

Batch 237020, train_perplexity=18.655087, train_loss=2.9261189

Batch 237030, train_perplexity=18.65509, train_loss=2.926119

Batch 237040, train_perplexity=18.65509, train_loss=2.926119

Batch 237050, train_perplexity=18.65509, train_loss=2.926119

Batch 237060, train_perplexity=18.65509, train_loss=2.926119

Batch 237070, train_perplexity=18.655087, train_loss=2.9261189

Batch 237080, train_perplexity=18.655087, train_loss=2.9261189

Batch 237090, train_perplexity=18.655087, train_loss=2.9261189

Batch 237100, train_perplexity=18.655087, train_loss=2.9261189

Batch 237110, train_perplexity=18.655087, train_loss=2.9261189

Batch 237120, train_perplexity=18.655087, train_loss=2.9261189

Batch 237130, train_perplexity=18.655087, train_loss=2.9261189

Batch 237140, train_perplexity=18.655087, train_loss=2.9261189

Batch 237150, train_perplexity=18.655087, train_loss=2.9261189

Batch 237160, train_perplexity=18.655087, train_loss=2.9261189

Batch 237170, train_perplexity=18.655087, train_loss=2.9261189

Batch 237180, train_perplexity=18.655087, train_loss=2.9261189

Batch 237190, train_perplexity=18.655087, train_loss=2.9261189

Batch 237200, train_perplexity=18.655087, train_loss=2.9261189

Batch 237210, train_perplexity=18.655083, train_loss=2.9261186

Batch 237220, train_perplexity=18.655087, train_loss=2.9261189

Batch 237230, train_perplexity=18.655083, train_loss=2.9261186

Batch 237240, train_perplexity=18.655083, train_loss=2.9261186

Batch 237250, train_perplexity=18.655083, train_loss=2.9261186

Batch 237260, train_perplexity=18.655087, train_loss=2.9261189

Batch 237270, train_perplexity=18.655087, train_loss=2.9261189

Batch 237280, train_perplexity=18.655077, train_loss=2.9261184

Batch 237290, train_perplexity=18.655083, train_loss=2.9261186

Batch 237300, train_perplexity=18.655087, train_loss=2.9261189

Batch 237310, train_perplexity=18.655087, train_loss=2.9261189

Batch 237320, train_perplexity=18.655087, train_loss=2.9261189

Batch 237330, train_perplexity=18.655077, train_loss=2.9261184

Batch 237340, train_perplexity=18.655083, train_loss=2.9261186

Batch 237350, train_perplexity=18.655077, train_loss=2.9261184

Batch 237360, train_perplexity=18.655077, train_loss=2.9261184

Batch 237370, train_perplexity=18.655077, train_loss=2.9261184

Batch 237380, train_perplexity=18.655077, train_loss=2.9261184

Batch 237390, train_perplexity=18.655077, train_loss=2.9261184

Batch 237400, train_perplexity=18.655073, train_loss=2.9261181

Batch 237410, train_perplexity=18.655077, train_loss=2.9261184

Batch 237420, train_perplexity=18.655077, train_loss=2.9261184

Batch 237430, train_perplexity=18.655077, train_loss=2.9261184

Batch 237440, train_perplexity=18.655077, train_loss=2.9261184

Batch 237450, train_perplexity=18.655077, train_loss=2.9261184

Batch 237460, train_perplexity=18.655073, train_loss=2.9261181

Batch 237470, train_perplexity=18.655077, train_loss=2.9261184

Batch 237480, train_perplexity=18.655077, train_loss=2.9261184

Batch 237490, train_perplexity=18.655077, train_loss=2.9261184

Batch 237500, train_perplexity=18.655077, train_loss=2.9261184

Batch 237510, train_perplexity=18.655077, train_loss=2.9261184

Batch 237520, train_perplexity=18.655073, train_loss=2.9261181

Batch 237530, train_perplexity=18.655073, train_loss=2.9261181

Batch 237540, train_perplexity=18.65507, train_loss=2.926118

Batch 237550, train_perplexity=18.655073, train_loss=2.9261181

Batch 237560, train_perplexity=18.655073, train_loss=2.9261181

Batch 237570, train_perplexity=18.65507, train_loss=2.926118

Batch 237580, train_perplexity=18.65507, train_loss=2.926118

Batch 237590, train_perplexity=18.655073, train_loss=2.9261181

Batch 237600, train_perplexity=18.65507, train_loss=2.926118

Batch 237610, train_perplexity=18.655077, train_loss=2.9261184

Batch 237620, train_perplexity=18.65507, train_loss=2.926118

Batch 237630, train_perplexity=18.65507, train_loss=2.926118

Batch 237640, train_perplexity=18.65507, train_loss=2.926118

Batch 237650, train_perplexity=18.655077, train_loss=2.9261184

Batch 237660, train_perplexity=18.65507, train_loss=2.926118

Batch 237670, train_perplexity=18.65507, train_loss=2.926118

Batch 237680, train_perplexity=18.65507, train_loss=2.926118

Batch 237690, train_perplexity=18.655073, train_loss=2.9261181

Batch 237700, train_perplexity=18.65507, train_loss=2.926118

Batch 237710, train_perplexity=18.655073, train_loss=2.9261181

Batch 237720, train_perplexity=18.65507, train_loss=2.926118

Batch 237730, train_perplexity=18.65507, train_loss=2.926118

Batch 237740, train_perplexity=18.65507, train_loss=2.926118

Batch 237750, train_perplexity=18.65507, train_loss=2.926118

Batch 237760, train_perplexity=18.65507, train_loss=2.926118

Batch 237770, train_perplexity=18.65507, train_loss=2.926118

Batch 237780, train_perplexity=18.65507, train_loss=2.926118

Batch 237790, train_perplexity=18.65507, train_loss=2.926118

Batch 237800, train_perplexity=18.65507, train_loss=2.926118

Batch 237810, train_perplexity=18.655064, train_loss=2.9261177

Batch 237820, train_perplexity=18.65507, train_loss=2.926118

Batch 237830, train_perplexity=18.65507, train_loss=2.926118

Batch 237840, train_perplexity=18.65507, train_loss=2.926118

Batch 237850, train_perplexity=18.65507, train_loss=2.926118

Batch 237860, train_perplexity=18.655064, train_loss=2.9261177
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 237870, train_perplexity=18.65507, train_loss=2.926118

Batch 237880, train_perplexity=18.655064, train_loss=2.9261177

Batch 237890, train_perplexity=18.655064, train_loss=2.9261177

Batch 237900, train_perplexity=18.65506, train_loss=2.9261174

Batch 237910, train_perplexity=18.65506, train_loss=2.9261174

Batch 237920, train_perplexity=18.65507, train_loss=2.926118

Batch 237930, train_perplexity=18.65507, train_loss=2.926118

Batch 237940, train_perplexity=18.65507, train_loss=2.926118

Batch 237950, train_perplexity=18.65506, train_loss=2.9261174

Batch 237960, train_perplexity=18.65506, train_loss=2.9261174

Batch 237970, train_perplexity=18.65506, train_loss=2.9261174

Batch 237980, train_perplexity=18.65506, train_loss=2.9261174

Batch 237990, train_perplexity=18.65506, train_loss=2.9261174

Batch 238000, train_perplexity=18.65506, train_loss=2.9261174

Batch 238010, train_perplexity=18.65506, train_loss=2.9261174

Batch 238020, train_perplexity=18.65506, train_loss=2.9261174

Batch 238030, train_perplexity=18.65506, train_loss=2.9261174

Batch 238040, train_perplexity=18.65506, train_loss=2.9261174

Batch 238050, train_perplexity=18.65506, train_loss=2.9261174

Batch 238060, train_perplexity=18.65506, train_loss=2.9261174

Batch 238070, train_perplexity=18.65506, train_loss=2.9261174

Batch 238080, train_perplexity=18.65506, train_loss=2.9261174

Batch 238090, train_perplexity=18.65506, train_loss=2.9261174

Batch 238100, train_perplexity=18.655056, train_loss=2.9261172

Batch 238110, train_perplexity=18.65506, train_loss=2.9261174

Batch 238120, train_perplexity=18.65506, train_loss=2.9261174

Batch 238130, train_perplexity=18.655056, train_loss=2.9261172

Batch 238140, train_perplexity=18.65505, train_loss=2.926117

Batch 238150, train_perplexity=18.655056, train_loss=2.9261172

Batch 238160, train_perplexity=18.655056, train_loss=2.9261172

Batch 238170, train_perplexity=18.65506, train_loss=2.9261174

Batch 238180, train_perplexity=18.65505, train_loss=2.926117

Batch 238190, train_perplexity=18.65506, train_loss=2.9261174

Batch 238200, train_perplexity=18.65506, train_loss=2.9261174

Batch 238210, train_perplexity=18.65506, train_loss=2.9261174

Batch 238220, train_perplexity=18.65505, train_loss=2.926117

Batch 238230, train_perplexity=18.65506, train_loss=2.9261174

Batch 238240, train_perplexity=18.65505, train_loss=2.926117

Batch 238250, train_perplexity=18.65506, train_loss=2.9261174

Batch 238260, train_perplexity=18.65505, train_loss=2.926117

Batch 238270, train_perplexity=18.65505, train_loss=2.926117

Batch 238280, train_perplexity=18.655056, train_loss=2.9261172

Batch 238290, train_perplexity=18.65505, train_loss=2.926117

Batch 238300, train_perplexity=18.65505, train_loss=2.926117

Batch 238310, train_perplexity=18.65505, train_loss=2.926117

Batch 238320, train_perplexity=18.65505, train_loss=2.926117

Batch 238330, train_perplexity=18.65505, train_loss=2.926117

Batch 238340, train_perplexity=18.65505, train_loss=2.926117

Batch 238350, train_perplexity=18.655043, train_loss=2.9261165

Batch 238360, train_perplexity=18.65505, train_loss=2.926117

Batch 238370, train_perplexity=18.65505, train_loss=2.926117

Batch 238380, train_perplexity=18.65505, train_loss=2.926117

Batch 238390, train_perplexity=18.65505, train_loss=2.926117

Batch 238400, train_perplexity=18.65505, train_loss=2.926117

Batch 238410, train_perplexity=18.65505, train_loss=2.926117

Batch 238420, train_perplexity=18.65505, train_loss=2.926117

Batch 238430, train_perplexity=18.65505, train_loss=2.926117

Batch 238440, train_perplexity=18.655046, train_loss=2.9261167

Batch 238450, train_perplexity=18.655046, train_loss=2.9261167

Batch 238460, train_perplexity=18.655043, train_loss=2.9261165

Batch 238470, train_perplexity=18.655046, train_loss=2.9261167

Batch 238480, train_perplexity=18.655046, train_loss=2.9261167

Batch 238490, train_perplexity=18.65505, train_loss=2.926117

Batch 238500, train_perplexity=18.65505, train_loss=2.926117

Batch 238510, train_perplexity=18.65505, train_loss=2.926117

Batch 238520, train_perplexity=18.65505, train_loss=2.926117

Batch 238530, train_perplexity=18.65505, train_loss=2.926117

Batch 238540, train_perplexity=18.655046, train_loss=2.9261167

Batch 238550, train_perplexity=18.655043, train_loss=2.9261165

Batch 238560, train_perplexity=18.655043, train_loss=2.9261165

Batch 238570, train_perplexity=18.655043, train_loss=2.9261165

Batch 238580, train_perplexity=18.655043, train_loss=2.9261165

Batch 238590, train_perplexity=18.655043, train_loss=2.9261165

Batch 238600, train_perplexity=18.655046, train_loss=2.9261167

Batch 238610, train_perplexity=18.655043, train_loss=2.9261165

Batch 238620, train_perplexity=18.655043, train_loss=2.9261165

Batch 238630, train_perplexity=18.655043, train_loss=2.9261165

Batch 238640, train_perplexity=18.65505, train_loss=2.926117

Batch 238650, train_perplexity=18.655043, train_loss=2.9261165

Batch 238660, train_perplexity=18.655037, train_loss=2.9261162

Batch 238670, train_perplexity=18.655043, train_loss=2.9261165

Batch 238680, train_perplexity=18.655037, train_loss=2.9261162

Batch 238690, train_perplexity=18.655037, train_loss=2.9261162

Batch 238700, train_perplexity=18.655043, train_loss=2.9261165

Batch 238710, train_perplexity=18.655043, train_loss=2.9261165

Batch 238720, train_perplexity=18.655043, train_loss=2.9261165

Batch 238730, train_perplexity=18.655043, train_loss=2.9261165

Batch 238740, train_perplexity=18.655037, train_loss=2.9261162

Batch 238750, train_perplexity=18.655037, train_loss=2.9261162

Batch 238760, train_perplexity=18.655043, train_loss=2.9261165

Batch 238770, train_perplexity=18.655043, train_loss=2.9261165

Batch 238780, train_perplexity=18.655043, train_loss=2.9261165

Batch 238790, train_perplexity=18.655043, train_loss=2.9261165

Batch 238800, train_perplexity=18.655037, train_loss=2.9261162

Batch 238810, train_perplexity=18.655043, train_loss=2.9261165

Batch 238820, train_perplexity=18.655033, train_loss=2.926116

Batch 238830, train_perplexity=18.655033, train_loss=2.926116

Batch 238840, train_perplexity=18.655037, train_loss=2.9261162

Batch 238850, train_perplexity=18.655033, train_loss=2.926116

Batch 238860, train_perplexity=18.655037, train_loss=2.9261162

Batch 238870, train_perplexity=18.65503, train_loss=2.9261158

Batch 238880, train_perplexity=18.65503, train_loss=2.9261158

Batch 238890, train_perplexity=18.655033, train_loss=2.926116

Batch 238900, train_perplexity=18.65503, train_loss=2.9261158

Batch 238910, train_perplexity=18.655037, train_loss=2.9261162

Batch 238920, train_perplexity=18.655033, train_loss=2.926116

Batch 238930, train_perplexity=18.655037, train_loss=2.9261162

Batch 238940, train_perplexity=18.65503, train_loss=2.9261158

Batch 238950, train_perplexity=18.65503, train_loss=2.9261158

Batch 238960, train_perplexity=18.65503, train_loss=2.9261158

Batch 238970, train_perplexity=18.655033, train_loss=2.926116

Batch 238980, train_perplexity=18.65503, train_loss=2.9261158

Batch 238990, train_perplexity=18.655024, train_loss=2.9261155

Batch 239000, train_perplexity=18.655033, train_loss=2.926116

Batch 239010, train_perplexity=18.65503, train_loss=2.9261158

Batch 239020, train_perplexity=18.65503, train_loss=2.9261158

Batch 239030, train_perplexity=18.655033, train_loss=2.926116

Batch 239040, train_perplexity=18.65503, train_loss=2.9261158

Batch 239050, train_perplexity=18.655033, train_loss=2.926116

Batch 239060, train_perplexity=18.65503, train_loss=2.9261158

Batch 239070, train_perplexity=18.655024, train_loss=2.9261155

Batch 239080, train_perplexity=18.655024, train_loss=2.9261155

Batch 239090, train_perplexity=18.65503, train_loss=2.9261158

Batch 239100, train_perplexity=18.655033, train_loss=2.926116

Batch 239110, train_perplexity=18.655024, train_loss=2.9261155

Batch 239120, train_perplexity=18.655024, train_loss=2.9261155

Batch 239130, train_perplexity=18.655024, train_loss=2.9261155

Batch 239140, train_perplexity=18.655024, train_loss=2.9261155

Batch 239150, train_perplexity=18.65503, train_loss=2.9261158

Batch 239160, train_perplexity=18.655024, train_loss=2.9261155
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 239170, train_perplexity=18.655024, train_loss=2.9261155

Batch 239180, train_perplexity=18.655024, train_loss=2.9261155

Batch 239190, train_perplexity=18.655024, train_loss=2.9261155

Batch 239200, train_perplexity=18.655033, train_loss=2.926116

Batch 239210, train_perplexity=18.655024, train_loss=2.9261155

Batch 239220, train_perplexity=18.65502, train_loss=2.9261153

Batch 239230, train_perplexity=18.65502, train_loss=2.9261153

Batch 239240, train_perplexity=18.655024, train_loss=2.9261155

Batch 239250, train_perplexity=18.655024, train_loss=2.9261155

Batch 239260, train_perplexity=18.65502, train_loss=2.9261153

Batch 239270, train_perplexity=18.655024, train_loss=2.9261155

Batch 239280, train_perplexity=18.65502, train_loss=2.9261153

Batch 239290, train_perplexity=18.655024, train_loss=2.9261155

Batch 239300, train_perplexity=18.655024, train_loss=2.9261155

Batch 239310, train_perplexity=18.655024, train_loss=2.9261155

Batch 239320, train_perplexity=18.655024, train_loss=2.9261155

Batch 239330, train_perplexity=18.65502, train_loss=2.9261153

Batch 239340, train_perplexity=18.655024, train_loss=2.9261155

Batch 239350, train_perplexity=18.655024, train_loss=2.9261155

Batch 239360, train_perplexity=18.655024, train_loss=2.9261155

Batch 239370, train_perplexity=18.655016, train_loss=2.926115

Batch 239380, train_perplexity=18.65502, train_loss=2.9261153

Batch 239390, train_perplexity=18.655016, train_loss=2.926115

Batch 239400, train_perplexity=18.65502, train_loss=2.9261153

Batch 239410, train_perplexity=18.655024, train_loss=2.9261155

Batch 239420, train_perplexity=18.65502, train_loss=2.9261153

Batch 239430, train_perplexity=18.655016, train_loss=2.926115

Batch 239440, train_perplexity=18.655016, train_loss=2.926115

Batch 239450, train_perplexity=18.655016, train_loss=2.926115

Batch 239460, train_perplexity=18.655016, train_loss=2.926115

Batch 239470, train_perplexity=18.655016, train_loss=2.926115

Batch 239480, train_perplexity=18.655016, train_loss=2.926115

Batch 239490, train_perplexity=18.655016, train_loss=2.926115

Batch 239500, train_perplexity=18.65501, train_loss=2.9261148

Batch 239510, train_perplexity=18.655024, train_loss=2.9261155

Batch 239520, train_perplexity=18.655016, train_loss=2.926115

Batch 239530, train_perplexity=18.655016, train_loss=2.926115

Batch 239540, train_perplexity=18.65501, train_loss=2.9261148

Batch 239550, train_perplexity=18.655016, train_loss=2.926115

Batch 239560, train_perplexity=18.655016, train_loss=2.926115

Batch 239570, train_perplexity=18.655016, train_loss=2.926115

Batch 239580, train_perplexity=18.65501, train_loss=2.9261148

Batch 239590, train_perplexity=18.65501, train_loss=2.9261148

Batch 239600, train_perplexity=18.65501, train_loss=2.9261148

Batch 239610, train_perplexity=18.65501, train_loss=2.9261148

Batch 239620, train_perplexity=18.655016, train_loss=2.926115

Batch 239630, train_perplexity=18.655006, train_loss=2.9261146

Batch 239640, train_perplexity=18.65501, train_loss=2.9261148

Batch 239650, train_perplexity=18.65501, train_loss=2.9261148

Batch 239660, train_perplexity=18.655016, train_loss=2.926115

Batch 239670, train_perplexity=18.65501, train_loss=2.9261148

Batch 239680, train_perplexity=18.65501, train_loss=2.9261148

Batch 239690, train_perplexity=18.655006, train_loss=2.9261146

Batch 239700, train_perplexity=18.655016, train_loss=2.926115

Batch 239710, train_perplexity=18.65501, train_loss=2.9261148

Batch 239720, train_perplexity=18.65501, train_loss=2.9261148

Batch 239730, train_perplexity=18.655006, train_loss=2.9261146

Batch 239740, train_perplexity=18.65501, train_loss=2.9261148

Batch 239750, train_perplexity=18.655006, train_loss=2.9261146

Batch 239760, train_perplexity=18.655006, train_loss=2.9261146

Batch 239770, train_perplexity=18.65501, train_loss=2.9261148

Batch 239780, train_perplexity=18.655006, train_loss=2.9261146

Batch 239790, train_perplexity=18.655006, train_loss=2.9261146

Batch 239800, train_perplexity=18.65501, train_loss=2.9261148

Batch 239810, train_perplexity=18.655006, train_loss=2.9261146

Batch 239820, train_perplexity=18.65501, train_loss=2.9261148

Batch 239830, train_perplexity=18.655006, train_loss=2.9261146

Batch 239840, train_perplexity=18.655006, train_loss=2.9261146

Batch 239850, train_perplexity=18.655006, train_loss=2.9261146

Batch 239860, train_perplexity=18.65501, train_loss=2.9261148

Batch 239870, train_perplexity=18.65501, train_loss=2.9261148

Batch 239880, train_perplexity=18.655003, train_loss=2.9261143

Batch 239890, train_perplexity=18.655006, train_loss=2.9261146

Batch 239900, train_perplexity=18.655006, train_loss=2.9261146

Batch 239910, train_perplexity=18.655006, train_loss=2.9261146

Batch 239920, train_perplexity=18.655006, train_loss=2.9261146

Batch 239930, train_perplexity=18.655006, train_loss=2.9261146

Batch 239940, train_perplexity=18.655006, train_loss=2.9261146

Batch 239950, train_perplexity=18.655003, train_loss=2.9261143

Batch 239960, train_perplexity=18.655003, train_loss=2.9261143

Batch 239970, train_perplexity=18.655006, train_loss=2.9261146

Batch 239980, train_perplexity=18.654997, train_loss=2.926114

Batch 239990, train_perplexity=18.655006, train_loss=2.9261146

Batch 240000, train_perplexity=18.654997, train_loss=2.926114

Batch 240010, train_perplexity=18.655006, train_loss=2.9261146

Batch 240020, train_perplexity=18.655003, train_loss=2.9261143

Batch 240030, train_perplexity=18.654997, train_loss=2.926114

Batch 240040, train_perplexity=18.655003, train_loss=2.9261143

Batch 240050, train_perplexity=18.654997, train_loss=2.926114

Batch 240060, train_perplexity=18.654997, train_loss=2.926114

Batch 240070, train_perplexity=18.654997, train_loss=2.926114

Batch 240080, train_perplexity=18.655003, train_loss=2.9261143

Batch 240090, train_perplexity=18.654997, train_loss=2.926114

Batch 240100, train_perplexity=18.654997, train_loss=2.926114

Batch 240110, train_perplexity=18.654993, train_loss=2.9261138

Batch 240120, train_perplexity=18.654997, train_loss=2.926114

Batch 240130, train_perplexity=18.654997, train_loss=2.926114

Batch 240140, train_perplexity=18.654997, train_loss=2.926114

Batch 240150, train_perplexity=18.654997, train_loss=2.926114

Batch 240160, train_perplexity=18.654997, train_loss=2.926114

Batch 240170, train_perplexity=18.654997, train_loss=2.926114

Batch 240180, train_perplexity=18.654993, train_loss=2.9261138

Batch 240190, train_perplexity=18.65499, train_loss=2.9261136

Batch 240200, train_perplexity=18.654997, train_loss=2.926114

Batch 240210, train_perplexity=18.654997, train_loss=2.926114

Batch 240220, train_perplexity=18.654997, train_loss=2.926114

Batch 240230, train_perplexity=18.654993, train_loss=2.9261138

Batch 240240, train_perplexity=18.654993, train_loss=2.9261138

Batch 240250, train_perplexity=18.654997, train_loss=2.926114

Batch 240260, train_perplexity=18.654993, train_loss=2.9261138

Batch 240270, train_perplexity=18.654993, train_loss=2.9261138

Batch 240280, train_perplexity=18.654993, train_loss=2.9261138

Batch 240290, train_perplexity=18.65499, train_loss=2.9261136

Batch 240300, train_perplexity=18.65499, train_loss=2.9261136

Batch 240310, train_perplexity=18.654993, train_loss=2.9261138

Batch 240320, train_perplexity=18.654993, train_loss=2.9261138

Batch 240330, train_perplexity=18.65499, train_loss=2.9261136

Batch 240340, train_perplexity=18.654997, train_loss=2.926114

Batch 240350, train_perplexity=18.65499, train_loss=2.9261136

Batch 240360, train_perplexity=18.654993, train_loss=2.9261138

Batch 240370, train_perplexity=18.65499, train_loss=2.9261136

Batch 240380, train_perplexity=18.65499, train_loss=2.9261136

Batch 240390, train_perplexity=18.654993, train_loss=2.9261138

Batch 240400, train_perplexity=18.65499, train_loss=2.9261136

Batch 240410, train_perplexity=18.65499, train_loss=2.9261136

Batch 240420, train_perplexity=18.65499, train_loss=2.9261136

Batch 240430, train_perplexity=18.65499, train_loss=2.9261136

Batch 240440, train_perplexity=18.65499, train_loss=2.9261136

Batch 240450, train_perplexity=18.654993, train_loss=2.9261138

Batch 240460, train_perplexity=18.65499, train_loss=2.9261136
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 240470, train_perplexity=18.65499, train_loss=2.9261136

Batch 240480, train_perplexity=18.65499, train_loss=2.9261136

Batch 240490, train_perplexity=18.65499, train_loss=2.9261136

Batch 240500, train_perplexity=18.65499, train_loss=2.9261136

Batch 240510, train_perplexity=18.65499, train_loss=2.9261136

Batch 240520, train_perplexity=18.654984, train_loss=2.9261134

Batch 240530, train_perplexity=18.65499, train_loss=2.9261136

Batch 240540, train_perplexity=18.654984, train_loss=2.9261134

Batch 240550, train_perplexity=18.654984, train_loss=2.9261134

Batch 240560, train_perplexity=18.654984, train_loss=2.9261134

Batch 240570, train_perplexity=18.65498, train_loss=2.9261131

Batch 240580, train_perplexity=18.654984, train_loss=2.9261134

Batch 240590, train_perplexity=18.65499, train_loss=2.9261136

Batch 240600, train_perplexity=18.654984, train_loss=2.9261134

Batch 240610, train_perplexity=18.654984, train_loss=2.9261134

Batch 240620, train_perplexity=18.654984, train_loss=2.9261134

Batch 240630, train_perplexity=18.654984, train_loss=2.9261134

Batch 240640, train_perplexity=18.654984, train_loss=2.9261134

Batch 240650, train_perplexity=18.65498, train_loss=2.9261131

Batch 240660, train_perplexity=18.654984, train_loss=2.9261134

Batch 240670, train_perplexity=18.654984, train_loss=2.9261134

Batch 240680, train_perplexity=18.654984, train_loss=2.9261134

Batch 240690, train_perplexity=18.65498, train_loss=2.9261131

Batch 240700, train_perplexity=18.65498, train_loss=2.9261131

Batch 240710, train_perplexity=18.65498, train_loss=2.9261131

Batch 240720, train_perplexity=18.654984, train_loss=2.9261134

Batch 240730, train_perplexity=18.65498, train_loss=2.9261131

Batch 240740, train_perplexity=18.65498, train_loss=2.9261131

Batch 240750, train_perplexity=18.65498, train_loss=2.9261131

Batch 240760, train_perplexity=18.65498, train_loss=2.9261131

Batch 240770, train_perplexity=18.65498, train_loss=2.9261131

Batch 240780, train_perplexity=18.65498, train_loss=2.9261131

Batch 240790, train_perplexity=18.65498, train_loss=2.9261131

Batch 240800, train_perplexity=18.654976, train_loss=2.926113

Batch 240810, train_perplexity=18.65498, train_loss=2.9261131

Batch 240820, train_perplexity=18.654976, train_loss=2.926113

Batch 240830, train_perplexity=18.654976, train_loss=2.926113

Batch 240840, train_perplexity=18.654976, train_loss=2.926113

Batch 240850, train_perplexity=18.654976, train_loss=2.926113

Batch 240860, train_perplexity=18.654976, train_loss=2.926113

Batch 240870, train_perplexity=18.654976, train_loss=2.926113

Batch 240880, train_perplexity=18.65497, train_loss=2.9261127

Batch 240890, train_perplexity=18.65498, train_loss=2.9261131

Batch 240900, train_perplexity=18.65498, train_loss=2.9261131

Batch 240910, train_perplexity=18.654976, train_loss=2.926113

Batch 240920, train_perplexity=18.654976, train_loss=2.926113

Batch 240930, train_perplexity=18.65497, train_loss=2.9261127

Batch 240940, train_perplexity=18.65497, train_loss=2.9261127

Batch 240950, train_perplexity=18.654976, train_loss=2.926113

Batch 240960, train_perplexity=18.654976, train_loss=2.926113

Batch 240970, train_perplexity=18.654976, train_loss=2.926113

Batch 240980, train_perplexity=18.65497, train_loss=2.9261127

Batch 240990, train_perplexity=18.654976, train_loss=2.926113

Batch 241000, train_perplexity=18.654976, train_loss=2.926113

Batch 241010, train_perplexity=18.65497, train_loss=2.9261127

Batch 241020, train_perplexity=18.65497, train_loss=2.9261127

Batch 241030, train_perplexity=18.65497, train_loss=2.9261127

Batch 241040, train_perplexity=18.654976, train_loss=2.926113

Batch 241050, train_perplexity=18.65497, train_loss=2.9261127

Batch 241060, train_perplexity=18.65497, train_loss=2.9261127

Batch 241070, train_perplexity=18.65497, train_loss=2.9261127

Batch 241080, train_perplexity=18.65497, train_loss=2.9261127

Batch 241090, train_perplexity=18.65497, train_loss=2.9261127

Batch 241100, train_perplexity=18.65497, train_loss=2.9261127

Batch 241110, train_perplexity=18.65497, train_loss=2.9261127

Batch 241120, train_perplexity=18.654966, train_loss=2.9261124

Batch 241130, train_perplexity=18.65497, train_loss=2.9261127

Batch 241140, train_perplexity=18.654966, train_loss=2.9261124

Batch 241150, train_perplexity=18.65497, train_loss=2.9261127

Batch 241160, train_perplexity=18.65497, train_loss=2.9261127

Batch 241170, train_perplexity=18.654966, train_loss=2.9261124

Batch 241180, train_perplexity=18.654966, train_loss=2.9261124

Batch 241190, train_perplexity=18.654966, train_loss=2.9261124

Batch 241200, train_perplexity=18.65497, train_loss=2.9261127

Batch 241210, train_perplexity=18.65497, train_loss=2.9261127

Batch 241220, train_perplexity=18.654966, train_loss=2.9261124

Batch 241230, train_perplexity=18.65497, train_loss=2.9261127

Batch 241240, train_perplexity=18.654966, train_loss=2.9261124

Batch 241250, train_perplexity=18.654966, train_loss=2.9261124

Batch 241260, train_perplexity=18.654966, train_loss=2.9261124

Batch 241270, train_perplexity=18.654963, train_loss=2.9261122

Batch 241280, train_perplexity=18.654963, train_loss=2.9261122

Batch 241290, train_perplexity=18.654966, train_loss=2.9261124

Batch 241300, train_perplexity=18.654966, train_loss=2.9261124

Batch 241310, train_perplexity=18.654963, train_loss=2.9261122

Batch 241320, train_perplexity=18.654963, train_loss=2.9261122

Batch 241330, train_perplexity=18.654963, train_loss=2.9261122

Batch 241340, train_perplexity=18.654963, train_loss=2.9261122

Batch 241350, train_perplexity=18.654963, train_loss=2.9261122

Batch 241360, train_perplexity=18.654963, train_loss=2.9261122

Batch 241370, train_perplexity=18.654963, train_loss=2.9261122

Batch 241380, train_perplexity=18.654963, train_loss=2.9261122

Batch 241390, train_perplexity=18.654963, train_loss=2.9261122

Batch 241400, train_perplexity=18.654963, train_loss=2.9261122

Batch 241410, train_perplexity=18.654953, train_loss=2.9261117

Batch 241420, train_perplexity=18.654963, train_loss=2.9261122

Batch 241430, train_perplexity=18.654963, train_loss=2.9261122

Batch 241440, train_perplexity=18.654953, train_loss=2.9261117

Batch 241450, train_perplexity=18.654963, train_loss=2.9261122

Batch 241460, train_perplexity=18.654963, train_loss=2.9261122

Batch 241470, train_perplexity=18.654957, train_loss=2.926112

Batch 241480, train_perplexity=18.654957, train_loss=2.926112

Batch 241490, train_perplexity=18.654953, train_loss=2.9261117

Batch 241500, train_perplexity=18.654957, train_loss=2.926112

Batch 241510, train_perplexity=18.654957, train_loss=2.926112

Batch 241520, train_perplexity=18.654957, train_loss=2.926112

Batch 241530, train_perplexity=18.654957, train_loss=2.926112

Batch 241540, train_perplexity=18.654963, train_loss=2.9261122

Batch 241550, train_perplexity=18.654953, train_loss=2.9261117

Batch 241560, train_perplexity=18.654957, train_loss=2.926112

Batch 241570, train_perplexity=18.654957, train_loss=2.926112

Batch 241580, train_perplexity=18.654953, train_loss=2.9261117

Batch 241590, train_perplexity=18.654957, train_loss=2.926112

Batch 241600, train_perplexity=18.654953, train_loss=2.9261117

Batch 241610, train_perplexity=18.654953, train_loss=2.9261117

Batch 241620, train_perplexity=18.654953, train_loss=2.9261117

Batch 241630, train_perplexity=18.65495, train_loss=2.9261115

Batch 241640, train_perplexity=18.654953, train_loss=2.9261117

Batch 241650, train_perplexity=18.654953, train_loss=2.9261117

Batch 241660, train_perplexity=18.654953, train_loss=2.9261117

Batch 241670, train_perplexity=18.654953, train_loss=2.9261117

Batch 241680, train_perplexity=18.654953, train_loss=2.9261117

Batch 241690, train_perplexity=18.654953, train_loss=2.9261117

Batch 241700, train_perplexity=18.654953, train_loss=2.9261117

Batch 241710, train_perplexity=18.654953, train_loss=2.9261117

Batch 241720, train_perplexity=18.654953, train_loss=2.9261117

Batch 241730, train_perplexity=18.654953, train_loss=2.9261117

Batch 241740, train_perplexity=18.654953, train_loss=2.9261117

Batch 241750, train_perplexity=18.654953, train_loss=2.9261117

Batch 241760, train_perplexity=18.654953, train_loss=2.9261117
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 241770, train_perplexity=18.654953, train_loss=2.9261117

Batch 241780, train_perplexity=18.654943, train_loss=2.9261112

Batch 241790, train_perplexity=18.65495, train_loss=2.9261115

Batch 241800, train_perplexity=18.654953, train_loss=2.9261117

Batch 241810, train_perplexity=18.654953, train_loss=2.9261117

Batch 241820, train_perplexity=18.65495, train_loss=2.9261115

Batch 241830, train_perplexity=18.654953, train_loss=2.9261117

Batch 241840, train_perplexity=18.65495, train_loss=2.9261115

Batch 241850, train_perplexity=18.65495, train_loss=2.9261115

Batch 241860, train_perplexity=18.65495, train_loss=2.9261115

Batch 241870, train_perplexity=18.654943, train_loss=2.9261112

Batch 241880, train_perplexity=18.654943, train_loss=2.9261112

Batch 241890, train_perplexity=18.65495, train_loss=2.9261115

Batch 241900, train_perplexity=18.654943, train_loss=2.9261112

Batch 241910, train_perplexity=18.654943, train_loss=2.9261112

Batch 241920, train_perplexity=18.654943, train_loss=2.9261112

Batch 241930, train_perplexity=18.65495, train_loss=2.9261115

Batch 241940, train_perplexity=18.65495, train_loss=2.9261115

Batch 241950, train_perplexity=18.654943, train_loss=2.9261112

Batch 241960, train_perplexity=18.654943, train_loss=2.9261112

Batch 241970, train_perplexity=18.65495, train_loss=2.9261115

Batch 241980, train_perplexity=18.654943, train_loss=2.9261112

Batch 241990, train_perplexity=18.654943, train_loss=2.9261112

Batch 242000, train_perplexity=18.654943, train_loss=2.9261112

Batch 242010, train_perplexity=18.654943, train_loss=2.9261112

Batch 242020, train_perplexity=18.654943, train_loss=2.9261112

Batch 242030, train_perplexity=18.65494, train_loss=2.926111

Batch 242040, train_perplexity=18.654943, train_loss=2.9261112

Batch 242050, train_perplexity=18.654943, train_loss=2.9261112

Batch 242060, train_perplexity=18.654943, train_loss=2.9261112

Batch 242070, train_perplexity=18.654936, train_loss=2.9261107

Batch 242080, train_perplexity=18.65494, train_loss=2.926111

Batch 242090, train_perplexity=18.65494, train_loss=2.926111

Batch 242100, train_perplexity=18.65494, train_loss=2.926111

Batch 242110, train_perplexity=18.65494, train_loss=2.926111

Batch 242120, train_perplexity=18.65494, train_loss=2.926111

Batch 242130, train_perplexity=18.65494, train_loss=2.926111

Batch 242140, train_perplexity=18.65494, train_loss=2.926111

Batch 242150, train_perplexity=18.654936, train_loss=2.9261107

Batch 242160, train_perplexity=18.654936, train_loss=2.9261107

Batch 242170, train_perplexity=18.65494, train_loss=2.926111

Batch 242180, train_perplexity=18.654936, train_loss=2.9261107

Batch 242190, train_perplexity=18.654936, train_loss=2.9261107

Batch 242200, train_perplexity=18.65494, train_loss=2.926111

Batch 242210, train_perplexity=18.654936, train_loss=2.9261107

Batch 242220, train_perplexity=18.654936, train_loss=2.9261107

Batch 242230, train_perplexity=18.654936, train_loss=2.9261107

Batch 242240, train_perplexity=18.654936, train_loss=2.9261107

Batch 242250, train_perplexity=18.65494, train_loss=2.926111

Batch 242260, train_perplexity=18.654936, train_loss=2.9261107

Batch 242270, train_perplexity=18.654936, train_loss=2.9261107

Batch 242280, train_perplexity=18.654936, train_loss=2.9261107

Batch 242290, train_perplexity=18.654936, train_loss=2.9261107

Batch 242300, train_perplexity=18.654936, train_loss=2.9261107

Batch 242310, train_perplexity=18.654936, train_loss=2.9261107

Batch 242320, train_perplexity=18.654926, train_loss=2.9261103

Batch 242330, train_perplexity=18.654936, train_loss=2.9261107

Batch 242340, train_perplexity=18.654926, train_loss=2.9261103

Batch 242350, train_perplexity=18.654936, train_loss=2.9261107

Batch 242360, train_perplexity=18.65493, train_loss=2.9261105

Batch 242370, train_perplexity=18.65493, train_loss=2.9261105

Batch 242380, train_perplexity=18.65493, train_loss=2.9261105

Batch 242390, train_perplexity=18.654926, train_loss=2.9261103

Batch 242400, train_perplexity=18.65493, train_loss=2.9261105

Batch 242410, train_perplexity=18.654936, train_loss=2.9261107

Batch 242420, train_perplexity=18.65493, train_loss=2.9261105

Batch 242430, train_perplexity=18.65493, train_loss=2.9261105

Batch 242440, train_perplexity=18.65493, train_loss=2.9261105

Batch 242450, train_perplexity=18.65493, train_loss=2.9261105

Batch 242460, train_perplexity=18.65493, train_loss=2.9261105

Batch 242470, train_perplexity=18.654926, train_loss=2.9261103

Batch 242480, train_perplexity=18.65493, train_loss=2.9261105

Batch 242490, train_perplexity=18.654926, train_loss=2.9261103

Batch 242500, train_perplexity=18.654926, train_loss=2.9261103

Batch 242510, train_perplexity=18.654926, train_loss=2.9261103

Batch 242520, train_perplexity=18.65493, train_loss=2.9261105

Batch 242530, train_perplexity=18.654926, train_loss=2.9261103

Batch 242540, train_perplexity=18.654926, train_loss=2.9261103

Batch 242550, train_perplexity=18.654926, train_loss=2.9261103

Batch 242560, train_perplexity=18.65493, train_loss=2.9261105

Batch 242570, train_perplexity=18.654926, train_loss=2.9261103

Batch 242580, train_perplexity=18.654926, train_loss=2.9261103

Batch 242590, train_perplexity=18.654926, train_loss=2.9261103

Batch 242600, train_perplexity=18.654926, train_loss=2.9261103

Batch 242610, train_perplexity=18.654926, train_loss=2.9261103

Batch 242620, train_perplexity=18.654926, train_loss=2.9261103

Batch 242630, train_perplexity=18.654917, train_loss=2.9261098

Batch 242640, train_perplexity=18.654926, train_loss=2.9261103

Batch 242650, train_perplexity=18.654926, train_loss=2.9261103

Batch 242660, train_perplexity=18.654922, train_loss=2.92611

Batch 242670, train_perplexity=18.654926, train_loss=2.9261103

Batch 242680, train_perplexity=18.654926, train_loss=2.9261103

Batch 242690, train_perplexity=18.654917, train_loss=2.9261098

Batch 242700, train_perplexity=18.654926, train_loss=2.9261103

Batch 242710, train_perplexity=18.654922, train_loss=2.92611

Batch 242720, train_perplexity=18.654926, train_loss=2.9261103

Batch 242730, train_perplexity=18.654926, train_loss=2.9261103

Batch 242740, train_perplexity=18.654922, train_loss=2.92611

Batch 242750, train_perplexity=18.654917, train_loss=2.9261098

Batch 242760, train_perplexity=18.654922, train_loss=2.92611

Batch 242770, train_perplexity=18.654922, train_loss=2.92611

Batch 242780, train_perplexity=18.654917, train_loss=2.9261098

Batch 242790, train_perplexity=18.654917, train_loss=2.9261098

Batch 242800, train_perplexity=18.654922, train_loss=2.92611

Batch 242810, train_perplexity=18.654922, train_loss=2.92611

Batch 242820, train_perplexity=18.654922, train_loss=2.92611

Batch 242830, train_perplexity=18.654917, train_loss=2.9261098

Batch 242840, train_perplexity=18.654926, train_loss=2.9261103

Batch 242850, train_perplexity=18.654917, train_loss=2.9261098

Batch 242860, train_perplexity=18.654917, train_loss=2.9261098

Batch 242870, train_perplexity=18.654922, train_loss=2.92611

Batch 242880, train_perplexity=18.654917, train_loss=2.9261098

Batch 242890, train_perplexity=18.654917, train_loss=2.9261098

Batch 242900, train_perplexity=18.654917, train_loss=2.9261098

Batch 242910, train_perplexity=18.654917, train_loss=2.9261098

Batch 242920, train_perplexity=18.654917, train_loss=2.9261098

Batch 242930, train_perplexity=18.654917, train_loss=2.9261098

Batch 242940, train_perplexity=18.654917, train_loss=2.9261098

Batch 242950, train_perplexity=18.654917, train_loss=2.9261098

Batch 242960, train_perplexity=18.654913, train_loss=2.9261096

Batch 242970, train_perplexity=18.654917, train_loss=2.9261098

Batch 242980, train_perplexity=18.654917, train_loss=2.9261098

Batch 242990, train_perplexity=18.65491, train_loss=2.9261093

Batch 243000, train_perplexity=18.65491, train_loss=2.9261093

Batch 243010, train_perplexity=18.654917, train_loss=2.9261098

Batch 243020, train_perplexity=18.654917, train_loss=2.9261098

Batch 243030, train_perplexity=18.654917, train_loss=2.9261098

Batch 243040, train_perplexity=18.65491, train_loss=2.9261093

Batch 243050, train_perplexity=18.65491, train_loss=2.9261093

Batch 243060, train_perplexity=18.654913, train_loss=2.9261096
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 243070, train_perplexity=18.65491, train_loss=2.9261093

Batch 243080, train_perplexity=18.654913, train_loss=2.9261096

Batch 243090, train_perplexity=18.65491, train_loss=2.9261093

Batch 243100, train_perplexity=18.654913, train_loss=2.9261096

Batch 243110, train_perplexity=18.65491, train_loss=2.9261093

Batch 243120, train_perplexity=18.65491, train_loss=2.9261093

Batch 243130, train_perplexity=18.65491, train_loss=2.9261093

Batch 243140, train_perplexity=18.654913, train_loss=2.9261096

Batch 243150, train_perplexity=18.65491, train_loss=2.9261093

Batch 243160, train_perplexity=18.65491, train_loss=2.9261093

Batch 243170, train_perplexity=18.654913, train_loss=2.9261096

Batch 243180, train_perplexity=18.654913, train_loss=2.9261096

Batch 243190, train_perplexity=18.65491, train_loss=2.9261093

Batch 243200, train_perplexity=18.65491, train_loss=2.9261093

Batch 243210, train_perplexity=18.654913, train_loss=2.9261096

Batch 243220, train_perplexity=18.65491, train_loss=2.9261093

Batch 243230, train_perplexity=18.654903, train_loss=2.926109

Batch 243240, train_perplexity=18.65491, train_loss=2.9261093

Batch 243250, train_perplexity=18.65491, train_loss=2.9261093

Batch 243260, train_perplexity=18.65491, train_loss=2.9261093

Batch 243270, train_perplexity=18.65491, train_loss=2.9261093

Batch 243280, train_perplexity=18.65491, train_loss=2.9261093

Batch 243290, train_perplexity=18.6549, train_loss=2.9261088

Batch 243300, train_perplexity=18.654903, train_loss=2.926109

Batch 243310, train_perplexity=18.65491, train_loss=2.9261093

Batch 243320, train_perplexity=18.65491, train_loss=2.9261093

Batch 243330, train_perplexity=18.65491, train_loss=2.9261093

Batch 243340, train_perplexity=18.654903, train_loss=2.926109

Batch 243350, train_perplexity=18.6549, train_loss=2.9261088

Batch 243360, train_perplexity=18.654903, train_loss=2.926109

Batch 243370, train_perplexity=18.654903, train_loss=2.926109

Batch 243380, train_perplexity=18.65491, train_loss=2.9261093

Batch 243390, train_perplexity=18.65491, train_loss=2.9261093

Batch 243400, train_perplexity=18.654903, train_loss=2.926109

Batch 243410, train_perplexity=18.654903, train_loss=2.926109

Batch 243420, train_perplexity=18.6549, train_loss=2.9261088

Batch 243430, train_perplexity=18.6549, train_loss=2.9261088

Batch 243440, train_perplexity=18.654903, train_loss=2.926109

Batch 243450, train_perplexity=18.6549, train_loss=2.9261088

Batch 243460, train_perplexity=18.6549, train_loss=2.9261088

Batch 243470, train_perplexity=18.654903, train_loss=2.926109

Batch 243480, train_perplexity=18.6549, train_loss=2.9261088

Batch 243490, train_perplexity=18.6549, train_loss=2.9261088

Batch 243500, train_perplexity=18.6549, train_loss=2.9261088

Batch 243510, train_perplexity=18.6549, train_loss=2.9261088

Batch 243520, train_perplexity=18.6549, train_loss=2.9261088

Batch 243530, train_perplexity=18.6549, train_loss=2.9261088

Batch 243540, train_perplexity=18.6549, train_loss=2.9261088

Batch 243550, train_perplexity=18.6549, train_loss=2.9261088

Batch 243560, train_perplexity=18.6549, train_loss=2.9261088

Batch 243570, train_perplexity=18.654896, train_loss=2.9261086

Batch 243580, train_perplexity=18.654896, train_loss=2.9261086

Batch 243590, train_perplexity=18.6549, train_loss=2.9261088

Batch 243600, train_perplexity=18.6549, train_loss=2.9261088

Batch 243610, train_perplexity=18.6549, train_loss=2.9261088

Batch 243620, train_perplexity=18.65489, train_loss=2.9261084

Batch 243630, train_perplexity=18.654896, train_loss=2.9261086

Batch 243640, train_perplexity=18.654896, train_loss=2.9261086

Batch 243650, train_perplexity=18.6549, train_loss=2.9261088

Batch 243660, train_perplexity=18.65489, train_loss=2.9261084

Batch 243670, train_perplexity=18.6549, train_loss=2.9261088

Batch 243680, train_perplexity=18.65489, train_loss=2.9261084

Batch 243690, train_perplexity=18.654896, train_loss=2.9261086

Batch 243700, train_perplexity=18.6549, train_loss=2.9261088

Batch 243710, train_perplexity=18.654896, train_loss=2.9261086

Batch 243720, train_perplexity=18.654896, train_loss=2.9261086

Batch 243730, train_perplexity=18.654896, train_loss=2.9261086

Batch 243740, train_perplexity=18.65489, train_loss=2.9261084

Batch 243750, train_perplexity=18.654896, train_loss=2.9261086

Batch 243760, train_perplexity=18.654896, train_loss=2.9261086

Batch 243770, train_perplexity=18.654896, train_loss=2.9261086

Batch 243780, train_perplexity=18.654896, train_loss=2.9261086

Batch 243790, train_perplexity=18.65489, train_loss=2.9261084

Batch 243800, train_perplexity=18.65489, train_loss=2.9261084

Batch 243810, train_perplexity=18.65489, train_loss=2.9261084

Batch 243820, train_perplexity=18.654886, train_loss=2.9261081

Batch 243830, train_perplexity=18.65489, train_loss=2.9261084

Batch 243840, train_perplexity=18.65489, train_loss=2.9261084

Batch 243850, train_perplexity=18.65489, train_loss=2.9261084

Batch 243860, train_perplexity=18.65489, train_loss=2.9261084

Batch 243870, train_perplexity=18.65489, train_loss=2.9261084

Batch 243880, train_perplexity=18.65489, train_loss=2.9261084

Batch 243890, train_perplexity=18.65489, train_loss=2.9261084

Batch 243900, train_perplexity=18.65489, train_loss=2.9261084

Batch 243910, train_perplexity=18.65489, train_loss=2.9261084

Batch 243920, train_perplexity=18.654886, train_loss=2.9261081

Batch 243930, train_perplexity=18.65489, train_loss=2.9261084

Batch 243940, train_perplexity=18.654886, train_loss=2.9261081

Batch 243950, train_perplexity=18.65489, train_loss=2.9261084

Batch 243960, train_perplexity=18.654886, train_loss=2.9261081

Batch 243970, train_perplexity=18.654886, train_loss=2.9261081

Batch 243980, train_perplexity=18.654882, train_loss=2.926108

Batch 243990, train_perplexity=18.65489, train_loss=2.9261084

Batch 244000, train_perplexity=18.654886, train_loss=2.9261081

Batch 244010, train_perplexity=18.654882, train_loss=2.926108

Batch 244020, train_perplexity=18.654886, train_loss=2.9261081

Batch 244030, train_perplexity=18.654882, train_loss=2.926108

Batch 244040, train_perplexity=18.654882, train_loss=2.926108

Batch 244050, train_perplexity=18.654886, train_loss=2.9261081

Batch 244060, train_perplexity=18.654886, train_loss=2.9261081

Batch 244070, train_perplexity=18.654882, train_loss=2.926108

Batch 244080, train_perplexity=18.654882, train_loss=2.926108

Batch 244090, train_perplexity=18.654882, train_loss=2.926108

Batch 244100, train_perplexity=18.654882, train_loss=2.926108

Batch 244110, train_perplexity=18.654882, train_loss=2.926108

Batch 244120, train_perplexity=18.654882, train_loss=2.926108

Batch 244130, train_perplexity=18.654886, train_loss=2.9261081

Batch 244140, train_perplexity=18.654882, train_loss=2.926108

Batch 244150, train_perplexity=18.654882, train_loss=2.926108

Batch 244160, train_perplexity=18.654882, train_loss=2.926108

Batch 244170, train_perplexity=18.654882, train_loss=2.926108

Batch 244180, train_perplexity=18.654882, train_loss=2.926108

Batch 244190, train_perplexity=18.654882, train_loss=2.926108

Batch 244200, train_perplexity=18.654882, train_loss=2.926108

Batch 244210, train_perplexity=18.654882, train_loss=2.926108

Batch 244220, train_perplexity=18.654877, train_loss=2.9261076

Batch 244230, train_perplexity=18.654877, train_loss=2.9261076

Batch 244240, train_perplexity=18.654873, train_loss=2.9261074

Batch 244250, train_perplexity=18.654882, train_loss=2.926108

Batch 244260, train_perplexity=18.654877, train_loss=2.9261076

Batch 244270, train_perplexity=18.654882, train_loss=2.926108

Batch 244280, train_perplexity=18.654877, train_loss=2.9261076

Batch 244290, train_perplexity=18.654877, train_loss=2.9261076

Batch 244300, train_perplexity=18.654877, train_loss=2.9261076

Batch 244310, train_perplexity=18.654877, train_loss=2.9261076

Batch 244320, train_perplexity=18.654877, train_loss=2.9261076

Batch 244330, train_perplexity=18.654882, train_loss=2.926108

Batch 244340, train_perplexity=18.654882, train_loss=2.926108

Batch 244350, train_perplexity=18.654882, train_loss=2.926108

Batch 244360, train_perplexity=18.654873, train_loss=2.9261074
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 244370, train_perplexity=18.654873, train_loss=2.9261074

Batch 244380, train_perplexity=18.654877, train_loss=2.9261076

Batch 244390, train_perplexity=18.654877, train_loss=2.9261076

Batch 244400, train_perplexity=18.654877, train_loss=2.9261076

Batch 244410, train_perplexity=18.654873, train_loss=2.9261074

Batch 244420, train_perplexity=18.654873, train_loss=2.9261074

Batch 244430, train_perplexity=18.654877, train_loss=2.9261076

Batch 244440, train_perplexity=18.654873, train_loss=2.9261074

Batch 244450, train_perplexity=18.654877, train_loss=2.9261076

Batch 244460, train_perplexity=18.654873, train_loss=2.9261074

Batch 244470, train_perplexity=18.654877, train_loss=2.9261076

Batch 244480, train_perplexity=18.654873, train_loss=2.9261074

Batch 244490, train_perplexity=18.654873, train_loss=2.9261074

Batch 244500, train_perplexity=18.654873, train_loss=2.9261074

Batch 244510, train_perplexity=18.654873, train_loss=2.9261074

Batch 244520, train_perplexity=18.654873, train_loss=2.9261074

Batch 244530, train_perplexity=18.65487, train_loss=2.9261072

Batch 244540, train_perplexity=18.654873, train_loss=2.9261074

Batch 244550, train_perplexity=18.65487, train_loss=2.9261072

Batch 244560, train_perplexity=18.654863, train_loss=2.926107

Batch 244570, train_perplexity=18.654873, train_loss=2.9261074

Batch 244580, train_perplexity=18.654873, train_loss=2.9261074

Batch 244590, train_perplexity=18.65487, train_loss=2.9261072

Batch 244600, train_perplexity=18.65487, train_loss=2.9261072

Batch 244610, train_perplexity=18.654873, train_loss=2.9261074

Batch 244620, train_perplexity=18.65487, train_loss=2.9261072

Batch 244630, train_perplexity=18.654873, train_loss=2.9261074

Batch 244640, train_perplexity=18.65487, train_loss=2.9261072

Batch 244650, train_perplexity=18.65487, train_loss=2.9261072

Batch 244660, train_perplexity=18.654863, train_loss=2.926107

Batch 244670, train_perplexity=18.65487, train_loss=2.9261072

Batch 244680, train_perplexity=18.65487, train_loss=2.9261072

Batch 244690, train_perplexity=18.65487, train_loss=2.9261072

Batch 244700, train_perplexity=18.65487, train_loss=2.9261072

Batch 244710, train_perplexity=18.654863, train_loss=2.926107

Batch 244720, train_perplexity=18.654863, train_loss=2.926107

Batch 244730, train_perplexity=18.654863, train_loss=2.926107

Batch 244740, train_perplexity=18.65487, train_loss=2.9261072

Batch 244750, train_perplexity=18.654863, train_loss=2.926107

Batch 244760, train_perplexity=18.654863, train_loss=2.926107

Batch 244770, train_perplexity=18.654863, train_loss=2.926107

Batch 244780, train_perplexity=18.654863, train_loss=2.926107

Batch 244790, train_perplexity=18.654863, train_loss=2.926107

Batch 244800, train_perplexity=18.654863, train_loss=2.926107

Batch 244810, train_perplexity=18.654856, train_loss=2.9261065

Batch 244820, train_perplexity=18.65486, train_loss=2.9261067

Batch 244830, train_perplexity=18.654863, train_loss=2.926107

Batch 244840, train_perplexity=18.654863, train_loss=2.926107

Batch 244850, train_perplexity=18.654863, train_loss=2.926107

Batch 244860, train_perplexity=18.654863, train_loss=2.926107

Batch 244870, train_perplexity=18.654863, train_loss=2.926107

Batch 244880, train_perplexity=18.654863, train_loss=2.926107

Batch 244890, train_perplexity=18.654863, train_loss=2.926107

Batch 244900, train_perplexity=18.654856, train_loss=2.9261065

Batch 244910, train_perplexity=18.65486, train_loss=2.9261067

Batch 244920, train_perplexity=18.654856, train_loss=2.9261065

Batch 244930, train_perplexity=18.65486, train_loss=2.9261067

Batch 244940, train_perplexity=18.654863, train_loss=2.926107

Batch 244950, train_perplexity=18.65486, train_loss=2.9261067

Batch 244960, train_perplexity=18.654856, train_loss=2.9261065

Batch 244970, train_perplexity=18.654863, train_loss=2.926107

Batch 244980, train_perplexity=18.654856, train_loss=2.9261065

Batch 244990, train_perplexity=18.65486, train_loss=2.9261067

Batch 245000, train_perplexity=18.654856, train_loss=2.9261065

Batch 245010, train_perplexity=18.654863, train_loss=2.926107

Batch 245020, train_perplexity=18.654856, train_loss=2.9261065

Batch 245030, train_perplexity=18.654856, train_loss=2.9261065

Batch 245040, train_perplexity=18.65486, train_loss=2.9261067

Batch 245050, train_perplexity=18.654856, train_loss=2.9261065

Batch 245060, train_perplexity=18.654856, train_loss=2.9261065

Batch 245070, train_perplexity=18.654856, train_loss=2.9261065

Batch 245080, train_perplexity=18.654856, train_loss=2.9261065

Batch 245090, train_perplexity=18.654856, train_loss=2.9261065

Batch 245100, train_perplexity=18.65485, train_loss=2.9261062

Batch 245110, train_perplexity=18.654856, train_loss=2.9261065

Batch 245120, train_perplexity=18.654856, train_loss=2.9261065

Batch 245130, train_perplexity=18.654856, train_loss=2.9261065

Batch 245140, train_perplexity=18.654856, train_loss=2.9261065

Batch 245150, train_perplexity=18.65486, train_loss=2.9261067

Batch 245160, train_perplexity=18.654856, train_loss=2.9261065

Batch 245170, train_perplexity=18.654846, train_loss=2.926106

Batch 245180, train_perplexity=18.65485, train_loss=2.9261062

Batch 245190, train_perplexity=18.654856, train_loss=2.9261065

Batch 245200, train_perplexity=18.654846, train_loss=2.926106

Batch 245210, train_perplexity=18.65485, train_loss=2.9261062

Batch 245220, train_perplexity=18.654846, train_loss=2.926106

Batch 245230, train_perplexity=18.65485, train_loss=2.9261062

Batch 245240, train_perplexity=18.654846, train_loss=2.926106

Batch 245250, train_perplexity=18.654846, train_loss=2.926106

Batch 245260, train_perplexity=18.654846, train_loss=2.926106

Batch 245270, train_perplexity=18.654856, train_loss=2.9261065

Batch 245280, train_perplexity=18.65485, train_loss=2.9261062

Batch 245290, train_perplexity=18.654846, train_loss=2.926106

Batch 245300, train_perplexity=18.65485, train_loss=2.9261062

Batch 245310, train_perplexity=18.654846, train_loss=2.926106

Batch 245320, train_perplexity=18.65485, train_loss=2.9261062

Batch 245330, train_perplexity=18.654846, train_loss=2.926106

Batch 245340, train_perplexity=18.654846, train_loss=2.926106

Batch 245350, train_perplexity=18.654842, train_loss=2.9261057

Batch 245360, train_perplexity=18.65485, train_loss=2.9261062

Batch 245370, train_perplexity=18.654846, train_loss=2.926106

Batch 245380, train_perplexity=18.65485, train_loss=2.9261062

Batch 245390, train_perplexity=18.654846, train_loss=2.926106

Batch 245400, train_perplexity=18.654846, train_loss=2.926106

Batch 245410, train_perplexity=18.654846, train_loss=2.926106

Batch 245420, train_perplexity=18.654846, train_loss=2.926106

Batch 245430, train_perplexity=18.654846, train_loss=2.926106

Batch 245440, train_perplexity=18.654846, train_loss=2.926106

Batch 245450, train_perplexity=18.654846, train_loss=2.926106

Batch 245460, train_perplexity=18.654846, train_loss=2.926106

Batch 245470, train_perplexity=18.654846, train_loss=2.926106

Batch 245480, train_perplexity=18.654842, train_loss=2.9261057

Batch 245490, train_perplexity=18.654846, train_loss=2.926106

Batch 245500, train_perplexity=18.654837, train_loss=2.9261055

Batch 245510, train_perplexity=18.654846, train_loss=2.926106

Batch 245520, train_perplexity=18.654846, train_loss=2.926106

Batch 245530, train_perplexity=18.654837, train_loss=2.9261055

Batch 245540, train_perplexity=18.654837, train_loss=2.9261055

Batch 245550, train_perplexity=18.654842, train_loss=2.9261057

Batch 245560, train_perplexity=18.654842, train_loss=2.9261057

Batch 245570, train_perplexity=18.654842, train_loss=2.9261057

Batch 245580, train_perplexity=18.654837, train_loss=2.9261055

Batch 245590, train_perplexity=18.654837, train_loss=2.9261055

Batch 245600, train_perplexity=18.654837, train_loss=2.9261055

Batch 245610, train_perplexity=18.654837, train_loss=2.9261055

Batch 245620, train_perplexity=18.654837, train_loss=2.9261055

Batch 245630, train_perplexity=18.654837, train_loss=2.9261055

Batch 245640, train_perplexity=18.654837, train_loss=2.9261055

Batch 245650, train_perplexity=18.654837, train_loss=2.9261055

Batch 245660, train_perplexity=18.654837, train_loss=2.9261055
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 245670, train_perplexity=18.654842, train_loss=2.9261057

Batch 245680, train_perplexity=18.654846, train_loss=2.926106

Batch 245690, train_perplexity=18.654837, train_loss=2.9261055

Batch 245700, train_perplexity=18.654837, train_loss=2.9261055

Batch 245710, train_perplexity=18.654837, train_loss=2.9261055

Batch 245720, train_perplexity=18.654837, train_loss=2.9261055

Batch 245730, train_perplexity=18.654837, train_loss=2.9261055

Batch 245740, train_perplexity=18.654837, train_loss=2.9261055

Batch 245750, train_perplexity=18.654837, train_loss=2.9261055

Batch 245760, train_perplexity=18.654837, train_loss=2.9261055

Batch 245770, train_perplexity=18.654837, train_loss=2.9261055

Batch 245780, train_perplexity=18.654833, train_loss=2.9261053

Batch 245790, train_perplexity=18.654833, train_loss=2.9261053

Batch 245800, train_perplexity=18.654837, train_loss=2.9261055

Batch 245810, train_perplexity=18.654837, train_loss=2.9261055

Batch 245820, train_perplexity=18.654837, train_loss=2.9261055

Batch 245830, train_perplexity=18.654837, train_loss=2.9261055

Batch 245840, train_perplexity=18.654833, train_loss=2.9261053

Batch 245850, train_perplexity=18.654829, train_loss=2.926105

Batch 245860, train_perplexity=18.654833, train_loss=2.9261053

Batch 245870, train_perplexity=18.654829, train_loss=2.926105

Batch 245880, train_perplexity=18.654833, train_loss=2.9261053

Batch 245890, train_perplexity=18.654837, train_loss=2.9261055

Batch 245900, train_perplexity=18.654837, train_loss=2.9261055

Batch 245910, train_perplexity=18.654833, train_loss=2.9261053

Batch 245920, train_perplexity=18.654833, train_loss=2.9261053

Batch 245930, train_perplexity=18.654829, train_loss=2.926105

Batch 245940, train_perplexity=18.654829, train_loss=2.926105

Batch 245950, train_perplexity=18.654829, train_loss=2.926105

Batch 245960, train_perplexity=18.654833, train_loss=2.9261053

Batch 245970, train_perplexity=18.654837, train_loss=2.9261055

Batch 245980, train_perplexity=18.654829, train_loss=2.926105

Batch 245990, train_perplexity=18.654829, train_loss=2.926105

Batch 246000, train_perplexity=18.654829, train_loss=2.926105

Batch 246010, train_perplexity=18.654829, train_loss=2.926105

Batch 246020, train_perplexity=18.654829, train_loss=2.926105

Batch 246030, train_perplexity=18.654829, train_loss=2.926105

Batch 246040, train_perplexity=18.654829, train_loss=2.926105

Batch 246050, train_perplexity=18.654829, train_loss=2.926105

Batch 246060, train_perplexity=18.65482, train_loss=2.9261045

Batch 246070, train_perplexity=18.654829, train_loss=2.926105

Batch 246080, train_perplexity=18.654829, train_loss=2.926105

Batch 246090, train_perplexity=18.654829, train_loss=2.926105

Batch 246100, train_perplexity=18.654829, train_loss=2.926105

Batch 246110, train_perplexity=18.654829, train_loss=2.926105

Batch 246120, train_perplexity=18.654829, train_loss=2.926105

Batch 246130, train_perplexity=18.654829, train_loss=2.926105

Batch 246140, train_perplexity=18.654823, train_loss=2.9261048

Batch 246150, train_perplexity=18.654829, train_loss=2.926105

Batch 246160, train_perplexity=18.654829, train_loss=2.926105

Batch 246170, train_perplexity=18.654829, train_loss=2.926105

Batch 246180, train_perplexity=18.654829, train_loss=2.926105

Batch 246190, train_perplexity=18.65482, train_loss=2.9261045

Batch 246200, train_perplexity=18.654823, train_loss=2.9261048

Batch 246210, train_perplexity=18.65482, train_loss=2.9261045

Batch 246220, train_perplexity=18.65482, train_loss=2.9261045

Batch 246230, train_perplexity=18.654823, train_loss=2.9261048

Batch 246240, train_perplexity=18.654823, train_loss=2.9261048

Batch 246250, train_perplexity=18.654823, train_loss=2.9261048

Batch 246260, train_perplexity=18.654823, train_loss=2.9261048

Batch 246270, train_perplexity=18.65482, train_loss=2.9261045

Batch 246280, train_perplexity=18.65482, train_loss=2.9261045

Batch 246290, train_perplexity=18.65482, train_loss=2.9261045

Batch 246300, train_perplexity=18.65482, train_loss=2.9261045

Batch 246310, train_perplexity=18.654823, train_loss=2.9261048

Batch 246320, train_perplexity=18.654823, train_loss=2.9261048

Batch 246330, train_perplexity=18.654829, train_loss=2.926105

Batch 246340, train_perplexity=18.654823, train_loss=2.9261048

Batch 246350, train_perplexity=18.65482, train_loss=2.9261045

Batch 246360, train_perplexity=18.65482, train_loss=2.9261045

Batch 246370, train_perplexity=18.65482, train_loss=2.9261045

Batch 246380, train_perplexity=18.65482, train_loss=2.9261045

Batch 246390, train_perplexity=18.65482, train_loss=2.9261045

Batch 246400, train_perplexity=18.654816, train_loss=2.9261043

Batch 246410, train_perplexity=18.65482, train_loss=2.9261045

Batch 246420, train_perplexity=18.654816, train_loss=2.9261043

Batch 246430, train_perplexity=18.65481, train_loss=2.926104

Batch 246440, train_perplexity=18.65482, train_loss=2.9261045

Batch 246450, train_perplexity=18.654816, train_loss=2.9261043

Batch 246460, train_perplexity=18.65482, train_loss=2.9261045

Batch 246470, train_perplexity=18.65482, train_loss=2.9261045

Batch 246480, train_perplexity=18.65481, train_loss=2.926104

Batch 246490, train_perplexity=18.654816, train_loss=2.9261043

Batch 246500, train_perplexity=18.65481, train_loss=2.926104

Batch 246510, train_perplexity=18.65482, train_loss=2.9261045

Batch 246520, train_perplexity=18.654816, train_loss=2.9261043

Batch 246530, train_perplexity=18.65481, train_loss=2.926104

Batch 246540, train_perplexity=18.65481, train_loss=2.926104

Batch 246550, train_perplexity=18.65481, train_loss=2.926104

Batch 246560, train_perplexity=18.654816, train_loss=2.9261043

Batch 246570, train_perplexity=18.65481, train_loss=2.926104

Batch 246580, train_perplexity=18.654816, train_loss=2.9261043

Batch 246590, train_perplexity=18.654816, train_loss=2.9261043

Batch 246600, train_perplexity=18.65481, train_loss=2.926104

Batch 246610, train_perplexity=18.65481, train_loss=2.926104

Batch 246620, train_perplexity=18.65481, train_loss=2.926104

Batch 246630, train_perplexity=18.65481, train_loss=2.926104

Batch 246640, train_perplexity=18.65481, train_loss=2.926104

Batch 246650, train_perplexity=18.65481, train_loss=2.926104

Batch 246660, train_perplexity=18.65481, train_loss=2.926104

Batch 246670, train_perplexity=18.65481, train_loss=2.926104

Batch 246680, train_perplexity=18.65481, train_loss=2.926104

Batch 246690, train_perplexity=18.654802, train_loss=2.9261036

Batch 246700, train_perplexity=18.65481, train_loss=2.926104

Batch 246710, train_perplexity=18.65481, train_loss=2.926104

Batch 246720, train_perplexity=18.65481, train_loss=2.926104

Batch 246730, train_perplexity=18.65481, train_loss=2.926104

Batch 246740, train_perplexity=18.654806, train_loss=2.9261038

Batch 246750, train_perplexity=18.65481, train_loss=2.926104

Batch 246760, train_perplexity=18.65481, train_loss=2.926104

Batch 246770, train_perplexity=18.65481, train_loss=2.926104

Batch 246780, train_perplexity=18.65481, train_loss=2.926104

Batch 246790, train_perplexity=18.654806, train_loss=2.9261038

Batch 246800, train_perplexity=18.654806, train_loss=2.9261038

Batch 246810, train_perplexity=18.654806, train_loss=2.9261038

Batch 246820, train_perplexity=18.65481, train_loss=2.926104

Batch 246830, train_perplexity=18.654806, train_loss=2.9261038

Batch 246840, train_perplexity=18.654806, train_loss=2.9261038

Batch 246850, train_perplexity=18.654806, train_loss=2.9261038

Batch 246860, train_perplexity=18.654802, train_loss=2.9261036

Batch 246870, train_perplexity=18.654806, train_loss=2.9261038

Batch 246880, train_perplexity=18.654806, train_loss=2.9261038

Batch 246890, train_perplexity=18.654802, train_loss=2.9261036

Batch 246900, train_perplexity=18.654802, train_loss=2.9261036

Batch 246910, train_perplexity=18.654802, train_loss=2.9261036

Batch 246920, train_perplexity=18.654802, train_loss=2.9261036

Batch 246930, train_perplexity=18.654797, train_loss=2.9261034

Batch 246940, train_perplexity=18.654802, train_loss=2.9261036

Batch 246950, train_perplexity=18.654802, train_loss=2.9261036

Batch 246960, train_perplexity=18.654802, train_loss=2.9261036
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 246970, train_perplexity=18.654802, train_loss=2.9261036

Batch 246980, train_perplexity=18.654802, train_loss=2.9261036

Batch 246990, train_perplexity=18.654802, train_loss=2.9261036

Batch 247000, train_perplexity=18.654802, train_loss=2.9261036

Batch 247010, train_perplexity=18.654797, train_loss=2.9261034

Batch 247020, train_perplexity=18.654802, train_loss=2.9261036

Batch 247030, train_perplexity=18.654802, train_loss=2.9261036

Batch 247040, train_perplexity=18.654802, train_loss=2.9261036

Batch 247050, train_perplexity=18.654797, train_loss=2.9261034

Batch 247060, train_perplexity=18.654797, train_loss=2.9261034

Batch 247070, train_perplexity=18.654802, train_loss=2.9261036

Batch 247080, train_perplexity=18.654793, train_loss=2.926103

Batch 247090, train_perplexity=18.654802, train_loss=2.9261036

Batch 247100, train_perplexity=18.654802, train_loss=2.9261036

Batch 247110, train_perplexity=18.654802, train_loss=2.9261036

Batch 247120, train_perplexity=18.654793, train_loss=2.926103

Batch 247130, train_perplexity=18.654797, train_loss=2.9261034

Batch 247140, train_perplexity=18.654802, train_loss=2.9261036

Batch 247150, train_perplexity=18.654797, train_loss=2.9261034

Batch 247160, train_perplexity=18.654797, train_loss=2.9261034

Batch 247170, train_perplexity=18.654797, train_loss=2.9261034

Batch 247180, train_perplexity=18.654793, train_loss=2.926103

Batch 247190, train_perplexity=18.654793, train_loss=2.926103

Batch 247200, train_perplexity=18.654797, train_loss=2.9261034

Batch 247210, train_perplexity=18.654793, train_loss=2.926103

Batch 247220, train_perplexity=18.654797, train_loss=2.9261034

Batch 247230, train_perplexity=18.654793, train_loss=2.926103

Batch 247240, train_perplexity=18.654793, train_loss=2.926103

Batch 247250, train_perplexity=18.654797, train_loss=2.9261034

Batch 247260, train_perplexity=18.654797, train_loss=2.9261034

Batch 247270, train_perplexity=18.654797, train_loss=2.9261034

Batch 247280, train_perplexity=18.654789, train_loss=2.9261029

Batch 247290, train_perplexity=18.654793, train_loss=2.926103

Batch 247300, train_perplexity=18.654789, train_loss=2.9261029

Batch 247310, train_perplexity=18.654793, train_loss=2.926103

Batch 247320, train_perplexity=18.654797, train_loss=2.9261034

Batch 247330, train_perplexity=18.654793, train_loss=2.926103

Batch 247340, train_perplexity=18.654789, train_loss=2.9261029

Batch 247350, train_perplexity=18.654789, train_loss=2.9261029

Batch 247360, train_perplexity=18.654793, train_loss=2.926103

Batch 247370, train_perplexity=18.654793, train_loss=2.926103

Batch 247380, train_perplexity=18.654793, train_loss=2.926103

Batch 247390, train_perplexity=18.654789, train_loss=2.9261029

Batch 247400, train_perplexity=18.654793, train_loss=2.926103

Batch 247410, train_perplexity=18.654793, train_loss=2.926103

Batch 247420, train_perplexity=18.654793, train_loss=2.926103

Batch 247430, train_perplexity=18.654789, train_loss=2.9261029

Batch 247440, train_perplexity=18.654783, train_loss=2.9261026

Batch 247450, train_perplexity=18.654783, train_loss=2.9261026

Batch 247460, train_perplexity=18.654793, train_loss=2.926103

Batch 247470, train_perplexity=18.654793, train_loss=2.926103

Batch 247480, train_perplexity=18.654783, train_loss=2.9261026

Batch 247490, train_perplexity=18.654783, train_loss=2.9261026

Batch 247500, train_perplexity=18.654789, train_loss=2.9261029

Batch 247510, train_perplexity=18.654783, train_loss=2.9261026

Batch 247520, train_perplexity=18.654783, train_loss=2.9261026

Batch 247530, train_perplexity=18.654783, train_loss=2.9261026

Batch 247540, train_perplexity=18.654789, train_loss=2.9261029

Batch 247550, train_perplexity=18.654783, train_loss=2.9261026

Batch 247560, train_perplexity=18.654783, train_loss=2.9261026

Batch 247570, train_perplexity=18.654789, train_loss=2.9261029

Batch 247580, train_perplexity=18.654783, train_loss=2.9261026

Batch 247590, train_perplexity=18.654783, train_loss=2.9261026

Batch 247600, train_perplexity=18.654789, train_loss=2.9261029

Batch 247610, train_perplexity=18.654783, train_loss=2.9261026

Batch 247620, train_perplexity=18.654783, train_loss=2.9261026

Batch 247630, train_perplexity=18.654776, train_loss=2.9261022

Batch 247640, train_perplexity=18.654783, train_loss=2.9261026

Batch 247650, train_perplexity=18.654783, train_loss=2.9261026

Batch 247660, train_perplexity=18.654783, train_loss=2.9261026

Batch 247670, train_perplexity=18.654783, train_loss=2.9261026

Batch 247680, train_perplexity=18.654783, train_loss=2.9261026

Batch 247690, train_perplexity=18.65478, train_loss=2.9261024

Batch 247700, train_perplexity=18.65478, train_loss=2.9261024

Batch 247710, train_perplexity=18.654783, train_loss=2.9261026

Batch 247720, train_perplexity=18.654783, train_loss=2.9261026

Batch 247730, train_perplexity=18.65478, train_loss=2.9261024

Batch 247740, train_perplexity=18.65478, train_loss=2.9261024

Batch 247750, train_perplexity=18.65478, train_loss=2.9261024

Batch 247760, train_perplexity=18.65478, train_loss=2.9261024

Batch 247770, train_perplexity=18.654783, train_loss=2.9261026

Batch 247780, train_perplexity=18.65478, train_loss=2.9261024

Batch 247790, train_perplexity=18.654783, train_loss=2.9261026

Batch 247800, train_perplexity=18.654776, train_loss=2.9261022

Batch 247810, train_perplexity=18.65478, train_loss=2.9261024

Batch 247820, train_perplexity=18.654776, train_loss=2.9261022

Batch 247830, train_perplexity=18.654776, train_loss=2.9261022

Batch 247840, train_perplexity=18.65478, train_loss=2.9261024

Batch 247850, train_perplexity=18.65478, train_loss=2.9261024

Batch 247860, train_perplexity=18.654776, train_loss=2.9261022

Batch 247870, train_perplexity=18.654776, train_loss=2.9261022

Batch 247880, train_perplexity=18.654776, train_loss=2.9261022

Batch 247890, train_perplexity=18.654776, train_loss=2.9261022

Batch 247900, train_perplexity=18.654776, train_loss=2.9261022

Batch 247910, train_perplexity=18.654776, train_loss=2.9261022

Batch 247920, train_perplexity=18.654776, train_loss=2.9261022

Batch 247930, train_perplexity=18.654776, train_loss=2.9261022

Batch 247940, train_perplexity=18.654776, train_loss=2.9261022

Batch 247950, train_perplexity=18.654776, train_loss=2.9261022

Batch 247960, train_perplexity=18.654766, train_loss=2.9261017

Batch 247970, train_perplexity=18.654776, train_loss=2.9261022

Batch 247980, train_perplexity=18.654776, train_loss=2.9261022

Batch 247990, train_perplexity=18.654776, train_loss=2.9261022

Batch 248000, train_perplexity=18.654776, train_loss=2.9261022

Batch 248010, train_perplexity=18.654776, train_loss=2.9261022

Batch 248020, train_perplexity=18.65477, train_loss=2.926102

Batch 248030, train_perplexity=18.654776, train_loss=2.9261022

Batch 248040, train_perplexity=18.654766, train_loss=2.9261017

Batch 248050, train_perplexity=18.654766, train_loss=2.9261017

Batch 248060, train_perplexity=18.654766, train_loss=2.9261017

Batch 248070, train_perplexity=18.654776, train_loss=2.9261022

Batch 248080, train_perplexity=18.654766, train_loss=2.9261017

Batch 248090, train_perplexity=18.654776, train_loss=2.9261022

Batch 248100, train_perplexity=18.654766, train_loss=2.9261017

Batch 248110, train_perplexity=18.654766, train_loss=2.9261017

Batch 248120, train_perplexity=18.65477, train_loss=2.926102

Batch 248130, train_perplexity=18.65477, train_loss=2.926102

Batch 248140, train_perplexity=18.654766, train_loss=2.9261017

Batch 248150, train_perplexity=18.654766, train_loss=2.9261017

Batch 248160, train_perplexity=18.654766, train_loss=2.9261017

Batch 248170, train_perplexity=18.654766, train_loss=2.9261017

Batch 248180, train_perplexity=18.65477, train_loss=2.926102

Batch 248190, train_perplexity=18.654766, train_loss=2.9261017

Batch 248200, train_perplexity=18.654766, train_loss=2.9261017

Batch 248210, train_perplexity=18.654762, train_loss=2.9261014

Batch 248220, train_perplexity=18.654766, train_loss=2.9261017

Batch 248230, train_perplexity=18.654766, train_loss=2.9261017

Batch 248240, train_perplexity=18.654766, train_loss=2.9261017

Batch 248250, train_perplexity=18.654766, train_loss=2.9261017
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 248260, train_perplexity=18.654766, train_loss=2.9261017

Batch 248270, train_perplexity=18.654766, train_loss=2.9261017

Batch 248280, train_perplexity=18.654766, train_loss=2.9261017

Batch 248290, train_perplexity=18.654766, train_loss=2.9261017

Batch 248300, train_perplexity=18.654766, train_loss=2.9261017

Batch 248310, train_perplexity=18.654766, train_loss=2.9261017

Batch 248320, train_perplexity=18.654766, train_loss=2.9261017

Batch 248330, train_perplexity=18.654762, train_loss=2.9261014

Batch 248340, train_perplexity=18.654758, train_loss=2.9261012

Batch 248350, train_perplexity=18.654766, train_loss=2.9261017

Batch 248360, train_perplexity=18.654762, train_loss=2.9261014

Batch 248370, train_perplexity=18.654758, train_loss=2.9261012

Batch 248380, train_perplexity=18.654758, train_loss=2.9261012

Batch 248390, train_perplexity=18.654758, train_loss=2.9261012

Batch 248400, train_perplexity=18.654762, train_loss=2.9261014

Batch 248410, train_perplexity=18.654766, train_loss=2.9261017

Batch 248420, train_perplexity=18.654762, train_loss=2.9261014

Batch 248430, train_perplexity=18.654762, train_loss=2.9261014

Batch 248440, train_perplexity=18.654758, train_loss=2.9261012

Batch 248450, train_perplexity=18.654762, train_loss=2.9261014

Batch 248460, train_perplexity=18.654758, train_loss=2.9261012

Batch 248470, train_perplexity=18.654758, train_loss=2.9261012

Batch 248480, train_perplexity=18.654758, train_loss=2.9261012

Batch 248490, train_perplexity=18.654758, train_loss=2.9261012

Batch 248500, train_perplexity=18.654758, train_loss=2.9261012

Batch 248510, train_perplexity=18.654758, train_loss=2.9261012

Batch 248520, train_perplexity=18.654758, train_loss=2.9261012

Batch 248530, train_perplexity=18.654758, train_loss=2.9261012

Batch 248540, train_perplexity=18.654753, train_loss=2.926101

Batch 248550, train_perplexity=18.654758, train_loss=2.9261012

Batch 248560, train_perplexity=18.654758, train_loss=2.9261012

Batch 248570, train_perplexity=18.654758, train_loss=2.9261012

Batch 248580, train_perplexity=18.654758, train_loss=2.9261012

Batch 248590, train_perplexity=18.654758, train_loss=2.9261012

Batch 248600, train_perplexity=18.654758, train_loss=2.9261012

Batch 248610, train_perplexity=18.654758, train_loss=2.9261012

Batch 248620, train_perplexity=18.654749, train_loss=2.9261007

Batch 248630, train_perplexity=18.654758, train_loss=2.9261012

Batch 248640, train_perplexity=18.654753, train_loss=2.926101

Batch 248650, train_perplexity=18.654758, train_loss=2.9261012

Batch 248660, train_perplexity=18.654749, train_loss=2.9261007

Batch 248670, train_perplexity=18.654758, train_loss=2.9261012

Batch 248680, train_perplexity=18.654753, train_loss=2.926101

Batch 248690, train_perplexity=18.654749, train_loss=2.9261007

Batch 248700, train_perplexity=18.654753, train_loss=2.926101

Batch 248710, train_perplexity=18.654749, train_loss=2.9261007

Batch 248720, train_perplexity=18.654749, train_loss=2.9261007

Batch 248730, train_perplexity=18.654749, train_loss=2.9261007

Batch 248740, train_perplexity=18.654749, train_loss=2.9261007

Batch 248750, train_perplexity=18.654749, train_loss=2.9261007

Batch 248760, train_perplexity=18.654749, train_loss=2.9261007

Batch 248770, train_perplexity=18.654749, train_loss=2.9261007

Batch 248780, train_perplexity=18.654753, train_loss=2.926101

Batch 248790, train_perplexity=18.654749, train_loss=2.9261007

Batch 248800, train_perplexity=18.654745, train_loss=2.9261005

Batch 248810, train_perplexity=18.654753, train_loss=2.926101

Batch 248820, train_perplexity=18.654749, train_loss=2.9261007

Batch 248830, train_perplexity=18.654758, train_loss=2.9261012

Batch 248840, train_perplexity=18.654753, train_loss=2.926101

Batch 248850, train_perplexity=18.654749, train_loss=2.9261007

Batch 248860, train_perplexity=18.654753, train_loss=2.926101

Batch 248870, train_perplexity=18.654753, train_loss=2.926101

Batch 248880, train_perplexity=18.654749, train_loss=2.9261007

Batch 248890, train_perplexity=18.654749, train_loss=2.9261007

Batch 248900, train_perplexity=18.654749, train_loss=2.9261007

Batch 248910, train_perplexity=18.654749, train_loss=2.9261007

Batch 248920, train_perplexity=18.654745, train_loss=2.9261005

Batch 248930, train_perplexity=18.65474, train_loss=2.9261003

Batch 248940, train_perplexity=18.654749, train_loss=2.9261007

Batch 248950, train_perplexity=18.654749, train_loss=2.9261007

Batch 248960, train_perplexity=18.65474, train_loss=2.9261003

Batch 248970, train_perplexity=18.65474, train_loss=2.9261003

Batch 248980, train_perplexity=18.65474, train_loss=2.9261003

Batch 248990, train_perplexity=18.654745, train_loss=2.9261005

Batch 249000, train_perplexity=18.654749, train_loss=2.9261007

Batch 249010, train_perplexity=18.654745, train_loss=2.9261005

Batch 249020, train_perplexity=18.654749, train_loss=2.9261007

Batch 249030, train_perplexity=18.65474, train_loss=2.9261003

Batch 249040, train_perplexity=18.654745, train_loss=2.9261005

Batch 249050, train_perplexity=18.65474, train_loss=2.9261003

Batch 249060, train_perplexity=18.65474, train_loss=2.9261003

Batch 249070, train_perplexity=18.65474, train_loss=2.9261003

Batch 249080, train_perplexity=18.654745, train_loss=2.9261005

Batch 249090, train_perplexity=18.654745, train_loss=2.9261005

Batch 249100, train_perplexity=18.65474, train_loss=2.9261003

Batch 249110, train_perplexity=18.65474, train_loss=2.9261003

Batch 249120, train_perplexity=18.65474, train_loss=2.9261003

Batch 249130, train_perplexity=18.65474, train_loss=2.9261003

Batch 249140, train_perplexity=18.65474, train_loss=2.9261003

Batch 249150, train_perplexity=18.65474, train_loss=2.9261003

Batch 249160, train_perplexity=18.65474, train_loss=2.9261003

Batch 249170, train_perplexity=18.65474, train_loss=2.9261003

Batch 249180, train_perplexity=18.65474, train_loss=2.9261003

Batch 249190, train_perplexity=18.65474, train_loss=2.9261003

Batch 249200, train_perplexity=18.65474, train_loss=2.9261003

Batch 249210, train_perplexity=18.65474, train_loss=2.9261003

Batch 249220, train_perplexity=18.65474, train_loss=2.9261003

Batch 249230, train_perplexity=18.65474, train_loss=2.9261003

Batch 249240, train_perplexity=18.65474, train_loss=2.9261003

Batch 249250, train_perplexity=18.65474, train_loss=2.9261003

Batch 249260, train_perplexity=18.654736, train_loss=2.9261

Batch 249270, train_perplexity=18.65474, train_loss=2.9261003

Batch 249280, train_perplexity=18.654732, train_loss=2.9260998

Batch 249290, train_perplexity=18.654732, train_loss=2.9260998

Batch 249300, train_perplexity=18.65474, train_loss=2.9261003

Batch 249310, train_perplexity=18.65474, train_loss=2.9261003

Batch 249320, train_perplexity=18.654736, train_loss=2.9261

Batch 249330, train_perplexity=18.654732, train_loss=2.9260998

Batch 249340, train_perplexity=18.654736, train_loss=2.9261

Batch 249350, train_perplexity=18.654736, train_loss=2.9261

Batch 249360, train_perplexity=18.654732, train_loss=2.9260998

Batch 249370, train_perplexity=18.654736, train_loss=2.9261

Batch 249380, train_perplexity=18.654732, train_loss=2.9260998

Batch 249390, train_perplexity=18.654736, train_loss=2.9261

Batch 249400, train_perplexity=18.654732, train_loss=2.9260998

Batch 249410, train_perplexity=18.654736, train_loss=2.9261

Batch 249420, train_perplexity=18.654736, train_loss=2.9261

Batch 249430, train_perplexity=18.654736, train_loss=2.9261

Batch 249440, train_perplexity=18.654732, train_loss=2.9260998

Batch 249450, train_perplexity=18.654736, train_loss=2.9261

Batch 249460, train_perplexity=18.654732, train_loss=2.9260998

Batch 249470, train_perplexity=18.654732, train_loss=2.9260998

Batch 249480, train_perplexity=18.654732, train_loss=2.9260998

Batch 249490, train_perplexity=18.654732, train_loss=2.9260998

Batch 249500, train_perplexity=18.654726, train_loss=2.9260995

Batch 249510, train_perplexity=18.654732, train_loss=2.9260998

Batch 249520, train_perplexity=18.654732, train_loss=2.9260998

Batch 249530, train_perplexity=18.654732, train_loss=2.9260998

Batch 249540, train_perplexity=18.654732, train_loss=2.9260998

Batch 249550, train_perplexity=18.654732, train_loss=2.9260998
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 249560, train_perplexity=18.654732, train_loss=2.9260998

Batch 249570, train_perplexity=18.654732, train_loss=2.9260998

Batch 249580, train_perplexity=18.654722, train_loss=2.9260993

Batch 249590, train_perplexity=18.654732, train_loss=2.9260998

Batch 249600, train_perplexity=18.654726, train_loss=2.9260995

Batch 249610, train_perplexity=18.654722, train_loss=2.9260993

Batch 249620, train_perplexity=18.654726, train_loss=2.9260995

Batch 249630, train_perplexity=18.654726, train_loss=2.9260995

Batch 249640, train_perplexity=18.654722, train_loss=2.9260993

Batch 249650, train_perplexity=18.654722, train_loss=2.9260993

Batch 249660, train_perplexity=18.654722, train_loss=2.9260993

Batch 249670, train_perplexity=18.654722, train_loss=2.9260993

Batch 249680, train_perplexity=18.654726, train_loss=2.9260995

Batch 249690, train_perplexity=18.654722, train_loss=2.9260993

Batch 249700, train_perplexity=18.654726, train_loss=2.9260995

Batch 249710, train_perplexity=18.654722, train_loss=2.9260993

Batch 249720, train_perplexity=18.654722, train_loss=2.9260993

Batch 249730, train_perplexity=18.654726, train_loss=2.9260995

Batch 249740, train_perplexity=18.654726, train_loss=2.9260995

Batch 249750, train_perplexity=18.654726, train_loss=2.9260995

Batch 249760, train_perplexity=18.654726, train_loss=2.9260995

Batch 249770, train_perplexity=18.654726, train_loss=2.9260995

Batch 249780, train_perplexity=18.654722, train_loss=2.9260993

Batch 249790, train_perplexity=18.654726, train_loss=2.9260995

Batch 249800, train_perplexity=18.654722, train_loss=2.9260993

Batch 249810, train_perplexity=18.654722, train_loss=2.9260993

Batch 249820, train_perplexity=18.654722, train_loss=2.9260993

Batch 249830, train_perplexity=18.654713, train_loss=2.9260988

Batch 249840, train_perplexity=18.654722, train_loss=2.9260993

Batch 249850, train_perplexity=18.654722, train_loss=2.9260993

Batch 249860, train_perplexity=18.654718, train_loss=2.926099

Batch 249870, train_perplexity=18.654722, train_loss=2.9260993

Batch 249880, train_perplexity=18.654722, train_loss=2.9260993

Batch 249890, train_perplexity=18.654718, train_loss=2.926099

Batch 249900, train_perplexity=18.654722, train_loss=2.9260993

Batch 249910, train_perplexity=18.654722, train_loss=2.9260993

Batch 249920, train_perplexity=18.654718, train_loss=2.926099

Batch 249930, train_perplexity=18.654722, train_loss=2.9260993

Batch 249940, train_perplexity=18.654713, train_loss=2.9260988

Batch 249950, train_perplexity=18.654718, train_loss=2.926099

Batch 249960, train_perplexity=18.654718, train_loss=2.926099

Batch 249970, train_perplexity=18.654718, train_loss=2.926099

Batch 249980, train_perplexity=18.654713, train_loss=2.9260988

Batch 249990, train_perplexity=18.654718, train_loss=2.926099

Batch 250000, train_perplexity=18.654713, train_loss=2.9260988

Batch 250010, train_perplexity=18.654722, train_loss=2.9260993

Batch 250020, train_perplexity=18.654713, train_loss=2.9260988

Batch 250030, train_perplexity=18.654713, train_loss=2.9260988

Batch 250040, train_perplexity=18.654718, train_loss=2.926099

Batch 250050, train_perplexity=18.654713, train_loss=2.9260988

Batch 250060, train_perplexity=18.654713, train_loss=2.9260988

Batch 250070, train_perplexity=18.654713, train_loss=2.9260988

Batch 250080, train_perplexity=18.654718, train_loss=2.926099

Batch 250090, train_perplexity=18.654718, train_loss=2.926099

Batch 250100, train_perplexity=18.654713, train_loss=2.9260988

Batch 250110, train_perplexity=18.654713, train_loss=2.9260988

Batch 250120, train_perplexity=18.654709, train_loss=2.9260986

Batch 250130, train_perplexity=18.654713, train_loss=2.9260988

Batch 250140, train_perplexity=18.654709, train_loss=2.9260986

Batch 250150, train_perplexity=18.654713, train_loss=2.9260988

Batch 250160, train_perplexity=18.654713, train_loss=2.9260988

Batch 250170, train_perplexity=18.654705, train_loss=2.9260983

Batch 250180, train_perplexity=18.654713, train_loss=2.9260988

Batch 250190, train_perplexity=18.654713, train_loss=2.9260988

Batch 250200, train_perplexity=18.654713, train_loss=2.9260988

Batch 250210, train_perplexity=18.654709, train_loss=2.9260986

Batch 250220, train_perplexity=18.654705, train_loss=2.9260983

Batch 250230, train_perplexity=18.654705, train_loss=2.9260983

Batch 250240, train_perplexity=18.654705, train_loss=2.9260983

Batch 250250, train_perplexity=18.654713, train_loss=2.9260988

Batch 250260, train_perplexity=18.654709, train_loss=2.9260986

Batch 250270, train_perplexity=18.654709, train_loss=2.9260986

Batch 250280, train_perplexity=18.654705, train_loss=2.9260983

Batch 250290, train_perplexity=18.654705, train_loss=2.9260983

Batch 250300, train_perplexity=18.654713, train_loss=2.9260988

Batch 250310, train_perplexity=18.654705, train_loss=2.9260983

Batch 250320, train_perplexity=18.654705, train_loss=2.9260983

Batch 250330, train_perplexity=18.654705, train_loss=2.9260983

Batch 250340, train_perplexity=18.654709, train_loss=2.9260986

Batch 250350, train_perplexity=18.654705, train_loss=2.9260983

Batch 250360, train_perplexity=18.654705, train_loss=2.9260983

Batch 250370, train_perplexity=18.654705, train_loss=2.9260983

Batch 250380, train_perplexity=18.654705, train_loss=2.9260983

Batch 250390, train_perplexity=18.654705, train_loss=2.9260983

Batch 250400, train_perplexity=18.654705, train_loss=2.9260983

Batch 250410, train_perplexity=18.654705, train_loss=2.9260983

Batch 250420, train_perplexity=18.654705, train_loss=2.9260983

Batch 250430, train_perplexity=18.654705, train_loss=2.9260983

Batch 250440, train_perplexity=18.654705, train_loss=2.9260983

Batch 250450, train_perplexity=18.654696, train_loss=2.9260979

Batch 250460, train_perplexity=18.654705, train_loss=2.9260983

Batch 250470, train_perplexity=18.654705, train_loss=2.9260983

Batch 250480, train_perplexity=18.654705, train_loss=2.9260983

Batch 250490, train_perplexity=18.6547, train_loss=2.926098

Batch 250500, train_perplexity=18.654696, train_loss=2.9260979

Batch 250510, train_perplexity=18.654705, train_loss=2.9260983

Batch 250520, train_perplexity=18.654705, train_loss=2.9260983

Batch 250530, train_perplexity=18.654696, train_loss=2.9260979

Batch 250540, train_perplexity=18.6547, train_loss=2.926098

Batch 250550, train_perplexity=18.654705, train_loss=2.9260983

Batch 250560, train_perplexity=18.654696, train_loss=2.9260979

Batch 250570, train_perplexity=18.654696, train_loss=2.9260979

Batch 250580, train_perplexity=18.654696, train_loss=2.9260979

Batch 250590, train_perplexity=18.6547, train_loss=2.926098

Batch 250600, train_perplexity=18.654696, train_loss=2.9260979

Batch 250610, train_perplexity=18.6547, train_loss=2.926098

Batch 250620, train_perplexity=18.654696, train_loss=2.9260979

Batch 250630, train_perplexity=18.654696, train_loss=2.9260979

Batch 250640, train_perplexity=18.654696, train_loss=2.9260979

Batch 250650, train_perplexity=18.6547, train_loss=2.926098

Batch 250660, train_perplexity=18.6547, train_loss=2.926098

Batch 250670, train_perplexity=18.654696, train_loss=2.9260979

Batch 250680, train_perplexity=18.6547, train_loss=2.926098

Batch 250690, train_perplexity=18.654692, train_loss=2.9260976

Batch 250700, train_perplexity=18.654696, train_loss=2.9260979

Batch 250710, train_perplexity=18.654696, train_loss=2.9260979

Batch 250720, train_perplexity=18.654696, train_loss=2.9260979

Batch 250730, train_perplexity=18.654696, train_loss=2.9260979

Batch 250740, train_perplexity=18.654692, train_loss=2.9260976

Batch 250750, train_perplexity=18.654696, train_loss=2.9260979

Batch 250760, train_perplexity=18.654696, train_loss=2.9260979

Batch 250770, train_perplexity=18.654692, train_loss=2.9260976

Batch 250780, train_perplexity=18.654686, train_loss=2.9260974

Batch 250790, train_perplexity=18.654686, train_loss=2.9260974

Batch 250800, train_perplexity=18.654696, train_loss=2.9260979

Batch 250810, train_perplexity=18.654696, train_loss=2.9260979

Batch 250820, train_perplexity=18.654692, train_loss=2.9260976

Batch 250830, train_perplexity=18.654692, train_loss=2.9260976

Batch 250840, train_perplexity=18.654696, train_loss=2.9260979
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 250850, train_perplexity=18.654696, train_loss=2.9260979

Batch 250860, train_perplexity=18.654686, train_loss=2.9260974

Batch 250870, train_perplexity=18.654696, train_loss=2.9260979

Batch 250880, train_perplexity=18.654696, train_loss=2.9260979

Batch 250890, train_perplexity=18.654686, train_loss=2.9260974

Batch 250900, train_perplexity=18.654686, train_loss=2.9260974

Batch 250910, train_perplexity=18.654692, train_loss=2.9260976

Batch 250920, train_perplexity=18.654686, train_loss=2.9260974

Batch 250930, train_perplexity=18.654686, train_loss=2.9260974

Batch 250940, train_perplexity=18.654692, train_loss=2.9260976

Batch 250950, train_perplexity=18.654692, train_loss=2.9260976

Batch 250960, train_perplexity=18.654696, train_loss=2.9260979

Batch 250970, train_perplexity=18.654692, train_loss=2.9260976

Batch 250980, train_perplexity=18.654686, train_loss=2.9260974

Batch 250990, train_perplexity=18.654696, train_loss=2.9260979

Batch 251000, train_perplexity=18.654692, train_loss=2.9260976

Batch 251010, train_perplexity=18.654686, train_loss=2.9260974

Batch 251020, train_perplexity=18.654692, train_loss=2.9260976

Batch 251030, train_perplexity=18.654686, train_loss=2.9260974

Batch 251040, train_perplexity=18.654686, train_loss=2.9260974

Batch 251050, train_perplexity=18.654678, train_loss=2.926097

Batch 251060, train_perplexity=18.654686, train_loss=2.9260974

Batch 251070, train_perplexity=18.654686, train_loss=2.9260974

Batch 251080, train_perplexity=18.654678, train_loss=2.926097

Batch 251090, train_perplexity=18.654692, train_loss=2.9260976

Batch 251100, train_perplexity=18.654686, train_loss=2.9260974

Batch 251110, train_perplexity=18.654686, train_loss=2.9260974

Batch 251120, train_perplexity=18.654686, train_loss=2.9260974

Batch 251130, train_perplexity=18.654682, train_loss=2.9260972

Batch 251140, train_perplexity=18.654686, train_loss=2.9260974

Batch 251150, train_perplexity=18.654686, train_loss=2.9260974

Batch 251160, train_perplexity=18.654686, train_loss=2.9260974

Batch 251170, train_perplexity=18.654686, train_loss=2.9260974

Batch 251180, train_perplexity=18.654682, train_loss=2.9260972

Batch 251190, train_perplexity=18.654686, train_loss=2.9260974

Batch 251200, train_perplexity=18.654682, train_loss=2.9260972

Batch 251210, train_perplexity=18.654682, train_loss=2.9260972

Batch 251220, train_perplexity=18.654678, train_loss=2.926097

Batch 251230, train_perplexity=18.654682, train_loss=2.9260972

Batch 251240, train_perplexity=18.654686, train_loss=2.9260974

Batch 251250, train_perplexity=18.654682, train_loss=2.9260972

Batch 251260, train_perplexity=18.654678, train_loss=2.926097

Batch 251270, train_perplexity=18.654682, train_loss=2.9260972

Batch 251280, train_perplexity=18.654678, train_loss=2.926097

Batch 251290, train_perplexity=18.654678, train_loss=2.926097

Batch 251300, train_perplexity=18.654678, train_loss=2.926097

Batch 251310, train_perplexity=18.654673, train_loss=2.9260967

Batch 251320, train_perplexity=18.654678, train_loss=2.926097

Batch 251330, train_perplexity=18.654678, train_loss=2.926097

Batch 251340, train_perplexity=18.654678, train_loss=2.926097

Batch 251350, train_perplexity=18.654678, train_loss=2.926097

Batch 251360, train_perplexity=18.654678, train_loss=2.926097

Batch 251370, train_perplexity=18.654678, train_loss=2.926097

Batch 251380, train_perplexity=18.654678, train_loss=2.926097

Batch 251390, train_perplexity=18.654682, train_loss=2.9260972

Batch 251400, train_perplexity=18.654669, train_loss=2.9260964

Batch 251410, train_perplexity=18.654669, train_loss=2.9260964

Batch 251420, train_perplexity=18.654673, train_loss=2.9260967

Batch 251430, train_perplexity=18.654678, train_loss=2.926097

Batch 251440, train_perplexity=18.654678, train_loss=2.926097

Batch 251450, train_perplexity=18.654673, train_loss=2.9260967

Batch 251460, train_perplexity=18.654678, train_loss=2.926097

Batch 251470, train_perplexity=18.654669, train_loss=2.9260964

Batch 251480, train_perplexity=18.654678, train_loss=2.926097

Batch 251490, train_perplexity=18.654673, train_loss=2.9260967

Batch 251500, train_perplexity=18.654678, train_loss=2.926097

Batch 251510, train_perplexity=18.654669, train_loss=2.9260964

Batch 251520, train_perplexity=18.654673, train_loss=2.9260967

Batch 251530, train_perplexity=18.654669, train_loss=2.9260964

Batch 251540, train_perplexity=18.654669, train_loss=2.9260964

Batch 251550, train_perplexity=18.654669, train_loss=2.9260964

Batch 251560, train_perplexity=18.654669, train_loss=2.9260964

Batch 251570, train_perplexity=18.654669, train_loss=2.9260964

Batch 251580, train_perplexity=18.654669, train_loss=2.9260964

Batch 251590, train_perplexity=18.654673, train_loss=2.9260967

Batch 251600, train_perplexity=18.654673, train_loss=2.9260967

Batch 251610, train_perplexity=18.654678, train_loss=2.926097

Batch 251620, train_perplexity=18.654669, train_loss=2.9260964

Batch 251630, train_perplexity=18.654669, train_loss=2.9260964

Batch 251640, train_perplexity=18.654669, train_loss=2.9260964

Batch 251650, train_perplexity=18.654669, train_loss=2.9260964

Batch 251660, train_perplexity=18.654669, train_loss=2.9260964

Batch 251670, train_perplexity=18.654669, train_loss=2.9260964

Batch 251680, train_perplexity=18.654669, train_loss=2.9260964

Batch 251690, train_perplexity=18.654669, train_loss=2.9260964

Batch 251700, train_perplexity=18.654665, train_loss=2.9260962

Batch 251710, train_perplexity=18.65466, train_loss=2.926096

Batch 251720, train_perplexity=18.654669, train_loss=2.9260964

Batch 251730, train_perplexity=18.654665, train_loss=2.9260962

Batch 251740, train_perplexity=18.654669, train_loss=2.9260964

Batch 251750, train_perplexity=18.654665, train_loss=2.9260962

Batch 251760, train_perplexity=18.654665, train_loss=2.9260962

Batch 251770, train_perplexity=18.654669, train_loss=2.9260964

Batch 251780, train_perplexity=18.654665, train_loss=2.9260962

Batch 251790, train_perplexity=18.654665, train_loss=2.9260962

Batch 251800, train_perplexity=18.654665, train_loss=2.9260962

Batch 251810, train_perplexity=18.65466, train_loss=2.926096

Batch 251820, train_perplexity=18.654665, train_loss=2.9260962

Batch 251830, train_perplexity=18.654669, train_loss=2.9260964

Batch 251840, train_perplexity=18.654669, train_loss=2.9260964

Batch 251850, train_perplexity=18.65466, train_loss=2.926096

Batch 251860, train_perplexity=18.654665, train_loss=2.9260962

Batch 251870, train_perplexity=18.654669, train_loss=2.9260964

Batch 251880, train_perplexity=18.65466, train_loss=2.926096

Batch 251890, train_perplexity=18.65466, train_loss=2.926096

Batch 251900, train_perplexity=18.654665, train_loss=2.9260962

Batch 251910, train_perplexity=18.65466, train_loss=2.926096

Batch 251920, train_perplexity=18.654655, train_loss=2.9260957

Batch 251930, train_perplexity=18.65466, train_loss=2.926096

Batch 251940, train_perplexity=18.65466, train_loss=2.926096

Batch 251950, train_perplexity=18.65466, train_loss=2.926096

Batch 251960, train_perplexity=18.65466, train_loss=2.926096

Batch 251970, train_perplexity=18.65466, train_loss=2.926096

Batch 251980, train_perplexity=18.65466, train_loss=2.926096

Batch 251990, train_perplexity=18.654669, train_loss=2.9260964

Batch 252000, train_perplexity=18.654665, train_loss=2.9260962

Batch 252010, train_perplexity=18.65466, train_loss=2.926096

Batch 252020, train_perplexity=18.654655, train_loss=2.9260957

Batch 252030, train_perplexity=18.654655, train_loss=2.9260957

Batch 252040, train_perplexity=18.65466, train_loss=2.926096

Batch 252050, train_perplexity=18.65466, train_loss=2.926096

Batch 252060, train_perplexity=18.654655, train_loss=2.9260957

Batch 252070, train_perplexity=18.654652, train_loss=2.9260955

Batch 252080, train_perplexity=18.654652, train_loss=2.9260955

Batch 252090, train_perplexity=18.65466, train_loss=2.926096

Batch 252100, train_perplexity=18.65466, train_loss=2.926096

Batch 252110, train_perplexity=18.65466, train_loss=2.926096

Batch 252120, train_perplexity=18.654655, train_loss=2.9260957

Batch 252130, train_perplexity=18.654655, train_loss=2.9260957
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 252140, train_perplexity=18.654652, train_loss=2.9260955

Batch 252150, train_perplexity=18.654652, train_loss=2.9260955

Batch 252160, train_perplexity=18.654652, train_loss=2.9260955

Batch 252170, train_perplexity=18.654652, train_loss=2.9260955

Batch 252180, train_perplexity=18.654652, train_loss=2.9260955

Batch 252190, train_perplexity=18.654652, train_loss=2.9260955

Batch 252200, train_perplexity=18.654652, train_loss=2.9260955

Batch 252210, train_perplexity=18.654652, train_loss=2.9260955

Batch 252220, train_perplexity=18.654655, train_loss=2.9260957

Batch 252230, train_perplexity=18.654652, train_loss=2.9260955

Batch 252240, train_perplexity=18.654652, train_loss=2.9260955

Batch 252250, train_perplexity=18.654652, train_loss=2.9260955

Batch 252260, train_perplexity=18.654652, train_loss=2.9260955

Batch 252270, train_perplexity=18.654652, train_loss=2.9260955

Batch 252280, train_perplexity=18.654655, train_loss=2.9260957

Batch 252290, train_perplexity=18.654655, train_loss=2.9260957

Batch 252300, train_perplexity=18.654652, train_loss=2.9260955

Batch 252310, train_perplexity=18.654652, train_loss=2.9260955

Batch 252320, train_perplexity=18.654652, train_loss=2.9260955

Batch 252330, train_perplexity=18.654652, train_loss=2.9260955

Batch 252340, train_perplexity=18.654652, train_loss=2.9260955

Batch 252350, train_perplexity=18.654642, train_loss=2.926095

Batch 252360, train_perplexity=18.654642, train_loss=2.926095

Batch 252370, train_perplexity=18.654646, train_loss=2.9260952

Batch 252380, train_perplexity=18.654652, train_loss=2.9260955

Batch 252390, train_perplexity=18.654652, train_loss=2.9260955

Batch 252400, train_perplexity=18.654642, train_loss=2.926095

Batch 252410, train_perplexity=18.654646, train_loss=2.9260952

Batch 252420, train_perplexity=18.654646, train_loss=2.9260952

Batch 252430, train_perplexity=18.654646, train_loss=2.9260952

Batch 252440, train_perplexity=18.654646, train_loss=2.9260952

Batch 252450, train_perplexity=18.654642, train_loss=2.926095

Batch 252460, train_perplexity=18.654642, train_loss=2.926095

Batch 252470, train_perplexity=18.654642, train_loss=2.926095

Batch 252480, train_perplexity=18.654646, train_loss=2.9260952

Batch 252490, train_perplexity=18.654642, train_loss=2.926095

Batch 252500, train_perplexity=18.654642, train_loss=2.926095

Batch 252510, train_perplexity=18.654642, train_loss=2.926095

Batch 252520, train_perplexity=18.654642, train_loss=2.926095

Batch 252530, train_perplexity=18.654642, train_loss=2.926095

Batch 252540, train_perplexity=18.654646, train_loss=2.9260952

Batch 252550, train_perplexity=18.654642, train_loss=2.926095

Batch 252560, train_perplexity=18.654642, train_loss=2.926095

Batch 252570, train_perplexity=18.654646, train_loss=2.9260952

Batch 252580, train_perplexity=18.654638, train_loss=2.9260948

Batch 252590, train_perplexity=18.654642, train_loss=2.926095

Batch 252600, train_perplexity=18.654642, train_loss=2.926095

Batch 252610, train_perplexity=18.654642, train_loss=2.926095

Batch 252620, train_perplexity=18.654642, train_loss=2.926095

Batch 252630, train_perplexity=18.654638, train_loss=2.9260948

Batch 252640, train_perplexity=18.654642, train_loss=2.926095

Batch 252650, train_perplexity=18.654642, train_loss=2.926095

Batch 252660, train_perplexity=18.654638, train_loss=2.9260948

Batch 252670, train_perplexity=18.654633, train_loss=2.9260945

Batch 252680, train_perplexity=18.654642, train_loss=2.926095

Batch 252690, train_perplexity=18.654642, train_loss=2.926095

Batch 252700, train_perplexity=18.654638, train_loss=2.9260948

Batch 252710, train_perplexity=18.654638, train_loss=2.9260948

Batch 252720, train_perplexity=18.654638, train_loss=2.9260948

Batch 252730, train_perplexity=18.654642, train_loss=2.926095

Batch 252740, train_perplexity=18.654633, train_loss=2.9260945

Batch 252750, train_perplexity=18.654638, train_loss=2.9260948

Batch 252760, train_perplexity=18.654638, train_loss=2.9260948

Batch 252770, train_perplexity=18.654633, train_loss=2.9260945

Batch 252780, train_perplexity=18.654633, train_loss=2.9260945

Batch 252790, train_perplexity=18.654642, train_loss=2.926095

Batch 252800, train_perplexity=18.654633, train_loss=2.9260945

Batch 252810, train_perplexity=18.654638, train_loss=2.9260948

Batch 252820, train_perplexity=18.654638, train_loss=2.9260948

Batch 252830, train_perplexity=18.654633, train_loss=2.9260945

Batch 252840, train_perplexity=18.654638, train_loss=2.9260948

Batch 252850, train_perplexity=18.654633, train_loss=2.9260945

Batch 252860, train_perplexity=18.654633, train_loss=2.9260945

Batch 252870, train_perplexity=18.654638, train_loss=2.9260948

Batch 252880, train_perplexity=18.654633, train_loss=2.9260945

Batch 252890, train_perplexity=18.654638, train_loss=2.9260948

Batch 252900, train_perplexity=18.654638, train_loss=2.9260948

Batch 252910, train_perplexity=18.654638, train_loss=2.9260948

Batch 252920, train_perplexity=18.654633, train_loss=2.9260945

Batch 252930, train_perplexity=18.654633, train_loss=2.9260945

Batch 252940, train_perplexity=18.654633, train_loss=2.9260945

Batch 252950, train_perplexity=18.654629, train_loss=2.9260943

Batch 252960, train_perplexity=18.654629, train_loss=2.9260943

Batch 252970, train_perplexity=18.654633, train_loss=2.9260945

Batch 252980, train_perplexity=18.654633, train_loss=2.9260945

Batch 252990, train_perplexity=18.654629, train_loss=2.9260943

Batch 253000, train_perplexity=18.654633, train_loss=2.9260945

Batch 253010, train_perplexity=18.654625, train_loss=2.926094

Batch 253020, train_perplexity=18.654633, train_loss=2.9260945

Batch 253030, train_perplexity=18.654633, train_loss=2.9260945

Batch 253040, train_perplexity=18.654633, train_loss=2.9260945

Batch 253050, train_perplexity=18.654625, train_loss=2.926094

Batch 253060, train_perplexity=18.654629, train_loss=2.9260943

Batch 253070, train_perplexity=18.654625, train_loss=2.926094

Batch 253080, train_perplexity=18.654629, train_loss=2.9260943

Batch 253090, train_perplexity=18.654629, train_loss=2.9260943

Batch 253100, train_perplexity=18.654629, train_loss=2.9260943

Batch 253110, train_perplexity=18.654629, train_loss=2.9260943

Batch 253120, train_perplexity=18.654625, train_loss=2.926094

Batch 253130, train_perplexity=18.654625, train_loss=2.926094

Batch 253140, train_perplexity=18.654625, train_loss=2.926094

Batch 253150, train_perplexity=18.654625, train_loss=2.926094

Batch 253160, train_perplexity=18.654625, train_loss=2.926094

Batch 253170, train_perplexity=18.654629, train_loss=2.9260943

Batch 253180, train_perplexity=18.654625, train_loss=2.926094

Batch 253190, train_perplexity=18.654629, train_loss=2.9260943

Batch 253200, train_perplexity=18.654625, train_loss=2.926094

Batch 253210, train_perplexity=18.654625, train_loss=2.926094

Batch 253220, train_perplexity=18.654629, train_loss=2.9260943

Batch 253230, train_perplexity=18.654629, train_loss=2.9260943

Batch 253240, train_perplexity=18.654625, train_loss=2.926094

Batch 253250, train_perplexity=18.65462, train_loss=2.9260938

Batch 253260, train_perplexity=18.654625, train_loss=2.926094

Batch 253270, train_perplexity=18.654625, train_loss=2.926094

Batch 253280, train_perplexity=18.654625, train_loss=2.926094

Batch 253290, train_perplexity=18.654615, train_loss=2.9260936

Batch 253300, train_perplexity=18.654615, train_loss=2.9260936

Batch 253310, train_perplexity=18.654625, train_loss=2.926094

Batch 253320, train_perplexity=18.65462, train_loss=2.9260938

Batch 253330, train_perplexity=18.65462, train_loss=2.9260938

Batch 253340, train_perplexity=18.654625, train_loss=2.926094

Batch 253350, train_perplexity=18.654625, train_loss=2.926094

Batch 253360, train_perplexity=18.654625, train_loss=2.926094

Batch 253370, train_perplexity=18.65462, train_loss=2.9260938

Batch 253380, train_perplexity=18.654615, train_loss=2.9260936

Batch 253390, train_perplexity=18.654615, train_loss=2.9260936

Batch 253400, train_perplexity=18.654615, train_loss=2.9260936

Batch 253410, train_perplexity=18.65462, train_loss=2.9260938

Batch 253420, train_perplexity=18.654615, train_loss=2.9260936
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 253430, train_perplexity=18.65462, train_loss=2.9260938

Batch 253440, train_perplexity=18.65462, train_loss=2.9260938

Batch 253450, train_perplexity=18.65462, train_loss=2.9260938

Batch 253460, train_perplexity=18.654615, train_loss=2.9260936

Batch 253470, train_perplexity=18.65462, train_loss=2.9260938

Batch 253480, train_perplexity=18.654615, train_loss=2.9260936

Batch 253490, train_perplexity=18.654615, train_loss=2.9260936

Batch 253500, train_perplexity=18.654615, train_loss=2.9260936

Batch 253510, train_perplexity=18.654615, train_loss=2.9260936

Batch 253520, train_perplexity=18.654615, train_loss=2.9260936

Batch 253530, train_perplexity=18.654615, train_loss=2.9260936

Batch 253540, train_perplexity=18.654615, train_loss=2.9260936

Batch 253550, train_perplexity=18.654615, train_loss=2.9260936

Batch 253560, train_perplexity=18.654615, train_loss=2.9260936

Batch 253570, train_perplexity=18.654615, train_loss=2.9260936

Batch 253580, train_perplexity=18.654615, train_loss=2.9260936

Batch 253590, train_perplexity=18.654615, train_loss=2.9260936

Batch 253600, train_perplexity=18.654615, train_loss=2.9260936

Batch 253610, train_perplexity=18.654615, train_loss=2.9260936

Batch 253620, train_perplexity=18.654612, train_loss=2.9260933

Batch 253630, train_perplexity=18.654606, train_loss=2.926093

Batch 253640, train_perplexity=18.654606, train_loss=2.926093

Batch 253650, train_perplexity=18.654606, train_loss=2.926093

Batch 253660, train_perplexity=18.654612, train_loss=2.9260933

Batch 253670, train_perplexity=18.654606, train_loss=2.926093

Batch 253680, train_perplexity=18.654612, train_loss=2.9260933

Batch 253690, train_perplexity=18.654606, train_loss=2.926093

Batch 253700, train_perplexity=18.654612, train_loss=2.9260933

Batch 253710, train_perplexity=18.654606, train_loss=2.926093

Batch 253720, train_perplexity=18.654606, train_loss=2.926093

Batch 253730, train_perplexity=18.654612, train_loss=2.9260933

Batch 253740, train_perplexity=18.654612, train_loss=2.9260933

Batch 253750, train_perplexity=18.654612, train_loss=2.9260933

Batch 253760, train_perplexity=18.654615, train_loss=2.9260936

Batch 253770, train_perplexity=18.654606, train_loss=2.926093

Batch 253780, train_perplexity=18.654606, train_loss=2.926093

Batch 253790, train_perplexity=18.654606, train_loss=2.926093

Batch 253800, train_perplexity=18.654602, train_loss=2.9260929

Batch 253810, train_perplexity=18.654606, train_loss=2.926093

Batch 253820, train_perplexity=18.654606, train_loss=2.926093

Batch 253830, train_perplexity=18.654606, train_loss=2.926093

Batch 253840, train_perplexity=18.654606, train_loss=2.926093

Batch 253850, train_perplexity=18.654602, train_loss=2.9260929

Batch 253860, train_perplexity=18.654612, train_loss=2.9260933

Batch 253870, train_perplexity=18.654602, train_loss=2.9260929

Batch 253880, train_perplexity=18.654598, train_loss=2.9260926

Batch 253890, train_perplexity=18.654606, train_loss=2.926093

Batch 253900, train_perplexity=18.654606, train_loss=2.926093

Batch 253910, train_perplexity=18.654606, train_loss=2.926093

Batch 253920, train_perplexity=18.654602, train_loss=2.9260929

Batch 253930, train_perplexity=18.654606, train_loss=2.926093

Batch 253940, train_perplexity=18.654606, train_loss=2.926093

Batch 253950, train_perplexity=18.654598, train_loss=2.9260926

Batch 253960, train_perplexity=18.654598, train_loss=2.9260926

Batch 253970, train_perplexity=18.654606, train_loss=2.926093

Batch 253980, train_perplexity=18.654602, train_loss=2.9260929

Batch 253990, train_perplexity=18.654598, train_loss=2.9260926

Batch 254000, train_perplexity=18.654606, train_loss=2.926093

Batch 254010, train_perplexity=18.654598, train_loss=2.9260926

Batch 254020, train_perplexity=18.654602, train_loss=2.9260929

Batch 254030, train_perplexity=18.654598, train_loss=2.9260926

Batch 254040, train_perplexity=18.654602, train_loss=2.9260929

Batch 254050, train_perplexity=18.654602, train_loss=2.9260929

Batch 254060, train_perplexity=18.654598, train_loss=2.9260926

Batch 254070, train_perplexity=18.654598, train_loss=2.9260926

Batch 254080, train_perplexity=18.654602, train_loss=2.9260929

Batch 254090, train_perplexity=18.654598, train_loss=2.9260926

Batch 254100, train_perplexity=18.654598, train_loss=2.9260926

Batch 254110, train_perplexity=18.654598, train_loss=2.9260926

Batch 254120, train_perplexity=18.654606, train_loss=2.926093

Batch 254130, train_perplexity=18.654598, train_loss=2.9260926

Batch 254140, train_perplexity=18.654602, train_loss=2.9260929

Batch 254150, train_perplexity=18.654602, train_loss=2.9260929

Batch 254160, train_perplexity=18.654598, train_loss=2.9260926

Batch 254170, train_perplexity=18.654598, train_loss=2.9260926

Batch 254180, train_perplexity=18.654602, train_loss=2.9260929

Batch 254190, train_perplexity=18.654602, train_loss=2.9260929

Batch 254200, train_perplexity=18.654593, train_loss=2.9260924

Batch 254210, train_perplexity=18.654598, train_loss=2.9260926

Batch 254220, train_perplexity=18.654598, train_loss=2.9260926

Batch 254230, train_perplexity=18.654589, train_loss=2.9260921

Batch 254240, train_perplexity=18.654589, train_loss=2.9260921

Batch 254250, train_perplexity=18.654602, train_loss=2.9260929

Batch 254260, train_perplexity=18.654589, train_loss=2.9260921

Batch 254270, train_perplexity=18.654589, train_loss=2.9260921

Batch 254280, train_perplexity=18.654589, train_loss=2.9260921

Batch 254290, train_perplexity=18.654593, train_loss=2.9260924

Batch 254300, train_perplexity=18.654598, train_loss=2.9260926

Batch 254310, train_perplexity=18.654593, train_loss=2.9260924

Batch 254320, train_perplexity=18.654593, train_loss=2.9260924

Batch 254330, train_perplexity=18.654598, train_loss=2.9260926

Batch 254340, train_perplexity=18.654589, train_loss=2.9260921

Batch 254350, train_perplexity=18.654589, train_loss=2.9260921

Batch 254360, train_perplexity=18.654589, train_loss=2.9260921

Batch 254370, train_perplexity=18.654589, train_loss=2.9260921

Batch 254380, train_perplexity=18.654589, train_loss=2.9260921

Batch 254390, train_perplexity=18.654593, train_loss=2.9260924

Batch 254400, train_perplexity=18.654589, train_loss=2.9260921

Batch 254410, train_perplexity=18.654593, train_loss=2.9260924

Batch 254420, train_perplexity=18.654589, train_loss=2.9260921

Batch 254430, train_perplexity=18.654589, train_loss=2.9260921

Batch 254440, train_perplexity=18.654593, train_loss=2.9260924

Batch 254450, train_perplexity=18.654598, train_loss=2.9260926

Batch 254460, train_perplexity=18.654589, train_loss=2.9260921

Batch 254470, train_perplexity=18.654585, train_loss=2.926092

Batch 254480, train_perplexity=18.654589, train_loss=2.9260921

Batch 254490, train_perplexity=18.654589, train_loss=2.9260921

Batch 254500, train_perplexity=18.654589, train_loss=2.9260921

Batch 254510, train_perplexity=18.654589, train_loss=2.9260921

Batch 254520, train_perplexity=18.654589, train_loss=2.9260921

Batch 254530, train_perplexity=18.654589, train_loss=2.9260921

Batch 254540, train_perplexity=18.654589, train_loss=2.9260921

Batch 254550, train_perplexity=18.654585, train_loss=2.926092

Batch 254560, train_perplexity=18.654589, train_loss=2.9260921

Batch 254570, train_perplexity=18.654585, train_loss=2.926092

Batch 254580, train_perplexity=18.654589, train_loss=2.9260921

Batch 254590, train_perplexity=18.654589, train_loss=2.9260921

Batch 254600, train_perplexity=18.654589, train_loss=2.9260921

Batch 254610, train_perplexity=18.65458, train_loss=2.9260917

Batch 254620, train_perplexity=18.654589, train_loss=2.9260921

Batch 254630, train_perplexity=18.654589, train_loss=2.9260921

Batch 254640, train_perplexity=18.654585, train_loss=2.926092

Batch 254650, train_perplexity=18.65458, train_loss=2.9260917

Batch 254660, train_perplexity=18.65458, train_loss=2.9260917

Batch 254670, train_perplexity=18.654585, train_loss=2.926092

Batch 254680, train_perplexity=18.65458, train_loss=2.9260917

Batch 254690, train_perplexity=18.654585, train_loss=2.926092

Batch 254700, train_perplexity=18.654585, train_loss=2.926092

Batch 254710, train_perplexity=18.65458, train_loss=2.9260917
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 254720, train_perplexity=18.65458, train_loss=2.9260917

Batch 254730, train_perplexity=18.65458, train_loss=2.9260917

Batch 254740, train_perplexity=18.65458, train_loss=2.9260917

Batch 254750, train_perplexity=18.65458, train_loss=2.9260917

Batch 254760, train_perplexity=18.65458, train_loss=2.9260917

Batch 254770, train_perplexity=18.65458, train_loss=2.9260917

Batch 254780, train_perplexity=18.65458, train_loss=2.9260917

Batch 254790, train_perplexity=18.65458, train_loss=2.9260917

Batch 254800, train_perplexity=18.65458, train_loss=2.9260917

Batch 254810, train_perplexity=18.65458, train_loss=2.9260917

Batch 254820, train_perplexity=18.65458, train_loss=2.9260917

Batch 254830, train_perplexity=18.65458, train_loss=2.9260917

Batch 254840, train_perplexity=18.65458, train_loss=2.9260917

Batch 254850, train_perplexity=18.65458, train_loss=2.9260917

Batch 254860, train_perplexity=18.65458, train_loss=2.9260917

Batch 254870, train_perplexity=18.654572, train_loss=2.9260912

Batch 254880, train_perplexity=18.654572, train_loss=2.9260912

Batch 254890, train_perplexity=18.654585, train_loss=2.926092

Batch 254900, train_perplexity=18.65458, train_loss=2.9260917

Batch 254910, train_perplexity=18.654572, train_loss=2.9260912

Batch 254920, train_perplexity=18.654572, train_loss=2.9260912

Batch 254930, train_perplexity=18.654575, train_loss=2.9260914

Batch 254940, train_perplexity=18.65458, train_loss=2.9260917

Batch 254950, train_perplexity=18.654572, train_loss=2.9260912

Batch 254960, train_perplexity=18.654575, train_loss=2.9260914

Batch 254970, train_perplexity=18.65458, train_loss=2.9260917

Batch 254980, train_perplexity=18.654575, train_loss=2.9260914

Batch 254990, train_perplexity=18.654572, train_loss=2.9260912

Batch 255000, train_perplexity=18.654572, train_loss=2.9260912

Batch 255010, train_perplexity=18.654572, train_loss=2.9260912

Batch 255020, train_perplexity=18.654575, train_loss=2.9260914

Batch 255030, train_perplexity=18.654572, train_loss=2.9260912

Batch 255040, train_perplexity=18.654572, train_loss=2.9260912

Batch 255050, train_perplexity=18.654575, train_loss=2.9260914

Batch 255060, train_perplexity=18.654575, train_loss=2.9260914

Batch 255070, train_perplexity=18.654572, train_loss=2.9260912

Batch 255080, train_perplexity=18.654572, train_loss=2.9260912

Batch 255090, train_perplexity=18.654572, train_loss=2.9260912

Batch 255100, train_perplexity=18.654572, train_loss=2.9260912

Batch 255110, train_perplexity=18.654572, train_loss=2.9260912

Batch 255120, train_perplexity=18.654566, train_loss=2.926091

Batch 255130, train_perplexity=18.654572, train_loss=2.9260912

Batch 255140, train_perplexity=18.654572, train_loss=2.9260912

Batch 255150, train_perplexity=18.654572, train_loss=2.9260912

Batch 255160, train_perplexity=18.654572, train_loss=2.9260912

Batch 255170, train_perplexity=18.654572, train_loss=2.9260912

Batch 255180, train_perplexity=18.654572, train_loss=2.9260912

Batch 255190, train_perplexity=18.654566, train_loss=2.926091

Batch 255200, train_perplexity=18.654566, train_loss=2.926091

Batch 255210, train_perplexity=18.654572, train_loss=2.9260912

Batch 255220, train_perplexity=18.654562, train_loss=2.9260907

Batch 255230, train_perplexity=18.654562, train_loss=2.9260907

Batch 255240, train_perplexity=18.654572, train_loss=2.9260912

Batch 255250, train_perplexity=18.654566, train_loss=2.926091

Batch 255260, train_perplexity=18.654572, train_loss=2.9260912

Batch 255270, train_perplexity=18.654562, train_loss=2.9260907

Batch 255280, train_perplexity=18.654566, train_loss=2.926091

Batch 255290, train_perplexity=18.654572, train_loss=2.9260912

Batch 255300, train_perplexity=18.654562, train_loss=2.9260907

Batch 255310, train_perplexity=18.654562, train_loss=2.9260907

Batch 255320, train_perplexity=18.654566, train_loss=2.926091

Batch 255330, train_perplexity=18.654566, train_loss=2.926091

Batch 255340, train_perplexity=18.654562, train_loss=2.9260907

Batch 255350, train_perplexity=18.654562, train_loss=2.9260907

Batch 255360, train_perplexity=18.654562, train_loss=2.9260907

Batch 255370, train_perplexity=18.654562, train_loss=2.9260907

Batch 255380, train_perplexity=18.654562, train_loss=2.9260907

Batch 255390, train_perplexity=18.654562, train_loss=2.9260907

Batch 255400, train_perplexity=18.654562, train_loss=2.9260907

Batch 255410, train_perplexity=18.654562, train_loss=2.9260907

Batch 255420, train_perplexity=18.654562, train_loss=2.9260907

Batch 255430, train_perplexity=18.654562, train_loss=2.9260907

Batch 255440, train_perplexity=18.654552, train_loss=2.9260902

Batch 255450, train_perplexity=18.654566, train_loss=2.926091

Batch 255460, train_perplexity=18.654562, train_loss=2.9260907

Batch 255470, train_perplexity=18.654558, train_loss=2.9260905

Batch 255480, train_perplexity=18.654562, train_loss=2.9260907

Batch 255490, train_perplexity=18.654562, train_loss=2.9260907

Batch 255500, train_perplexity=18.654562, train_loss=2.9260907

Batch 255510, train_perplexity=18.654558, train_loss=2.9260905

Batch 255520, train_perplexity=18.654552, train_loss=2.9260902

Batch 255530, train_perplexity=18.654558, train_loss=2.9260905

Batch 255540, train_perplexity=18.654562, train_loss=2.9260907

Batch 255550, train_perplexity=18.654552, train_loss=2.9260902

Batch 255560, train_perplexity=18.654552, train_loss=2.9260902

Batch 255570, train_perplexity=18.654552, train_loss=2.9260902

Batch 255580, train_perplexity=18.654562, train_loss=2.9260907

Batch 255590, train_perplexity=18.654558, train_loss=2.9260905

Batch 255600, train_perplexity=18.654552, train_loss=2.9260902

Batch 255610, train_perplexity=18.654558, train_loss=2.9260905

Batch 255620, train_perplexity=18.654552, train_loss=2.9260902

Batch 255630, train_perplexity=18.654552, train_loss=2.9260902

Batch 255640, train_perplexity=18.654552, train_loss=2.9260902

Batch 255650, train_perplexity=18.654552, train_loss=2.9260902

Batch 255660, train_perplexity=18.654558, train_loss=2.9260905

Batch 255670, train_perplexity=18.654558, train_loss=2.9260905

Batch 255680, train_perplexity=18.654549, train_loss=2.92609

Batch 255690, train_perplexity=18.654552, train_loss=2.9260902

Batch 255700, train_perplexity=18.654552, train_loss=2.9260902

Batch 255710, train_perplexity=18.654552, train_loss=2.9260902

Batch 255720, train_perplexity=18.654552, train_loss=2.9260902

Batch 255730, train_perplexity=18.654552, train_loss=2.9260902

Batch 255740, train_perplexity=18.654558, train_loss=2.9260905

Batch 255750, train_perplexity=18.654558, train_loss=2.9260905

Batch 255760, train_perplexity=18.654552, train_loss=2.9260902

Batch 255770, train_perplexity=18.654552, train_loss=2.9260902

Batch 255780, train_perplexity=18.654552, train_loss=2.9260902

Batch 255790, train_perplexity=18.654549, train_loss=2.92609

Batch 255800, train_perplexity=18.654552, train_loss=2.9260902

Batch 255810, train_perplexity=18.654549, train_loss=2.92609

Batch 255820, train_perplexity=18.654552, train_loss=2.9260902

Batch 255830, train_perplexity=18.654545, train_loss=2.9260898

Batch 255840, train_perplexity=18.654552, train_loss=2.9260902

Batch 255850, train_perplexity=18.654545, train_loss=2.9260898

Batch 255860, train_perplexity=18.654549, train_loss=2.92609

Batch 255870, train_perplexity=18.654549, train_loss=2.92609

Batch 255880, train_perplexity=18.654549, train_loss=2.92609

Batch 255890, train_perplexity=18.654545, train_loss=2.9260898

Batch 255900, train_perplexity=18.654545, train_loss=2.9260898

Batch 255910, train_perplexity=18.654545, train_loss=2.9260898

Batch 255920, train_perplexity=18.654545, train_loss=2.9260898

Batch 255930, train_perplexity=18.654549, train_loss=2.92609

Batch 255940, train_perplexity=18.654549, train_loss=2.92609

Batch 255950, train_perplexity=18.654545, train_loss=2.9260898

Batch 255960, train_perplexity=18.654545, train_loss=2.9260898

Batch 255970, train_perplexity=18.654545, train_loss=2.9260898

Batch 255980, train_perplexity=18.654545, train_loss=2.9260898

Batch 255990, train_perplexity=18.654552, train_loss=2.9260902

Batch 256000, train_perplexity=18.654545, train_loss=2.9260898
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 256010, train_perplexity=18.654545, train_loss=2.9260898

Batch 256020, train_perplexity=18.654545, train_loss=2.9260898

Batch 256030, train_perplexity=18.654545, train_loss=2.9260898

Batch 256040, train_perplexity=18.654545, train_loss=2.9260898

Batch 256050, train_perplexity=18.654545, train_loss=2.9260898

Batch 256060, train_perplexity=18.654545, train_loss=2.9260898

Batch 256070, train_perplexity=18.654545, train_loss=2.9260898

Batch 256080, train_perplexity=18.654545, train_loss=2.9260898

Batch 256090, train_perplexity=18.654545, train_loss=2.9260898

Batch 256100, train_perplexity=18.654545, train_loss=2.9260898

Batch 256110, train_perplexity=18.654545, train_loss=2.9260898

Batch 256120, train_perplexity=18.654545, train_loss=2.9260898

Batch 256130, train_perplexity=18.654545, train_loss=2.9260898

Batch 256140, train_perplexity=18.654545, train_loss=2.9260898

Batch 256150, train_perplexity=18.65454, train_loss=2.9260895

Batch 256160, train_perplexity=18.654545, train_loss=2.9260898

Batch 256170, train_perplexity=18.654535, train_loss=2.9260893

Batch 256180, train_perplexity=18.654535, train_loss=2.9260893

Batch 256190, train_perplexity=18.654535, train_loss=2.9260893

Batch 256200, train_perplexity=18.654535, train_loss=2.9260893

Batch 256210, train_perplexity=18.65454, train_loss=2.9260895

Batch 256220, train_perplexity=18.654545, train_loss=2.9260898

Batch 256230, train_perplexity=18.654535, train_loss=2.9260893

Batch 256240, train_perplexity=18.654545, train_loss=2.9260898

Batch 256250, train_perplexity=18.65454, train_loss=2.9260895

Batch 256260, train_perplexity=18.65454, train_loss=2.9260895

Batch 256270, train_perplexity=18.654535, train_loss=2.9260893

Batch 256280, train_perplexity=18.654535, train_loss=2.9260893

Batch 256290, train_perplexity=18.65454, train_loss=2.9260895

Batch 256300, train_perplexity=18.654535, train_loss=2.9260893

Batch 256310, train_perplexity=18.654535, train_loss=2.9260893

Batch 256320, train_perplexity=18.654535, train_loss=2.9260893

Batch 256330, train_perplexity=18.654535, train_loss=2.9260893

Batch 256340, train_perplexity=18.654535, train_loss=2.9260893

Batch 256350, train_perplexity=18.65454, train_loss=2.9260895

Batch 256360, train_perplexity=18.654535, train_loss=2.9260893

Batch 256370, train_perplexity=18.654535, train_loss=2.9260893

Batch 256380, train_perplexity=18.65454, train_loss=2.9260895

Batch 256390, train_perplexity=18.654535, train_loss=2.9260893

Batch 256400, train_perplexity=18.654535, train_loss=2.9260893

Batch 256410, train_perplexity=18.654535, train_loss=2.9260893

Batch 256420, train_perplexity=18.654535, train_loss=2.9260893

Batch 256430, train_perplexity=18.654535, train_loss=2.9260893

Batch 256440, train_perplexity=18.654535, train_loss=2.9260893

Batch 256450, train_perplexity=18.654526, train_loss=2.9260888

Batch 256460, train_perplexity=18.654535, train_loss=2.9260893

Batch 256470, train_perplexity=18.654535, train_loss=2.9260893

Batch 256480, train_perplexity=18.654531, train_loss=2.926089

Batch 256490, train_perplexity=18.654531, train_loss=2.926089

Batch 256500, train_perplexity=18.654526, train_loss=2.9260888

Batch 256510, train_perplexity=18.654535, train_loss=2.9260893

Batch 256520, train_perplexity=18.654526, train_loss=2.9260888

Batch 256530, train_perplexity=18.654526, train_loss=2.9260888

Batch 256540, train_perplexity=18.654526, train_loss=2.9260888

Batch 256550, train_perplexity=18.654526, train_loss=2.9260888

Batch 256560, train_perplexity=18.654526, train_loss=2.9260888

Batch 256570, train_perplexity=18.654526, train_loss=2.9260888

Batch 256580, train_perplexity=18.654526, train_loss=2.9260888

Batch 256590, train_perplexity=18.654531, train_loss=2.926089

Batch 256600, train_perplexity=18.654526, train_loss=2.9260888

Batch 256610, train_perplexity=18.654526, train_loss=2.9260888

Batch 256620, train_perplexity=18.654526, train_loss=2.9260888

Batch 256630, train_perplexity=18.654526, train_loss=2.9260888

Batch 256640, train_perplexity=18.654526, train_loss=2.9260888

Batch 256650, train_perplexity=18.654526, train_loss=2.9260888

Batch 256660, train_perplexity=18.654526, train_loss=2.9260888

Batch 256670, train_perplexity=18.654526, train_loss=2.9260888

Batch 256680, train_perplexity=18.654526, train_loss=2.9260888

Batch 256690, train_perplexity=18.654526, train_loss=2.9260888

Batch 256700, train_perplexity=18.654526, train_loss=2.9260888

Batch 256710, train_perplexity=18.654526, train_loss=2.9260888

Batch 256720, train_perplexity=18.654526, train_loss=2.9260888

Batch 256730, train_perplexity=18.654518, train_loss=2.9260883

Batch 256740, train_perplexity=18.654522, train_loss=2.9260886

Batch 256750, train_perplexity=18.654522, train_loss=2.9260886

Batch 256760, train_perplexity=18.654526, train_loss=2.9260888

Batch 256770, train_perplexity=18.654526, train_loss=2.9260888

Batch 256780, train_perplexity=18.654526, train_loss=2.9260888

Batch 256790, train_perplexity=18.654522, train_loss=2.9260886

Batch 256800, train_perplexity=18.654526, train_loss=2.9260888

Batch 256810, train_perplexity=18.654526, train_loss=2.9260888

Batch 256820, train_perplexity=18.654522, train_loss=2.9260886

Batch 256830, train_perplexity=18.654518, train_loss=2.9260883

Batch 256840, train_perplexity=18.654526, train_loss=2.9260888

Batch 256850, train_perplexity=18.654526, train_loss=2.9260888

Batch 256860, train_perplexity=18.654522, train_loss=2.9260886

Batch 256870, train_perplexity=18.654526, train_loss=2.9260888

Batch 256880, train_perplexity=18.654522, train_loss=2.9260886

Batch 256890, train_perplexity=18.654522, train_loss=2.9260886

Batch 256900, train_perplexity=18.654518, train_loss=2.9260883

Batch 256910, train_perplexity=18.654518, train_loss=2.9260883

Batch 256920, train_perplexity=18.654518, train_loss=2.9260883

Batch 256930, train_perplexity=18.654512, train_loss=2.926088

Batch 256940, train_perplexity=18.654518, train_loss=2.9260883

Batch 256950, train_perplexity=18.654518, train_loss=2.9260883

Batch 256960, train_perplexity=18.654518, train_loss=2.9260883

Batch 256970, train_perplexity=18.654518, train_loss=2.9260883

Batch 256980, train_perplexity=18.654518, train_loss=2.9260883

Batch 256990, train_perplexity=18.654518, train_loss=2.9260883

Batch 257000, train_perplexity=18.654522, train_loss=2.9260886

Batch 257010, train_perplexity=18.654518, train_loss=2.9260883

Batch 257020, train_perplexity=18.654518, train_loss=2.9260883

Batch 257030, train_perplexity=18.654518, train_loss=2.9260883

Batch 257040, train_perplexity=18.654518, train_loss=2.9260883

Batch 257050, train_perplexity=18.654512, train_loss=2.926088

Batch 257060, train_perplexity=18.654518, train_loss=2.9260883

Batch 257070, train_perplexity=18.654512, train_loss=2.926088

Batch 257080, train_perplexity=18.654518, train_loss=2.9260883

Batch 257090, train_perplexity=18.654512, train_loss=2.926088

Batch 257100, train_perplexity=18.654518, train_loss=2.9260883

Batch 257110, train_perplexity=18.654512, train_loss=2.926088

Batch 257120, train_perplexity=18.654512, train_loss=2.926088

Batch 257130, train_perplexity=18.654518, train_loss=2.9260883

Batch 257140, train_perplexity=18.654512, train_loss=2.926088

Batch 257150, train_perplexity=18.654509, train_loss=2.9260879

Batch 257160, train_perplexity=18.654512, train_loss=2.926088

Batch 257170, train_perplexity=18.654509, train_loss=2.9260879

Batch 257180, train_perplexity=18.654512, train_loss=2.926088

Batch 257190, train_perplexity=18.654509, train_loss=2.9260879

Batch 257200, train_perplexity=18.654512, train_loss=2.926088

Batch 257210, train_perplexity=18.654512, train_loss=2.926088

Batch 257220, train_perplexity=18.654509, train_loss=2.9260879

Batch 257230, train_perplexity=18.654509, train_loss=2.9260879

Batch 257240, train_perplexity=18.654512, train_loss=2.926088

Batch 257250, train_perplexity=18.654509, train_loss=2.9260879

Batch 257260, train_perplexity=18.654509, train_loss=2.9260879

Batch 257270, train_perplexity=18.654509, train_loss=2.9260879

Batch 257280, train_perplexity=18.654509, train_loss=2.9260879

Batch 257290, train_perplexity=18.654509, train_loss=2.9260879
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 257300, train_perplexity=18.654509, train_loss=2.9260879

Batch 257310, train_perplexity=18.654512, train_loss=2.926088

Batch 257320, train_perplexity=18.654509, train_loss=2.9260879

Batch 257330, train_perplexity=18.654509, train_loss=2.9260879

Batch 257340, train_perplexity=18.654509, train_loss=2.9260879

Batch 257350, train_perplexity=18.654509, train_loss=2.9260879

Batch 257360, train_perplexity=18.654509, train_loss=2.9260879

Batch 257370, train_perplexity=18.654509, train_loss=2.9260879

Batch 257380, train_perplexity=18.6545, train_loss=2.9260874

Batch 257390, train_perplexity=18.654505, train_loss=2.9260876

Batch 257400, train_perplexity=18.6545, train_loss=2.9260874

Batch 257410, train_perplexity=18.654505, train_loss=2.9260876

Batch 257420, train_perplexity=18.654509, train_loss=2.9260879

Batch 257430, train_perplexity=18.654505, train_loss=2.9260876

Batch 257440, train_perplexity=18.6545, train_loss=2.9260874

Batch 257450, train_perplexity=18.6545, train_loss=2.9260874

Batch 257460, train_perplexity=18.6545, train_loss=2.9260874

Batch 257470, train_perplexity=18.6545, train_loss=2.9260874

Batch 257480, train_perplexity=18.6545, train_loss=2.9260874

Batch 257490, train_perplexity=18.6545, train_loss=2.9260874

Batch 257500, train_perplexity=18.654505, train_loss=2.9260876

Batch 257510, train_perplexity=18.654509, train_loss=2.9260879

Batch 257520, train_perplexity=18.6545, train_loss=2.9260874

Batch 257530, train_perplexity=18.6545, train_loss=2.9260874

Batch 257540, train_perplexity=18.6545, train_loss=2.9260874

Batch 257550, train_perplexity=18.6545, train_loss=2.9260874

Batch 257560, train_perplexity=18.6545, train_loss=2.9260874

Batch 257570, train_perplexity=18.6545, train_loss=2.9260874

Batch 257580, train_perplexity=18.654495, train_loss=2.9260871

Batch 257590, train_perplexity=18.6545, train_loss=2.9260874

Batch 257600, train_perplexity=18.654505, train_loss=2.9260876

Batch 257610, train_perplexity=18.6545, train_loss=2.9260874

Batch 257620, train_perplexity=18.654495, train_loss=2.9260871

Batch 257630, train_perplexity=18.654505, train_loss=2.9260876

Batch 257640, train_perplexity=18.654495, train_loss=2.9260871

Batch 257650, train_perplexity=18.654495, train_loss=2.9260871

Batch 257660, train_perplexity=18.6545, train_loss=2.9260874

Batch 257670, train_perplexity=18.6545, train_loss=2.9260874

Batch 257680, train_perplexity=18.6545, train_loss=2.9260874

Batch 257690, train_perplexity=18.6545, train_loss=2.9260874

Batch 257700, train_perplexity=18.654495, train_loss=2.9260871

Batch 257710, train_perplexity=18.654491, train_loss=2.926087

Batch 257720, train_perplexity=18.6545, train_loss=2.9260874

Batch 257730, train_perplexity=18.654495, train_loss=2.9260871

Batch 257740, train_perplexity=18.654491, train_loss=2.926087

Batch 257750, train_perplexity=18.654495, train_loss=2.9260871

Batch 257760, train_perplexity=18.654491, train_loss=2.926087

Batch 257770, train_perplexity=18.6545, train_loss=2.9260874

Batch 257780, train_perplexity=18.654491, train_loss=2.926087

Batch 257790, train_perplexity=18.654491, train_loss=2.926087

Batch 257800, train_perplexity=18.654491, train_loss=2.926087

Batch 257810, train_perplexity=18.654495, train_loss=2.9260871

Batch 257820, train_perplexity=18.654495, train_loss=2.9260871

Batch 257830, train_perplexity=18.654491, train_loss=2.926087

Batch 257840, train_perplexity=18.654491, train_loss=2.926087

Batch 257850, train_perplexity=18.654491, train_loss=2.926087

Batch 257860, train_perplexity=18.654491, train_loss=2.926087

Batch 257870, train_perplexity=18.654491, train_loss=2.926087

Batch 257880, train_perplexity=18.654491, train_loss=2.926087

Batch 257890, train_perplexity=18.654491, train_loss=2.926087

Batch 257900, train_perplexity=18.654491, train_loss=2.926087

Batch 257910, train_perplexity=18.654491, train_loss=2.926087

Batch 257920, train_perplexity=18.654491, train_loss=2.926087

Batch 257930, train_perplexity=18.654491, train_loss=2.926087

Batch 257940, train_perplexity=18.654495, train_loss=2.9260871

Batch 257950, train_perplexity=18.654491, train_loss=2.926087

Batch 257960, train_perplexity=18.654491, train_loss=2.926087

Batch 257970, train_perplexity=18.654486, train_loss=2.9260867

Batch 257980, train_perplexity=18.654491, train_loss=2.926087

Batch 257990, train_perplexity=18.654491, train_loss=2.926087

Batch 258000, train_perplexity=18.654482, train_loss=2.9260864

Batch 258010, train_perplexity=18.654491, train_loss=2.926087

Batch 258020, train_perplexity=18.654491, train_loss=2.926087

Batch 258030, train_perplexity=18.654486, train_loss=2.9260867

Batch 258040, train_perplexity=18.654486, train_loss=2.9260867

Batch 258050, train_perplexity=18.654482, train_loss=2.9260864

Batch 258060, train_perplexity=18.654482, train_loss=2.9260864

Batch 258070, train_perplexity=18.654491, train_loss=2.926087

Batch 258080, train_perplexity=18.654491, train_loss=2.926087

Batch 258090, train_perplexity=18.654486, train_loss=2.9260867

Batch 258100, train_perplexity=18.654482, train_loss=2.9260864

Batch 258110, train_perplexity=18.654482, train_loss=2.9260864

Batch 258120, train_perplexity=18.654482, train_loss=2.9260864

Batch 258130, train_perplexity=18.654482, train_loss=2.9260864

Batch 258140, train_perplexity=18.654486, train_loss=2.9260867

Batch 258150, train_perplexity=18.654482, train_loss=2.9260864

Batch 258160, train_perplexity=18.654486, train_loss=2.9260867

Batch 258170, train_perplexity=18.654491, train_loss=2.926087

Batch 258180, train_perplexity=18.654482, train_loss=2.9260864

Batch 258190, train_perplexity=18.654482, train_loss=2.9260864

Batch 258200, train_perplexity=18.654482, train_loss=2.9260864

Batch 258210, train_perplexity=18.654482, train_loss=2.9260864

Batch 258220, train_perplexity=18.654482, train_loss=2.9260864

Batch 258230, train_perplexity=18.654482, train_loss=2.9260864

Batch 258240, train_perplexity=18.654482, train_loss=2.9260864

Batch 258250, train_perplexity=18.654478, train_loss=2.9260862

Batch 258260, train_perplexity=18.654472, train_loss=2.926086

Batch 258270, train_perplexity=18.654482, train_loss=2.9260864

Batch 258280, train_perplexity=18.654478, train_loss=2.9260862

Batch 258290, train_perplexity=18.654478, train_loss=2.9260862

Batch 258300, train_perplexity=18.654482, train_loss=2.9260864

Batch 258310, train_perplexity=18.654482, train_loss=2.9260864

Batch 258320, train_perplexity=18.654482, train_loss=2.9260864

Batch 258330, train_perplexity=18.654478, train_loss=2.9260862

Batch 258340, train_perplexity=18.654482, train_loss=2.9260864

Batch 258350, train_perplexity=18.654472, train_loss=2.926086

Batch 258360, train_perplexity=18.654472, train_loss=2.926086

Batch 258370, train_perplexity=18.654482, train_loss=2.9260864

Batch 258380, train_perplexity=18.654472, train_loss=2.926086

Batch 258390, train_perplexity=18.654478, train_loss=2.9260862

Batch 258400, train_perplexity=18.654482, train_loss=2.9260864

Batch 258410, train_perplexity=18.654472, train_loss=2.926086

Batch 258420, train_perplexity=18.654472, train_loss=2.926086

Batch 258430, train_perplexity=18.654472, train_loss=2.926086

Batch 258440, train_perplexity=18.654472, train_loss=2.926086

Batch 258450, train_perplexity=18.654472, train_loss=2.926086

Batch 258460, train_perplexity=18.654472, train_loss=2.926086

Batch 258470, train_perplexity=18.654478, train_loss=2.9260862

Batch 258480, train_perplexity=18.654472, train_loss=2.926086

Batch 258490, train_perplexity=18.654472, train_loss=2.926086

Batch 258500, train_perplexity=18.654472, train_loss=2.926086

Batch 258510, train_perplexity=18.654472, train_loss=2.926086

Batch 258520, train_perplexity=18.654478, train_loss=2.9260862

Batch 258530, train_perplexity=18.654472, train_loss=2.926086

Batch 258540, train_perplexity=18.654472, train_loss=2.926086

Batch 258550, train_perplexity=18.654472, train_loss=2.926086

Batch 258560, train_perplexity=18.654478, train_loss=2.9260862

Batch 258570, train_perplexity=18.654472, train_loss=2.926086

Batch 258580, train_perplexity=18.654472, train_loss=2.926086

Batch 258590, train_perplexity=18.654472, train_loss=2.926086
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 258600, train_perplexity=18.654472, train_loss=2.926086

Batch 258610, train_perplexity=18.654465, train_loss=2.9260855

Batch 258620, train_perplexity=18.654472, train_loss=2.926086

Batch 258630, train_perplexity=18.654465, train_loss=2.9260855

Batch 258640, train_perplexity=18.654469, train_loss=2.9260857

Batch 258650, train_perplexity=18.654469, train_loss=2.9260857

Batch 258660, train_perplexity=18.654469, train_loss=2.9260857

Batch 258670, train_perplexity=18.654472, train_loss=2.926086

Batch 258680, train_perplexity=18.654472, train_loss=2.926086

Batch 258690, train_perplexity=18.654465, train_loss=2.9260855

Batch 258700, train_perplexity=18.654465, train_loss=2.9260855

Batch 258710, train_perplexity=18.654465, train_loss=2.9260855

Batch 258720, train_perplexity=18.654469, train_loss=2.9260857

Batch 258730, train_perplexity=18.654465, train_loss=2.9260855

Batch 258740, train_perplexity=18.654469, train_loss=2.9260857

Batch 258750, train_perplexity=18.654472, train_loss=2.926086

Batch 258760, train_perplexity=18.654465, train_loss=2.9260855

Batch 258770, train_perplexity=18.654465, train_loss=2.9260855

Batch 258780, train_perplexity=18.654472, train_loss=2.926086

Batch 258790, train_perplexity=18.654465, train_loss=2.9260855

Batch 258800, train_perplexity=18.654465, train_loss=2.9260855

Batch 258810, train_perplexity=18.654465, train_loss=2.9260855

Batch 258820, train_perplexity=18.654469, train_loss=2.9260857

Batch 258830, train_perplexity=18.654465, train_loss=2.9260855

Batch 258840, train_perplexity=18.654459, train_loss=2.9260852

Batch 258850, train_perplexity=18.654465, train_loss=2.9260855

Batch 258860, train_perplexity=18.654465, train_loss=2.9260855

Batch 258870, train_perplexity=18.654465, train_loss=2.9260855

Batch 258880, train_perplexity=18.654465, train_loss=2.9260855

Batch 258890, train_perplexity=18.654465, train_loss=2.9260855

Batch 258900, train_perplexity=18.654465, train_loss=2.9260855

Batch 258910, train_perplexity=18.654465, train_loss=2.9260855

Batch 258920, train_perplexity=18.654465, train_loss=2.9260855

Batch 258930, train_perplexity=18.654465, train_loss=2.9260855

Batch 258940, train_perplexity=18.654459, train_loss=2.9260852

Batch 258950, train_perplexity=18.654469, train_loss=2.9260857

Batch 258960, train_perplexity=18.654465, train_loss=2.9260855

Batch 258970, train_perplexity=18.654455, train_loss=2.926085

Batch 258980, train_perplexity=18.654455, train_loss=2.926085

Batch 258990, train_perplexity=18.654465, train_loss=2.9260855

Batch 259000, train_perplexity=18.654459, train_loss=2.9260852

Batch 259010, train_perplexity=18.654465, train_loss=2.9260855

Batch 259020, train_perplexity=18.654465, train_loss=2.9260855

Batch 259030, train_perplexity=18.654455, train_loss=2.926085

Batch 259040, train_perplexity=18.654459, train_loss=2.9260852

Batch 259050, train_perplexity=18.654455, train_loss=2.926085

Batch 259060, train_perplexity=18.654455, train_loss=2.926085

Batch 259070, train_perplexity=18.654455, train_loss=2.926085

Batch 259080, train_perplexity=18.654455, train_loss=2.926085

Batch 259090, train_perplexity=18.654455, train_loss=2.926085

Batch 259100, train_perplexity=18.654455, train_loss=2.926085

Batch 259110, train_perplexity=18.654455, train_loss=2.926085

Batch 259120, train_perplexity=18.654459, train_loss=2.9260852

Batch 259130, train_perplexity=18.654455, train_loss=2.926085

Batch 259140, train_perplexity=18.654455, train_loss=2.926085

Batch 259150, train_perplexity=18.654455, train_loss=2.926085

Batch 259160, train_perplexity=18.654455, train_loss=2.926085

Batch 259170, train_perplexity=18.654455, train_loss=2.926085

Batch 259180, train_perplexity=18.654455, train_loss=2.926085

Batch 259190, train_perplexity=18.654455, train_loss=2.926085

Batch 259200, train_perplexity=18.654455, train_loss=2.926085

Batch 259210, train_perplexity=18.654459, train_loss=2.9260852

Batch 259220, train_perplexity=18.654455, train_loss=2.926085

Batch 259230, train_perplexity=18.654451, train_loss=2.9260848

Batch 259240, train_perplexity=18.654455, train_loss=2.926085

Batch 259250, train_perplexity=18.654446, train_loss=2.9260845

Batch 259260, train_perplexity=18.654459, train_loss=2.9260852

Batch 259270, train_perplexity=18.654446, train_loss=2.9260845

Batch 259280, train_perplexity=18.654446, train_loss=2.9260845

Batch 259290, train_perplexity=18.654451, train_loss=2.9260848

Batch 259300, train_perplexity=18.654451, train_loss=2.9260848

Batch 259310, train_perplexity=18.654455, train_loss=2.926085

Batch 259320, train_perplexity=18.654455, train_loss=2.926085

Batch 259330, train_perplexity=18.654455, train_loss=2.926085

Batch 259340, train_perplexity=18.654446, train_loss=2.9260845

Batch 259350, train_perplexity=18.654446, train_loss=2.9260845

Batch 259360, train_perplexity=18.654451, train_loss=2.9260848

Batch 259370, train_perplexity=18.654446, train_loss=2.9260845

Batch 259380, train_perplexity=18.654446, train_loss=2.9260845

Batch 259390, train_perplexity=18.654446, train_loss=2.9260845

Batch 259400, train_perplexity=18.654446, train_loss=2.9260845

Batch 259410, train_perplexity=18.654446, train_loss=2.9260845

Batch 259420, train_perplexity=18.654446, train_loss=2.9260845

Batch 259430, train_perplexity=18.654451, train_loss=2.9260848

Batch 259440, train_perplexity=18.654446, train_loss=2.9260845

Batch 259450, train_perplexity=18.654451, train_loss=2.9260848

Batch 259460, train_perplexity=18.654455, train_loss=2.926085

Batch 259470, train_perplexity=18.654446, train_loss=2.9260845

Batch 259480, train_perplexity=18.654446, train_loss=2.9260845

Batch 259490, train_perplexity=18.654442, train_loss=2.9260843

Batch 259500, train_perplexity=18.654446, train_loss=2.9260845

Batch 259510, train_perplexity=18.654446, train_loss=2.9260845

Batch 259520, train_perplexity=18.654446, train_loss=2.9260845

Batch 259530, train_perplexity=18.654446, train_loss=2.9260845

Batch 259540, train_perplexity=18.654442, train_loss=2.9260843

Batch 259550, train_perplexity=18.654442, train_loss=2.9260843

Batch 259560, train_perplexity=18.654442, train_loss=2.9260843

Batch 259570, train_perplexity=18.654442, train_loss=2.9260843

Batch 259580, train_perplexity=18.654442, train_loss=2.9260843

Batch 259590, train_perplexity=18.654446, train_loss=2.9260845

Batch 259600, train_perplexity=18.654446, train_loss=2.9260845

Batch 259610, train_perplexity=18.654446, train_loss=2.9260845

Batch 259620, train_perplexity=18.654442, train_loss=2.9260843

Batch 259630, train_perplexity=18.654438, train_loss=2.926084

Batch 259640, train_perplexity=18.654442, train_loss=2.9260843

Batch 259650, train_perplexity=18.654442, train_loss=2.9260843

Batch 259660, train_perplexity=18.654442, train_loss=2.9260843

Batch 259670, train_perplexity=18.654438, train_loss=2.926084

Batch 259680, train_perplexity=18.654438, train_loss=2.926084

Batch 259690, train_perplexity=18.654438, train_loss=2.926084

Batch 259700, train_perplexity=18.654438, train_loss=2.926084

Batch 259710, train_perplexity=18.654438, train_loss=2.926084

Batch 259720, train_perplexity=18.654438, train_loss=2.926084

Batch 259730, train_perplexity=18.654438, train_loss=2.926084

Batch 259740, train_perplexity=18.654438, train_loss=2.926084

Batch 259750, train_perplexity=18.654438, train_loss=2.926084

Batch 259760, train_perplexity=18.654446, train_loss=2.9260845

Batch 259770, train_perplexity=18.654442, train_loss=2.9260843

Batch 259780, train_perplexity=18.654438, train_loss=2.926084

Batch 259790, train_perplexity=18.654438, train_loss=2.926084

Batch 259800, train_perplexity=18.654438, train_loss=2.926084

Batch 259810, train_perplexity=18.654438, train_loss=2.926084

Batch 259820, train_perplexity=18.654438, train_loss=2.926084

Batch 259830, train_perplexity=18.654438, train_loss=2.926084

Batch 259840, train_perplexity=18.654438, train_loss=2.926084

Batch 259850, train_perplexity=18.654438, train_loss=2.926084

Batch 259860, train_perplexity=18.654438, train_loss=2.926084

Batch 259870, train_perplexity=18.654432, train_loss=2.9260838

Batch 259880, train_perplexity=18.654428, train_loss=2.9260836
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 259890, train_perplexity=18.654438, train_loss=2.926084

Batch 259900, train_perplexity=18.654438, train_loss=2.926084

Batch 259910, train_perplexity=18.654428, train_loss=2.9260836

Batch 259920, train_perplexity=18.654432, train_loss=2.9260838

Batch 259930, train_perplexity=18.654428, train_loss=2.9260836

Batch 259940, train_perplexity=18.654428, train_loss=2.9260836

Batch 259950, train_perplexity=18.654428, train_loss=2.9260836

Batch 259960, train_perplexity=18.654438, train_loss=2.926084

Batch 259970, train_perplexity=18.654432, train_loss=2.9260838

Batch 259980, train_perplexity=18.654432, train_loss=2.9260838

Batch 259990, train_perplexity=18.654428, train_loss=2.9260836

Batch 260000, train_perplexity=18.654428, train_loss=2.9260836

Batch 260010, train_perplexity=18.654428, train_loss=2.9260836

Batch 260020, train_perplexity=18.654428, train_loss=2.9260836

Batch 260030, train_perplexity=18.654432, train_loss=2.9260838

Batch 260040, train_perplexity=18.654428, train_loss=2.9260836

Batch 260050, train_perplexity=18.654428, train_loss=2.9260836

Batch 260060, train_perplexity=18.654428, train_loss=2.9260836

Batch 260070, train_perplexity=18.654432, train_loss=2.9260838

Batch 260080, train_perplexity=18.654428, train_loss=2.9260836

Batch 260090, train_perplexity=18.654428, train_loss=2.9260836

Batch 260100, train_perplexity=18.654428, train_loss=2.9260836

Batch 260110, train_perplexity=18.654432, train_loss=2.9260838

Batch 260120, train_perplexity=18.654432, train_loss=2.9260838

Batch 260130, train_perplexity=18.654428, train_loss=2.9260836

Batch 260140, train_perplexity=18.654425, train_loss=2.9260833

Batch 260150, train_perplexity=18.654428, train_loss=2.9260836

Batch 260160, train_perplexity=18.654428, train_loss=2.9260836

Batch 260170, train_perplexity=18.654419, train_loss=2.926083

Batch 260180, train_perplexity=18.654428, train_loss=2.9260836

Batch 260190, train_perplexity=18.654428, train_loss=2.9260836

Batch 260200, train_perplexity=18.654428, train_loss=2.9260836

Batch 260210, train_perplexity=18.654425, train_loss=2.9260833

Batch 260220, train_perplexity=18.654425, train_loss=2.9260833

Batch 260230, train_perplexity=18.654419, train_loss=2.926083

Batch 260240, train_perplexity=18.654419, train_loss=2.926083

Batch 260250, train_perplexity=18.654419, train_loss=2.926083

Batch 260260, train_perplexity=18.654425, train_loss=2.9260833

Batch 260270, train_perplexity=18.654428, train_loss=2.9260836

Batch 260280, train_perplexity=18.654419, train_loss=2.926083

Batch 260290, train_perplexity=18.654419, train_loss=2.926083

Batch 260300, train_perplexity=18.654419, train_loss=2.926083

Batch 260310, train_perplexity=18.654425, train_loss=2.9260833

Batch 260320, train_perplexity=18.654419, train_loss=2.926083

Batch 260330, train_perplexity=18.654419, train_loss=2.926083

Batch 260340, train_perplexity=18.654419, train_loss=2.926083

Batch 260350, train_perplexity=18.654425, train_loss=2.9260833

Batch 260360, train_perplexity=18.654419, train_loss=2.926083

Batch 260370, train_perplexity=18.654419, train_loss=2.926083

Batch 260380, train_perplexity=18.654419, train_loss=2.926083

Batch 260390, train_perplexity=18.654419, train_loss=2.926083

Batch 260400, train_perplexity=18.654419, train_loss=2.926083

Batch 260410, train_perplexity=18.654419, train_loss=2.926083

Batch 260420, train_perplexity=18.654419, train_loss=2.926083

Batch 260430, train_perplexity=18.654419, train_loss=2.926083

Batch 260440, train_perplexity=18.654419, train_loss=2.926083

Batch 260450, train_perplexity=18.654419, train_loss=2.926083

Batch 260460, train_perplexity=18.654419, train_loss=2.926083

Batch 260470, train_perplexity=18.654419, train_loss=2.926083

Batch 260480, train_perplexity=18.654419, train_loss=2.926083

Batch 260490, train_perplexity=18.654419, train_loss=2.926083

Batch 260500, train_perplexity=18.654419, train_loss=2.926083

Batch 260510, train_perplexity=18.654415, train_loss=2.9260828

Batch 260520, train_perplexity=18.654419, train_loss=2.926083

Batch 260530, train_perplexity=18.654419, train_loss=2.926083

Batch 260540, train_perplexity=18.654419, train_loss=2.926083

Batch 260550, train_perplexity=18.654415, train_loss=2.9260828

Batch 260560, train_perplexity=18.654419, train_loss=2.926083

Batch 260570, train_perplexity=18.654411, train_loss=2.9260826

Batch 260580, train_perplexity=18.654419, train_loss=2.926083

Batch 260590, train_perplexity=18.654411, train_loss=2.9260826

Batch 260600, train_perplexity=18.654415, train_loss=2.9260828

Batch 260610, train_perplexity=18.654419, train_loss=2.926083

Batch 260620, train_perplexity=18.654415, train_loss=2.9260828

Batch 260630, train_perplexity=18.654415, train_loss=2.9260828

Batch 260640, train_perplexity=18.654411, train_loss=2.9260826

Batch 260650, train_perplexity=18.654411, train_loss=2.9260826

Batch 260660, train_perplexity=18.654411, train_loss=2.9260826

Batch 260670, train_perplexity=18.654411, train_loss=2.9260826

Batch 260680, train_perplexity=18.654415, train_loss=2.9260828

Batch 260690, train_perplexity=18.654415, train_loss=2.9260828

Batch 260700, train_perplexity=18.654411, train_loss=2.9260826

Batch 260710, train_perplexity=18.654411, train_loss=2.9260826

Batch 260720, train_perplexity=18.654411, train_loss=2.9260826

Batch 260730, train_perplexity=18.654411, train_loss=2.9260826

Batch 260740, train_perplexity=18.654411, train_loss=2.9260826

Batch 260750, train_perplexity=18.654411, train_loss=2.9260826

Batch 260760, train_perplexity=18.654411, train_loss=2.9260826

Batch 260770, train_perplexity=18.654411, train_loss=2.9260826

Batch 260780, train_perplexity=18.654411, train_loss=2.9260826

Batch 260790, train_perplexity=18.654411, train_loss=2.9260826

Batch 260800, train_perplexity=18.654411, train_loss=2.9260826

Batch 260810, train_perplexity=18.654411, train_loss=2.9260826

Batch 260820, train_perplexity=18.654406, train_loss=2.9260824

Batch 260830, train_perplexity=18.654402, train_loss=2.9260821

Batch 260840, train_perplexity=18.654406, train_loss=2.9260824

Batch 260850, train_perplexity=18.654411, train_loss=2.9260826

Batch 260860, train_perplexity=18.654406, train_loss=2.9260824

Batch 260870, train_perplexity=18.654406, train_loss=2.9260824

Batch 260880, train_perplexity=18.654402, train_loss=2.9260821

Batch 260890, train_perplexity=18.654406, train_loss=2.9260824

Batch 260900, train_perplexity=18.654406, train_loss=2.9260824

Batch 260910, train_perplexity=18.654406, train_loss=2.9260824

Batch 260920, train_perplexity=18.654402, train_loss=2.9260821

Batch 260930, train_perplexity=18.654402, train_loss=2.9260821

Batch 260940, train_perplexity=18.654402, train_loss=2.9260821

Batch 260950, train_perplexity=18.654402, train_loss=2.9260821

Batch 260960, train_perplexity=18.654406, train_loss=2.9260824

Batch 260970, train_perplexity=18.654402, train_loss=2.9260821

Batch 260980, train_perplexity=18.654402, train_loss=2.9260821

Batch 260990, train_perplexity=18.654402, train_loss=2.9260821

Batch 261000, train_perplexity=18.654402, train_loss=2.9260821

Batch 261010, train_perplexity=18.654402, train_loss=2.9260821

Batch 261020, train_perplexity=18.654406, train_loss=2.9260824

Batch 261030, train_perplexity=18.654406, train_loss=2.9260824

Batch 261040, train_perplexity=18.654402, train_loss=2.9260821

Batch 261050, train_perplexity=18.654402, train_loss=2.9260821

Batch 261060, train_perplexity=18.654402, train_loss=2.9260821

Batch 261070, train_perplexity=18.654402, train_loss=2.9260821

Batch 261080, train_perplexity=18.654402, train_loss=2.9260821

Batch 261090, train_perplexity=18.654402, train_loss=2.9260821

Batch 261100, train_perplexity=18.654392, train_loss=2.9260817

Batch 261110, train_perplexity=18.654402, train_loss=2.9260821

Batch 261120, train_perplexity=18.654392, train_loss=2.9260817

Batch 261130, train_perplexity=18.654398, train_loss=2.926082

Batch 261140, train_perplexity=18.654392, train_loss=2.9260817

Batch 261150, train_perplexity=18.654402, train_loss=2.9260821

Batch 261160, train_perplexity=18.654402, train_loss=2.9260821

Batch 261170, train_perplexity=18.654398, train_loss=2.926082
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 261180, train_perplexity=18.654402, train_loss=2.9260821

Batch 261190, train_perplexity=18.654392, train_loss=2.9260817

Batch 261200, train_perplexity=18.654402, train_loss=2.9260821

Batch 261210, train_perplexity=18.654398, train_loss=2.926082

Batch 261220, train_perplexity=18.654392, train_loss=2.9260817

Batch 261230, train_perplexity=18.654392, train_loss=2.9260817

Batch 261240, train_perplexity=18.654392, train_loss=2.9260817

Batch 261250, train_perplexity=18.654392, train_loss=2.9260817

Batch 261260, train_perplexity=18.654392, train_loss=2.9260817

Batch 261270, train_perplexity=18.654392, train_loss=2.9260817

Batch 261280, train_perplexity=18.654398, train_loss=2.926082

Batch 261290, train_perplexity=18.654392, train_loss=2.9260817

Batch 261300, train_perplexity=18.654392, train_loss=2.9260817

Batch 261310, train_perplexity=18.654392, train_loss=2.9260817

Batch 261320, train_perplexity=18.654392, train_loss=2.9260817

Batch 261330, train_perplexity=18.654392, train_loss=2.9260817

Batch 261340, train_perplexity=18.654392, train_loss=2.9260817

Batch 261350, train_perplexity=18.654398, train_loss=2.926082

Batch 261360, train_perplexity=18.654392, train_loss=2.9260817

Batch 261370, train_perplexity=18.654392, train_loss=2.9260817

Batch 261380, train_perplexity=18.654392, train_loss=2.9260817

Batch 261390, train_perplexity=18.654398, train_loss=2.926082

Batch 261400, train_perplexity=18.654392, train_loss=2.9260817

Batch 261410, train_perplexity=18.654392, train_loss=2.9260817

Batch 261420, train_perplexity=18.654392, train_loss=2.9260817

Batch 261430, train_perplexity=18.654392, train_loss=2.9260817

Batch 261440, train_perplexity=18.654392, train_loss=2.9260817

Batch 261450, train_perplexity=18.654388, train_loss=2.9260814

Batch 261460, train_perplexity=18.654392, train_loss=2.9260817

Batch 261470, train_perplexity=18.654385, train_loss=2.9260812

Batch 261480, train_perplexity=18.654392, train_loss=2.9260817

Batch 261490, train_perplexity=18.654392, train_loss=2.9260817

Batch 261500, train_perplexity=18.654385, train_loss=2.9260812

Batch 261510, train_perplexity=18.654385, train_loss=2.9260812

Batch 261520, train_perplexity=18.654388, train_loss=2.9260814

Batch 261530, train_perplexity=18.654385, train_loss=2.9260812

Batch 261540, train_perplexity=18.654392, train_loss=2.9260817

Batch 261550, train_perplexity=18.654392, train_loss=2.9260817

Batch 261560, train_perplexity=18.654385, train_loss=2.9260812

Batch 261570, train_perplexity=18.654385, train_loss=2.9260812

Batch 261580, train_perplexity=18.654385, train_loss=2.9260812

Batch 261590, train_perplexity=18.654385, train_loss=2.9260812

Batch 261600, train_perplexity=18.654385, train_loss=2.9260812

Batch 261610, train_perplexity=18.654388, train_loss=2.9260814

Batch 261620, train_perplexity=18.654385, train_loss=2.9260812

Batch 261630, train_perplexity=18.654385, train_loss=2.9260812

Batch 261640, train_perplexity=18.654385, train_loss=2.9260812

Batch 261650, train_perplexity=18.654385, train_loss=2.9260812

Batch 261660, train_perplexity=18.654379, train_loss=2.926081

Batch 261670, train_perplexity=18.654385, train_loss=2.9260812

Batch 261680, train_perplexity=18.654375, train_loss=2.9260807

Batch 261690, train_perplexity=18.654385, train_loss=2.9260812

Batch 261700, train_perplexity=18.654385, train_loss=2.9260812

Batch 261710, train_perplexity=18.654385, train_loss=2.9260812

Batch 261720, train_perplexity=18.654385, train_loss=2.9260812

Batch 261730, train_perplexity=18.654375, train_loss=2.9260807

Batch 261740, train_perplexity=18.654379, train_loss=2.926081

Batch 261750, train_perplexity=18.654385, train_loss=2.9260812

Batch 261760, train_perplexity=18.654379, train_loss=2.926081

Batch 261770, train_perplexity=18.654379, train_loss=2.926081

Batch 261780, train_perplexity=18.654385, train_loss=2.9260812

Batch 261790, train_perplexity=18.654379, train_loss=2.926081

Batch 261800, train_perplexity=18.654375, train_loss=2.9260807

Batch 261810, train_perplexity=18.654385, train_loss=2.9260812

Batch 261820, train_perplexity=18.654375, train_loss=2.9260807

Batch 261830, train_perplexity=18.654375, train_loss=2.9260807

Batch 261840, train_perplexity=18.654375, train_loss=2.9260807

Batch 261850, train_perplexity=18.654379, train_loss=2.926081

Batch 261860, train_perplexity=18.654385, train_loss=2.9260812

Batch 261870, train_perplexity=18.654375, train_loss=2.9260807

Batch 261880, train_perplexity=18.654385, train_loss=2.9260812

Batch 261890, train_perplexity=18.654385, train_loss=2.9260812

Batch 261900, train_perplexity=18.654375, train_loss=2.9260807

Batch 261910, train_perplexity=18.654375, train_loss=2.9260807

Batch 261920, train_perplexity=18.654379, train_loss=2.926081

Batch 261930, train_perplexity=18.654375, train_loss=2.9260807

Batch 261940, train_perplexity=18.654379, train_loss=2.926081

Batch 261950, train_perplexity=18.654375, train_loss=2.9260807

Batch 261960, train_perplexity=18.654375, train_loss=2.9260807

Batch 261970, train_perplexity=18.654375, train_loss=2.9260807

Batch 261980, train_perplexity=18.654385, train_loss=2.9260812

Batch 261990, train_perplexity=18.654375, train_loss=2.9260807

Batch 262000, train_perplexity=18.654371, train_loss=2.9260805

Batch 262010, train_perplexity=18.654375, train_loss=2.9260807

Batch 262020, train_perplexity=18.654375, train_loss=2.9260807

Batch 262030, train_perplexity=18.654371, train_loss=2.9260805

Batch 262040, train_perplexity=18.654375, train_loss=2.9260807

Batch 262050, train_perplexity=18.654379, train_loss=2.926081

Batch 262060, train_perplexity=18.654371, train_loss=2.9260805

Batch 262070, train_perplexity=18.654375, train_loss=2.9260807

Batch 262080, train_perplexity=18.654371, train_loss=2.9260805

Batch 262090, train_perplexity=18.654375, train_loss=2.9260807

Batch 262100, train_perplexity=18.654371, train_loss=2.9260805

Batch 262110, train_perplexity=18.654366, train_loss=2.9260802

Batch 262120, train_perplexity=18.654366, train_loss=2.9260802

Batch 262130, train_perplexity=18.654375, train_loss=2.9260807

Batch 262140, train_perplexity=18.654375, train_loss=2.9260807

Batch 262150, train_perplexity=18.654366, train_loss=2.9260802

Batch 262160, train_perplexity=18.654366, train_loss=2.9260802

Batch 262170, train_perplexity=18.654375, train_loss=2.9260807

Batch 262180, train_perplexity=18.654366, train_loss=2.9260802

Batch 262190, train_perplexity=18.654371, train_loss=2.9260805

Batch 262200, train_perplexity=18.654371, train_loss=2.9260805

Batch 262210, train_perplexity=18.654366, train_loss=2.9260802

Batch 262220, train_perplexity=18.654366, train_loss=2.9260802

Batch 262230, train_perplexity=18.654366, train_loss=2.9260802

Batch 262240, train_perplexity=18.654371, train_loss=2.9260805

Batch 262250, train_perplexity=18.654366, train_loss=2.9260802

Batch 262260, train_perplexity=18.654366, train_loss=2.9260802

Batch 262270, train_perplexity=18.654366, train_loss=2.9260802

Batch 262280, train_perplexity=18.654366, train_loss=2.9260802

Batch 262290, train_perplexity=18.654366, train_loss=2.9260802

Batch 262300, train_perplexity=18.654366, train_loss=2.9260802

Batch 262310, train_perplexity=18.654366, train_loss=2.9260802

Batch 262320, train_perplexity=18.654362, train_loss=2.92608

Batch 262330, train_perplexity=18.654366, train_loss=2.9260802

Batch 262340, train_perplexity=18.654366, train_loss=2.9260802

Batch 262350, train_perplexity=18.654366, train_loss=2.9260802

Batch 262360, train_perplexity=18.654362, train_loss=2.92608

Batch 262370, train_perplexity=18.654366, train_loss=2.9260802

Batch 262380, train_perplexity=18.654358, train_loss=2.9260798

Batch 262390, train_perplexity=18.654366, train_loss=2.9260802

Batch 262400, train_perplexity=18.654362, train_loss=2.92608

Batch 262410, train_perplexity=18.654362, train_loss=2.92608

Batch 262420, train_perplexity=18.654366, train_loss=2.9260802

Batch 262430, train_perplexity=18.654358, train_loss=2.9260798

Batch 262440, train_perplexity=18.654366, train_loss=2.9260802

Batch 262450, train_perplexity=18.654358, train_loss=2.9260798

Batch 262460, train_perplexity=18.654358, train_loss=2.9260798
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 262470, train_perplexity=18.654366, train_loss=2.9260802

Batch 262480, train_perplexity=18.654366, train_loss=2.9260802

Batch 262490, train_perplexity=18.654358, train_loss=2.9260798

Batch 262500, train_perplexity=18.654358, train_loss=2.9260798

Batch 262510, train_perplexity=18.654358, train_loss=2.9260798

Batch 262520, train_perplexity=18.654362, train_loss=2.92608

Batch 262530, train_perplexity=18.654362, train_loss=2.92608

Batch 262540, train_perplexity=18.654358, train_loss=2.9260798

Batch 262550, train_perplexity=18.654362, train_loss=2.92608

Batch 262560, train_perplexity=18.654358, train_loss=2.9260798

Batch 262570, train_perplexity=18.654358, train_loss=2.9260798

Batch 262580, train_perplexity=18.654358, train_loss=2.9260798

Batch 262590, train_perplexity=18.654358, train_loss=2.9260798

Batch 262600, train_perplexity=18.654358, train_loss=2.9260798

Batch 262610, train_perplexity=18.654358, train_loss=2.9260798

Batch 262620, train_perplexity=18.654358, train_loss=2.9260798

Batch 262630, train_perplexity=18.654358, train_loss=2.9260798

Batch 262640, train_perplexity=18.654358, train_loss=2.9260798

Batch 262650, train_perplexity=18.654358, train_loss=2.9260798

Batch 262660, train_perplexity=18.654358, train_loss=2.9260798

Batch 262670, train_perplexity=18.654358, train_loss=2.9260798

Batch 262680, train_perplexity=18.654358, train_loss=2.9260798

Batch 262690, train_perplexity=18.654358, train_loss=2.9260798

Batch 262700, train_perplexity=18.654358, train_loss=2.9260798

Batch 262710, train_perplexity=18.654348, train_loss=2.9260793

Batch 262720, train_perplexity=18.654358, train_loss=2.9260798

Batch 262730, train_perplexity=18.654348, train_loss=2.9260793

Batch 262740, train_perplexity=18.654348, train_loss=2.9260793

Batch 262750, train_perplexity=18.654358, train_loss=2.9260798

Batch 262760, train_perplexity=18.654352, train_loss=2.9260795

Batch 262770, train_perplexity=18.654348, train_loss=2.9260793

Batch 262780, train_perplexity=18.654358, train_loss=2.9260798

Batch 262790, train_perplexity=18.654352, train_loss=2.9260795

Batch 262800, train_perplexity=18.654348, train_loss=2.9260793

Batch 262810, train_perplexity=18.654352, train_loss=2.9260795

Batch 262820, train_perplexity=18.654348, train_loss=2.9260793

Batch 262830, train_perplexity=18.654348, train_loss=2.9260793

Batch 262840, train_perplexity=18.654348, train_loss=2.9260793

Batch 262850, train_perplexity=18.654348, train_loss=2.9260793

Batch 262860, train_perplexity=18.654345, train_loss=2.926079

Batch 262870, train_perplexity=18.654348, train_loss=2.9260793

Batch 262880, train_perplexity=18.654348, train_loss=2.9260793

Batch 262890, train_perplexity=18.654348, train_loss=2.9260793

Batch 262900, train_perplexity=18.654348, train_loss=2.9260793

Batch 262910, train_perplexity=18.654348, train_loss=2.9260793

Batch 262920, train_perplexity=18.654348, train_loss=2.9260793

Batch 262930, train_perplexity=18.654348, train_loss=2.9260793

Batch 262940, train_perplexity=18.654348, train_loss=2.9260793

Batch 262950, train_perplexity=18.654339, train_loss=2.9260788

Batch 262960, train_perplexity=18.654348, train_loss=2.9260793

Batch 262970, train_perplexity=18.654348, train_loss=2.9260793

Batch 262980, train_perplexity=18.654348, train_loss=2.9260793

Batch 262990, train_perplexity=18.654348, train_loss=2.9260793

Batch 263000, train_perplexity=18.654348, train_loss=2.9260793

Batch 263010, train_perplexity=18.654339, train_loss=2.9260788

Batch 263020, train_perplexity=18.654345, train_loss=2.926079

Batch 263030, train_perplexity=18.654348, train_loss=2.9260793

Batch 263040, train_perplexity=18.654339, train_loss=2.9260788

Batch 263050, train_perplexity=18.654339, train_loss=2.9260788

Batch 263060, train_perplexity=18.654339, train_loss=2.9260788

Batch 263070, train_perplexity=18.654339, train_loss=2.9260788

Batch 263080, train_perplexity=18.654345, train_loss=2.926079

Batch 263090, train_perplexity=18.654345, train_loss=2.926079

Batch 263100, train_perplexity=18.654345, train_loss=2.926079

Batch 263110, train_perplexity=18.654345, train_loss=2.926079

Batch 263120, train_perplexity=18.654339, train_loss=2.9260788

Batch 263130, train_perplexity=18.654339, train_loss=2.9260788

Batch 263140, train_perplexity=18.654339, train_loss=2.9260788

Batch 263150, train_perplexity=18.654339, train_loss=2.9260788

Batch 263160, train_perplexity=18.654345, train_loss=2.926079

Batch 263170, train_perplexity=18.654339, train_loss=2.9260788

Batch 263180, train_perplexity=18.654339, train_loss=2.9260788

Batch 263190, train_perplexity=18.654339, train_loss=2.9260788

Batch 263200, train_perplexity=18.654339, train_loss=2.9260788

Batch 263210, train_perplexity=18.654339, train_loss=2.9260788

Batch 263220, train_perplexity=18.654345, train_loss=2.926079

Batch 263230, train_perplexity=18.654339, train_loss=2.9260788

Batch 263240, train_perplexity=18.654339, train_loss=2.9260788

Batch 263250, train_perplexity=18.654345, train_loss=2.926079

Batch 263260, train_perplexity=18.654339, train_loss=2.9260788

Batch 263270, train_perplexity=18.654339, train_loss=2.9260788

Batch 263280, train_perplexity=18.654339, train_loss=2.9260788

Batch 263290, train_perplexity=18.654335, train_loss=2.9260786

Batch 263300, train_perplexity=18.654339, train_loss=2.9260788

Batch 263310, train_perplexity=18.654335, train_loss=2.9260786

Batch 263320, train_perplexity=18.654339, train_loss=2.9260788

Batch 263330, train_perplexity=18.654339, train_loss=2.9260788

Batch 263340, train_perplexity=18.654335, train_loss=2.9260786

Batch 263350, train_perplexity=18.654335, train_loss=2.9260786

Batch 263360, train_perplexity=18.654339, train_loss=2.9260788

Batch 263370, train_perplexity=18.654335, train_loss=2.9260786

Batch 263380, train_perplexity=18.654339, train_loss=2.9260788

Batch 263390, train_perplexity=18.654331, train_loss=2.9260783

Batch 263400, train_perplexity=18.654335, train_loss=2.9260786

Batch 263410, train_perplexity=18.654331, train_loss=2.9260783

Batch 263420, train_perplexity=18.654331, train_loss=2.9260783

Batch 263430, train_perplexity=18.654331, train_loss=2.9260783

Batch 263440, train_perplexity=18.654331, train_loss=2.9260783

Batch 263450, train_perplexity=18.654335, train_loss=2.9260786

Batch 263460, train_perplexity=18.654331, train_loss=2.9260783

Batch 263470, train_perplexity=18.654335, train_loss=2.9260786

Batch 263480, train_perplexity=18.654331, train_loss=2.9260783

Batch 263490, train_perplexity=18.654331, train_loss=2.9260783

Batch 263500, train_perplexity=18.654331, train_loss=2.9260783

Batch 263510, train_perplexity=18.654331, train_loss=2.9260783

Batch 263520, train_perplexity=18.654331, train_loss=2.9260783

Batch 263530, train_perplexity=18.654331, train_loss=2.9260783

Batch 263540, train_perplexity=18.654331, train_loss=2.9260783

Batch 263550, train_perplexity=18.654331, train_loss=2.9260783

Batch 263560, train_perplexity=18.654331, train_loss=2.9260783

Batch 263570, train_perplexity=18.654331, train_loss=2.9260783

Batch 263580, train_perplexity=18.654331, train_loss=2.9260783

Batch 263590, train_perplexity=18.654331, train_loss=2.9260783

Batch 263600, train_perplexity=18.654325, train_loss=2.926078

Batch 263610, train_perplexity=18.654331, train_loss=2.9260783

Batch 263620, train_perplexity=18.654331, train_loss=2.9260783

Batch 263630, train_perplexity=18.654325, train_loss=2.926078

Batch 263640, train_perplexity=18.654322, train_loss=2.9260778

Batch 263650, train_perplexity=18.654322, train_loss=2.9260778

Batch 263660, train_perplexity=18.654322, train_loss=2.9260778

Batch 263670, train_perplexity=18.654322, train_loss=2.9260778

Batch 263680, train_perplexity=18.654322, train_loss=2.9260778

Batch 263690, train_perplexity=18.654325, train_loss=2.926078

Batch 263700, train_perplexity=18.654322, train_loss=2.9260778

Batch 263710, train_perplexity=18.654331, train_loss=2.9260783

Batch 263720, train_perplexity=18.654325, train_loss=2.926078

Batch 263730, train_perplexity=18.654325, train_loss=2.926078

Batch 263740, train_perplexity=18.654325, train_loss=2.926078

Batch 263750, train_perplexity=18.654331, train_loss=2.9260783
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 263760, train_perplexity=18.654325, train_loss=2.926078

Batch 263770, train_perplexity=18.654322, train_loss=2.9260778

Batch 263780, train_perplexity=18.654322, train_loss=2.9260778

Batch 263790, train_perplexity=18.654322, train_loss=2.9260778

Batch 263800, train_perplexity=18.654322, train_loss=2.9260778

Batch 263810, train_perplexity=18.654331, train_loss=2.9260783

Batch 263820, train_perplexity=18.654322, train_loss=2.9260778

Batch 263830, train_perplexity=18.654325, train_loss=2.926078

Batch 263840, train_perplexity=18.654322, train_loss=2.9260778

Batch 263850, train_perplexity=18.654322, train_loss=2.9260778

Batch 263860, train_perplexity=18.654318, train_loss=2.9260776

Batch 263870, train_perplexity=18.654322, train_loss=2.9260778

Batch 263880, train_perplexity=18.654322, train_loss=2.9260778

Batch 263890, train_perplexity=18.654322, train_loss=2.9260778

Batch 263900, train_perplexity=18.654322, train_loss=2.9260778

Batch 263910, train_perplexity=18.654325, train_loss=2.926078

Batch 263920, train_perplexity=18.654318, train_loss=2.9260776

Batch 263930, train_perplexity=18.654318, train_loss=2.9260776

Batch 263940, train_perplexity=18.654322, train_loss=2.9260778

Batch 263950, train_perplexity=18.654322, train_loss=2.9260778

Batch 263960, train_perplexity=18.654322, train_loss=2.9260778

Batch 263970, train_perplexity=18.654322, train_loss=2.9260778

Batch 263980, train_perplexity=18.654312, train_loss=2.9260774

Batch 263990, train_perplexity=18.654312, train_loss=2.9260774

Batch 264000, train_perplexity=18.654322, train_loss=2.9260778

Batch 264010, train_perplexity=18.654312, train_loss=2.9260774

Batch 264020, train_perplexity=18.654322, train_loss=2.9260778

Batch 264030, train_perplexity=18.654312, train_loss=2.9260774

Batch 264040, train_perplexity=18.654318, train_loss=2.9260776

Batch 264050, train_perplexity=18.654312, train_loss=2.9260774

Batch 264060, train_perplexity=18.654312, train_loss=2.9260774

Batch 264070, train_perplexity=18.654312, train_loss=2.9260774

Batch 264080, train_perplexity=18.654322, train_loss=2.9260778

Batch 264090, train_perplexity=18.654312, train_loss=2.9260774

Batch 264100, train_perplexity=18.654312, train_loss=2.9260774

Batch 264110, train_perplexity=18.654312, train_loss=2.9260774

Batch 264120, train_perplexity=18.654312, train_loss=2.9260774

Batch 264130, train_perplexity=18.654318, train_loss=2.9260776

Batch 264140, train_perplexity=18.654312, train_loss=2.9260774

Batch 264150, train_perplexity=18.654312, train_loss=2.9260774

Batch 264160, train_perplexity=18.654312, train_loss=2.9260774

Batch 264170, train_perplexity=18.654312, train_loss=2.9260774

Batch 264180, train_perplexity=18.654312, train_loss=2.9260774

Batch 264190, train_perplexity=18.654312, train_loss=2.9260774

Batch 264200, train_perplexity=18.654312, train_loss=2.9260774

Batch 264210, train_perplexity=18.654312, train_loss=2.9260774

Batch 264220, train_perplexity=18.654312, train_loss=2.9260774

Batch 264230, train_perplexity=18.654305, train_loss=2.926077

Batch 264240, train_perplexity=18.654312, train_loss=2.9260774

Batch 264250, train_perplexity=18.654308, train_loss=2.9260771

Batch 264260, train_perplexity=18.654312, train_loss=2.9260774

Batch 264270, train_perplexity=18.654312, train_loss=2.9260774

Batch 264280, train_perplexity=18.654305, train_loss=2.926077

Batch 264290, train_perplexity=18.654312, train_loss=2.9260774

Batch 264300, train_perplexity=18.654308, train_loss=2.9260771

Batch 264310, train_perplexity=18.654308, train_loss=2.9260771

Batch 264320, train_perplexity=18.654308, train_loss=2.9260771

Batch 264330, train_perplexity=18.654305, train_loss=2.926077

Batch 264340, train_perplexity=18.654305, train_loss=2.926077

Batch 264350, train_perplexity=18.654305, train_loss=2.926077

Batch 264360, train_perplexity=18.654305, train_loss=2.926077

Batch 264370, train_perplexity=18.654305, train_loss=2.926077

Batch 264380, train_perplexity=18.654305, train_loss=2.926077

Batch 264390, train_perplexity=18.654305, train_loss=2.926077

Batch 264400, train_perplexity=18.654305, train_loss=2.926077

Batch 264410, train_perplexity=18.654312, train_loss=2.9260774

Batch 264420, train_perplexity=18.654305, train_loss=2.926077

Batch 264430, train_perplexity=18.654305, train_loss=2.926077

Batch 264440, train_perplexity=18.654305, train_loss=2.926077

Batch 264450, train_perplexity=18.654305, train_loss=2.926077

Batch 264460, train_perplexity=18.654305, train_loss=2.926077

Batch 264470, train_perplexity=18.654305, train_loss=2.926077

Batch 264480, train_perplexity=18.654305, train_loss=2.926077

Batch 264490, train_perplexity=18.654308, train_loss=2.9260771

Batch 264500, train_perplexity=18.654305, train_loss=2.926077

Batch 264510, train_perplexity=18.654305, train_loss=2.926077

Batch 264520, train_perplexity=18.654299, train_loss=2.9260767

Batch 264530, train_perplexity=18.654305, train_loss=2.926077

Batch 264540, train_perplexity=18.654305, train_loss=2.926077

Batch 264550, train_perplexity=18.654305, train_loss=2.926077

Batch 264560, train_perplexity=18.654305, train_loss=2.926077

Batch 264570, train_perplexity=18.654305, train_loss=2.926077

Batch 264580, train_perplexity=18.654305, train_loss=2.926077

Batch 264590, train_perplexity=18.654305, train_loss=2.926077

Batch 264600, train_perplexity=18.654299, train_loss=2.9260767

Batch 264610, train_perplexity=18.654299, train_loss=2.9260767

Batch 264620, train_perplexity=18.654295, train_loss=2.9260764

Batch 264630, train_perplexity=18.654299, train_loss=2.9260767

Batch 264640, train_perplexity=18.654305, train_loss=2.926077

Batch 264650, train_perplexity=18.654295, train_loss=2.9260764

Batch 264660, train_perplexity=18.654299, train_loss=2.9260767

Batch 264670, train_perplexity=18.654299, train_loss=2.9260767

Batch 264680, train_perplexity=18.654295, train_loss=2.9260764

Batch 264690, train_perplexity=18.654295, train_loss=2.9260764

Batch 264700, train_perplexity=18.654295, train_loss=2.9260764

Batch 264710, train_perplexity=18.654295, train_loss=2.9260764

Batch 264720, train_perplexity=18.654295, train_loss=2.9260764

Batch 264730, train_perplexity=18.654295, train_loss=2.9260764

Batch 264740, train_perplexity=18.654295, train_loss=2.9260764

Batch 264750, train_perplexity=18.654299, train_loss=2.9260767

Batch 264760, train_perplexity=18.654295, train_loss=2.9260764

Batch 264770, train_perplexity=18.654295, train_loss=2.9260764

Batch 264780, train_perplexity=18.654295, train_loss=2.9260764

Batch 264790, train_perplexity=18.654295, train_loss=2.9260764

Batch 264800, train_perplexity=18.654295, train_loss=2.9260764

Batch 264810, train_perplexity=18.654299, train_loss=2.9260767

Batch 264820, train_perplexity=18.654295, train_loss=2.9260764

Batch 264830, train_perplexity=18.654291, train_loss=2.9260762

Batch 264840, train_perplexity=18.654295, train_loss=2.9260764

Batch 264850, train_perplexity=18.654295, train_loss=2.9260764

Batch 264860, train_perplexity=18.654299, train_loss=2.9260767

Batch 264870, train_perplexity=18.654295, train_loss=2.9260764

Batch 264880, train_perplexity=18.654291, train_loss=2.9260762

Batch 264890, train_perplexity=18.654291, train_loss=2.9260762

Batch 264900, train_perplexity=18.654295, train_loss=2.9260764

Batch 264910, train_perplexity=18.654291, train_loss=2.9260762

Batch 264920, train_perplexity=18.654295, train_loss=2.9260764

Batch 264930, train_perplexity=18.654291, train_loss=2.9260762

Batch 264940, train_perplexity=18.654295, train_loss=2.9260764

Batch 264950, train_perplexity=18.654295, train_loss=2.9260764

Batch 264960, train_perplexity=18.654295, train_loss=2.9260764

Batch 264970, train_perplexity=18.654285, train_loss=2.926076

Batch 264980, train_perplexity=18.654285, train_loss=2.926076

Batch 264990, train_perplexity=18.654285, train_loss=2.926076

Batch 265000, train_perplexity=18.654285, train_loss=2.926076

Batch 265010, train_perplexity=18.654291, train_loss=2.9260762

Batch 265020, train_perplexity=18.654291, train_loss=2.9260762

Batch 265030, train_perplexity=18.654291, train_loss=2.9260762

Batch 265040, train_perplexity=18.654285, train_loss=2.926076
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 265050, train_perplexity=18.654285, train_loss=2.926076

Batch 265060, train_perplexity=18.654285, train_loss=2.926076

Batch 265070, train_perplexity=18.654285, train_loss=2.926076

Batch 265080, train_perplexity=18.654285, train_loss=2.926076

Batch 265090, train_perplexity=18.654285, train_loss=2.926076

Batch 265100, train_perplexity=18.654291, train_loss=2.9260762

Batch 265110, train_perplexity=18.654285, train_loss=2.926076

Batch 265120, train_perplexity=18.654285, train_loss=2.926076

Batch 265130, train_perplexity=18.654285, train_loss=2.926076

Batch 265140, train_perplexity=18.654282, train_loss=2.9260757

Batch 265150, train_perplexity=18.654285, train_loss=2.926076

Batch 265160, train_perplexity=18.654285, train_loss=2.926076

Batch 265170, train_perplexity=18.654285, train_loss=2.926076

Batch 265180, train_perplexity=18.654285, train_loss=2.926076

Batch 265190, train_perplexity=18.654285, train_loss=2.926076

Batch 265200, train_perplexity=18.654285, train_loss=2.926076

Batch 265210, train_perplexity=18.654278, train_loss=2.9260755

Batch 265220, train_perplexity=18.654278, train_loss=2.9260755

Batch 265230, train_perplexity=18.654278, train_loss=2.9260755

Batch 265240, train_perplexity=18.654285, train_loss=2.926076

Batch 265250, train_perplexity=18.654278, train_loss=2.9260755

Batch 265260, train_perplexity=18.654282, train_loss=2.9260757

Batch 265270, train_perplexity=18.654278, train_loss=2.9260755

Batch 265280, train_perplexity=18.654282, train_loss=2.9260757

Batch 265290, train_perplexity=18.654282, train_loss=2.9260757

Batch 265300, train_perplexity=18.654285, train_loss=2.926076

Batch 265310, train_perplexity=18.654282, train_loss=2.9260757

Batch 265320, train_perplexity=18.654278, train_loss=2.9260755

Batch 265330, train_perplexity=18.654278, train_loss=2.9260755

Batch 265340, train_perplexity=18.654278, train_loss=2.9260755

Batch 265350, train_perplexity=18.654282, train_loss=2.9260757

Batch 265360, train_perplexity=18.654282, train_loss=2.9260757

Batch 265370, train_perplexity=18.654282, train_loss=2.9260757

Batch 265380, train_perplexity=18.654282, train_loss=2.9260757

Batch 265390, train_perplexity=18.654278, train_loss=2.9260755

Batch 265400, train_perplexity=18.654278, train_loss=2.9260755

Batch 265410, train_perplexity=18.654278, train_loss=2.9260755

Batch 265420, train_perplexity=18.654272, train_loss=2.9260752

Batch 265430, train_perplexity=18.654278, train_loss=2.9260755

Batch 265440, train_perplexity=18.654278, train_loss=2.9260755

Batch 265450, train_perplexity=18.654278, train_loss=2.9260755

Batch 265460, train_perplexity=18.654282, train_loss=2.9260757

Batch 265470, train_perplexity=18.654282, train_loss=2.9260757

Batch 265480, train_perplexity=18.654278, train_loss=2.9260755

Batch 265490, train_perplexity=18.654272, train_loss=2.9260752

Batch 265500, train_perplexity=18.654278, train_loss=2.9260755

Batch 265510, train_perplexity=18.654268, train_loss=2.926075

Batch 265520, train_perplexity=18.654268, train_loss=2.926075

Batch 265530, train_perplexity=18.654278, train_loss=2.9260755

Batch 265540, train_perplexity=18.654268, train_loss=2.926075

Batch 265550, train_perplexity=18.654272, train_loss=2.9260752

Batch 265560, train_perplexity=18.654278, train_loss=2.9260755

Batch 265570, train_perplexity=18.654268, train_loss=2.926075

Batch 265580, train_perplexity=18.654268, train_loss=2.926075

Batch 265590, train_perplexity=18.654278, train_loss=2.9260755

Batch 265600, train_perplexity=18.654268, train_loss=2.926075

Batch 265610, train_perplexity=18.654268, train_loss=2.926075

Batch 265620, train_perplexity=18.654268, train_loss=2.926075

Batch 265630, train_perplexity=18.654272, train_loss=2.9260752

Batch 265640, train_perplexity=18.654268, train_loss=2.926075

Batch 265650, train_perplexity=18.654268, train_loss=2.926075

Batch 265660, train_perplexity=18.654268, train_loss=2.926075

Batch 265670, train_perplexity=18.654272, train_loss=2.9260752

Batch 265680, train_perplexity=18.654272, train_loss=2.9260752

Batch 265690, train_perplexity=18.654264, train_loss=2.9260747

Batch 265700, train_perplexity=18.654272, train_loss=2.9260752

Batch 265710, train_perplexity=18.654268, train_loss=2.926075

Batch 265720, train_perplexity=18.654268, train_loss=2.926075

Batch 265730, train_perplexity=18.654268, train_loss=2.926075

Batch 265740, train_perplexity=18.654268, train_loss=2.926075

Batch 265750, train_perplexity=18.654268, train_loss=2.926075

Batch 265760, train_perplexity=18.654264, train_loss=2.9260747

Batch 265770, train_perplexity=18.654268, train_loss=2.926075

Batch 265780, train_perplexity=18.654268, train_loss=2.926075

Batch 265790, train_perplexity=18.654268, train_loss=2.926075

Batch 265800, train_perplexity=18.654268, train_loss=2.926075

Batch 265810, train_perplexity=18.654268, train_loss=2.926075

Batch 265820, train_perplexity=18.654268, train_loss=2.926075

Batch 265830, train_perplexity=18.654268, train_loss=2.926075

Batch 265840, train_perplexity=18.654264, train_loss=2.9260747

Batch 265850, train_perplexity=18.654264, train_loss=2.9260747

Batch 265860, train_perplexity=18.654264, train_loss=2.9260747

Batch 265870, train_perplexity=18.654264, train_loss=2.9260747

Batch 265880, train_perplexity=18.654268, train_loss=2.926075

Batch 265890, train_perplexity=18.654259, train_loss=2.9260745

Batch 265900, train_perplexity=18.654259, train_loss=2.9260745

Batch 265910, train_perplexity=18.654259, train_loss=2.9260745

Batch 265920, train_perplexity=18.654268, train_loss=2.926075

Batch 265930, train_perplexity=18.654259, train_loss=2.9260745

Batch 265940, train_perplexity=18.654259, train_loss=2.9260745

Batch 265950, train_perplexity=18.654259, train_loss=2.9260745

Batch 265960, train_perplexity=18.654259, train_loss=2.9260745

Batch 265970, train_perplexity=18.654259, train_loss=2.9260745

Batch 265980, train_perplexity=18.654259, train_loss=2.9260745

Batch 265990, train_perplexity=18.654259, train_loss=2.9260745

Batch 266000, train_perplexity=18.654259, train_loss=2.9260745

Batch 266010, train_perplexity=18.654259, train_loss=2.9260745

Batch 266020, train_perplexity=18.654264, train_loss=2.9260747

Batch 266030, train_perplexity=18.654259, train_loss=2.9260745

Batch 266040, train_perplexity=18.654259, train_loss=2.9260745

Batch 266050, train_perplexity=18.654259, train_loss=2.9260745

Batch 266060, train_perplexity=18.654259, train_loss=2.9260745

Batch 266070, train_perplexity=18.654259, train_loss=2.9260745

Batch 266080, train_perplexity=18.654259, train_loss=2.9260745

Batch 266090, train_perplexity=18.654259, train_loss=2.9260745

Batch 266100, train_perplexity=18.654259, train_loss=2.9260745

Batch 266110, train_perplexity=18.654259, train_loss=2.9260745

Batch 266120, train_perplexity=18.654259, train_loss=2.9260745

Batch 266130, train_perplexity=18.654255, train_loss=2.9260743

Batch 266140, train_perplexity=18.654259, train_loss=2.9260745

Batch 266150, train_perplexity=18.654259, train_loss=2.9260745

Batch 266160, train_perplexity=18.654259, train_loss=2.9260745

Batch 266170, train_perplexity=18.654255, train_loss=2.9260743

Batch 266180, train_perplexity=18.654251, train_loss=2.926074

Batch 266190, train_perplexity=18.654259, train_loss=2.9260745

Batch 266200, train_perplexity=18.654251, train_loss=2.926074

Batch 266210, train_perplexity=18.654259, train_loss=2.9260745

Batch 266220, train_perplexity=18.654251, train_loss=2.926074

Batch 266230, train_perplexity=18.654251, train_loss=2.926074

Batch 266240, train_perplexity=18.654255, train_loss=2.9260743

Batch 266250, train_perplexity=18.654255, train_loss=2.9260743

Batch 266260, train_perplexity=18.654251, train_loss=2.926074

Batch 266270, train_perplexity=18.654251, train_loss=2.926074

Batch 266280, train_perplexity=18.654255, train_loss=2.9260743

Batch 266290, train_perplexity=18.654251, train_loss=2.926074

Batch 266300, train_perplexity=18.654251, train_loss=2.926074

Batch 266310, train_perplexity=18.654251, train_loss=2.926074

Batch 266320, train_perplexity=18.654251, train_loss=2.926074

Batch 266330, train_perplexity=18.654251, train_loss=2.926074
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 266340, train_perplexity=18.654251, train_loss=2.926074

Batch 266350, train_perplexity=18.654245, train_loss=2.9260738

Batch 266360, train_perplexity=18.654251, train_loss=2.926074

Batch 266370, train_perplexity=18.654251, train_loss=2.926074

Batch 266380, train_perplexity=18.654251, train_loss=2.926074

Batch 266390, train_perplexity=18.654251, train_loss=2.926074

Batch 266400, train_perplexity=18.654245, train_loss=2.9260738

Batch 266410, train_perplexity=18.654251, train_loss=2.926074

Batch 266420, train_perplexity=18.654251, train_loss=2.926074

Batch 266430, train_perplexity=18.654251, train_loss=2.926074

Batch 266440, train_perplexity=18.654251, train_loss=2.926074

Batch 266450, train_perplexity=18.654251, train_loss=2.926074

Batch 266460, train_perplexity=18.654251, train_loss=2.926074

Batch 266470, train_perplexity=18.654242, train_loss=2.9260736

Batch 266480, train_perplexity=18.654245, train_loss=2.9260738

Batch 266490, train_perplexity=18.654245, train_loss=2.9260738

Batch 266500, train_perplexity=18.654251, train_loss=2.926074

Batch 266510, train_perplexity=18.654245, train_loss=2.9260738

Batch 266520, train_perplexity=18.654242, train_loss=2.9260736

Batch 266530, train_perplexity=18.654251, train_loss=2.926074

Batch 266540, train_perplexity=18.654245, train_loss=2.9260738

Batch 266550, train_perplexity=18.654242, train_loss=2.9260736

Batch 266560, train_perplexity=18.654242, train_loss=2.9260736

Batch 266570, train_perplexity=18.654242, train_loss=2.9260736

Batch 266580, train_perplexity=18.654242, train_loss=2.9260736

Batch 266590, train_perplexity=18.654242, train_loss=2.9260736

Batch 266600, train_perplexity=18.654242, train_loss=2.9260736

Batch 266610, train_perplexity=18.654242, train_loss=2.9260736

Batch 266620, train_perplexity=18.654242, train_loss=2.9260736

Batch 266630, train_perplexity=18.654242, train_loss=2.9260736

Batch 266640, train_perplexity=18.654242, train_loss=2.9260736

Batch 266650, train_perplexity=18.654242, train_loss=2.9260736

Batch 266660, train_perplexity=18.654242, train_loss=2.9260736

Batch 266670, train_perplexity=18.654242, train_loss=2.9260736

Batch 266680, train_perplexity=18.654242, train_loss=2.9260736

Batch 266690, train_perplexity=18.654242, train_loss=2.9260736

Batch 266700, train_perplexity=18.654242, train_loss=2.9260736

Batch 266710, train_perplexity=18.654242, train_loss=2.9260736

Batch 266720, train_perplexity=18.654242, train_loss=2.9260736

Batch 266730, train_perplexity=18.654242, train_loss=2.9260736

Batch 266740, train_perplexity=18.654238, train_loss=2.9260733

Batch 266750, train_perplexity=18.654238, train_loss=2.9260733

Batch 266760, train_perplexity=18.654232, train_loss=2.926073

Batch 266770, train_perplexity=18.654232, train_loss=2.926073

Batch 266780, train_perplexity=18.654238, train_loss=2.9260733

Batch 266790, train_perplexity=18.654232, train_loss=2.926073

Batch 266800, train_perplexity=18.654232, train_loss=2.926073

Batch 266810, train_perplexity=18.654232, train_loss=2.926073

Batch 266820, train_perplexity=18.654232, train_loss=2.926073

Batch 266830, train_perplexity=18.654232, train_loss=2.926073

Batch 266840, train_perplexity=18.654242, train_loss=2.9260736

Batch 266850, train_perplexity=18.654232, train_loss=2.926073

Batch 266860, train_perplexity=18.654232, train_loss=2.926073

Batch 266870, train_perplexity=18.654232, train_loss=2.926073

Batch 266880, train_perplexity=18.654232, train_loss=2.926073

Batch 266890, train_perplexity=18.654238, train_loss=2.9260733

Batch 266900, train_perplexity=18.654232, train_loss=2.926073

Batch 266910, train_perplexity=18.654232, train_loss=2.926073

Batch 266920, train_perplexity=18.654232, train_loss=2.926073

Batch 266930, train_perplexity=18.654232, train_loss=2.926073

Batch 266940, train_perplexity=18.654232, train_loss=2.926073

Batch 266950, train_perplexity=18.654232, train_loss=2.926073

Batch 266960, train_perplexity=18.654232, train_loss=2.926073

Batch 266970, train_perplexity=18.654232, train_loss=2.926073

Batch 266980, train_perplexity=18.654232, train_loss=2.926073

Batch 266990, train_perplexity=18.654232, train_loss=2.926073

Batch 267000, train_perplexity=18.654232, train_loss=2.926073

Batch 267010, train_perplexity=18.654232, train_loss=2.926073

Batch 267020, train_perplexity=18.654232, train_loss=2.926073

Batch 267030, train_perplexity=18.654232, train_loss=2.926073

Batch 267040, train_perplexity=18.654232, train_loss=2.926073

Batch 267050, train_perplexity=18.654238, train_loss=2.9260733

Batch 267060, train_perplexity=18.654224, train_loss=2.9260726

Batch 267070, train_perplexity=18.654228, train_loss=2.9260728

Batch 267080, train_perplexity=18.654228, train_loss=2.9260728

Batch 267090, train_perplexity=18.654232, train_loss=2.926073

Batch 267100, train_perplexity=18.654232, train_loss=2.926073

Batch 267110, train_perplexity=18.654228, train_loss=2.9260728

Batch 267120, train_perplexity=18.654232, train_loss=2.926073

Batch 267130, train_perplexity=18.654228, train_loss=2.9260728

Batch 267140, train_perplexity=18.654228, train_loss=2.9260728

Batch 267150, train_perplexity=18.654224, train_loss=2.9260726

Batch 267160, train_perplexity=18.654228, train_loss=2.9260728

Batch 267170, train_perplexity=18.654224, train_loss=2.9260726

Batch 267180, train_perplexity=18.654228, train_loss=2.9260728

Batch 267190, train_perplexity=18.654232, train_loss=2.926073

Batch 267200, train_perplexity=18.654224, train_loss=2.9260726

Batch 267210, train_perplexity=18.654224, train_loss=2.9260726

Batch 267220, train_perplexity=18.654228, train_loss=2.9260728

Batch 267230, train_perplexity=18.654224, train_loss=2.9260726

Batch 267240, train_perplexity=18.654224, train_loss=2.9260726

Batch 267250, train_perplexity=18.654228, train_loss=2.9260728

Batch 267260, train_perplexity=18.654224, train_loss=2.9260726

Batch 267270, train_perplexity=18.654224, train_loss=2.9260726

Batch 267280, train_perplexity=18.654224, train_loss=2.9260726

Batch 267290, train_perplexity=18.654224, train_loss=2.9260726

Batch 267300, train_perplexity=18.654224, train_loss=2.9260726

Batch 267310, train_perplexity=18.654224, train_loss=2.9260726

Batch 267320, train_perplexity=18.654224, train_loss=2.9260726

Batch 267330, train_perplexity=18.654224, train_loss=2.9260726

Batch 267340, train_perplexity=18.654228, train_loss=2.9260728

Batch 267350, train_perplexity=18.654224, train_loss=2.9260726

Batch 267360, train_perplexity=18.654215, train_loss=2.9260721

Batch 267370, train_perplexity=18.654219, train_loss=2.9260724

Batch 267380, train_perplexity=18.654224, train_loss=2.9260726

Batch 267390, train_perplexity=18.654224, train_loss=2.9260726

Batch 267400, train_perplexity=18.654215, train_loss=2.9260721

Batch 267410, train_perplexity=18.654224, train_loss=2.9260726

Batch 267420, train_perplexity=18.654219, train_loss=2.9260724

Batch 267430, train_perplexity=18.654224, train_loss=2.9260726

Batch 267440, train_perplexity=18.654215, train_loss=2.9260721

Batch 267450, train_perplexity=18.654219, train_loss=2.9260724

Batch 267460, train_perplexity=18.654219, train_loss=2.9260724

Batch 267470, train_perplexity=18.654219, train_loss=2.9260724

Batch 267480, train_perplexity=18.654215, train_loss=2.9260721

Batch 267490, train_perplexity=18.654224, train_loss=2.9260726

Batch 267500, train_perplexity=18.654215, train_loss=2.9260721

Batch 267510, train_perplexity=18.654215, train_loss=2.9260721

Batch 267520, train_perplexity=18.654215, train_loss=2.9260721

Batch 267530, train_perplexity=18.654224, train_loss=2.9260726

Batch 267540, train_perplexity=18.654215, train_loss=2.9260721

Batch 267550, train_perplexity=18.654215, train_loss=2.9260721

Batch 267560, train_perplexity=18.654215, train_loss=2.9260721

Batch 267570, train_perplexity=18.654211, train_loss=2.926072

Batch 267580, train_perplexity=18.654215, train_loss=2.9260721

Batch 267590, train_perplexity=18.654215, train_loss=2.9260721

Batch 267600, train_perplexity=18.654215, train_loss=2.9260721

Batch 267610, train_perplexity=18.654215, train_loss=2.9260721

Batch 267620, train_perplexity=18.654211, train_loss=2.926072
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 267630, train_perplexity=18.654219, train_loss=2.9260724

Batch 267640, train_perplexity=18.654215, train_loss=2.9260721

Batch 267650, train_perplexity=18.654211, train_loss=2.926072

Batch 267660, train_perplexity=18.654215, train_loss=2.9260721

Batch 267670, train_perplexity=18.654205, train_loss=2.9260716

Batch 267680, train_perplexity=18.654215, train_loss=2.9260721

Batch 267690, train_perplexity=18.654215, train_loss=2.9260721

Batch 267700, train_perplexity=18.654215, train_loss=2.9260721

Batch 267710, train_perplexity=18.654215, train_loss=2.9260721

Batch 267720, train_perplexity=18.654205, train_loss=2.9260716

Batch 267730, train_perplexity=18.654211, train_loss=2.926072

Batch 267740, train_perplexity=18.654211, train_loss=2.926072

Batch 267750, train_perplexity=18.654205, train_loss=2.9260716

Batch 267760, train_perplexity=18.654211, train_loss=2.926072

Batch 267770, train_perplexity=18.654215, train_loss=2.9260721

Batch 267780, train_perplexity=18.654205, train_loss=2.9260716

Batch 267790, train_perplexity=18.654211, train_loss=2.926072

Batch 267800, train_perplexity=18.654205, train_loss=2.9260716

Batch 267810, train_perplexity=18.654211, train_loss=2.926072

Batch 267820, train_perplexity=18.654205, train_loss=2.9260716

Batch 267830, train_perplexity=18.654205, train_loss=2.9260716

Batch 267840, train_perplexity=18.654205, train_loss=2.9260716

Batch 267850, train_perplexity=18.654205, train_loss=2.9260716

Batch 267860, train_perplexity=18.654205, train_loss=2.9260716

Batch 267870, train_perplexity=18.654205, train_loss=2.9260716

Batch 267880, train_perplexity=18.654205, train_loss=2.9260716

Batch 267890, train_perplexity=18.654205, train_loss=2.9260716

Batch 267900, train_perplexity=18.654205, train_loss=2.9260716

Batch 267910, train_perplexity=18.654205, train_loss=2.9260716

Batch 267920, train_perplexity=18.654205, train_loss=2.9260716

Batch 267930, train_perplexity=18.654202, train_loss=2.9260714

Batch 267940, train_perplexity=18.654205, train_loss=2.9260716

Batch 267950, train_perplexity=18.654205, train_loss=2.9260716

Batch 267960, train_perplexity=18.654205, train_loss=2.9260716

Batch 267970, train_perplexity=18.654198, train_loss=2.9260712

Batch 267980, train_perplexity=18.654198, train_loss=2.9260712

Batch 267990, train_perplexity=18.654205, train_loss=2.9260716

Batch 268000, train_perplexity=18.654205, train_loss=2.9260716

Batch 268010, train_perplexity=18.654205, train_loss=2.9260716

Batch 268020, train_perplexity=18.654205, train_loss=2.9260716

Batch 268030, train_perplexity=18.654202, train_loss=2.9260714

Batch 268040, train_perplexity=18.654202, train_loss=2.9260714

Batch 268050, train_perplexity=18.654205, train_loss=2.9260716

Batch 268060, train_perplexity=18.654202, train_loss=2.9260714

Batch 268070, train_perplexity=18.654198, train_loss=2.9260712

Batch 268080, train_perplexity=18.654198, train_loss=2.9260712

Batch 268090, train_perplexity=18.654205, train_loss=2.9260716

Batch 268100, train_perplexity=18.654202, train_loss=2.9260714

Batch 268110, train_perplexity=18.654198, train_loss=2.9260712

Batch 268120, train_perplexity=18.654205, train_loss=2.9260716

Batch 268130, train_perplexity=18.654202, train_loss=2.9260714

Batch 268140, train_perplexity=18.654202, train_loss=2.9260714

Batch 268150, train_perplexity=18.654202, train_loss=2.9260714

Batch 268160, train_perplexity=18.654198, train_loss=2.9260712

Batch 268170, train_perplexity=18.654198, train_loss=2.9260712

Batch 268180, train_perplexity=18.654198, train_loss=2.9260712

Batch 268190, train_perplexity=18.654198, train_loss=2.9260712

Batch 268200, train_perplexity=18.654198, train_loss=2.9260712

Batch 268210, train_perplexity=18.654198, train_loss=2.9260712

Batch 268220, train_perplexity=18.654198, train_loss=2.9260712

Batch 268230, train_perplexity=18.654198, train_loss=2.9260712

Batch 268240, train_perplexity=18.654198, train_loss=2.9260712

Batch 268250, train_perplexity=18.654198, train_loss=2.9260712

Batch 268260, train_perplexity=18.654198, train_loss=2.9260712

Batch 268270, train_perplexity=18.654198, train_loss=2.9260712

Batch 268280, train_perplexity=18.654198, train_loss=2.9260712

Batch 268290, train_perplexity=18.654198, train_loss=2.9260712

Batch 268300, train_perplexity=18.654192, train_loss=2.926071

Batch 268310, train_perplexity=18.654198, train_loss=2.9260712

Batch 268320, train_perplexity=18.654198, train_loss=2.9260712

Batch 268330, train_perplexity=18.654198, train_loss=2.9260712

Batch 268340, train_perplexity=18.654198, train_loss=2.9260712

Batch 268350, train_perplexity=18.654188, train_loss=2.9260707

Batch 268360, train_perplexity=18.654192, train_loss=2.926071

Batch 268370, train_perplexity=18.654198, train_loss=2.9260712

Batch 268380, train_perplexity=18.654188, train_loss=2.9260707

Batch 268390, train_perplexity=18.654188, train_loss=2.9260707

Batch 268400, train_perplexity=18.654188, train_loss=2.9260707

Batch 268410, train_perplexity=18.654198, train_loss=2.9260712

Batch 268420, train_perplexity=18.654198, train_loss=2.9260712

Batch 268430, train_perplexity=18.654188, train_loss=2.9260707

Batch 268440, train_perplexity=18.654188, train_loss=2.9260707

Batch 268450, train_perplexity=18.654188, train_loss=2.9260707

Batch 268460, train_perplexity=18.654188, train_loss=2.9260707

Batch 268470, train_perplexity=18.654188, train_loss=2.9260707

Batch 268480, train_perplexity=18.654188, train_loss=2.9260707

Batch 268490, train_perplexity=18.654188, train_loss=2.9260707

Batch 268500, train_perplexity=18.654188, train_loss=2.9260707

Batch 268510, train_perplexity=18.654192, train_loss=2.926071

Batch 268520, train_perplexity=18.654188, train_loss=2.9260707

Batch 268530, train_perplexity=18.654188, train_loss=2.9260707

Batch 268540, train_perplexity=18.654188, train_loss=2.9260707

Batch 268550, train_perplexity=18.654188, train_loss=2.9260707

Batch 268560, train_perplexity=18.654188, train_loss=2.9260707

Batch 268570, train_perplexity=18.654188, train_loss=2.9260707

Batch 268580, train_perplexity=18.654184, train_loss=2.9260705

Batch 268590, train_perplexity=18.654188, train_loss=2.9260707

Batch 268600, train_perplexity=18.654184, train_loss=2.9260705

Batch 268610, train_perplexity=18.654188, train_loss=2.9260707

Batch 268620, train_perplexity=18.654188, train_loss=2.9260707

Batch 268630, train_perplexity=18.654188, train_loss=2.9260707

Batch 268640, train_perplexity=18.654188, train_loss=2.9260707

Batch 268650, train_perplexity=18.654179, train_loss=2.9260702

Batch 268660, train_perplexity=18.654188, train_loss=2.9260707

Batch 268670, train_perplexity=18.654188, train_loss=2.9260707

Batch 268680, train_perplexity=18.654188, train_loss=2.9260707

Batch 268690, train_perplexity=18.654184, train_loss=2.9260705

Batch 268700, train_perplexity=18.654188, train_loss=2.9260707

Batch 268710, train_perplexity=18.654188, train_loss=2.9260707

Batch 268720, train_perplexity=18.654188, train_loss=2.9260707

Batch 268730, train_perplexity=18.654184, train_loss=2.9260705

Batch 268740, train_perplexity=18.654188, train_loss=2.9260707

Batch 268750, train_perplexity=18.654179, train_loss=2.9260702

Batch 268760, train_perplexity=18.654184, train_loss=2.9260705

Batch 268770, train_perplexity=18.654184, train_loss=2.9260705

Batch 268780, train_perplexity=18.654179, train_loss=2.9260702

Batch 268790, train_perplexity=18.654184, train_loss=2.9260705

Batch 268800, train_perplexity=18.654179, train_loss=2.9260702

Batch 268810, train_perplexity=18.654179, train_loss=2.9260702

Batch 268820, train_perplexity=18.654179, train_loss=2.9260702

Batch 268830, train_perplexity=18.654179, train_loss=2.9260702

Batch 268840, train_perplexity=18.654179, train_loss=2.9260702

Batch 268850, train_perplexity=18.654179, train_loss=2.9260702

Batch 268860, train_perplexity=18.654179, train_loss=2.9260702

Batch 268870, train_perplexity=18.654179, train_loss=2.9260702

Batch 268880, train_perplexity=18.654179, train_loss=2.9260702

Batch 268890, train_perplexity=18.654179, train_loss=2.9260702

Batch 268900, train_perplexity=18.654179, train_loss=2.9260702

Batch 268910, train_perplexity=18.654179, train_loss=2.9260702
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 268920, train_perplexity=18.654179, train_loss=2.9260702

Batch 268930, train_perplexity=18.654179, train_loss=2.9260702

Batch 268940, train_perplexity=18.654175, train_loss=2.92607

Batch 268950, train_perplexity=18.654179, train_loss=2.9260702

Batch 268960, train_perplexity=18.654179, train_loss=2.9260702

Batch 268970, train_perplexity=18.654175, train_loss=2.92607

Batch 268980, train_perplexity=18.654175, train_loss=2.92607

Batch 268990, train_perplexity=18.654179, train_loss=2.9260702

Batch 269000, train_perplexity=18.654179, train_loss=2.9260702

Batch 269010, train_perplexity=18.654179, train_loss=2.9260702

Batch 269020, train_perplexity=18.654179, train_loss=2.9260702

Batch 269030, train_perplexity=18.654171, train_loss=2.9260697

Batch 269040, train_perplexity=18.654171, train_loss=2.9260697

Batch 269050, train_perplexity=18.654171, train_loss=2.9260697

Batch 269060, train_perplexity=18.654171, train_loss=2.9260697

Batch 269070, train_perplexity=18.654171, train_loss=2.9260697

Batch 269080, train_perplexity=18.654175, train_loss=2.92607

Batch 269090, train_perplexity=18.654179, train_loss=2.9260702

Batch 269100, train_perplexity=18.654171, train_loss=2.9260697

Batch 269110, train_perplexity=18.654171, train_loss=2.9260697

Batch 269120, train_perplexity=18.654171, train_loss=2.9260697

Batch 269130, train_perplexity=18.654171, train_loss=2.9260697

Batch 269140, train_perplexity=18.654171, train_loss=2.9260697

Batch 269150, train_perplexity=18.654171, train_loss=2.9260697

Batch 269160, train_perplexity=18.654171, train_loss=2.9260697

Batch 269170, train_perplexity=18.654171, train_loss=2.9260697

Batch 269180, train_perplexity=18.654171, train_loss=2.9260697

Batch 269190, train_perplexity=18.654171, train_loss=2.9260697

Batch 269200, train_perplexity=18.654171, train_loss=2.9260697

Batch 269210, train_perplexity=18.654171, train_loss=2.9260697

Batch 269220, train_perplexity=18.654171, train_loss=2.9260697

Batch 269230, train_perplexity=18.654171, train_loss=2.9260697

Batch 269240, train_perplexity=18.654171, train_loss=2.9260697

Batch 269250, train_perplexity=18.654171, train_loss=2.9260697

Batch 269260, train_perplexity=18.654171, train_loss=2.9260697

Batch 269270, train_perplexity=18.654171, train_loss=2.9260697

Batch 269280, train_perplexity=18.654171, train_loss=2.9260697

Batch 269290, train_perplexity=18.654171, train_loss=2.9260697

Batch 269300, train_perplexity=18.654161, train_loss=2.9260693

Batch 269310, train_perplexity=18.654171, train_loss=2.9260697

Batch 269320, train_perplexity=18.654171, train_loss=2.9260697

Batch 269330, train_perplexity=18.654161, train_loss=2.9260693

Batch 269340, train_perplexity=18.654161, train_loss=2.9260693

Batch 269350, train_perplexity=18.654171, train_loss=2.9260697

Batch 269360, train_perplexity=18.654161, train_loss=2.9260693

Batch 269370, train_perplexity=18.654165, train_loss=2.9260695

Batch 269380, train_perplexity=18.654171, train_loss=2.9260697

Batch 269390, train_perplexity=18.654165, train_loss=2.9260695

Batch 269400, train_perplexity=18.654171, train_loss=2.9260697

Batch 269410, train_perplexity=18.654161, train_loss=2.9260693

Batch 269420, train_perplexity=18.654161, train_loss=2.9260693

Batch 269430, train_perplexity=18.654161, train_loss=2.9260693

Batch 269440, train_perplexity=18.654165, train_loss=2.9260695

Batch 269450, train_perplexity=18.654161, train_loss=2.9260693

Batch 269460, train_perplexity=18.654161, train_loss=2.9260693

Batch 269470, train_perplexity=18.654165, train_loss=2.9260695

Batch 269480, train_perplexity=18.654161, train_loss=2.9260693

Batch 269490, train_perplexity=18.654161, train_loss=2.9260693

Batch 269500, train_perplexity=18.654161, train_loss=2.9260693

Batch 269510, train_perplexity=18.654161, train_loss=2.9260693

Batch 269520, train_perplexity=18.654161, train_loss=2.9260693

Batch 269530, train_perplexity=18.654158, train_loss=2.926069

Batch 269540, train_perplexity=18.654161, train_loss=2.9260693

Batch 269550, train_perplexity=18.654161, train_loss=2.9260693

Batch 269560, train_perplexity=18.654161, train_loss=2.9260693

Batch 269570, train_perplexity=18.654161, train_loss=2.9260693

Batch 269580, train_perplexity=18.654158, train_loss=2.926069

Batch 269590, train_perplexity=18.654152, train_loss=2.9260688

Batch 269600, train_perplexity=18.654152, train_loss=2.9260688

Batch 269610, train_perplexity=18.654161, train_loss=2.9260693

Batch 269620, train_perplexity=18.654161, train_loss=2.9260693

Batch 269630, train_perplexity=18.654161, train_loss=2.9260693

Batch 269640, train_perplexity=18.654161, train_loss=2.9260693

Batch 269650, train_perplexity=18.654152, train_loss=2.9260688

Batch 269660, train_perplexity=18.654152, train_loss=2.9260688

Batch 269670, train_perplexity=18.654152, train_loss=2.9260688

Batch 269680, train_perplexity=18.654161, train_loss=2.9260693

Batch 269690, train_perplexity=18.654152, train_loss=2.9260688

Batch 269700, train_perplexity=18.654152, train_loss=2.9260688

Batch 269710, train_perplexity=18.654152, train_loss=2.9260688

Batch 269720, train_perplexity=18.654158, train_loss=2.926069

Batch 269730, train_perplexity=18.654152, train_loss=2.9260688

Batch 269740, train_perplexity=18.654158, train_loss=2.926069

Batch 269750, train_perplexity=18.654152, train_loss=2.9260688

Batch 269760, train_perplexity=18.654152, train_loss=2.9260688

Batch 269770, train_perplexity=18.654152, train_loss=2.9260688

Batch 269780, train_perplexity=18.654152, train_loss=2.9260688

Batch 269790, train_perplexity=18.654148, train_loss=2.9260685

Batch 269800, train_perplexity=18.654152, train_loss=2.9260688

Batch 269810, train_perplexity=18.654152, train_loss=2.9260688

Batch 269820, train_perplexity=18.654152, train_loss=2.9260688

Batch 269830, train_perplexity=18.654152, train_loss=2.9260688

Batch 269840, train_perplexity=18.654152, train_loss=2.9260688

Batch 269850, train_perplexity=18.654152, train_loss=2.9260688

Batch 269860, train_perplexity=18.654152, train_loss=2.9260688

Batch 269870, train_perplexity=18.654152, train_loss=2.9260688

Batch 269880, train_perplexity=18.654148, train_loss=2.9260685

Batch 269890, train_perplexity=18.654152, train_loss=2.9260688

Batch 269900, train_perplexity=18.654152, train_loss=2.9260688

Batch 269910, train_perplexity=18.654144, train_loss=2.9260683

Batch 269920, train_perplexity=18.654152, train_loss=2.9260688

Batch 269930, train_perplexity=18.654144, train_loss=2.9260683

Batch 269940, train_perplexity=18.654152, train_loss=2.9260688

Batch 269950, train_perplexity=18.654152, train_loss=2.9260688

Batch 269960, train_perplexity=18.654148, train_loss=2.9260685

Batch 269970, train_perplexity=18.654152, train_loss=2.9260688

Batch 269980, train_perplexity=18.654144, train_loss=2.9260683

Batch 269990, train_perplexity=18.654144, train_loss=2.9260683

Batch 270000, train_perplexity=18.654144, train_loss=2.9260683

Batch 270010, train_perplexity=18.654144, train_loss=2.9260683

Batch 270020, train_perplexity=18.654144, train_loss=2.9260683

Batch 270030, train_perplexity=18.654144, train_loss=2.9260683

Batch 270040, train_perplexity=18.654148, train_loss=2.9260685

Batch 270050, train_perplexity=18.654148, train_loss=2.9260685

Batch 270060, train_perplexity=18.654144, train_loss=2.9260683

Batch 270070, train_perplexity=18.654144, train_loss=2.9260683

Batch 270080, train_perplexity=18.654148, train_loss=2.9260685

Batch 270090, train_perplexity=18.654148, train_loss=2.9260685

Batch 270100, train_perplexity=18.654144, train_loss=2.9260683

Batch 270110, train_perplexity=18.654144, train_loss=2.9260683

Batch 270120, train_perplexity=18.654144, train_loss=2.9260683

Batch 270130, train_perplexity=18.654144, train_loss=2.9260683

Batch 270140, train_perplexity=18.654144, train_loss=2.9260683

Batch 270150, train_perplexity=18.654144, train_loss=2.9260683

Batch 270160, train_perplexity=18.654144, train_loss=2.9260683

Batch 270170, train_perplexity=18.654144, train_loss=2.9260683

Batch 270180, train_perplexity=18.654144, train_loss=2.9260683

Batch 270190, train_perplexity=18.654144, train_loss=2.9260683

Batch 270200, train_perplexity=18.654139, train_loss=2.926068
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 270210, train_perplexity=18.654139, train_loss=2.926068

Batch 270220, train_perplexity=18.654135, train_loss=2.9260678

Batch 270230, train_perplexity=18.654139, train_loss=2.926068

Batch 270240, train_perplexity=18.654139, train_loss=2.926068

Batch 270250, train_perplexity=18.654144, train_loss=2.9260683

Batch 270260, train_perplexity=18.654139, train_loss=2.926068

Batch 270270, train_perplexity=18.654135, train_loss=2.9260678

Batch 270280, train_perplexity=18.654139, train_loss=2.926068

Batch 270290, train_perplexity=18.654139, train_loss=2.926068

Batch 270300, train_perplexity=18.654139, train_loss=2.926068

Batch 270310, train_perplexity=18.654139, train_loss=2.926068

Batch 270320, train_perplexity=18.654135, train_loss=2.9260678

Batch 270330, train_perplexity=18.654135, train_loss=2.9260678

Batch 270340, train_perplexity=18.654135, train_loss=2.9260678

Batch 270350, train_perplexity=18.654135, train_loss=2.9260678

Batch 270360, train_perplexity=18.654139, train_loss=2.926068

Batch 270370, train_perplexity=18.654135, train_loss=2.9260678

Batch 270380, train_perplexity=18.654135, train_loss=2.9260678

Batch 270390, train_perplexity=18.654135, train_loss=2.9260678

Batch 270400, train_perplexity=18.654139, train_loss=2.926068

Batch 270410, train_perplexity=18.654139, train_loss=2.926068

Batch 270420, train_perplexity=18.654135, train_loss=2.9260678

Batch 270430, train_perplexity=18.654135, train_loss=2.9260678

Batch 270440, train_perplexity=18.654135, train_loss=2.9260678

Batch 270450, train_perplexity=18.654135, train_loss=2.9260678

Batch 270460, train_perplexity=18.654135, train_loss=2.9260678

Batch 270470, train_perplexity=18.654135, train_loss=2.9260678

Batch 270480, train_perplexity=18.654139, train_loss=2.926068

Batch 270490, train_perplexity=18.65413, train_loss=2.9260676

Batch 270500, train_perplexity=18.654135, train_loss=2.9260678

Batch 270510, train_perplexity=18.654135, train_loss=2.9260678

Batch 270520, train_perplexity=18.654125, train_loss=2.9260674

Batch 270530, train_perplexity=18.654135, train_loss=2.9260678

Batch 270540, train_perplexity=18.654135, train_loss=2.9260678

Batch 270550, train_perplexity=18.654135, train_loss=2.9260678

Batch 270560, train_perplexity=18.654135, train_loss=2.9260678

Batch 270570, train_perplexity=18.654125, train_loss=2.9260674

Batch 270580, train_perplexity=18.654125, train_loss=2.9260674

Batch 270590, train_perplexity=18.65413, train_loss=2.9260676

Batch 270600, train_perplexity=18.65413, train_loss=2.9260676

Batch 270610, train_perplexity=18.654135, train_loss=2.9260678

Batch 270620, train_perplexity=18.65413, train_loss=2.9260676

Batch 270630, train_perplexity=18.654125, train_loss=2.9260674

Batch 270640, train_perplexity=18.654125, train_loss=2.9260674

Batch 270650, train_perplexity=18.654125, train_loss=2.9260674

Batch 270660, train_perplexity=18.654125, train_loss=2.9260674

Batch 270670, train_perplexity=18.654125, train_loss=2.9260674

Batch 270680, train_perplexity=18.654125, train_loss=2.9260674

Batch 270690, train_perplexity=18.654125, train_loss=2.9260674

Batch 270700, train_perplexity=18.65413, train_loss=2.9260676

Batch 270710, train_perplexity=18.654125, train_loss=2.9260674

Batch 270720, train_perplexity=18.654125, train_loss=2.9260674

Batch 270730, train_perplexity=18.654125, train_loss=2.9260674

Batch 270740, train_perplexity=18.654125, train_loss=2.9260674

Batch 270750, train_perplexity=18.65413, train_loss=2.9260676

Batch 270760, train_perplexity=18.654125, train_loss=2.9260674

Batch 270770, train_perplexity=18.654125, train_loss=2.9260674

Batch 270780, train_perplexity=18.654125, train_loss=2.9260674

Batch 270790, train_perplexity=18.654125, train_loss=2.9260674

Batch 270800, train_perplexity=18.654121, train_loss=2.926067

Batch 270810, train_perplexity=18.654118, train_loss=2.9260669

Batch 270820, train_perplexity=18.654121, train_loss=2.926067

Batch 270830, train_perplexity=18.654121, train_loss=2.926067

Batch 270840, train_perplexity=18.654121, train_loss=2.926067

Batch 270850, train_perplexity=18.654121, train_loss=2.926067

Batch 270860, train_perplexity=18.654125, train_loss=2.9260674

Batch 270870, train_perplexity=18.654121, train_loss=2.926067

Batch 270880, train_perplexity=18.654121, train_loss=2.926067

Batch 270890, train_perplexity=18.654118, train_loss=2.9260669

Batch 270900, train_perplexity=18.654125, train_loss=2.9260674

Batch 270910, train_perplexity=18.654118, train_loss=2.9260669

Batch 270920, train_perplexity=18.654118, train_loss=2.9260669

Batch 270930, train_perplexity=18.654118, train_loss=2.9260669

Batch 270940, train_perplexity=18.654125, train_loss=2.9260674

Batch 270950, train_perplexity=18.654121, train_loss=2.926067

Batch 270960, train_perplexity=18.654121, train_loss=2.926067

Batch 270970, train_perplexity=18.654118, train_loss=2.9260669

Batch 270980, train_perplexity=18.654118, train_loss=2.9260669

Batch 270990, train_perplexity=18.654118, train_loss=2.9260669

Batch 271000, train_perplexity=18.654118, train_loss=2.9260669

Batch 271010, train_perplexity=18.654118, train_loss=2.9260669

Batch 271020, train_perplexity=18.654118, train_loss=2.9260669

Batch 271030, train_perplexity=18.654118, train_loss=2.9260669

Batch 271040, train_perplexity=18.654118, train_loss=2.9260669

Batch 271050, train_perplexity=18.654118, train_loss=2.9260669

Batch 271060, train_perplexity=18.654121, train_loss=2.926067

Batch 271070, train_perplexity=18.654118, train_loss=2.9260669

Batch 271080, train_perplexity=18.654118, train_loss=2.9260669

Batch 271090, train_perplexity=18.654118, train_loss=2.9260669

Batch 271100, train_perplexity=18.654118, train_loss=2.9260669

Batch 271110, train_perplexity=18.654118, train_loss=2.9260669

Batch 271120, train_perplexity=18.654118, train_loss=2.9260669

Batch 271130, train_perplexity=18.654118, train_loss=2.9260669

Batch 271140, train_perplexity=18.654112, train_loss=2.9260666

Batch 271150, train_perplexity=18.654108, train_loss=2.9260664

Batch 271160, train_perplexity=18.654118, train_loss=2.9260669

Batch 271170, train_perplexity=18.654118, train_loss=2.9260669

Batch 271180, train_perplexity=18.654108, train_loss=2.9260664

Batch 271190, train_perplexity=18.654118, train_loss=2.9260669

Batch 271200, train_perplexity=18.654118, train_loss=2.9260669

Batch 271210, train_perplexity=18.654118, train_loss=2.9260669

Batch 271220, train_perplexity=18.654108, train_loss=2.9260664

Batch 271230, train_perplexity=18.654108, train_loss=2.9260664

Batch 271240, train_perplexity=18.654112, train_loss=2.9260666

Batch 271250, train_perplexity=18.654108, train_loss=2.9260664

Batch 271260, train_perplexity=18.654108, train_loss=2.9260664

Batch 271270, train_perplexity=18.654108, train_loss=2.9260664

Batch 271280, train_perplexity=18.654108, train_loss=2.9260664

Batch 271290, train_perplexity=18.654112, train_loss=2.9260666

Batch 271300, train_perplexity=18.654108, train_loss=2.9260664

Batch 271310, train_perplexity=18.654108, train_loss=2.9260664

Batch 271320, train_perplexity=18.654112, train_loss=2.9260666

Batch 271330, train_perplexity=18.654108, train_loss=2.9260664

Batch 271340, train_perplexity=18.654112, train_loss=2.9260666

Batch 271350, train_perplexity=18.654118, train_loss=2.9260669

Batch 271360, train_perplexity=18.654108, train_loss=2.9260664

Batch 271370, train_perplexity=18.654104, train_loss=2.9260662

Batch 271380, train_perplexity=18.654108, train_loss=2.9260664

Batch 271390, train_perplexity=18.654108, train_loss=2.9260664

Batch 271400, train_perplexity=18.654108, train_loss=2.9260664

Batch 271410, train_perplexity=18.654112, train_loss=2.9260666

Batch 271420, train_perplexity=18.654108, train_loss=2.9260664

Batch 271430, train_perplexity=18.654104, train_loss=2.9260662

Batch 271440, train_perplexity=18.654108, train_loss=2.9260664

Batch 271450, train_perplexity=18.654104, train_loss=2.9260662

Batch 271460, train_perplexity=18.654108, train_loss=2.9260664

Batch 271470, train_perplexity=18.654104, train_loss=2.9260662

Batch 271480, train_perplexity=18.654108, train_loss=2.9260664

Batch 271490, train_perplexity=18.654108, train_loss=2.9260664
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 271500, train_perplexity=18.654108, train_loss=2.9260664

Batch 271510, train_perplexity=18.654108, train_loss=2.9260664

Batch 271520, train_perplexity=18.654099, train_loss=2.926066

Batch 271530, train_perplexity=18.654104, train_loss=2.9260662

Batch 271540, train_perplexity=18.654099, train_loss=2.926066

Batch 271550, train_perplexity=18.654099, train_loss=2.926066

Batch 271560, train_perplexity=18.654099, train_loss=2.926066

Batch 271570, train_perplexity=18.654104, train_loss=2.9260662

Batch 271580, train_perplexity=18.654099, train_loss=2.926066

Batch 271590, train_perplexity=18.654099, train_loss=2.926066

Batch 271600, train_perplexity=18.654099, train_loss=2.926066

Batch 271610, train_perplexity=18.654099, train_loss=2.926066

Batch 271620, train_perplexity=18.654104, train_loss=2.9260662

Batch 271630, train_perplexity=18.654099, train_loss=2.926066

Batch 271640, train_perplexity=18.654099, train_loss=2.926066

Batch 271650, train_perplexity=18.654099, train_loss=2.926066

Batch 271660, train_perplexity=18.654099, train_loss=2.926066

Batch 271670, train_perplexity=18.654099, train_loss=2.926066

Batch 271680, train_perplexity=18.654099, train_loss=2.926066

Batch 271690, train_perplexity=18.654099, train_loss=2.926066

Batch 271700, train_perplexity=18.654099, train_loss=2.926066

Batch 271710, train_perplexity=18.654099, train_loss=2.926066

Batch 271720, train_perplexity=18.654099, train_loss=2.926066

Batch 271730, train_perplexity=18.654099, train_loss=2.926066

Batch 271740, train_perplexity=18.654099, train_loss=2.926066

Batch 271750, train_perplexity=18.654099, train_loss=2.926066

Batch 271760, train_perplexity=18.654099, train_loss=2.926066

Batch 271770, train_perplexity=18.654099, train_loss=2.926066

Batch 271780, train_perplexity=18.654099, train_loss=2.926066

Batch 271790, train_perplexity=18.654099, train_loss=2.926066

Batch 271800, train_perplexity=18.654099, train_loss=2.926066

Batch 271810, train_perplexity=18.654099, train_loss=2.926066

Batch 271820, train_perplexity=18.65409, train_loss=2.9260654

Batch 271830, train_perplexity=18.654099, train_loss=2.926066

Batch 271840, train_perplexity=18.654095, train_loss=2.9260657

Batch 271850, train_perplexity=18.65409, train_loss=2.9260654

Batch 271860, train_perplexity=18.65409, train_loss=2.9260654

Batch 271870, train_perplexity=18.654095, train_loss=2.9260657

Batch 271880, train_perplexity=18.65409, train_loss=2.9260654

Batch 271890, train_perplexity=18.65409, train_loss=2.9260654

Batch 271900, train_perplexity=18.65409, train_loss=2.9260654

Batch 271910, train_perplexity=18.65409, train_loss=2.9260654

Batch 271920, train_perplexity=18.65409, train_loss=2.9260654

Batch 271930, train_perplexity=18.65409, train_loss=2.9260654

Batch 271940, train_perplexity=18.65409, train_loss=2.9260654

Batch 271950, train_perplexity=18.65409, train_loss=2.9260654

Batch 271960, train_perplexity=18.654095, train_loss=2.9260657

Batch 271970, train_perplexity=18.654095, train_loss=2.9260657

Batch 271980, train_perplexity=18.654095, train_loss=2.9260657

Batch 271990, train_perplexity=18.654095, train_loss=2.9260657

Batch 272000, train_perplexity=18.65409, train_loss=2.9260654

Batch 272010, train_perplexity=18.65409, train_loss=2.9260654

Batch 272020, train_perplexity=18.654081, train_loss=2.926065

Batch 272030, train_perplexity=18.65409, train_loss=2.9260654

Batch 272040, train_perplexity=18.654085, train_loss=2.9260652

Batch 272050, train_perplexity=18.65409, train_loss=2.9260654

Batch 272060, train_perplexity=18.65409, train_loss=2.9260654

Batch 272070, train_perplexity=18.654095, train_loss=2.9260657

Batch 272080, train_perplexity=18.65409, train_loss=2.9260654

Batch 272090, train_perplexity=18.65409, train_loss=2.9260654

Batch 272100, train_perplexity=18.654085, train_loss=2.9260652

Batch 272110, train_perplexity=18.654085, train_loss=2.9260652

Batch 272120, train_perplexity=18.654085, train_loss=2.9260652

Batch 272130, train_perplexity=18.654081, train_loss=2.926065

Batch 272140, train_perplexity=18.654085, train_loss=2.9260652

Batch 272150, train_perplexity=18.654085, train_loss=2.9260652

Batch 272160, train_perplexity=18.654081, train_loss=2.926065

Batch 272170, train_perplexity=18.654081, train_loss=2.926065

Batch 272180, train_perplexity=18.654081, train_loss=2.926065

Batch 272190, train_perplexity=18.654081, train_loss=2.926065

Batch 272200, train_perplexity=18.654085, train_loss=2.9260652

Batch 272210, train_perplexity=18.654081, train_loss=2.926065

Batch 272220, train_perplexity=18.65409, train_loss=2.9260654

Batch 272230, train_perplexity=18.654081, train_loss=2.926065

Batch 272240, train_perplexity=18.654081, train_loss=2.926065

Batch 272250, train_perplexity=18.654081, train_loss=2.926065

Batch 272260, train_perplexity=18.654081, train_loss=2.926065

Batch 272270, train_perplexity=18.654085, train_loss=2.9260652

Batch 272280, train_perplexity=18.654081, train_loss=2.926065

Batch 272290, train_perplexity=18.654081, train_loss=2.926065

Batch 272300, train_perplexity=18.654081, train_loss=2.926065

Batch 272310, train_perplexity=18.654081, train_loss=2.926065

Batch 272320, train_perplexity=18.654081, train_loss=2.926065

Batch 272330, train_perplexity=18.654081, train_loss=2.926065

Batch 272340, train_perplexity=18.654081, train_loss=2.926065

Batch 272350, train_perplexity=18.654081, train_loss=2.926065

Batch 272360, train_perplexity=18.654081, train_loss=2.926065

Batch 272370, train_perplexity=18.654081, train_loss=2.926065

Batch 272380, train_perplexity=18.654081, train_loss=2.926065

Batch 272390, train_perplexity=18.654081, train_loss=2.926065

Batch 272400, train_perplexity=18.654072, train_loss=2.9260645

Batch 272410, train_perplexity=18.654072, train_loss=2.9260645

Batch 272420, train_perplexity=18.654078, train_loss=2.9260647

Batch 272430, train_perplexity=18.654072, train_loss=2.9260645

Batch 272440, train_perplexity=18.654081, train_loss=2.926065

Batch 272450, train_perplexity=18.654081, train_loss=2.926065

Batch 272460, train_perplexity=18.654078, train_loss=2.9260647

Batch 272470, train_perplexity=18.654078, train_loss=2.9260647

Batch 272480, train_perplexity=18.654078, train_loss=2.9260647

Batch 272490, train_perplexity=18.654072, train_loss=2.9260645

Batch 272500, train_perplexity=18.654072, train_loss=2.9260645

Batch 272510, train_perplexity=18.654072, train_loss=2.9260645

Batch 272520, train_perplexity=18.654072, train_loss=2.9260645

Batch 272530, train_perplexity=18.654078, train_loss=2.9260647

Batch 272540, train_perplexity=18.654078, train_loss=2.9260647

Batch 272550, train_perplexity=18.654072, train_loss=2.9260645

Batch 272560, train_perplexity=18.654072, train_loss=2.9260645

Batch 272570, train_perplexity=18.654078, train_loss=2.9260647

Batch 272580, train_perplexity=18.654072, train_loss=2.9260645

Batch 272590, train_perplexity=18.654072, train_loss=2.9260645

Batch 272600, train_perplexity=18.654078, train_loss=2.9260647

Batch 272610, train_perplexity=18.654072, train_loss=2.9260645

Batch 272620, train_perplexity=18.654072, train_loss=2.9260645

Batch 272630, train_perplexity=18.654072, train_loss=2.9260645

Batch 272640, train_perplexity=18.654072, train_loss=2.9260645

Batch 272650, train_perplexity=18.654072, train_loss=2.9260645

Batch 272660, train_perplexity=18.654072, train_loss=2.9260645

Batch 272670, train_perplexity=18.654072, train_loss=2.9260645

Batch 272680, train_perplexity=18.654072, train_loss=2.9260645

Batch 272690, train_perplexity=18.654068, train_loss=2.9260643

Batch 272700, train_perplexity=18.654068, train_loss=2.9260643

Batch 272710, train_perplexity=18.654072, train_loss=2.9260645

Batch 272720, train_perplexity=18.654068, train_loss=2.9260643

Batch 272730, train_perplexity=18.654072, train_loss=2.9260645

Batch 272740, train_perplexity=18.654072, train_loss=2.9260645

Batch 272750, train_perplexity=18.654068, train_loss=2.9260643

Batch 272760, train_perplexity=18.654064, train_loss=2.926064

Batch 272770, train_perplexity=18.654068, train_loss=2.9260643

Batch 272780, train_perplexity=18.654072, train_loss=2.9260645

Batch 272790, train_perplexity=18.654072, train_loss=2.9260645
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 272800, train_perplexity=18.654064, train_loss=2.926064

Batch 272810, train_perplexity=18.654068, train_loss=2.9260643

Batch 272820, train_perplexity=18.654064, train_loss=2.926064

Batch 272830, train_perplexity=18.654068, train_loss=2.9260643

Batch 272840, train_perplexity=18.654064, train_loss=2.926064

Batch 272850, train_perplexity=18.654064, train_loss=2.926064

Batch 272860, train_perplexity=18.654064, train_loss=2.926064

Batch 272870, train_perplexity=18.654064, train_loss=2.926064

Batch 272880, train_perplexity=18.654064, train_loss=2.926064

Batch 272890, train_perplexity=18.654064, train_loss=2.926064

Batch 272900, train_perplexity=18.654064, train_loss=2.926064

Batch 272910, train_perplexity=18.654064, train_loss=2.926064

Batch 272920, train_perplexity=18.654064, train_loss=2.926064

Batch 272930, train_perplexity=18.654064, train_loss=2.926064

Batch 272940, train_perplexity=18.654064, train_loss=2.926064

Batch 272950, train_perplexity=18.654058, train_loss=2.9260638

Batch 272960, train_perplexity=18.654064, train_loss=2.926064

Batch 272970, train_perplexity=18.654064, train_loss=2.926064

Batch 272980, train_perplexity=18.654064, train_loss=2.926064

Batch 272990, train_perplexity=18.654064, train_loss=2.926064

Batch 273000, train_perplexity=18.654064, train_loss=2.926064

Batch 273010, train_perplexity=18.654064, train_loss=2.926064

Batch 273020, train_perplexity=18.654055, train_loss=2.9260635

Batch 273030, train_perplexity=18.654064, train_loss=2.926064

Batch 273040, train_perplexity=18.654055, train_loss=2.9260635

Batch 273050, train_perplexity=18.654058, train_loss=2.9260638

Batch 273060, train_perplexity=18.654064, train_loss=2.926064

Batch 273070, train_perplexity=18.654055, train_loss=2.9260635

Batch 273080, train_perplexity=18.654058, train_loss=2.9260638

Batch 273090, train_perplexity=18.654055, train_loss=2.9260635

Batch 273100, train_perplexity=18.654055, train_loss=2.9260635

Batch 273110, train_perplexity=18.654064, train_loss=2.926064

Batch 273120, train_perplexity=18.654055, train_loss=2.9260635

Batch 273130, train_perplexity=18.654058, train_loss=2.9260638

Batch 273140, train_perplexity=18.654055, train_loss=2.9260635

Batch 273150, train_perplexity=18.654055, train_loss=2.9260635

Batch 273160, train_perplexity=18.654055, train_loss=2.9260635

Batch 273170, train_perplexity=18.654055, train_loss=2.9260635

Batch 273180, train_perplexity=18.654055, train_loss=2.9260635

Batch 273190, train_perplexity=18.654055, train_loss=2.9260635

Batch 273200, train_perplexity=18.654055, train_loss=2.9260635

Batch 273210, train_perplexity=18.654058, train_loss=2.9260638

Batch 273220, train_perplexity=18.654055, train_loss=2.9260635

Batch 273230, train_perplexity=18.654055, train_loss=2.9260635

Batch 273240, train_perplexity=18.654055, train_loss=2.9260635

Batch 273250, train_perplexity=18.654055, train_loss=2.9260635

Batch 273260, train_perplexity=18.654055, train_loss=2.9260635

Batch 273270, train_perplexity=18.654055, train_loss=2.9260635

Batch 273280, train_perplexity=18.654055, train_loss=2.9260635

Batch 273290, train_perplexity=18.654055, train_loss=2.9260635

Batch 273300, train_perplexity=18.654055, train_loss=2.9260635

Batch 273310, train_perplexity=18.654045, train_loss=2.926063

Batch 273320, train_perplexity=18.65405, train_loss=2.9260633

Batch 273330, train_perplexity=18.654055, train_loss=2.9260635

Batch 273340, train_perplexity=18.654055, train_loss=2.9260635

Batch 273350, train_perplexity=18.65405, train_loss=2.9260633

Batch 273360, train_perplexity=18.654055, train_loss=2.9260635

Batch 273370, train_perplexity=18.654045, train_loss=2.926063

Batch 273380, train_perplexity=18.654055, train_loss=2.9260635

Batch 273390, train_perplexity=18.65405, train_loss=2.9260633

Batch 273400, train_perplexity=18.65405, train_loss=2.9260633

Batch 273410, train_perplexity=18.65405, train_loss=2.9260633

Batch 273420, train_perplexity=18.65405, train_loss=2.9260633

Batch 273430, train_perplexity=18.654045, train_loss=2.926063

Batch 273440, train_perplexity=18.654045, train_loss=2.926063

Batch 273450, train_perplexity=18.65405, train_loss=2.9260633

Batch 273460, train_perplexity=18.654045, train_loss=2.926063

Batch 273470, train_perplexity=18.654045, train_loss=2.926063

Batch 273480, train_perplexity=18.654045, train_loss=2.926063

Batch 273490, train_perplexity=18.654045, train_loss=2.926063

Batch 273500, train_perplexity=18.65405, train_loss=2.9260633

Batch 273510, train_perplexity=18.654045, train_loss=2.926063

Batch 273520, train_perplexity=18.654045, train_loss=2.926063

Batch 273530, train_perplexity=18.654045, train_loss=2.926063

Batch 273540, train_perplexity=18.654045, train_loss=2.926063

Batch 273550, train_perplexity=18.654045, train_loss=2.926063

Batch 273560, train_perplexity=18.654045, train_loss=2.926063

Batch 273570, train_perplexity=18.654045, train_loss=2.926063

Batch 273580, train_perplexity=18.654045, train_loss=2.926063

Batch 273590, train_perplexity=18.654045, train_loss=2.926063

Batch 273600, train_perplexity=18.654045, train_loss=2.926063

Batch 273610, train_perplexity=18.654045, train_loss=2.926063

Batch 273620, train_perplexity=18.654045, train_loss=2.926063

Batch 273630, train_perplexity=18.654037, train_loss=2.9260626

Batch 273640, train_perplexity=18.654041, train_loss=2.9260628

Batch 273650, train_perplexity=18.654041, train_loss=2.9260628

Batch 273660, train_perplexity=18.654037, train_loss=2.9260626

Batch 273670, train_perplexity=18.654045, train_loss=2.926063

Batch 273680, train_perplexity=18.654045, train_loss=2.926063

Batch 273690, train_perplexity=18.654045, train_loss=2.926063

Batch 273700, train_perplexity=18.654045, train_loss=2.926063

Batch 273710, train_perplexity=18.654037, train_loss=2.9260626

Batch 273720, train_perplexity=18.654037, train_loss=2.9260626

Batch 273730, train_perplexity=18.654041, train_loss=2.9260628

Batch 273740, train_perplexity=18.654037, train_loss=2.9260626

Batch 273750, train_perplexity=18.654037, train_loss=2.9260626

Batch 273760, train_perplexity=18.654037, train_loss=2.9260626

Batch 273770, train_perplexity=18.654037, train_loss=2.9260626

Batch 273780, train_perplexity=18.654037, train_loss=2.9260626

Batch 273790, train_perplexity=18.654037, train_loss=2.9260626

Batch 273800, train_perplexity=18.654037, train_loss=2.9260626

Batch 273810, train_perplexity=18.654041, train_loss=2.9260628

Batch 273820, train_perplexity=18.654037, train_loss=2.9260626

Batch 273830, train_perplexity=18.654032, train_loss=2.9260623

Batch 273840, train_perplexity=18.654037, train_loss=2.9260626

Batch 273850, train_perplexity=18.654037, train_loss=2.9260626

Batch 273860, train_perplexity=18.654037, train_loss=2.9260626

Batch 273870, train_perplexity=18.654037, train_loss=2.9260626

Batch 273880, train_perplexity=18.654037, train_loss=2.9260626

Batch 273890, train_perplexity=18.654037, train_loss=2.9260626

Batch 273900, train_perplexity=18.654037, train_loss=2.9260626

Batch 273910, train_perplexity=18.654037, train_loss=2.9260626

Batch 273920, train_perplexity=18.654037, train_loss=2.9260626

Batch 273930, train_perplexity=18.654037, train_loss=2.9260626

Batch 273940, train_perplexity=18.654032, train_loss=2.9260623

Batch 273950, train_perplexity=18.654028, train_loss=2.926062

Batch 273960, train_perplexity=18.654037, train_loss=2.9260626

Batch 273970, train_perplexity=18.654037, train_loss=2.9260626

Batch 273980, train_perplexity=18.654037, train_loss=2.9260626

Batch 273990, train_perplexity=18.654037, train_loss=2.9260626

Batch 274000, train_perplexity=18.654028, train_loss=2.926062

Batch 274010, train_perplexity=18.654037, train_loss=2.9260626

Batch 274020, train_perplexity=18.654032, train_loss=2.9260623

Batch 274030, train_perplexity=18.654028, train_loss=2.926062

Batch 274040, train_perplexity=18.654028, train_loss=2.926062

Batch 274050, train_perplexity=18.654032, train_loss=2.9260623

Batch 274060, train_perplexity=18.654028, train_loss=2.926062

Batch 274070, train_perplexity=18.654028, train_loss=2.926062

Batch 274080, train_perplexity=18.654032, train_loss=2.9260623
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 274090, train_perplexity=18.654028, train_loss=2.926062

Batch 274100, train_perplexity=18.654028, train_loss=2.926062

Batch 274110, train_perplexity=18.654028, train_loss=2.926062

Batch 274120, train_perplexity=18.654028, train_loss=2.926062

Batch 274130, train_perplexity=18.654028, train_loss=2.926062

Batch 274140, train_perplexity=18.654028, train_loss=2.926062

Batch 274150, train_perplexity=18.654028, train_loss=2.926062

Batch 274160, train_perplexity=18.654028, train_loss=2.926062

Batch 274170, train_perplexity=18.654028, train_loss=2.926062

Batch 274180, train_perplexity=18.654032, train_loss=2.9260623

Batch 274190, train_perplexity=18.654028, train_loss=2.926062

Batch 274200, train_perplexity=18.654028, train_loss=2.926062

Batch 274210, train_perplexity=18.654028, train_loss=2.926062

Batch 274220, train_perplexity=18.654028, train_loss=2.926062

Batch 274230, train_perplexity=18.654028, train_loss=2.926062

Batch 274240, train_perplexity=18.654028, train_loss=2.926062

Batch 274250, train_perplexity=18.654018, train_loss=2.9260616

Batch 274260, train_perplexity=18.654028, train_loss=2.926062

Batch 274270, train_perplexity=18.654028, train_loss=2.926062

Batch 274280, train_perplexity=18.654028, train_loss=2.926062

Batch 274290, train_perplexity=18.654018, train_loss=2.9260616

Batch 274300, train_perplexity=18.654024, train_loss=2.9260619

Batch 274310, train_perplexity=18.654024, train_loss=2.9260619

Batch 274320, train_perplexity=18.654028, train_loss=2.926062

Batch 274330, train_perplexity=18.654028, train_loss=2.926062

Batch 274340, train_perplexity=18.654028, train_loss=2.926062

Batch 274350, train_perplexity=18.654028, train_loss=2.926062

Batch 274360, train_perplexity=18.654024, train_loss=2.9260619

Batch 274370, train_perplexity=18.654024, train_loss=2.9260619

Batch 274380, train_perplexity=18.654018, train_loss=2.9260616

Batch 274390, train_perplexity=18.654024, train_loss=2.9260619

Batch 274400, train_perplexity=18.654018, train_loss=2.9260616

Batch 274410, train_perplexity=18.654028, train_loss=2.926062

Batch 274420, train_perplexity=18.654024, train_loss=2.9260619

Batch 274430, train_perplexity=18.654018, train_loss=2.9260616

Batch 274440, train_perplexity=18.654024, train_loss=2.9260619

Batch 274450, train_perplexity=18.654018, train_loss=2.9260616

Batch 274460, train_perplexity=18.654018, train_loss=2.9260616

Batch 274470, train_perplexity=18.654018, train_loss=2.9260616

Batch 274480, train_perplexity=18.654018, train_loss=2.9260616

Batch 274490, train_perplexity=18.654018, train_loss=2.9260616

Batch 274500, train_perplexity=18.654018, train_loss=2.9260616

Batch 274510, train_perplexity=18.654018, train_loss=2.9260616

Batch 274520, train_perplexity=18.654018, train_loss=2.9260616

Batch 274530, train_perplexity=18.654018, train_loss=2.9260616

Batch 274540, train_perplexity=18.654018, train_loss=2.9260616

Batch 274550, train_perplexity=18.654018, train_loss=2.9260616

Batch 274560, train_perplexity=18.65401, train_loss=2.9260612

Batch 274570, train_perplexity=18.654018, train_loss=2.9260616

Batch 274580, train_perplexity=18.654015, train_loss=2.9260614

Batch 274590, train_perplexity=18.654015, train_loss=2.9260614

Batch 274600, train_perplexity=18.654018, train_loss=2.9260616

Batch 274610, train_perplexity=18.654018, train_loss=2.9260616

Batch 274620, train_perplexity=18.654018, train_loss=2.9260616

Batch 274630, train_perplexity=18.654018, train_loss=2.9260616

Batch 274640, train_perplexity=18.65401, train_loss=2.9260612

Batch 274650, train_perplexity=18.65401, train_loss=2.9260612

Batch 274660, train_perplexity=18.654015, train_loss=2.9260614

Batch 274670, train_perplexity=18.654015, train_loss=2.9260614

Batch 274680, train_perplexity=18.65401, train_loss=2.9260612

Batch 274690, train_perplexity=18.65401, train_loss=2.9260612

Batch 274700, train_perplexity=18.654015, train_loss=2.9260614

Batch 274710, train_perplexity=18.65401, train_loss=2.9260612

Batch 274720, train_perplexity=18.654015, train_loss=2.9260614

Batch 274730, train_perplexity=18.65401, train_loss=2.9260612

Batch 274740, train_perplexity=18.654015, train_loss=2.9260614

Batch 274750, train_perplexity=18.65401, train_loss=2.9260612

Batch 274760, train_perplexity=18.65401, train_loss=2.9260612

Batch 274770, train_perplexity=18.65401, train_loss=2.9260612

Batch 274780, train_perplexity=18.65401, train_loss=2.9260612

Batch 274790, train_perplexity=18.65401, train_loss=2.9260612

Batch 274800, train_perplexity=18.65401, train_loss=2.9260612

Batch 274810, train_perplexity=18.65401, train_loss=2.9260612

Batch 274820, train_perplexity=18.65401, train_loss=2.9260612

Batch 274830, train_perplexity=18.65401, train_loss=2.9260612

Batch 274840, train_perplexity=18.65401, train_loss=2.9260612

Batch 274850, train_perplexity=18.65401, train_loss=2.9260612

Batch 274860, train_perplexity=18.65401, train_loss=2.9260612

Batch 274870, train_perplexity=18.65401, train_loss=2.9260612

Batch 274880, train_perplexity=18.65401, train_loss=2.9260612

Batch 274890, train_perplexity=18.654005, train_loss=2.926061

Batch 274900, train_perplexity=18.65401, train_loss=2.9260612

Batch 274910, train_perplexity=18.65401, train_loss=2.9260612

Batch 274920, train_perplexity=18.654005, train_loss=2.926061

Batch 274930, train_perplexity=18.65401, train_loss=2.9260612

Batch 274940, train_perplexity=18.65401, train_loss=2.9260612

Batch 274950, train_perplexity=18.654005, train_loss=2.926061

Batch 274960, train_perplexity=18.654005, train_loss=2.926061

Batch 274970, train_perplexity=18.654005, train_loss=2.926061

Batch 274980, train_perplexity=18.654001, train_loss=2.9260607

Batch 274990, train_perplexity=18.654005, train_loss=2.926061

Batch 275000, train_perplexity=18.654001, train_loss=2.9260607

Batch 275010, train_perplexity=18.654001, train_loss=2.9260607

Batch 275020, train_perplexity=18.654005, train_loss=2.926061

Batch 275030, train_perplexity=18.654001, train_loss=2.9260607

Batch 275040, train_perplexity=18.654001, train_loss=2.9260607

Batch 275050, train_perplexity=18.654001, train_loss=2.9260607

Batch 275060, train_perplexity=18.654001, train_loss=2.9260607

Batch 275070, train_perplexity=18.654001, train_loss=2.9260607

Batch 275080, train_perplexity=18.654001, train_loss=2.9260607

Batch 275090, train_perplexity=18.654001, train_loss=2.9260607

Batch 275100, train_perplexity=18.654001, train_loss=2.9260607

Batch 275110, train_perplexity=18.654001, train_loss=2.9260607

Batch 275120, train_perplexity=18.654001, train_loss=2.9260607

Batch 275130, train_perplexity=18.654001, train_loss=2.9260607

Batch 275140, train_perplexity=18.654001, train_loss=2.9260607

Batch 275150, train_perplexity=18.654001, train_loss=2.9260607

Batch 275160, train_perplexity=18.654001, train_loss=2.9260607

Batch 275170, train_perplexity=18.654001, train_loss=2.9260607

Batch 275180, train_perplexity=18.653997, train_loss=2.9260604

Batch 275190, train_perplexity=18.654001, train_loss=2.9260607

Batch 275200, train_perplexity=18.654001, train_loss=2.9260607

Batch 275210, train_perplexity=18.654001, train_loss=2.9260607

Batch 275220, train_perplexity=18.653992, train_loss=2.9260602

Batch 275230, train_perplexity=18.653997, train_loss=2.9260604

Batch 275240, train_perplexity=18.654001, train_loss=2.9260607

Batch 275250, train_perplexity=18.653997, train_loss=2.9260604

Batch 275260, train_perplexity=18.653997, train_loss=2.9260604

Batch 275270, train_perplexity=18.654001, train_loss=2.9260607

Batch 275280, train_perplexity=18.653997, train_loss=2.9260604

Batch 275290, train_perplexity=18.653992, train_loss=2.9260602

Batch 275300, train_perplexity=18.653992, train_loss=2.9260602

Batch 275310, train_perplexity=18.653992, train_loss=2.9260602

Batch 275320, train_perplexity=18.653997, train_loss=2.9260604

Batch 275330, train_perplexity=18.653997, train_loss=2.9260604

Batch 275340, train_perplexity=18.653992, train_loss=2.9260602

Batch 275350, train_perplexity=18.653992, train_loss=2.9260602

Batch 275360, train_perplexity=18.653992, train_loss=2.9260602

Batch 275370, train_perplexity=18.653992, train_loss=2.9260602
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 275380, train_perplexity=18.653992, train_loss=2.9260602

Batch 275390, train_perplexity=18.653992, train_loss=2.9260602

Batch 275400, train_perplexity=18.653992, train_loss=2.9260602

Batch 275410, train_perplexity=18.653992, train_loss=2.9260602

Batch 275420, train_perplexity=18.653997, train_loss=2.9260604

Batch 275430, train_perplexity=18.653992, train_loss=2.9260602

Batch 275440, train_perplexity=18.653997, train_loss=2.9260604

Batch 275450, train_perplexity=18.653992, train_loss=2.9260602

Batch 275460, train_perplexity=18.653992, train_loss=2.9260602

Batch 275470, train_perplexity=18.653992, train_loss=2.9260602

Batch 275480, train_perplexity=18.653992, train_loss=2.9260602

Batch 275490, train_perplexity=18.653992, train_loss=2.9260602

Batch 275500, train_perplexity=18.653992, train_loss=2.9260602

Batch 275510, train_perplexity=18.653992, train_loss=2.9260602

Batch 275520, train_perplexity=18.653988, train_loss=2.92606

Batch 275530, train_perplexity=18.653992, train_loss=2.9260602

Batch 275540, train_perplexity=18.653988, train_loss=2.92606

Batch 275550, train_perplexity=18.653984, train_loss=2.9260597

Batch 275560, train_perplexity=18.653992, train_loss=2.9260602

Batch 275570, train_perplexity=18.653984, train_loss=2.9260597

Batch 275580, train_perplexity=18.653992, train_loss=2.9260602

Batch 275590, train_perplexity=18.653984, train_loss=2.9260597

Batch 275600, train_perplexity=18.653984, train_loss=2.9260597

Batch 275610, train_perplexity=18.653988, train_loss=2.92606

Batch 275620, train_perplexity=18.653984, train_loss=2.9260597

Batch 275630, train_perplexity=18.653988, train_loss=2.92606

Batch 275640, train_perplexity=18.653992, train_loss=2.9260602

Batch 275650, train_perplexity=18.653992, train_loss=2.9260602

Batch 275660, train_perplexity=18.653988, train_loss=2.92606

Batch 275670, train_perplexity=18.653984, train_loss=2.9260597

Batch 275680, train_perplexity=18.653984, train_loss=2.9260597

Batch 275690, train_perplexity=18.653988, train_loss=2.92606

Batch 275700, train_perplexity=18.653984, train_loss=2.9260597

Batch 275710, train_perplexity=18.653984, train_loss=2.9260597

Batch 275720, train_perplexity=18.653984, train_loss=2.9260597

Batch 275730, train_perplexity=18.653984, train_loss=2.9260597

Batch 275740, train_perplexity=18.653984, train_loss=2.9260597

Batch 275750, train_perplexity=18.653984, train_loss=2.9260597

Batch 275760, train_perplexity=18.653984, train_loss=2.9260597

Batch 275770, train_perplexity=18.653984, train_loss=2.9260597

Batch 275780, train_perplexity=18.653978, train_loss=2.9260595

Batch 275790, train_perplexity=18.653984, train_loss=2.9260597

Batch 275800, train_perplexity=18.653975, train_loss=2.9260592

Batch 275810, train_perplexity=18.653984, train_loss=2.9260597

Batch 275820, train_perplexity=18.653984, train_loss=2.9260597

Batch 275830, train_perplexity=18.653984, train_loss=2.9260597

Batch 275840, train_perplexity=18.653984, train_loss=2.9260597

Batch 275850, train_perplexity=18.653984, train_loss=2.9260597

Batch 275860, train_perplexity=18.653978, train_loss=2.9260595

Batch 275870, train_perplexity=18.653984, train_loss=2.9260597

Batch 275880, train_perplexity=18.653978, train_loss=2.9260595

Batch 275890, train_perplexity=18.653978, train_loss=2.9260595

Batch 275900, train_perplexity=18.653975, train_loss=2.9260592

Batch 275910, train_perplexity=18.653975, train_loss=2.9260592

Batch 275920, train_perplexity=18.653975, train_loss=2.9260592

Batch 275930, train_perplexity=18.653984, train_loss=2.9260597

Batch 275940, train_perplexity=18.653975, train_loss=2.9260592

Batch 275950, train_perplexity=18.653975, train_loss=2.9260592

Batch 275960, train_perplexity=18.653975, train_loss=2.9260592

Batch 275970, train_perplexity=18.653975, train_loss=2.9260592

Batch 275980, train_perplexity=18.653975, train_loss=2.9260592

Batch 275990, train_perplexity=18.653975, train_loss=2.9260592

Batch 276000, train_perplexity=18.653975, train_loss=2.9260592

Batch 276010, train_perplexity=18.653975, train_loss=2.9260592

Batch 276020, train_perplexity=18.653975, train_loss=2.9260592

Batch 276030, train_perplexity=18.653975, train_loss=2.9260592

Batch 276040, train_perplexity=18.653975, train_loss=2.9260592

Batch 276050, train_perplexity=18.653975, train_loss=2.9260592

Batch 276060, train_perplexity=18.653975, train_loss=2.9260592

Batch 276070, train_perplexity=18.653975, train_loss=2.9260592

Batch 276080, train_perplexity=18.65397, train_loss=2.926059

Batch 276090, train_perplexity=18.653975, train_loss=2.9260592

Batch 276100, train_perplexity=18.653975, train_loss=2.9260592

Batch 276110, train_perplexity=18.653975, train_loss=2.9260592

Batch 276120, train_perplexity=18.65397, train_loss=2.926059

Batch 276130, train_perplexity=18.653965, train_loss=2.9260588

Batch 276140, train_perplexity=18.65397, train_loss=2.926059

Batch 276150, train_perplexity=18.653965, train_loss=2.9260588

Batch 276160, train_perplexity=18.65397, train_loss=2.926059

Batch 276170, train_perplexity=18.653965, train_loss=2.9260588

Batch 276180, train_perplexity=18.653965, train_loss=2.9260588

Batch 276190, train_perplexity=18.653965, train_loss=2.9260588

Batch 276200, train_perplexity=18.65397, train_loss=2.926059

Batch 276210, train_perplexity=18.653975, train_loss=2.9260592

Batch 276220, train_perplexity=18.653965, train_loss=2.9260588

Batch 276230, train_perplexity=18.653965, train_loss=2.9260588

Batch 276240, train_perplexity=18.653965, train_loss=2.9260588

Batch 276250, train_perplexity=18.653965, train_loss=2.9260588

Batch 276260, train_perplexity=18.653965, train_loss=2.9260588

Batch 276270, train_perplexity=18.653965, train_loss=2.9260588

Batch 276280, train_perplexity=18.65397, train_loss=2.926059

Batch 276290, train_perplexity=18.653965, train_loss=2.9260588

Batch 276300, train_perplexity=18.653965, train_loss=2.9260588

Batch 276310, train_perplexity=18.653975, train_loss=2.9260592

Batch 276320, train_perplexity=18.653965, train_loss=2.9260588

Batch 276330, train_perplexity=18.653965, train_loss=2.9260588

Batch 276340, train_perplexity=18.653965, train_loss=2.9260588

Batch 276350, train_perplexity=18.653965, train_loss=2.9260588

Batch 276360, train_perplexity=18.653965, train_loss=2.9260588

Batch 276370, train_perplexity=18.653965, train_loss=2.9260588

Batch 276380, train_perplexity=18.653965, train_loss=2.9260588

Batch 276390, train_perplexity=18.65397, train_loss=2.926059

Batch 276400, train_perplexity=18.653965, train_loss=2.9260588

Batch 276410, train_perplexity=18.653965, train_loss=2.9260588

Batch 276420, train_perplexity=18.653965, train_loss=2.9260588

Batch 276430, train_perplexity=18.653965, train_loss=2.9260588

Batch 276440, train_perplexity=18.653957, train_loss=2.9260583

Batch 276450, train_perplexity=18.653961, train_loss=2.9260585

Batch 276460, train_perplexity=18.653965, train_loss=2.9260588

Batch 276470, train_perplexity=18.653965, train_loss=2.9260588

Batch 276480, train_perplexity=18.653957, train_loss=2.9260583

Batch 276490, train_perplexity=18.653957, train_loss=2.9260583

Batch 276500, train_perplexity=18.653957, train_loss=2.9260583

Batch 276510, train_perplexity=18.653965, train_loss=2.9260588

Batch 276520, train_perplexity=18.653965, train_loss=2.9260588

Batch 276530, train_perplexity=18.653961, train_loss=2.9260585

Batch 276540, train_perplexity=18.653957, train_loss=2.9260583

Batch 276550, train_perplexity=18.653957, train_loss=2.9260583

Batch 276560, train_perplexity=18.653961, train_loss=2.9260585

Batch 276570, train_perplexity=18.653957, train_loss=2.9260583

Batch 276580, train_perplexity=18.653961, train_loss=2.9260585

Batch 276590, train_perplexity=18.653957, train_loss=2.9260583

Batch 276600, train_perplexity=18.653957, train_loss=2.9260583

Batch 276610, train_perplexity=18.653961, train_loss=2.9260585

Batch 276620, train_perplexity=18.653957, train_loss=2.9260583

Batch 276630, train_perplexity=18.653957, train_loss=2.9260583

Batch 276640, train_perplexity=18.653957, train_loss=2.9260583

Batch 276650, train_perplexity=18.653957, train_loss=2.9260583

Batch 276660, train_perplexity=18.653957, train_loss=2.9260583
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 276670, train_perplexity=18.653957, train_loss=2.9260583

Batch 276680, train_perplexity=18.653957, train_loss=2.9260583

Batch 276690, train_perplexity=18.653948, train_loss=2.9260578

Batch 276700, train_perplexity=18.653957, train_loss=2.9260583

Batch 276710, train_perplexity=18.653957, train_loss=2.9260583

Batch 276720, train_perplexity=18.653957, train_loss=2.9260583

Batch 276730, train_perplexity=18.653957, train_loss=2.9260583

Batch 276740, train_perplexity=18.653952, train_loss=2.926058

Batch 276750, train_perplexity=18.653957, train_loss=2.9260583

Batch 276760, train_perplexity=18.653948, train_loss=2.9260578

Batch 276770, train_perplexity=18.653952, train_loss=2.926058

Batch 276780, train_perplexity=18.653957, train_loss=2.9260583

Batch 276790, train_perplexity=18.653957, train_loss=2.9260583

Batch 276800, train_perplexity=18.653948, train_loss=2.9260578

Batch 276810, train_perplexity=18.653952, train_loss=2.926058

Batch 276820, train_perplexity=18.653948, train_loss=2.9260578

Batch 276830, train_perplexity=18.653952, train_loss=2.926058

Batch 276840, train_perplexity=18.653948, train_loss=2.9260578

Batch 276850, train_perplexity=18.653948, train_loss=2.9260578

Batch 276860, train_perplexity=18.653948, train_loss=2.9260578

Batch 276870, train_perplexity=18.653952, train_loss=2.926058

Batch 276880, train_perplexity=18.653948, train_loss=2.9260578

Batch 276890, train_perplexity=18.653948, train_loss=2.9260578

Batch 276900, train_perplexity=18.653948, train_loss=2.9260578

Batch 276910, train_perplexity=18.653948, train_loss=2.9260578

Batch 276920, train_perplexity=18.653948, train_loss=2.9260578

Batch 276930, train_perplexity=18.653948, train_loss=2.9260578

Batch 276940, train_perplexity=18.653948, train_loss=2.9260578

Batch 276950, train_perplexity=18.653948, train_loss=2.9260578

Batch 276960, train_perplexity=18.653948, train_loss=2.9260578

Batch 276970, train_perplexity=18.653948, train_loss=2.9260578

Batch 276980, train_perplexity=18.653948, train_loss=2.9260578

Batch 276990, train_perplexity=18.653948, train_loss=2.9260578

Batch 277000, train_perplexity=18.653948, train_loss=2.9260578

Batch 277010, train_perplexity=18.653948, train_loss=2.9260578

Batch 277020, train_perplexity=18.653944, train_loss=2.9260576

Batch 277030, train_perplexity=18.653944, train_loss=2.9260576

Batch 277040, train_perplexity=18.653944, train_loss=2.9260576

Batch 277050, train_perplexity=18.653944, train_loss=2.9260576

Batch 277060, train_perplexity=18.653948, train_loss=2.9260578

Batch 277070, train_perplexity=18.653938, train_loss=2.9260573

Batch 277080, train_perplexity=18.653938, train_loss=2.9260573

Batch 277090, train_perplexity=18.653938, train_loss=2.9260573

Batch 277100, train_perplexity=18.653938, train_loss=2.9260573

Batch 277110, train_perplexity=18.653948, train_loss=2.9260578

Batch 277120, train_perplexity=18.653944, train_loss=2.9260576

Batch 277130, train_perplexity=18.653938, train_loss=2.9260573

Batch 277140, train_perplexity=18.653938, train_loss=2.9260573

Batch 277150, train_perplexity=18.653938, train_loss=2.9260573

Batch 277160, train_perplexity=18.653944, train_loss=2.9260576

Batch 277170, train_perplexity=18.653938, train_loss=2.9260573

Batch 277180, train_perplexity=18.653938, train_loss=2.9260573

Batch 277190, train_perplexity=18.653938, train_loss=2.9260573

Batch 277200, train_perplexity=18.653938, train_loss=2.9260573

Batch 277210, train_perplexity=18.653938, train_loss=2.9260573

Batch 277220, train_perplexity=18.653938, train_loss=2.9260573

Batch 277230, train_perplexity=18.653938, train_loss=2.9260573

Batch 277240, train_perplexity=18.653938, train_loss=2.9260573

Batch 277250, train_perplexity=18.653938, train_loss=2.9260573

Batch 277260, train_perplexity=18.653938, train_loss=2.9260573

Batch 277270, train_perplexity=18.653938, train_loss=2.9260573

Batch 277280, train_perplexity=18.653938, train_loss=2.9260573

Batch 277290, train_perplexity=18.653938, train_loss=2.9260573

Batch 277300, train_perplexity=18.653938, train_loss=2.9260573

Batch 277310, train_perplexity=18.653938, train_loss=2.9260573

Batch 277320, train_perplexity=18.65393, train_loss=2.9260569

Batch 277330, train_perplexity=18.65393, train_loss=2.9260569

Batch 277340, train_perplexity=18.653938, train_loss=2.9260573

Batch 277350, train_perplexity=18.653938, train_loss=2.9260573

Batch 277360, train_perplexity=18.65393, train_loss=2.9260569

Batch 277370, train_perplexity=18.65393, train_loss=2.9260569

Batch 277380, train_perplexity=18.65393, train_loss=2.9260569

Batch 277390, train_perplexity=18.65393, train_loss=2.9260569

Batch 277400, train_perplexity=18.653934, train_loss=2.926057

Batch 277410, train_perplexity=18.653934, train_loss=2.926057

Batch 277420, train_perplexity=18.65393, train_loss=2.9260569

Batch 277430, train_perplexity=18.65393, train_loss=2.9260569

Batch 277440, train_perplexity=18.653934, train_loss=2.926057

Batch 277450, train_perplexity=18.65393, train_loss=2.9260569

Batch 277460, train_perplexity=18.65393, train_loss=2.9260569

Batch 277470, train_perplexity=18.65393, train_loss=2.9260569

Batch 277480, train_perplexity=18.653934, train_loss=2.926057

Batch 277490, train_perplexity=18.65393, train_loss=2.9260569

Batch 277500, train_perplexity=18.653934, train_loss=2.926057

Batch 277510, train_perplexity=18.65393, train_loss=2.9260569

Batch 277520, train_perplexity=18.65393, train_loss=2.9260569

Batch 277530, train_perplexity=18.65393, train_loss=2.9260569

Batch 277540, train_perplexity=18.65393, train_loss=2.9260569

Batch 277550, train_perplexity=18.65393, train_loss=2.9260569

Batch 277560, train_perplexity=18.65393, train_loss=2.9260569

Batch 277570, train_perplexity=18.65393, train_loss=2.9260569

Batch 277580, train_perplexity=18.65393, train_loss=2.9260569

Batch 277590, train_perplexity=18.65393, train_loss=2.9260569

Batch 277600, train_perplexity=18.653925, train_loss=2.9260566

Batch 277610, train_perplexity=18.653921, train_loss=2.9260564

Batch 277620, train_perplexity=18.653925, train_loss=2.9260566

Batch 277630, train_perplexity=18.653925, train_loss=2.9260566

Batch 277640, train_perplexity=18.653925, train_loss=2.9260566

Batch 277650, train_perplexity=18.65393, train_loss=2.9260569

Batch 277660, train_perplexity=18.653925, train_loss=2.9260566

Batch 277670, train_perplexity=18.65393, train_loss=2.9260569

Batch 277680, train_perplexity=18.653925, train_loss=2.9260566

Batch 277690, train_perplexity=18.653921, train_loss=2.9260564

Batch 277700, train_perplexity=18.653921, train_loss=2.9260564

Batch 277710, train_perplexity=18.653925, train_loss=2.9260566

Batch 277720, train_perplexity=18.653925, train_loss=2.9260566

Batch 277730, train_perplexity=18.653925, train_loss=2.9260566

Batch 277740, train_perplexity=18.653921, train_loss=2.9260564

Batch 277750, train_perplexity=18.653921, train_loss=2.9260564

Batch 277760, train_perplexity=18.653921, train_loss=2.9260564

Batch 277770, train_perplexity=18.653921, train_loss=2.9260564

Batch 277780, train_perplexity=18.653921, train_loss=2.9260564

Batch 277790, train_perplexity=18.653921, train_loss=2.9260564

Batch 277800, train_perplexity=18.653921, train_loss=2.9260564

Batch 277810, train_perplexity=18.653921, train_loss=2.9260564

Batch 277820, train_perplexity=18.653921, train_loss=2.9260564

Batch 277830, train_perplexity=18.653921, train_loss=2.9260564

Batch 277840, train_perplexity=18.653921, train_loss=2.9260564

Batch 277850, train_perplexity=18.653921, train_loss=2.9260564

Batch 277860, train_perplexity=18.653921, train_loss=2.9260564

Batch 277870, train_perplexity=18.653921, train_loss=2.9260564

Batch 277880, train_perplexity=18.653921, train_loss=2.9260564

Batch 277890, train_perplexity=18.653921, train_loss=2.9260564

Batch 277900, train_perplexity=18.653917, train_loss=2.9260561

Batch 277910, train_perplexity=18.653917, train_loss=2.9260561

Batch 277920, train_perplexity=18.653912, train_loss=2.926056

Batch 277930, train_perplexity=18.653912, train_loss=2.926056

Batch 277940, train_perplexity=18.653912, train_loss=2.926056

Batch 277950, train_perplexity=18.653921, train_loss=2.9260564
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 277960, train_perplexity=18.653912, train_loss=2.926056

Batch 277970, train_perplexity=18.653912, train_loss=2.926056

Batch 277980, train_perplexity=18.653917, train_loss=2.9260561

Batch 277990, train_perplexity=18.653912, train_loss=2.926056

Batch 278000, train_perplexity=18.653912, train_loss=2.926056

Batch 278010, train_perplexity=18.653912, train_loss=2.926056

Batch 278020, train_perplexity=18.653912, train_loss=2.926056

Batch 278030, train_perplexity=18.653917, train_loss=2.9260561

Batch 278040, train_perplexity=18.653921, train_loss=2.9260564

Batch 278050, train_perplexity=18.653921, train_loss=2.9260564

Batch 278060, train_perplexity=18.653912, train_loss=2.926056

Batch 278070, train_perplexity=18.653912, train_loss=2.926056

Batch 278080, train_perplexity=18.653912, train_loss=2.926056

Batch 278090, train_perplexity=18.653912, train_loss=2.926056

Batch 278100, train_perplexity=18.653917, train_loss=2.9260561

Batch 278110, train_perplexity=18.653912, train_loss=2.926056

Batch 278120, train_perplexity=18.653912, train_loss=2.926056

Batch 278130, train_perplexity=18.653912, train_loss=2.926056

Batch 278140, train_perplexity=18.653912, train_loss=2.926056

Batch 278150, train_perplexity=18.653912, train_loss=2.926056

Batch 278160, train_perplexity=18.653912, train_loss=2.926056

Batch 278170, train_perplexity=18.653912, train_loss=2.926056

Batch 278180, train_perplexity=18.653912, train_loss=2.926056

Batch 278190, train_perplexity=18.653912, train_loss=2.926056

Batch 278200, train_perplexity=18.653912, train_loss=2.926056

Batch 278210, train_perplexity=18.653912, train_loss=2.926056

Batch 278220, train_perplexity=18.653912, train_loss=2.926056

Batch 278230, train_perplexity=18.653908, train_loss=2.9260557

Batch 278240, train_perplexity=18.653912, train_loss=2.926056

Batch 278250, train_perplexity=18.653912, train_loss=2.926056

Batch 278260, train_perplexity=18.653912, train_loss=2.926056

Batch 278270, train_perplexity=18.653904, train_loss=2.9260554

Batch 278280, train_perplexity=18.653908, train_loss=2.9260557

Batch 278290, train_perplexity=18.653904, train_loss=2.9260554

Batch 278300, train_perplexity=18.653904, train_loss=2.9260554

Batch 278310, train_perplexity=18.653904, train_loss=2.9260554

Batch 278320, train_perplexity=18.653908, train_loss=2.9260557

Batch 278330, train_perplexity=18.653904, train_loss=2.9260554

Batch 278340, train_perplexity=18.653904, train_loss=2.9260554

Batch 278350, train_perplexity=18.653904, train_loss=2.9260554

Batch 278360, train_perplexity=18.653904, train_loss=2.9260554

Batch 278370, train_perplexity=18.653904, train_loss=2.9260554

Batch 278380, train_perplexity=18.653904, train_loss=2.9260554

Batch 278390, train_perplexity=18.653904, train_loss=2.9260554

Batch 278400, train_perplexity=18.653904, train_loss=2.9260554

Batch 278410, train_perplexity=18.653904, train_loss=2.9260554

Batch 278420, train_perplexity=18.653908, train_loss=2.9260557

Batch 278430, train_perplexity=18.653904, train_loss=2.9260554

Batch 278440, train_perplexity=18.653904, train_loss=2.9260554

Batch 278450, train_perplexity=18.653904, train_loss=2.9260554

Batch 278460, train_perplexity=18.653904, train_loss=2.9260554

Batch 278470, train_perplexity=18.653904, train_loss=2.9260554

Batch 278480, train_perplexity=18.653904, train_loss=2.9260554

Batch 278490, train_perplexity=18.653904, train_loss=2.9260554

Batch 278500, train_perplexity=18.653898, train_loss=2.9260552

Batch 278510, train_perplexity=18.653904, train_loss=2.9260554

Batch 278520, train_perplexity=18.653904, train_loss=2.9260554

Batch 278530, train_perplexity=18.653904, train_loss=2.9260554

Batch 278540, train_perplexity=18.653898, train_loss=2.9260552

Batch 278550, train_perplexity=18.653898, train_loss=2.9260552

Batch 278560, train_perplexity=18.653894, train_loss=2.926055

Batch 278570, train_perplexity=18.653904, train_loss=2.9260554

Batch 278580, train_perplexity=18.653904, train_loss=2.9260554

Batch 278590, train_perplexity=18.653898, train_loss=2.9260552

Batch 278600, train_perplexity=18.653894, train_loss=2.926055

Batch 278610, train_perplexity=18.653894, train_loss=2.926055

Batch 278620, train_perplexity=18.653894, train_loss=2.926055

Batch 278630, train_perplexity=18.653894, train_loss=2.926055

Batch 278640, train_perplexity=18.653894, train_loss=2.926055

Batch 278650, train_perplexity=18.653894, train_loss=2.926055

Batch 278660, train_perplexity=18.653894, train_loss=2.926055

Batch 278670, train_perplexity=18.653894, train_loss=2.926055

Batch 278680, train_perplexity=18.653894, train_loss=2.926055

Batch 278690, train_perplexity=18.653894, train_loss=2.926055

Batch 278700, train_perplexity=18.653894, train_loss=2.926055

Batch 278710, train_perplexity=18.653894, train_loss=2.926055

Batch 278720, train_perplexity=18.653894, train_loss=2.926055

Batch 278730, train_perplexity=18.653894, train_loss=2.926055

Batch 278740, train_perplexity=18.653894, train_loss=2.926055

Batch 278750, train_perplexity=18.653894, train_loss=2.926055

Batch 278760, train_perplexity=18.65389, train_loss=2.9260547

Batch 278770, train_perplexity=18.653894, train_loss=2.926055

Batch 278780, train_perplexity=18.653894, train_loss=2.926055

Batch 278790, train_perplexity=18.65389, train_loss=2.9260547

Batch 278800, train_perplexity=18.65389, train_loss=2.9260547

Batch 278810, train_perplexity=18.65389, train_loss=2.9260547

Batch 278820, train_perplexity=18.65389, train_loss=2.9260547

Batch 278830, train_perplexity=18.65389, train_loss=2.9260547

Batch 278840, train_perplexity=18.65389, train_loss=2.9260547

Batch 278850, train_perplexity=18.653894, train_loss=2.926055

Batch 278860, train_perplexity=18.653894, train_loss=2.926055

Batch 278870, train_perplexity=18.65389, train_loss=2.9260547

Batch 278880, train_perplexity=18.653885, train_loss=2.9260545

Batch 278890, train_perplexity=18.65389, train_loss=2.9260547

Batch 278900, train_perplexity=18.653885, train_loss=2.9260545

Batch 278910, train_perplexity=18.65389, train_loss=2.9260547

Batch 278920, train_perplexity=18.653885, train_loss=2.9260545

Batch 278930, train_perplexity=18.653885, train_loss=2.9260545

Batch 278940, train_perplexity=18.653885, train_loss=2.9260545

Batch 278950, train_perplexity=18.653885, train_loss=2.9260545

Batch 278960, train_perplexity=18.653885, train_loss=2.9260545

Batch 278970, train_perplexity=18.653885, train_loss=2.9260545

Batch 278980, train_perplexity=18.653885, train_loss=2.9260545

Batch 278990, train_perplexity=18.653885, train_loss=2.9260545

Batch 279000, train_perplexity=18.653885, train_loss=2.9260545

Batch 279010, train_perplexity=18.653885, train_loss=2.9260545

Batch 279020, train_perplexity=18.653881, train_loss=2.9260542

Batch 279030, train_perplexity=18.653885, train_loss=2.9260545

Batch 279040, train_perplexity=18.653885, train_loss=2.9260545

Batch 279050, train_perplexity=18.653885, train_loss=2.9260545

Batch 279060, train_perplexity=18.653885, train_loss=2.9260545

Batch 279070, train_perplexity=18.653881, train_loss=2.9260542

Batch 279080, train_perplexity=18.653885, train_loss=2.9260545

Batch 279090, train_perplexity=18.653885, train_loss=2.9260545

Batch 279100, train_perplexity=18.653877, train_loss=2.926054

Batch 279110, train_perplexity=18.653881, train_loss=2.9260542

Batch 279120, train_perplexity=18.653885, train_loss=2.9260545

Batch 279130, train_perplexity=18.653885, train_loss=2.9260545

Batch 279140, train_perplexity=18.653885, train_loss=2.9260545

Batch 279150, train_perplexity=18.653885, train_loss=2.9260545

Batch 279160, train_perplexity=18.653877, train_loss=2.926054

Batch 279170, train_perplexity=18.653877, train_loss=2.926054

Batch 279180, train_perplexity=18.653881, train_loss=2.9260542

Batch 279190, train_perplexity=18.653877, train_loss=2.926054

Batch 279200, train_perplexity=18.653881, train_loss=2.9260542

Batch 279210, train_perplexity=18.653877, train_loss=2.926054

Batch 279220, train_perplexity=18.653877, train_loss=2.926054

Batch 279230, train_perplexity=18.653877, train_loss=2.926054

Batch 279240, train_perplexity=18.653877, train_loss=2.926054

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 279250, train_perplexity=18.653877, train_loss=2.926054

Batch 279260, train_perplexity=18.653877, train_loss=2.926054

Batch 279270, train_perplexity=18.653872, train_loss=2.9260538

Batch 279280, train_perplexity=18.653877, train_loss=2.926054

Batch 279290, train_perplexity=18.653877, train_loss=2.926054

Batch 279300, train_perplexity=18.653877, train_loss=2.926054

Batch 279310, train_perplexity=18.653877, train_loss=2.926054

Batch 279320, train_perplexity=18.653877, train_loss=2.926054

Batch 279330, train_perplexity=18.653877, train_loss=2.926054

Batch 279340, train_perplexity=18.653877, train_loss=2.926054

Batch 279350, train_perplexity=18.653877, train_loss=2.926054

Batch 279360, train_perplexity=18.653877, train_loss=2.926054

Batch 279370, train_perplexity=18.653877, train_loss=2.926054

Batch 279380, train_perplexity=18.653877, train_loss=2.926054

Batch 279390, train_perplexity=18.653872, train_loss=2.9260538

Batch 279400, train_perplexity=18.653872, train_loss=2.9260538

Batch 279410, train_perplexity=18.653872, train_loss=2.9260538

Batch 279420, train_perplexity=18.653868, train_loss=2.9260535

Batch 279430, train_perplexity=18.653877, train_loss=2.926054

Batch 279440, train_perplexity=18.653877, train_loss=2.926054

Batch 279450, train_perplexity=18.653868, train_loss=2.9260535

Batch 279460, train_perplexity=18.653877, train_loss=2.926054

Batch 279470, train_perplexity=18.653872, train_loss=2.9260538

Batch 279480, train_perplexity=18.653877, train_loss=2.926054

Batch 279490, train_perplexity=18.653872, train_loss=2.9260538

Batch 279500, train_perplexity=18.653872, train_loss=2.9260538

Batch 279510, train_perplexity=18.653872, train_loss=2.9260538

Batch 279520, train_perplexity=18.653872, train_loss=2.9260538

Batch 279530, train_perplexity=18.653872, train_loss=2.9260538

Batch 279540, train_perplexity=18.653868, train_loss=2.9260535

Batch 279550, train_perplexity=18.653872, train_loss=2.9260538

Batch 279560, train_perplexity=18.653868, train_loss=2.9260535

Batch 279570, train_perplexity=18.653868, train_loss=2.9260535

Batch 279580, train_perplexity=18.653868, train_loss=2.9260535

Batch 279590, train_perplexity=18.653868, train_loss=2.9260535

Batch 279600, train_perplexity=18.653868, train_loss=2.9260535

Batch 279610, train_perplexity=18.653868, train_loss=2.9260535

Batch 279620, train_perplexity=18.653868, train_loss=2.9260535

Batch 279630, train_perplexity=18.653868, train_loss=2.9260535

Batch 279640, train_perplexity=18.653868, train_loss=2.9260535

Batch 279650, train_perplexity=18.653868, train_loss=2.9260535

Batch 279660, train_perplexity=18.653868, train_loss=2.9260535

Batch 279670, train_perplexity=18.653868, train_loss=2.9260535

Batch 279680, train_perplexity=18.653868, train_loss=2.9260535

Batch 279690, train_perplexity=18.653868, train_loss=2.9260535

Batch 279700, train_perplexity=18.653868, train_loss=2.9260535

Batch 279710, train_perplexity=18.653858, train_loss=2.926053

Batch 279720, train_perplexity=18.653868, train_loss=2.9260535

Batch 279730, train_perplexity=18.653864, train_loss=2.9260533

Batch 279740, train_perplexity=18.653864, train_loss=2.9260533

Batch 279750, train_perplexity=18.653864, train_loss=2.9260533

Batch 279760, train_perplexity=18.653868, train_loss=2.9260535

Batch 279770, train_perplexity=18.653864, train_loss=2.9260533

Batch 279780, train_perplexity=18.653868, train_loss=2.9260535

Batch 279790, train_perplexity=18.653868, train_loss=2.9260535

Batch 279800, train_perplexity=18.653868, train_loss=2.9260535

Batch 279810, train_perplexity=18.653858, train_loss=2.926053

Batch 279820, train_perplexity=18.653858, train_loss=2.926053

Batch 279830, train_perplexity=18.653858, train_loss=2.926053

Batch 279840, train_perplexity=18.653864, train_loss=2.9260533

Batch 279850, train_perplexity=18.653858, train_loss=2.926053

Batch 279860, train_perplexity=18.653858, train_loss=2.926053

Batch 279870, train_perplexity=18.653864, train_loss=2.9260533

Batch 279880, train_perplexity=18.653858, train_loss=2.926053

Batch 279890, train_perplexity=18.653858, train_loss=2.926053

Batch 279900, train_perplexity=18.653858, train_loss=2.926053

Batch 279910, train_perplexity=18.653858, train_loss=2.926053

Batch 279920, train_perplexity=18.653858, train_loss=2.926053

Batch 279930, train_perplexity=18.653858, train_loss=2.926053

Batch 279940, train_perplexity=18.653858, train_loss=2.926053

Batch 279950, train_perplexity=18.653858, train_loss=2.926053

Batch 279960, train_perplexity=18.653858, train_loss=2.926053

Batch 279970, train_perplexity=18.653858, train_loss=2.926053

Batch 279980, train_perplexity=18.653858, train_loss=2.926053

Batch 279990, train_perplexity=18.653858, train_loss=2.926053

Batch 280000, train_perplexity=18.653858, train_loss=2.926053

Batch 280010, train_perplexity=18.653858, train_loss=2.926053

Batch 280020, train_perplexity=18.653858, train_loss=2.926053

Batch 280030, train_perplexity=18.653858, train_loss=2.926053

Batch 280040, train_perplexity=18.653854, train_loss=2.9260528

Batch 280050, train_perplexity=18.653854, train_loss=2.9260528

Batch 280060, train_perplexity=18.653854, train_loss=2.9260528

Batch 280070, train_perplexity=18.653858, train_loss=2.926053

Batch 280080, train_perplexity=18.653854, train_loss=2.9260528

Batch 280090, train_perplexity=18.653854, train_loss=2.9260528

Batch 280100, train_perplexity=18.653858, train_loss=2.926053

Batch 280110, train_perplexity=18.653858, train_loss=2.926053

Batch 280120, train_perplexity=18.653854, train_loss=2.9260528

Batch 280130, train_perplexity=18.65385, train_loss=2.9260526

Batch 280140, train_perplexity=18.65385, train_loss=2.9260526

Batch 280150, train_perplexity=18.65385, train_loss=2.9260526

Batch 280160, train_perplexity=18.65385, train_loss=2.9260526

Batch 280170, train_perplexity=18.65385, train_loss=2.9260526

Batch 280180, train_perplexity=18.65385, train_loss=2.9260526

Batch 280190, train_perplexity=18.65385, train_loss=2.9260526

Batch 280200, train_perplexity=18.65385, train_loss=2.9260526

Batch 280210, train_perplexity=18.65385, train_loss=2.9260526

Batch 280220, train_perplexity=18.65385, train_loss=2.9260526

Batch 280230, train_perplexity=18.65385, train_loss=2.9260526

Batch 280240, train_perplexity=18.65385, train_loss=2.9260526

Batch 280250, train_perplexity=18.65385, train_loss=2.9260526

Batch 280260, train_perplexity=18.65385, train_loss=2.9260526

Batch 280270, train_perplexity=18.65385, train_loss=2.9260526

Batch 280280, train_perplexity=18.65385, train_loss=2.9260526

Batch 280290, train_perplexity=18.65385, train_loss=2.9260526

Batch 280300, train_perplexity=18.65385, train_loss=2.9260526

Batch 280310, train_perplexity=18.653845, train_loss=2.9260523

Batch 280320, train_perplexity=18.65385, train_loss=2.9260526

Batch 280330, train_perplexity=18.65385, train_loss=2.9260526

Batch 280340, train_perplexity=18.65385, train_loss=2.9260526

Batch 280350, train_perplexity=18.65385, train_loss=2.9260526

Batch 280360, train_perplexity=18.653845, train_loss=2.9260523

Batch 280370, train_perplexity=18.65385, train_loss=2.9260526

Batch 280380, train_perplexity=18.65385, train_loss=2.9260526

Batch 280390, train_perplexity=18.65385, train_loss=2.9260526

Batch 280400, train_perplexity=18.65385, train_loss=2.9260526

Batch 280410, train_perplexity=18.653841, train_loss=2.926052

Batch 280420, train_perplexity=18.653841, train_loss=2.926052

Batch 280430, train_perplexity=18.653845, train_loss=2.9260523

Batch 280440, train_perplexity=18.653845, train_loss=2.9260523

Batch 280450, train_perplexity=18.653841, train_loss=2.926052

Batch 280460, train_perplexity=18.653841, train_loss=2.926052

Batch 280470, train_perplexity=18.653845, train_loss=2.9260523

Batch 280480, train_perplexity=18.65385, train_loss=2.9260526

Batch 280490, train_perplexity=18.653841, train_loss=2.926052

Batch 280500, train_perplexity=18.653841, train_loss=2.926052

Batch 280510, train_perplexity=18.653841, train_loss=2.926052

Batch 280520, train_perplexity=18.653841, train_loss=2.926052

Batch 280530, train_perplexity=18.653841, train_loss=2.926052

Batch 280540, train_perplexity=18.653845, train_loss=2.9260523
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 280550, train_perplexity=18.653841, train_loss=2.926052

Batch 280560, train_perplexity=18.653841, train_loss=2.926052

Batch 280570, train_perplexity=18.653841, train_loss=2.926052

Batch 280580, train_perplexity=18.653837, train_loss=2.9260519

Batch 280590, train_perplexity=18.653841, train_loss=2.926052

Batch 280600, train_perplexity=18.653841, train_loss=2.926052

Batch 280610, train_perplexity=18.653841, train_loss=2.926052

Batch 280620, train_perplexity=18.653837, train_loss=2.9260519

Batch 280630, train_perplexity=18.653841, train_loss=2.926052

Batch 280640, train_perplexity=18.653841, train_loss=2.926052

Batch 280650, train_perplexity=18.653841, train_loss=2.926052

Batch 280660, train_perplexity=18.653841, train_loss=2.926052

Batch 280670, train_perplexity=18.653841, train_loss=2.926052

Batch 280680, train_perplexity=18.653837, train_loss=2.9260519

Batch 280690, train_perplexity=18.653841, train_loss=2.926052

Batch 280700, train_perplexity=18.653837, train_loss=2.9260519

Batch 280710, train_perplexity=18.653841, train_loss=2.926052

Batch 280720, train_perplexity=18.653837, train_loss=2.9260519

Batch 280730, train_perplexity=18.653841, train_loss=2.926052

Batch 280740, train_perplexity=18.653841, train_loss=2.926052

Batch 280750, train_perplexity=18.653831, train_loss=2.9260516

Batch 280760, train_perplexity=18.653837, train_loss=2.9260519

Batch 280770, train_perplexity=18.653831, train_loss=2.9260516

Batch 280780, train_perplexity=18.653831, train_loss=2.9260516

Batch 280790, train_perplexity=18.653831, train_loss=2.9260516

Batch 280800, train_perplexity=18.653831, train_loss=2.9260516

Batch 280810, train_perplexity=18.653831, train_loss=2.9260516

Batch 280820, train_perplexity=18.653831, train_loss=2.9260516

Batch 280830, train_perplexity=18.653837, train_loss=2.9260519

Batch 280840, train_perplexity=18.653831, train_loss=2.9260516

Batch 280850, train_perplexity=18.653831, train_loss=2.9260516

Batch 280860, train_perplexity=18.653831, train_loss=2.9260516

Batch 280870, train_perplexity=18.653831, train_loss=2.9260516

Batch 280880, train_perplexity=18.653831, train_loss=2.9260516

Batch 280890, train_perplexity=18.653831, train_loss=2.9260516

Batch 280900, train_perplexity=18.653831, train_loss=2.9260516

Batch 280910, train_perplexity=18.653831, train_loss=2.9260516

Batch 280920, train_perplexity=18.653831, train_loss=2.9260516

Batch 280930, train_perplexity=18.653831, train_loss=2.9260516

Batch 280940, train_perplexity=18.653828, train_loss=2.9260514

Batch 280950, train_perplexity=18.653831, train_loss=2.9260516

Batch 280960, train_perplexity=18.653828, train_loss=2.9260514

Batch 280970, train_perplexity=18.653824, train_loss=2.9260511

Batch 280980, train_perplexity=18.653831, train_loss=2.9260516

Batch 280990, train_perplexity=18.653824, train_loss=2.9260511

Batch 281000, train_perplexity=18.653824, train_loss=2.9260511

Batch 281010, train_perplexity=18.653824, train_loss=2.9260511

Batch 281020, train_perplexity=18.653824, train_loss=2.9260511

Batch 281030, train_perplexity=18.653828, train_loss=2.9260514

Batch 281040, train_perplexity=18.653824, train_loss=2.9260511

Batch 281050, train_perplexity=18.653824, train_loss=2.9260511

Batch 281060, train_perplexity=18.653831, train_loss=2.9260516

Batch 281070, train_perplexity=18.653824, train_loss=2.9260511

Batch 281080, train_perplexity=18.653831, train_loss=2.9260516

Batch 281090, train_perplexity=18.653824, train_loss=2.9260511

Batch 281100, train_perplexity=18.653824, train_loss=2.9260511

Batch 281110, train_perplexity=18.653824, train_loss=2.9260511

Batch 281120, train_perplexity=18.653824, train_loss=2.9260511

Batch 281130, train_perplexity=18.653824, train_loss=2.9260511

Batch 281140, train_perplexity=18.653824, train_loss=2.9260511

Batch 281150, train_perplexity=18.653824, train_loss=2.9260511

Batch 281160, train_perplexity=18.653824, train_loss=2.9260511

Batch 281170, train_perplexity=18.653824, train_loss=2.9260511

Batch 281180, train_perplexity=18.653828, train_loss=2.9260514

Batch 281190, train_perplexity=18.653824, train_loss=2.9260511

Batch 281200, train_perplexity=18.653824, train_loss=2.9260511

Batch 281210, train_perplexity=18.653824, train_loss=2.9260511

Batch 281220, train_perplexity=18.653824, train_loss=2.9260511

Batch 281230, train_perplexity=18.653824, train_loss=2.9260511

Batch 281240, train_perplexity=18.653818, train_loss=2.926051

Batch 281250, train_perplexity=18.653824, train_loss=2.9260511

Batch 281260, train_perplexity=18.653824, train_loss=2.9260511

Batch 281270, train_perplexity=18.653824, train_loss=2.9260511

Batch 281280, train_perplexity=18.653824, train_loss=2.9260511

Batch 281290, train_perplexity=18.653814, train_loss=2.9260507

Batch 281300, train_perplexity=18.653824, train_loss=2.9260511

Batch 281310, train_perplexity=18.653824, train_loss=2.9260511

Batch 281320, train_perplexity=18.653818, train_loss=2.926051

Batch 281330, train_perplexity=18.653824, train_loss=2.9260511

Batch 281340, train_perplexity=18.653824, train_loss=2.9260511

Batch 281350, train_perplexity=18.653818, train_loss=2.926051

Batch 281360, train_perplexity=18.653824, train_loss=2.9260511

Batch 281370, train_perplexity=18.653814, train_loss=2.9260507

Batch 281380, train_perplexity=18.653818, train_loss=2.926051

Batch 281390, train_perplexity=18.653818, train_loss=2.926051

Batch 281400, train_perplexity=18.653818, train_loss=2.926051

Batch 281410, train_perplexity=18.653824, train_loss=2.9260511

Batch 281420, train_perplexity=18.653814, train_loss=2.9260507

Batch 281430, train_perplexity=18.653814, train_loss=2.9260507

Batch 281440, train_perplexity=18.653814, train_loss=2.9260507

Batch 281450, train_perplexity=18.653814, train_loss=2.9260507

Batch 281460, train_perplexity=18.653814, train_loss=2.9260507

Batch 281470, train_perplexity=18.653814, train_loss=2.9260507

Batch 281480, train_perplexity=18.653814, train_loss=2.9260507

Batch 281490, train_perplexity=18.653814, train_loss=2.9260507

Batch 281500, train_perplexity=18.653814, train_loss=2.9260507

Batch 281510, train_perplexity=18.653814, train_loss=2.9260507

Batch 281520, train_perplexity=18.653814, train_loss=2.9260507

Batch 281530, train_perplexity=18.653814, train_loss=2.9260507

Batch 281540, train_perplexity=18.653814, train_loss=2.9260507

Batch 281550, train_perplexity=18.653814, train_loss=2.9260507

Batch 281560, train_perplexity=18.653814, train_loss=2.9260507

Batch 281570, train_perplexity=18.653807, train_loss=2.9260502

Batch 281580, train_perplexity=18.653814, train_loss=2.9260507

Batch 281590, train_perplexity=18.653814, train_loss=2.9260507

Batch 281600, train_perplexity=18.653814, train_loss=2.9260507

Batch 281610, train_perplexity=18.65381, train_loss=2.9260504

Batch 281620, train_perplexity=18.653814, train_loss=2.9260507

Batch 281630, train_perplexity=18.653814, train_loss=2.9260507

Batch 281640, train_perplexity=18.653814, train_loss=2.9260507

Batch 281650, train_perplexity=18.653807, train_loss=2.9260502

Batch 281660, train_perplexity=18.65381, train_loss=2.9260504

Batch 281670, train_perplexity=18.653807, train_loss=2.9260502

Batch 281680, train_perplexity=18.653807, train_loss=2.9260502

Batch 281690, train_perplexity=18.653814, train_loss=2.9260507

Batch 281700, train_perplexity=18.65381, train_loss=2.9260504

Batch 281710, train_perplexity=18.65381, train_loss=2.9260504

Batch 281720, train_perplexity=18.653807, train_loss=2.9260502

Batch 281730, train_perplexity=18.653807, train_loss=2.9260502

Batch 281740, train_perplexity=18.653807, train_loss=2.9260502

Batch 281750, train_perplexity=18.653807, train_loss=2.9260502

Batch 281760, train_perplexity=18.653807, train_loss=2.9260502

Batch 281770, train_perplexity=18.653807, train_loss=2.9260502

Batch 281780, train_perplexity=18.653807, train_loss=2.9260502

Batch 281790, train_perplexity=18.653807, train_loss=2.9260502

Batch 281800, train_perplexity=18.653807, train_loss=2.9260502

Batch 281810, train_perplexity=18.653807, train_loss=2.9260502

Batch 281820, train_perplexity=18.653807, train_loss=2.9260502

Batch 281830, train_perplexity=18.653807, train_loss=2.9260502
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 281840, train_perplexity=18.653807, train_loss=2.9260502

Batch 281850, train_perplexity=18.653807, train_loss=2.9260502

Batch 281860, train_perplexity=18.653807, train_loss=2.9260502

Batch 281870, train_perplexity=18.653807, train_loss=2.9260502

Batch 281880, train_perplexity=18.653807, train_loss=2.9260502

Batch 281890, train_perplexity=18.653807, train_loss=2.9260502

Batch 281900, train_perplexity=18.653807, train_loss=2.9260502

Batch 281910, train_perplexity=18.653801, train_loss=2.92605

Batch 281920, train_perplexity=18.653801, train_loss=2.92605

Batch 281930, train_perplexity=18.653807, train_loss=2.9260502

Batch 281940, train_perplexity=18.653807, train_loss=2.9260502

Batch 281950, train_perplexity=18.653801, train_loss=2.92605

Batch 281960, train_perplexity=18.653801, train_loss=2.92605

Batch 281970, train_perplexity=18.653797, train_loss=2.9260497

Batch 281980, train_perplexity=18.653801, train_loss=2.92605

Batch 281990, train_perplexity=18.653801, train_loss=2.92605

Batch 282000, train_perplexity=18.653801, train_loss=2.92605

Batch 282010, train_perplexity=18.653801, train_loss=2.92605

Batch 282020, train_perplexity=18.653797, train_loss=2.9260497

Batch 282030, train_perplexity=18.653801, train_loss=2.92605

Batch 282040, train_perplexity=18.653801, train_loss=2.92605

Batch 282050, train_perplexity=18.653797, train_loss=2.9260497

Batch 282060, train_perplexity=18.653797, train_loss=2.9260497

Batch 282070, train_perplexity=18.653797, train_loss=2.9260497

Batch 282080, train_perplexity=18.653797, train_loss=2.9260497

Batch 282090, train_perplexity=18.653797, train_loss=2.9260497

Batch 282100, train_perplexity=18.653797, train_loss=2.9260497

Batch 282110, train_perplexity=18.653797, train_loss=2.9260497

Batch 282120, train_perplexity=18.653797, train_loss=2.9260497

Batch 282130, train_perplexity=18.653797, train_loss=2.9260497

Batch 282140, train_perplexity=18.653797, train_loss=2.9260497

Batch 282150, train_perplexity=18.653797, train_loss=2.9260497

Batch 282160, train_perplexity=18.653797, train_loss=2.9260497

Batch 282170, train_perplexity=18.653801, train_loss=2.92605

Batch 282180, train_perplexity=18.653797, train_loss=2.9260497

Batch 282190, train_perplexity=18.653797, train_loss=2.9260497

Batch 282200, train_perplexity=18.653797, train_loss=2.9260497

Batch 282210, train_perplexity=18.653788, train_loss=2.9260492

Batch 282220, train_perplexity=18.653797, train_loss=2.9260497

Batch 282230, train_perplexity=18.653788, train_loss=2.9260492

Batch 282240, train_perplexity=18.653793, train_loss=2.9260495

Batch 282250, train_perplexity=18.653788, train_loss=2.9260492

Batch 282260, train_perplexity=18.653788, train_loss=2.9260492

Batch 282270, train_perplexity=18.653788, train_loss=2.9260492

Batch 282280, train_perplexity=18.653788, train_loss=2.9260492

Batch 282290, train_perplexity=18.653797, train_loss=2.9260497

Batch 282300, train_perplexity=18.653788, train_loss=2.9260492

Batch 282310, train_perplexity=18.653788, train_loss=2.9260492

Batch 282320, train_perplexity=18.653788, train_loss=2.9260492

Batch 282330, train_perplexity=18.653788, train_loss=2.9260492

Batch 282340, train_perplexity=18.653793, train_loss=2.9260495

Batch 282350, train_perplexity=18.653788, train_loss=2.9260492

Batch 282360, train_perplexity=18.653788, train_loss=2.9260492

Batch 282370, train_perplexity=18.653788, train_loss=2.9260492

Batch 282380, train_perplexity=18.653788, train_loss=2.9260492

Batch 282390, train_perplexity=18.653788, train_loss=2.9260492

Batch 282400, train_perplexity=18.653788, train_loss=2.9260492

Batch 282410, train_perplexity=18.653788, train_loss=2.9260492

Batch 282420, train_perplexity=18.653788, train_loss=2.9260492

Batch 282430, train_perplexity=18.653788, train_loss=2.9260492

Batch 282440, train_perplexity=18.653788, train_loss=2.9260492

Batch 282450, train_perplexity=18.653788, train_loss=2.9260492

Batch 282460, train_perplexity=18.653788, train_loss=2.9260492

Batch 282470, train_perplexity=18.653788, train_loss=2.9260492

Batch 282480, train_perplexity=18.653788, train_loss=2.9260492

Batch 282490, train_perplexity=18.653788, train_loss=2.9260492

Batch 282500, train_perplexity=18.653788, train_loss=2.9260492

Batch 282510, train_perplexity=18.653784, train_loss=2.926049

Batch 282520, train_perplexity=18.65378, train_loss=2.9260488

Batch 282530, train_perplexity=18.653788, train_loss=2.9260492

Batch 282540, train_perplexity=18.653784, train_loss=2.926049

Batch 282550, train_perplexity=18.65378, train_loss=2.9260488

Batch 282560, train_perplexity=18.653784, train_loss=2.926049

Batch 282570, train_perplexity=18.65378, train_loss=2.9260488

Batch 282580, train_perplexity=18.653788, train_loss=2.9260492

Batch 282590, train_perplexity=18.653784, train_loss=2.926049

Batch 282600, train_perplexity=18.65378, train_loss=2.9260488

Batch 282610, train_perplexity=18.653784, train_loss=2.926049

Batch 282620, train_perplexity=18.65378, train_loss=2.9260488

Batch 282630, train_perplexity=18.65378, train_loss=2.9260488

Batch 282640, train_perplexity=18.65378, train_loss=2.9260488

Batch 282650, train_perplexity=18.65378, train_loss=2.9260488

Batch 282660, train_perplexity=18.65378, train_loss=2.9260488

Batch 282670, train_perplexity=18.65378, train_loss=2.9260488

Batch 282680, train_perplexity=18.65378, train_loss=2.9260488

Batch 282690, train_perplexity=18.65378, train_loss=2.9260488

Batch 282700, train_perplexity=18.65378, train_loss=2.9260488

Batch 282710, train_perplexity=18.653784, train_loss=2.926049

Batch 282720, train_perplexity=18.653784, train_loss=2.926049

Batch 282730, train_perplexity=18.65378, train_loss=2.9260488

Batch 282740, train_perplexity=18.65378, train_loss=2.9260488

Batch 282750, train_perplexity=18.65378, train_loss=2.9260488

Batch 282760, train_perplexity=18.65378, train_loss=2.9260488

Batch 282770, train_perplexity=18.65378, train_loss=2.9260488

Batch 282780, train_perplexity=18.65378, train_loss=2.9260488

Batch 282790, train_perplexity=18.65378, train_loss=2.9260488

Batch 282800, train_perplexity=18.65378, train_loss=2.9260488

Batch 282810, train_perplexity=18.65378, train_loss=2.9260488

Batch 282820, train_perplexity=18.653774, train_loss=2.9260485

Batch 282830, train_perplexity=18.653774, train_loss=2.9260485

Batch 282840, train_perplexity=18.65378, train_loss=2.9260488

Batch 282850, train_perplexity=18.65378, train_loss=2.9260488

Batch 282860, train_perplexity=18.65378, train_loss=2.9260488

Batch 282870, train_perplexity=18.653774, train_loss=2.9260485

Batch 282880, train_perplexity=18.65377, train_loss=2.9260483

Batch 282890, train_perplexity=18.65377, train_loss=2.9260483

Batch 282900, train_perplexity=18.653774, train_loss=2.9260485

Batch 282910, train_perplexity=18.65377, train_loss=2.9260483

Batch 282920, train_perplexity=18.65377, train_loss=2.9260483

Batch 282930, train_perplexity=18.65378, train_loss=2.9260488

Batch 282940, train_perplexity=18.653774, train_loss=2.9260485

Batch 282950, train_perplexity=18.65377, train_loss=2.9260483

Batch 282960, train_perplexity=18.65377, train_loss=2.9260483

Batch 282970, train_perplexity=18.65377, train_loss=2.9260483

Batch 282980, train_perplexity=18.65377, train_loss=2.9260483

Batch 282990, train_perplexity=18.65377, train_loss=2.9260483

Batch 283000, train_perplexity=18.65377, train_loss=2.9260483

Batch 283010, train_perplexity=18.65378, train_loss=2.9260488

Batch 283020, train_perplexity=18.65377, train_loss=2.9260483

Batch 283030, train_perplexity=18.65377, train_loss=2.9260483

Batch 283040, train_perplexity=18.65377, train_loss=2.9260483

Batch 283050, train_perplexity=18.65377, train_loss=2.9260483

Batch 283060, train_perplexity=18.65377, train_loss=2.9260483

Batch 283070, train_perplexity=18.65377, train_loss=2.9260483

Batch 283080, train_perplexity=18.65377, train_loss=2.9260483

Batch 283090, train_perplexity=18.65377, train_loss=2.9260483

Batch 283100, train_perplexity=18.65377, train_loss=2.9260483

Batch 283110, train_perplexity=18.65377, train_loss=2.9260483

Batch 283120, train_perplexity=18.653767, train_loss=2.926048

Batch 283130, train_perplexity=18.65377, train_loss=2.9260483
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 283140, train_perplexity=18.65376, train_loss=2.9260478

Batch 283150, train_perplexity=18.65377, train_loss=2.9260483

Batch 283160, train_perplexity=18.65377, train_loss=2.9260483

Batch 283170, train_perplexity=18.653767, train_loss=2.926048

Batch 283180, train_perplexity=18.653767, train_loss=2.926048

Batch 283190, train_perplexity=18.65377, train_loss=2.9260483

Batch 283200, train_perplexity=18.653767, train_loss=2.926048

Batch 283210, train_perplexity=18.65376, train_loss=2.9260478

Batch 283220, train_perplexity=18.65376, train_loss=2.9260478

Batch 283230, train_perplexity=18.65377, train_loss=2.9260483

Batch 283240, train_perplexity=18.653767, train_loss=2.926048

Batch 283250, train_perplexity=18.653767, train_loss=2.926048

Batch 283260, train_perplexity=18.653767, train_loss=2.926048

Batch 283270, train_perplexity=18.65377, train_loss=2.9260483

Batch 283280, train_perplexity=18.65376, train_loss=2.9260478

Batch 283290, train_perplexity=18.65376, train_loss=2.9260478

Batch 283300, train_perplexity=18.65377, train_loss=2.9260483

Batch 283310, train_perplexity=18.65376, train_loss=2.9260478

Batch 283320, train_perplexity=18.65376, train_loss=2.9260478

Batch 283330, train_perplexity=18.65376, train_loss=2.9260478

Batch 283340, train_perplexity=18.65376, train_loss=2.9260478

Batch 283350, train_perplexity=18.65376, train_loss=2.9260478

Batch 283360, train_perplexity=18.65376, train_loss=2.9260478

Batch 283370, train_perplexity=18.65376, train_loss=2.9260478

Batch 283380, train_perplexity=18.65376, train_loss=2.9260478

Batch 283390, train_perplexity=18.65376, train_loss=2.9260478

Batch 283400, train_perplexity=18.65376, train_loss=2.9260478

Batch 283410, train_perplexity=18.65376, train_loss=2.9260478

Batch 283420, train_perplexity=18.65376, train_loss=2.9260478

Batch 283430, train_perplexity=18.65376, train_loss=2.9260478

Batch 283440, train_perplexity=18.65376, train_loss=2.9260478

Batch 283450, train_perplexity=18.65376, train_loss=2.9260478

Batch 283460, train_perplexity=18.653753, train_loss=2.9260473

Batch 283470, train_perplexity=18.653757, train_loss=2.9260476

Batch 283480, train_perplexity=18.653753, train_loss=2.9260473

Batch 283490, train_perplexity=18.653757, train_loss=2.9260476

Batch 283500, train_perplexity=18.65376, train_loss=2.9260478

Batch 283510, train_perplexity=18.65376, train_loss=2.9260478

Batch 283520, train_perplexity=18.653753, train_loss=2.9260473

Batch 283530, train_perplexity=18.653757, train_loss=2.9260476

Batch 283540, train_perplexity=18.653753, train_loss=2.9260473

Batch 283550, train_perplexity=18.65376, train_loss=2.9260478

Batch 283560, train_perplexity=18.653753, train_loss=2.9260473

Batch 283570, train_perplexity=18.65376, train_loss=2.9260478

Batch 283580, train_perplexity=18.653753, train_loss=2.9260473

Batch 283590, train_perplexity=18.65376, train_loss=2.9260478

Batch 283600, train_perplexity=18.653753, train_loss=2.9260473

Batch 283610, train_perplexity=18.653757, train_loss=2.9260476

Batch 283620, train_perplexity=18.653753, train_loss=2.9260473

Batch 283630, train_perplexity=18.653753, train_loss=2.9260473

Batch 283640, train_perplexity=18.653753, train_loss=2.9260473

Batch 283650, train_perplexity=18.653753, train_loss=2.9260473

Batch 283660, train_perplexity=18.653748, train_loss=2.926047

Batch 283670, train_perplexity=18.653753, train_loss=2.9260473

Batch 283680, train_perplexity=18.653753, train_loss=2.9260473

Batch 283690, train_perplexity=18.653753, train_loss=2.9260473

Batch 283700, train_perplexity=18.653753, train_loss=2.9260473

Batch 283710, train_perplexity=18.653753, train_loss=2.9260473

Batch 283720, train_perplexity=18.653753, train_loss=2.9260473

Batch 283730, train_perplexity=18.653753, train_loss=2.9260473

Batch 283740, train_perplexity=18.653753, train_loss=2.9260473

Batch 283750, train_perplexity=18.653753, train_loss=2.9260473

Batch 283760, train_perplexity=18.653753, train_loss=2.9260473

Batch 283770, train_perplexity=18.653753, train_loss=2.9260473

Batch 283780, train_perplexity=18.653753, train_loss=2.9260473

Batch 283790, train_perplexity=18.653748, train_loss=2.926047

Batch 283800, train_perplexity=18.653748, train_loss=2.926047

Batch 283810, train_perplexity=18.653753, train_loss=2.9260473

Batch 283820, train_perplexity=18.653744, train_loss=2.9260468

Batch 283830, train_perplexity=18.653748, train_loss=2.926047

Batch 283840, train_perplexity=18.653748, train_loss=2.926047

Batch 283850, train_perplexity=18.653748, train_loss=2.926047

Batch 283860, train_perplexity=18.653744, train_loss=2.9260468

Batch 283870, train_perplexity=18.653748, train_loss=2.926047

Batch 283880, train_perplexity=18.653744, train_loss=2.9260468

Batch 283890, train_perplexity=18.653748, train_loss=2.926047

Batch 283900, train_perplexity=18.653748, train_loss=2.926047

Batch 283910, train_perplexity=18.653748, train_loss=2.926047

Batch 283920, train_perplexity=18.653753, train_loss=2.9260473

Batch 283930, train_perplexity=18.653744, train_loss=2.9260468

Batch 283940, train_perplexity=18.653744, train_loss=2.9260468

Batch 283950, train_perplexity=18.653744, train_loss=2.9260468

Batch 283960, train_perplexity=18.653744, train_loss=2.9260468

Batch 283970, train_perplexity=18.653744, train_loss=2.9260468

Batch 283980, train_perplexity=18.653744, train_loss=2.9260468

Batch 283990, train_perplexity=18.653744, train_loss=2.9260468

Batch 284000, train_perplexity=18.653744, train_loss=2.9260468

Batch 284010, train_perplexity=18.65374, train_loss=2.9260466

Batch 284020, train_perplexity=18.653748, train_loss=2.926047

Batch 284030, train_perplexity=18.653744, train_loss=2.9260468

Batch 284040, train_perplexity=18.653744, train_loss=2.9260468

Batch 284050, train_perplexity=18.653744, train_loss=2.9260468

Batch 284060, train_perplexity=18.653744, train_loss=2.9260468

Batch 284070, train_perplexity=18.65374, train_loss=2.9260466

Batch 284080, train_perplexity=18.65374, train_loss=2.9260466

Batch 284090, train_perplexity=18.653744, train_loss=2.9260468

Batch 284100, train_perplexity=18.653734, train_loss=2.9260464

Batch 284110, train_perplexity=18.653744, train_loss=2.9260468

Batch 284120, train_perplexity=18.65374, train_loss=2.9260466

Batch 284130, train_perplexity=18.653744, train_loss=2.9260468

Batch 284140, train_perplexity=18.653744, train_loss=2.9260468

Batch 284150, train_perplexity=18.653734, train_loss=2.9260464

Batch 284160, train_perplexity=18.653734, train_loss=2.9260464

Batch 284170, train_perplexity=18.65374, train_loss=2.9260466

Batch 284180, train_perplexity=18.653734, train_loss=2.9260464

Batch 284190, train_perplexity=18.653734, train_loss=2.9260464

Batch 284200, train_perplexity=18.653734, train_loss=2.9260464

Batch 284210, train_perplexity=18.653734, train_loss=2.9260464

Batch 284220, train_perplexity=18.65374, train_loss=2.9260466

Batch 284230, train_perplexity=18.653734, train_loss=2.9260464

Batch 284240, train_perplexity=18.653734, train_loss=2.9260464

Batch 284250, train_perplexity=18.653734, train_loss=2.9260464

Batch 284260, train_perplexity=18.653734, train_loss=2.9260464

Batch 284270, train_perplexity=18.653734, train_loss=2.9260464

Batch 284280, train_perplexity=18.653734, train_loss=2.9260464

Batch 284290, train_perplexity=18.653734, train_loss=2.9260464

Batch 284300, train_perplexity=18.653734, train_loss=2.9260464

Batch 284310, train_perplexity=18.65374, train_loss=2.9260466

Batch 284320, train_perplexity=18.653734, train_loss=2.9260464

Batch 284330, train_perplexity=18.65373, train_loss=2.9260461

Batch 284340, train_perplexity=18.653734, train_loss=2.9260464

Batch 284350, train_perplexity=18.65373, train_loss=2.9260461

Batch 284360, train_perplexity=18.653734, train_loss=2.9260464

Batch 284370, train_perplexity=18.65373, train_loss=2.9260461

Batch 284380, train_perplexity=18.653734, train_loss=2.9260464

Batch 284390, train_perplexity=18.653734, train_loss=2.9260464

Batch 284400, train_perplexity=18.65373, train_loss=2.9260461

Batch 284410, train_perplexity=18.653734, train_loss=2.9260464

Batch 284420, train_perplexity=18.653727, train_loss=2.926046
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 284430, train_perplexity=18.65373, train_loss=2.9260461

Batch 284440, train_perplexity=18.653734, train_loss=2.9260464

Batch 284450, train_perplexity=18.65373, train_loss=2.9260461

Batch 284460, train_perplexity=18.653727, train_loss=2.926046

Batch 284470, train_perplexity=18.65373, train_loss=2.9260461

Batch 284480, train_perplexity=18.65373, train_loss=2.9260461

Batch 284490, train_perplexity=18.65373, train_loss=2.9260461

Batch 284500, train_perplexity=18.653727, train_loss=2.926046

Batch 284510, train_perplexity=18.653727, train_loss=2.926046

Batch 284520, train_perplexity=18.65373, train_loss=2.9260461

Batch 284530, train_perplexity=18.653727, train_loss=2.926046

Batch 284540, train_perplexity=18.653727, train_loss=2.926046

Batch 284550, train_perplexity=18.653727, train_loss=2.926046

Batch 284560, train_perplexity=18.65373, train_loss=2.9260461

Batch 284570, train_perplexity=18.653727, train_loss=2.926046

Batch 284580, train_perplexity=18.653727, train_loss=2.926046

Batch 284590, train_perplexity=18.653727, train_loss=2.926046

Batch 284600, train_perplexity=18.653727, train_loss=2.926046

Batch 284610, train_perplexity=18.653727, train_loss=2.926046

Batch 284620, train_perplexity=18.653727, train_loss=2.926046

Batch 284630, train_perplexity=18.653727, train_loss=2.926046

Batch 284640, train_perplexity=18.653727, train_loss=2.926046

Batch 284650, train_perplexity=18.653727, train_loss=2.926046

Batch 284660, train_perplexity=18.653727, train_loss=2.926046

Batch 284670, train_perplexity=18.653727, train_loss=2.926046

Batch 284680, train_perplexity=18.653727, train_loss=2.926046

Batch 284690, train_perplexity=18.653727, train_loss=2.926046

Batch 284700, train_perplexity=18.653727, train_loss=2.926046

Batch 284710, train_perplexity=18.653727, train_loss=2.926046

Batch 284720, train_perplexity=18.653727, train_loss=2.926046

Batch 284730, train_perplexity=18.653727, train_loss=2.926046

Batch 284740, train_perplexity=18.65372, train_loss=2.9260457

Batch 284750, train_perplexity=18.653727, train_loss=2.926046

Batch 284760, train_perplexity=18.653717, train_loss=2.9260454

Batch 284770, train_perplexity=18.653717, train_loss=2.9260454

Batch 284780, train_perplexity=18.65372, train_loss=2.9260457

Batch 284790, train_perplexity=18.653717, train_loss=2.9260454

Batch 284800, train_perplexity=18.653717, train_loss=2.9260454

Batch 284810, train_perplexity=18.653727, train_loss=2.926046

Batch 284820, train_perplexity=18.653717, train_loss=2.9260454

Batch 284830, train_perplexity=18.653717, train_loss=2.9260454

Batch 284840, train_perplexity=18.653717, train_loss=2.9260454

Batch 284850, train_perplexity=18.653717, train_loss=2.9260454

Batch 284860, train_perplexity=18.653717, train_loss=2.9260454

Batch 284870, train_perplexity=18.653717, train_loss=2.9260454

Batch 284880, train_perplexity=18.653717, train_loss=2.9260454

Batch 284890, train_perplexity=18.653717, train_loss=2.9260454

Batch 284900, train_perplexity=18.653717, train_loss=2.9260454

Batch 284910, train_perplexity=18.653717, train_loss=2.9260454

Batch 284920, train_perplexity=18.653717, train_loss=2.9260454

Batch 284930, train_perplexity=18.653717, train_loss=2.9260454

Batch 284940, train_perplexity=18.653717, train_loss=2.9260454

Batch 284950, train_perplexity=18.653717, train_loss=2.9260454

Batch 284960, train_perplexity=18.653713, train_loss=2.9260452

Batch 284970, train_perplexity=18.653717, train_loss=2.9260454

Batch 284980, train_perplexity=18.653708, train_loss=2.926045

Batch 284990, train_perplexity=18.653717, train_loss=2.9260454

Batch 285000, train_perplexity=18.653713, train_loss=2.9260452

Batch 285010, train_perplexity=18.653717, train_loss=2.9260454

Batch 285020, train_perplexity=18.653717, train_loss=2.9260454

Batch 285030, train_perplexity=18.653717, train_loss=2.9260454

Batch 285040, train_perplexity=18.653717, train_loss=2.9260454

Batch 285050, train_perplexity=18.653713, train_loss=2.9260452

Batch 285060, train_perplexity=18.653713, train_loss=2.9260452

Batch 285070, train_perplexity=18.653713, train_loss=2.9260452

Batch 285080, train_perplexity=18.653717, train_loss=2.9260454

Batch 285090, train_perplexity=18.653708, train_loss=2.926045

Batch 285100, train_perplexity=18.653708, train_loss=2.926045

Batch 285110, train_perplexity=18.653708, train_loss=2.926045

Batch 285120, train_perplexity=18.653708, train_loss=2.926045

Batch 285130, train_perplexity=18.653708, train_loss=2.926045

Batch 285140, train_perplexity=18.653708, train_loss=2.926045

Batch 285150, train_perplexity=18.653708, train_loss=2.926045

Batch 285160, train_perplexity=18.653708, train_loss=2.926045

Batch 285170, train_perplexity=18.653708, train_loss=2.926045

Batch 285180, train_perplexity=18.653708, train_loss=2.926045

Batch 285190, train_perplexity=18.653708, train_loss=2.926045

Batch 285200, train_perplexity=18.653708, train_loss=2.926045

Batch 285210, train_perplexity=18.653708, train_loss=2.926045

Batch 285220, train_perplexity=18.653708, train_loss=2.926045

Batch 285230, train_perplexity=18.653708, train_loss=2.926045

Batch 285240, train_perplexity=18.653708, train_loss=2.926045

Batch 285250, train_perplexity=18.653708, train_loss=2.926045

Batch 285260, train_perplexity=18.653708, train_loss=2.926045

Batch 285270, train_perplexity=18.653708, train_loss=2.926045

Batch 285280, train_perplexity=18.653708, train_loss=2.926045

Batch 285290, train_perplexity=18.653708, train_loss=2.926045

Batch 285300, train_perplexity=18.653708, train_loss=2.926045

Batch 285310, train_perplexity=18.6537, train_loss=2.9260445

Batch 285320, train_perplexity=18.653708, train_loss=2.926045

Batch 285330, train_perplexity=18.653708, train_loss=2.926045

Batch 285340, train_perplexity=18.653708, train_loss=2.926045

Batch 285350, train_perplexity=18.653708, train_loss=2.926045

Batch 285360, train_perplexity=18.653704, train_loss=2.9260447

Batch 285370, train_perplexity=18.653708, train_loss=2.926045

Batch 285380, train_perplexity=18.653708, train_loss=2.926045

Batch 285390, train_perplexity=18.653704, train_loss=2.9260447

Batch 285400, train_perplexity=18.6537, train_loss=2.9260445

Batch 285410, train_perplexity=18.6537, train_loss=2.9260445

Batch 285420, train_perplexity=18.6537, train_loss=2.9260445

Batch 285430, train_perplexity=18.653704, train_loss=2.9260447

Batch 285440, train_perplexity=18.653704, train_loss=2.9260447

Batch 285450, train_perplexity=18.6537, train_loss=2.9260445

Batch 285460, train_perplexity=18.6537, train_loss=2.9260445

Batch 285470, train_perplexity=18.6537, train_loss=2.9260445

Batch 285480, train_perplexity=18.6537, train_loss=2.9260445

Batch 285490, train_perplexity=18.6537, train_loss=2.9260445

Batch 285500, train_perplexity=18.6537, train_loss=2.9260445

Batch 285510, train_perplexity=18.6537, train_loss=2.9260445

Batch 285520, train_perplexity=18.6537, train_loss=2.9260445

Batch 285530, train_perplexity=18.653704, train_loss=2.9260447

Batch 285540, train_perplexity=18.6537, train_loss=2.9260445

Batch 285550, train_perplexity=18.6537, train_loss=2.9260445

Batch 285560, train_perplexity=18.6537, train_loss=2.9260445

Batch 285570, train_perplexity=18.6537, train_loss=2.9260445

Batch 285580, train_perplexity=18.6537, train_loss=2.9260445

Batch 285590, train_perplexity=18.6537, train_loss=2.9260445

Batch 285600, train_perplexity=18.6537, train_loss=2.9260445

Batch 285610, train_perplexity=18.6537, train_loss=2.9260445

Batch 285620, train_perplexity=18.6537, train_loss=2.9260445

Batch 285630, train_perplexity=18.6537, train_loss=2.9260445

Batch 285640, train_perplexity=18.6537, train_loss=2.9260445

Batch 285650, train_perplexity=18.6537, train_loss=2.9260445

Batch 285660, train_perplexity=18.653694, train_loss=2.9260442

Batch 285670, train_perplexity=18.65369, train_loss=2.926044

Batch 285680, train_perplexity=18.6537, train_loss=2.9260445

Batch 285690, train_perplexity=18.653694, train_loss=2.9260442

Batch 285700, train_perplexity=18.653694, train_loss=2.9260442

Batch 285710, train_perplexity=18.65369, train_loss=2.926044

Batch 285720, train_perplexity=18.653694, train_loss=2.9260442
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 285730, train_perplexity=18.6537, train_loss=2.9260445

Batch 285740, train_perplexity=18.6537, train_loss=2.9260445

Batch 285750, train_perplexity=18.65369, train_loss=2.926044

Batch 285760, train_perplexity=18.65369, train_loss=2.926044

Batch 285770, train_perplexity=18.653694, train_loss=2.9260442

Batch 285780, train_perplexity=18.65369, train_loss=2.926044

Batch 285790, train_perplexity=18.65369, train_loss=2.926044

Batch 285800, train_perplexity=18.65369, train_loss=2.926044

Batch 285810, train_perplexity=18.65369, train_loss=2.926044

Batch 285820, train_perplexity=18.65369, train_loss=2.926044

Batch 285830, train_perplexity=18.65369, train_loss=2.926044

Batch 285840, train_perplexity=18.65369, train_loss=2.926044

Batch 285850, train_perplexity=18.65369, train_loss=2.926044

Batch 285860, train_perplexity=18.65369, train_loss=2.926044

Batch 285870, train_perplexity=18.65369, train_loss=2.926044

Batch 285880, train_perplexity=18.653687, train_loss=2.9260437

Batch 285890, train_perplexity=18.65369, train_loss=2.926044

Batch 285900, train_perplexity=18.65369, train_loss=2.926044

Batch 285910, train_perplexity=18.65369, train_loss=2.926044

Batch 285920, train_perplexity=18.65369, train_loss=2.926044

Batch 285930, train_perplexity=18.653687, train_loss=2.9260437

Batch 285940, train_perplexity=18.65369, train_loss=2.926044

Batch 285950, train_perplexity=18.65368, train_loss=2.9260435

Batch 285960, train_perplexity=18.65368, train_loss=2.9260435

Batch 285970, train_perplexity=18.653687, train_loss=2.9260437

Batch 285980, train_perplexity=18.65369, train_loss=2.926044

Batch 285990, train_perplexity=18.65369, train_loss=2.926044

Batch 286000, train_perplexity=18.653687, train_loss=2.9260437

Batch 286010, train_perplexity=18.65369, train_loss=2.926044

Batch 286020, train_perplexity=18.65369, train_loss=2.926044

Batch 286030, train_perplexity=18.653687, train_loss=2.9260437

Batch 286040, train_perplexity=18.65368, train_loss=2.9260435

Batch 286050, train_perplexity=18.653687, train_loss=2.9260437

Batch 286060, train_perplexity=18.65368, train_loss=2.9260435

Batch 286070, train_perplexity=18.65368, train_loss=2.9260435

Batch 286080, train_perplexity=18.65368, train_loss=2.9260435

Batch 286090, train_perplexity=18.65368, train_loss=2.9260435

Batch 286100, train_perplexity=18.65368, train_loss=2.9260435

Batch 286110, train_perplexity=18.65368, train_loss=2.9260435

Batch 286120, train_perplexity=18.65368, train_loss=2.9260435

Batch 286130, train_perplexity=18.65368, train_loss=2.9260435

Batch 286140, train_perplexity=18.65368, train_loss=2.9260435

Batch 286150, train_perplexity=18.653687, train_loss=2.9260437

Batch 286160, train_perplexity=18.65368, train_loss=2.9260435

Batch 286170, train_perplexity=18.65368, train_loss=2.9260435

Batch 286180, train_perplexity=18.65368, train_loss=2.9260435

Batch 286190, train_perplexity=18.65368, train_loss=2.9260435

Batch 286200, train_perplexity=18.65368, train_loss=2.9260435

Batch 286210, train_perplexity=18.65368, train_loss=2.9260435

Batch 286220, train_perplexity=18.65368, train_loss=2.9260435

Batch 286230, train_perplexity=18.65368, train_loss=2.9260435

Batch 286240, train_perplexity=18.65368, train_loss=2.9260435

Batch 286250, train_perplexity=18.65368, train_loss=2.9260435

Batch 286260, train_perplexity=18.65368, train_loss=2.9260435

Batch 286270, train_perplexity=18.653677, train_loss=2.9260433

Batch 286280, train_perplexity=18.653677, train_loss=2.9260433

Batch 286290, train_perplexity=18.653677, train_loss=2.9260433

Batch 286300, train_perplexity=18.653673, train_loss=2.926043

Batch 286310, train_perplexity=18.653677, train_loss=2.9260433

Batch 286320, train_perplexity=18.653673, train_loss=2.926043

Batch 286330, train_perplexity=18.653677, train_loss=2.9260433

Batch 286340, train_perplexity=18.653677, train_loss=2.9260433

Batch 286350, train_perplexity=18.653673, train_loss=2.926043

Batch 286360, train_perplexity=18.653677, train_loss=2.9260433

Batch 286370, train_perplexity=18.653673, train_loss=2.926043

Batch 286380, train_perplexity=18.653673, train_loss=2.926043

Batch 286390, train_perplexity=18.653673, train_loss=2.926043

Batch 286400, train_perplexity=18.653673, train_loss=2.926043

Batch 286410, train_perplexity=18.653673, train_loss=2.926043

Batch 286420, train_perplexity=18.653673, train_loss=2.926043

Batch 286430, train_perplexity=18.653673, train_loss=2.926043

Batch 286440, train_perplexity=18.653673, train_loss=2.926043

Batch 286450, train_perplexity=18.653673, train_loss=2.926043

Batch 286460, train_perplexity=18.653673, train_loss=2.926043

Batch 286470, train_perplexity=18.653673, train_loss=2.926043

Batch 286480, train_perplexity=18.653673, train_loss=2.926043

Batch 286490, train_perplexity=18.653673, train_loss=2.926043

Batch 286500, train_perplexity=18.653673, train_loss=2.926043

Batch 286510, train_perplexity=18.653673, train_loss=2.926043

Batch 286520, train_perplexity=18.653673, train_loss=2.926043

Batch 286530, train_perplexity=18.653673, train_loss=2.926043

Batch 286540, train_perplexity=18.653673, train_loss=2.926043

Batch 286550, train_perplexity=18.653673, train_loss=2.926043

Batch 286560, train_perplexity=18.653673, train_loss=2.926043

Batch 286570, train_perplexity=18.653673, train_loss=2.926043

Batch 286580, train_perplexity=18.653673, train_loss=2.926043

Batch 286590, train_perplexity=18.653664, train_loss=2.9260426

Batch 286600, train_perplexity=18.653667, train_loss=2.9260428

Batch 286610, train_perplexity=18.653673, train_loss=2.926043

Batch 286620, train_perplexity=18.653673, train_loss=2.926043

Batch 286630, train_perplexity=18.653673, train_loss=2.926043

Batch 286640, train_perplexity=18.653667, train_loss=2.9260428

Batch 286650, train_perplexity=18.653664, train_loss=2.9260426

Batch 286660, train_perplexity=18.653664, train_loss=2.9260426

Batch 286670, train_perplexity=18.653673, train_loss=2.926043

Batch 286680, train_perplexity=18.653664, train_loss=2.9260426

Batch 286690, train_perplexity=18.653673, train_loss=2.926043

Batch 286700, train_perplexity=18.653664, train_loss=2.9260426

Batch 286710, train_perplexity=18.653664, train_loss=2.9260426

Batch 286720, train_perplexity=18.653667, train_loss=2.9260428

Batch 286730, train_perplexity=18.653667, train_loss=2.9260428

Batch 286740, train_perplexity=18.653664, train_loss=2.9260426

Batch 286750, train_perplexity=18.653664, train_loss=2.9260426

Batch 286760, train_perplexity=18.653664, train_loss=2.9260426

Batch 286770, train_perplexity=18.653664, train_loss=2.9260426

Batch 286780, train_perplexity=18.653664, train_loss=2.9260426

Batch 286790, train_perplexity=18.653664, train_loss=2.9260426

Batch 286800, train_perplexity=18.653664, train_loss=2.9260426

Batch 286810, train_perplexity=18.653664, train_loss=2.9260426

Batch 286820, train_perplexity=18.653664, train_loss=2.9260426

Batch 286830, train_perplexity=18.653664, train_loss=2.9260426

Batch 286840, train_perplexity=18.653664, train_loss=2.9260426

Batch 286850, train_perplexity=18.653664, train_loss=2.9260426

Batch 286860, train_perplexity=18.653664, train_loss=2.9260426

Batch 286870, train_perplexity=18.653664, train_loss=2.9260426

Batch 286880, train_perplexity=18.653654, train_loss=2.926042

Batch 286890, train_perplexity=18.65366, train_loss=2.9260423

Batch 286900, train_perplexity=18.65366, train_loss=2.9260423

Batch 286910, train_perplexity=18.653664, train_loss=2.9260426

Batch 286920, train_perplexity=18.65366, train_loss=2.9260423

Batch 286930, train_perplexity=18.653664, train_loss=2.9260426

Batch 286940, train_perplexity=18.653664, train_loss=2.9260426

Batch 286950, train_perplexity=18.653664, train_loss=2.9260426

Batch 286960, train_perplexity=18.653654, train_loss=2.926042

Batch 286970, train_perplexity=18.653664, train_loss=2.9260426

Batch 286980, train_perplexity=18.653654, train_loss=2.926042

Batch 286990, train_perplexity=18.65366, train_loss=2.9260423

Batch 287000, train_perplexity=18.653654, train_loss=2.926042

Batch 287010, train_perplexity=18.653654, train_loss=2.926042

Batch 287020, train_perplexity=18.65366, train_loss=2.9260423
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 287030, train_perplexity=18.65366, train_loss=2.9260423

Batch 287040, train_perplexity=18.653654, train_loss=2.926042

Batch 287050, train_perplexity=18.65366, train_loss=2.9260423

Batch 287060, train_perplexity=18.653654, train_loss=2.926042

Batch 287070, train_perplexity=18.653654, train_loss=2.926042

Batch 287080, train_perplexity=18.65366, train_loss=2.9260423

Batch 287090, train_perplexity=18.653654, train_loss=2.926042

Batch 287100, train_perplexity=18.653654, train_loss=2.926042

Batch 287110, train_perplexity=18.653654, train_loss=2.926042

Batch 287120, train_perplexity=18.653654, train_loss=2.926042

Batch 287130, train_perplexity=18.653654, train_loss=2.926042

Batch 287140, train_perplexity=18.653654, train_loss=2.926042

Batch 287150, train_perplexity=18.653654, train_loss=2.926042

Batch 287160, train_perplexity=18.653654, train_loss=2.926042

Batch 287170, train_perplexity=18.65365, train_loss=2.9260418

Batch 287180, train_perplexity=18.653654, train_loss=2.926042

Batch 287190, train_perplexity=18.65365, train_loss=2.9260418

Batch 287200, train_perplexity=18.653654, train_loss=2.926042

Batch 287210, train_perplexity=18.653654, train_loss=2.926042

Batch 287220, train_perplexity=18.653654, train_loss=2.926042

Batch 287230, train_perplexity=18.65365, train_loss=2.9260418

Batch 287240, train_perplexity=18.65365, train_loss=2.9260418

Batch 287250, train_perplexity=18.653646, train_loss=2.9260416

Batch 287260, train_perplexity=18.653654, train_loss=2.926042

Batch 287270, train_perplexity=18.653646, train_loss=2.9260416

Batch 287280, train_perplexity=18.65365, train_loss=2.9260418

Batch 287290, train_perplexity=18.65365, train_loss=2.9260418

Batch 287300, train_perplexity=18.65365, train_loss=2.9260418

Batch 287310, train_perplexity=18.65365, train_loss=2.9260418

Batch 287320, train_perplexity=18.653646, train_loss=2.9260416

Batch 287330, train_perplexity=18.653646, train_loss=2.9260416

Batch 287340, train_perplexity=18.65365, train_loss=2.9260418

Batch 287350, train_perplexity=18.653646, train_loss=2.9260416

Batch 287360, train_perplexity=18.653654, train_loss=2.926042

Batch 287370, train_perplexity=18.653646, train_loss=2.9260416

Batch 287380, train_perplexity=18.653646, train_loss=2.9260416

Batch 287390, train_perplexity=18.653646, train_loss=2.9260416

Batch 287400, train_perplexity=18.653646, train_loss=2.9260416

Batch 287410, train_perplexity=18.653646, train_loss=2.9260416

Batch 287420, train_perplexity=18.653646, train_loss=2.9260416

Batch 287430, train_perplexity=18.653646, train_loss=2.9260416

Batch 287440, train_perplexity=18.653646, train_loss=2.9260416

Batch 287450, train_perplexity=18.653646, train_loss=2.9260416

Batch 287460, train_perplexity=18.653646, train_loss=2.9260416

Batch 287470, train_perplexity=18.65364, train_loss=2.9260414

Batch 287480, train_perplexity=18.653646, train_loss=2.9260416

Batch 287490, train_perplexity=18.65364, train_loss=2.9260414

Batch 287500, train_perplexity=18.653646, train_loss=2.9260416

Batch 287510, train_perplexity=18.653646, train_loss=2.9260416

Batch 287520, train_perplexity=18.653646, train_loss=2.9260416

Batch 287530, train_perplexity=18.653646, train_loss=2.9260416

Batch 287540, train_perplexity=18.653646, train_loss=2.9260416

Batch 287550, train_perplexity=18.65364, train_loss=2.9260414

Batch 287560, train_perplexity=18.65364, train_loss=2.9260414

Batch 287570, train_perplexity=18.653637, train_loss=2.9260411

Batch 287580, train_perplexity=18.65364, train_loss=2.9260414

Batch 287590, train_perplexity=18.65364, train_loss=2.9260414

Batch 287600, train_perplexity=18.653637, train_loss=2.9260411

Batch 287610, train_perplexity=18.65364, train_loss=2.9260414

Batch 287620, train_perplexity=18.653637, train_loss=2.9260411

Batch 287630, train_perplexity=18.65364, train_loss=2.9260414

Batch 287640, train_perplexity=18.65364, train_loss=2.9260414

Batch 287650, train_perplexity=18.653637, train_loss=2.9260411

Batch 287660, train_perplexity=18.653637, train_loss=2.9260411

Batch 287670, train_perplexity=18.65364, train_loss=2.9260414

Batch 287680, train_perplexity=18.65364, train_loss=2.9260414

Batch 287690, train_perplexity=18.653637, train_loss=2.9260411

Batch 287700, train_perplexity=18.65364, train_loss=2.9260414

Batch 287710, train_perplexity=18.653637, train_loss=2.9260411

Batch 287720, train_perplexity=18.65364, train_loss=2.9260414

Batch 287730, train_perplexity=18.653637, train_loss=2.9260411

Batch 287740, train_perplexity=18.65364, train_loss=2.9260414

Batch 287750, train_perplexity=18.653637, train_loss=2.9260411

Batch 287760, train_perplexity=18.653637, train_loss=2.9260411

Batch 287770, train_perplexity=18.653637, train_loss=2.9260411

Batch 287780, train_perplexity=18.653637, train_loss=2.9260411

Batch 287790, train_perplexity=18.653637, train_loss=2.9260411

Batch 287800, train_perplexity=18.653637, train_loss=2.9260411

Batch 287810, train_perplexity=18.653637, train_loss=2.9260411

Batch 287820, train_perplexity=18.653633, train_loss=2.926041

Batch 287830, train_perplexity=18.653633, train_loss=2.926041

Batch 287840, train_perplexity=18.653637, train_loss=2.9260411

Batch 287850, train_perplexity=18.653633, train_loss=2.926041

Batch 287860, train_perplexity=18.653637, train_loss=2.9260411

Batch 287870, train_perplexity=18.653633, train_loss=2.926041

Batch 287880, train_perplexity=18.653637, train_loss=2.9260411

Batch 287890, train_perplexity=18.653637, train_loss=2.9260411

Batch 287900, train_perplexity=18.653633, train_loss=2.926041

Batch 287910, train_perplexity=18.653637, train_loss=2.9260411

Batch 287920, train_perplexity=18.653627, train_loss=2.9260406

Batch 287930, train_perplexity=18.653637, train_loss=2.9260411

Batch 287940, train_perplexity=18.653637, train_loss=2.9260411

Batch 287950, train_perplexity=18.653633, train_loss=2.926041

Batch 287960, train_perplexity=18.653627, train_loss=2.9260406

Batch 287970, train_perplexity=18.653627, train_loss=2.9260406

Batch 287980, train_perplexity=18.653627, train_loss=2.9260406

Batch 287990, train_perplexity=18.653627, train_loss=2.9260406

Batch 288000, train_perplexity=18.653627, train_loss=2.9260406

Batch 288010, train_perplexity=18.653627, train_loss=2.9260406

Batch 288020, train_perplexity=18.653627, train_loss=2.9260406

Batch 288030, train_perplexity=18.653627, train_loss=2.9260406

Batch 288040, train_perplexity=18.653633, train_loss=2.926041

Batch 288050, train_perplexity=18.653627, train_loss=2.9260406

Batch 288060, train_perplexity=18.653627, train_loss=2.9260406

Batch 288070, train_perplexity=18.653627, train_loss=2.9260406

Batch 288080, train_perplexity=18.653627, train_loss=2.9260406

Batch 288090, train_perplexity=18.653627, train_loss=2.9260406

Batch 288100, train_perplexity=18.653627, train_loss=2.9260406

Batch 288110, train_perplexity=18.653627, train_loss=2.9260406

Batch 288120, train_perplexity=18.653627, train_loss=2.9260406

Batch 288130, train_perplexity=18.653627, train_loss=2.9260406

Batch 288140, train_perplexity=18.653627, train_loss=2.9260406

Batch 288150, train_perplexity=18.653624, train_loss=2.9260404

Batch 288160, train_perplexity=18.653627, train_loss=2.9260406

Batch 288170, train_perplexity=18.653627, train_loss=2.9260406

Batch 288180, train_perplexity=18.653627, train_loss=2.9260406

Batch 288190, train_perplexity=18.653627, train_loss=2.9260406

Batch 288200, train_perplexity=18.653627, train_loss=2.9260406

Batch 288210, train_perplexity=18.653627, train_loss=2.9260406

Batch 288220, train_perplexity=18.653624, train_loss=2.9260404

Batch 288230, train_perplexity=18.653624, train_loss=2.9260404

Batch 288240, train_perplexity=18.653627, train_loss=2.9260406

Batch 288250, train_perplexity=18.653624, train_loss=2.9260404

Batch 288260, train_perplexity=18.653624, train_loss=2.9260404

Batch 288270, train_perplexity=18.65362, train_loss=2.9260402

Batch 288280, train_perplexity=18.653624, train_loss=2.9260404

Batch 288290, train_perplexity=18.65362, train_loss=2.9260402

Batch 288300, train_perplexity=18.65362, train_loss=2.9260402

Batch 288310, train_perplexity=18.653627, train_loss=2.9260406
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 288320, train_perplexity=18.65362, train_loss=2.9260402

Batch 288330, train_perplexity=18.65362, train_loss=2.9260402

Batch 288340, train_perplexity=18.653624, train_loss=2.9260404

Batch 288350, train_perplexity=18.65362, train_loss=2.9260402

Batch 288360, train_perplexity=18.65362, train_loss=2.9260402

Batch 288370, train_perplexity=18.65362, train_loss=2.9260402

Batch 288380, train_perplexity=18.65362, train_loss=2.9260402

Batch 288390, train_perplexity=18.65362, train_loss=2.9260402

Batch 288400, train_perplexity=18.65362, train_loss=2.9260402

Batch 288410, train_perplexity=18.65362, train_loss=2.9260402

Batch 288420, train_perplexity=18.653614, train_loss=2.92604

Batch 288430, train_perplexity=18.653614, train_loss=2.92604

Batch 288440, train_perplexity=18.65362, train_loss=2.9260402

Batch 288450, train_perplexity=18.65362, train_loss=2.9260402

Batch 288460, train_perplexity=18.65362, train_loss=2.9260402

Batch 288470, train_perplexity=18.65362, train_loss=2.9260402

Batch 288480, train_perplexity=18.65362, train_loss=2.9260402

Batch 288490, train_perplexity=18.653614, train_loss=2.92604

Batch 288500, train_perplexity=18.65361, train_loss=2.9260397

Batch 288510, train_perplexity=18.653614, train_loss=2.92604

Batch 288520, train_perplexity=18.65361, train_loss=2.9260397

Batch 288530, train_perplexity=18.653614, train_loss=2.92604

Batch 288540, train_perplexity=18.653614, train_loss=2.92604

Batch 288550, train_perplexity=18.65362, train_loss=2.9260402

Batch 288560, train_perplexity=18.65362, train_loss=2.9260402

Batch 288570, train_perplexity=18.65361, train_loss=2.9260397

Batch 288580, train_perplexity=18.653614, train_loss=2.92604

Batch 288590, train_perplexity=18.65362, train_loss=2.9260402

Batch 288600, train_perplexity=18.65361, train_loss=2.9260397

Batch 288610, train_perplexity=18.65361, train_loss=2.9260397

Batch 288620, train_perplexity=18.65362, train_loss=2.9260402

Batch 288630, train_perplexity=18.65361, train_loss=2.9260397

Batch 288640, train_perplexity=18.65361, train_loss=2.9260397

Batch 288650, train_perplexity=18.65361, train_loss=2.9260397

Batch 288660, train_perplexity=18.65361, train_loss=2.9260397

Batch 288670, train_perplexity=18.65361, train_loss=2.9260397

Batch 288680, train_perplexity=18.65361, train_loss=2.9260397

Batch 288690, train_perplexity=18.65361, train_loss=2.9260397

Batch 288700, train_perplexity=18.65361, train_loss=2.9260397

Batch 288710, train_perplexity=18.65361, train_loss=2.9260397

Batch 288720, train_perplexity=18.653606, train_loss=2.9260395

Batch 288730, train_perplexity=18.65361, train_loss=2.9260397

Batch 288740, train_perplexity=18.65361, train_loss=2.9260397

Batch 288750, train_perplexity=18.65361, train_loss=2.9260397

Batch 288760, train_perplexity=18.65361, train_loss=2.9260397

Batch 288770, train_perplexity=18.6536, train_loss=2.9260392

Batch 288780, train_perplexity=18.65361, train_loss=2.9260397

Batch 288790, train_perplexity=18.65361, train_loss=2.9260397

Batch 288800, train_perplexity=18.65361, train_loss=2.9260397

Batch 288810, train_perplexity=18.653606, train_loss=2.9260395

Batch 288820, train_perplexity=18.65361, train_loss=2.9260397

Batch 288830, train_perplexity=18.653606, train_loss=2.9260395

Batch 288840, train_perplexity=18.65361, train_loss=2.9260397

Batch 288850, train_perplexity=18.6536, train_loss=2.9260392

Batch 288860, train_perplexity=18.653606, train_loss=2.9260395

Batch 288870, train_perplexity=18.653606, train_loss=2.9260395

Batch 288880, train_perplexity=18.6536, train_loss=2.9260392

Batch 288890, train_perplexity=18.653606, train_loss=2.9260395

Batch 288900, train_perplexity=18.6536, train_loss=2.9260392

Batch 288910, train_perplexity=18.653606, train_loss=2.9260395

Batch 288920, train_perplexity=18.6536, train_loss=2.9260392

Batch 288930, train_perplexity=18.6536, train_loss=2.9260392

Batch 288940, train_perplexity=18.6536, train_loss=2.9260392

Batch 288950, train_perplexity=18.6536, train_loss=2.9260392

Batch 288960, train_perplexity=18.6536, train_loss=2.9260392

Batch 288970, train_perplexity=18.6536, train_loss=2.9260392

Batch 288980, train_perplexity=18.6536, train_loss=2.9260392

Batch 288990, train_perplexity=18.6536, train_loss=2.9260392

Batch 289000, train_perplexity=18.6536, train_loss=2.9260392

Batch 289010, train_perplexity=18.6536, train_loss=2.9260392

Batch 289020, train_perplexity=18.6536, train_loss=2.9260392

Batch 289030, train_perplexity=18.6536, train_loss=2.9260392

Batch 289040, train_perplexity=18.6536, train_loss=2.9260392

Batch 289050, train_perplexity=18.6536, train_loss=2.9260392

Batch 289060, train_perplexity=18.6536, train_loss=2.9260392

Batch 289070, train_perplexity=18.6536, train_loss=2.9260392

Batch 289080, train_perplexity=18.6536, train_loss=2.9260392

Batch 289090, train_perplexity=18.6536, train_loss=2.9260392

Batch 289100, train_perplexity=18.6536, train_loss=2.9260392

Batch 289110, train_perplexity=18.6536, train_loss=2.9260392

Batch 289120, train_perplexity=18.6536, train_loss=2.9260392

Batch 289130, train_perplexity=18.653597, train_loss=2.926039

Batch 289140, train_perplexity=18.653593, train_loss=2.9260387

Batch 289150, train_perplexity=18.6536, train_loss=2.9260392

Batch 289160, train_perplexity=18.6536, train_loss=2.9260392

Batch 289170, train_perplexity=18.6536, train_loss=2.9260392

Batch 289180, train_perplexity=18.653597, train_loss=2.926039

Batch 289190, train_perplexity=18.653593, train_loss=2.9260387

Batch 289200, train_perplexity=18.653593, train_loss=2.9260387

Batch 289210, train_perplexity=18.653593, train_loss=2.9260387

Batch 289220, train_perplexity=18.653593, train_loss=2.9260387

Batch 289230, train_perplexity=18.653593, train_loss=2.9260387

Batch 289240, train_perplexity=18.653597, train_loss=2.926039

Batch 289250, train_perplexity=18.653593, train_loss=2.9260387

Batch 289260, train_perplexity=18.653584, train_loss=2.9260383

Batch 289270, train_perplexity=18.653593, train_loss=2.9260387

Batch 289280, train_perplexity=18.653593, train_loss=2.9260387

Batch 289290, train_perplexity=18.653593, train_loss=2.9260387

Batch 289300, train_perplexity=18.653593, train_loss=2.9260387

Batch 289310, train_perplexity=18.653593, train_loss=2.9260387

Batch 289320, train_perplexity=18.653593, train_loss=2.9260387

Batch 289330, train_perplexity=18.653593, train_loss=2.9260387

Batch 289340, train_perplexity=18.653587, train_loss=2.9260385

Batch 289350, train_perplexity=18.653593, train_loss=2.9260387

Batch 289360, train_perplexity=18.653587, train_loss=2.9260385

Batch 289370, train_perplexity=18.653593, train_loss=2.9260387

Batch 289380, train_perplexity=18.653593, train_loss=2.9260387

Batch 289390, train_perplexity=18.653587, train_loss=2.9260385

Batch 289400, train_perplexity=18.653593, train_loss=2.9260387

Batch 289410, train_perplexity=18.653593, train_loss=2.9260387

Batch 289420, train_perplexity=18.653597, train_loss=2.926039

Batch 289430, train_perplexity=18.653587, train_loss=2.9260385

Batch 289440, train_perplexity=18.653587, train_loss=2.9260385

Batch 289450, train_perplexity=18.653587, train_loss=2.9260385

Batch 289460, train_perplexity=18.653593, train_loss=2.9260387

Batch 289470, train_perplexity=18.653587, train_loss=2.9260385

Batch 289480, train_perplexity=18.653587, train_loss=2.9260385

Batch 289490, train_perplexity=18.653593, train_loss=2.9260387

Batch 289500, train_perplexity=18.653584, train_loss=2.9260383

Batch 289510, train_perplexity=18.653587, train_loss=2.9260385

Batch 289520, train_perplexity=18.653584, train_loss=2.9260383

Batch 289530, train_perplexity=18.653593, train_loss=2.9260387

Batch 289540, train_perplexity=18.653584, train_loss=2.9260383

Batch 289550, train_perplexity=18.653584, train_loss=2.9260383

Batch 289560, train_perplexity=18.653584, train_loss=2.9260383

Batch 289570, train_perplexity=18.653584, train_loss=2.9260383

Batch 289580, train_perplexity=18.653584, train_loss=2.9260383

Batch 289590, train_perplexity=18.653587, train_loss=2.9260385

Batch 289600, train_perplexity=18.653584, train_loss=2.9260383

Batch 289610, train_perplexity=18.653584, train_loss=2.9260383
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 289620, train_perplexity=18.653584, train_loss=2.9260383

Batch 289630, train_perplexity=18.653587, train_loss=2.9260385

Batch 289640, train_perplexity=18.653584, train_loss=2.9260383

Batch 289650, train_perplexity=18.653584, train_loss=2.9260383

Batch 289660, train_perplexity=18.653584, train_loss=2.9260383

Batch 289670, train_perplexity=18.653584, train_loss=2.9260383

Batch 289680, train_perplexity=18.653584, train_loss=2.9260383

Batch 289690, train_perplexity=18.65358, train_loss=2.926038

Batch 289700, train_perplexity=18.653584, train_loss=2.9260383

Batch 289710, train_perplexity=18.653584, train_loss=2.9260383

Batch 289720, train_perplexity=18.653584, train_loss=2.9260383

Batch 289730, train_perplexity=18.653574, train_loss=2.9260378

Batch 289740, train_perplexity=18.65358, train_loss=2.926038

Batch 289750, train_perplexity=18.653574, train_loss=2.9260378

Batch 289760, train_perplexity=18.653574, train_loss=2.9260378

Batch 289770, train_perplexity=18.653584, train_loss=2.9260383

Batch 289780, train_perplexity=18.653584, train_loss=2.9260383

Batch 289790, train_perplexity=18.653574, train_loss=2.9260378

Batch 289800, train_perplexity=18.65358, train_loss=2.926038

Batch 289810, train_perplexity=18.653574, train_loss=2.9260378

Batch 289820, train_perplexity=18.653574, train_loss=2.9260378

Batch 289830, train_perplexity=18.65358, train_loss=2.926038

Batch 289840, train_perplexity=18.653584, train_loss=2.9260383

Batch 289850, train_perplexity=18.653574, train_loss=2.9260378

Batch 289860, train_perplexity=18.653574, train_loss=2.9260378

Batch 289870, train_perplexity=18.653574, train_loss=2.9260378

Batch 289880, train_perplexity=18.653584, train_loss=2.9260383

Batch 289890, train_perplexity=18.653574, train_loss=2.9260378

Batch 289900, train_perplexity=18.653574, train_loss=2.9260378

Batch 289910, train_perplexity=18.653574, train_loss=2.9260378

Batch 289920, train_perplexity=18.65358, train_loss=2.926038

Batch 289930, train_perplexity=18.653574, train_loss=2.9260378

Batch 289940, train_perplexity=18.653574, train_loss=2.9260378

Batch 289950, train_perplexity=18.653574, train_loss=2.9260378

Batch 289960, train_perplexity=18.653574, train_loss=2.9260378

Batch 289970, train_perplexity=18.653574, train_loss=2.9260378

Batch 289980, train_perplexity=18.653574, train_loss=2.9260378

Batch 289990, train_perplexity=18.65357, train_loss=2.9260375

Batch 290000, train_perplexity=18.653574, train_loss=2.9260378

Batch 290010, train_perplexity=18.653574, train_loss=2.9260378

Batch 290020, train_perplexity=18.653574, train_loss=2.9260378

Batch 290030, train_perplexity=18.65357, train_loss=2.9260375

Batch 290040, train_perplexity=18.653574, train_loss=2.9260378

Batch 290050, train_perplexity=18.65357, train_loss=2.9260375

Batch 290060, train_perplexity=18.653574, train_loss=2.9260378

Batch 290070, train_perplexity=18.653566, train_loss=2.9260373

Batch 290080, train_perplexity=18.653574, train_loss=2.9260378

Batch 290090, train_perplexity=18.65357, train_loss=2.9260375

Batch 290100, train_perplexity=18.65357, train_loss=2.9260375

Batch 290110, train_perplexity=18.653574, train_loss=2.9260378

Batch 290120, train_perplexity=18.653566, train_loss=2.9260373

Batch 290130, train_perplexity=18.65357, train_loss=2.9260375

Batch 290140, train_perplexity=18.653566, train_loss=2.9260373

Batch 290150, train_perplexity=18.65357, train_loss=2.9260375

Batch 290160, train_perplexity=18.653566, train_loss=2.9260373

Batch 290170, train_perplexity=18.653566, train_loss=2.9260373

Batch 290180, train_perplexity=18.65357, train_loss=2.9260375

Batch 290190, train_perplexity=18.653566, train_loss=2.9260373

Batch 290200, train_perplexity=18.653566, train_loss=2.9260373

Batch 290210, train_perplexity=18.653566, train_loss=2.9260373

Batch 290220, train_perplexity=18.653566, train_loss=2.9260373

Batch 290230, train_perplexity=18.653566, train_loss=2.9260373

Batch 290240, train_perplexity=18.653566, train_loss=2.9260373

Batch 290250, train_perplexity=18.653566, train_loss=2.9260373

Batch 290260, train_perplexity=18.65357, train_loss=2.9260375

Batch 290270, train_perplexity=18.65357, train_loss=2.9260375

Batch 290280, train_perplexity=18.653566, train_loss=2.9260373

Batch 290290, train_perplexity=18.653566, train_loss=2.9260373

Batch 290300, train_perplexity=18.653566, train_loss=2.9260373

Batch 290310, train_perplexity=18.653566, train_loss=2.9260373

Batch 290320, train_perplexity=18.653566, train_loss=2.9260373

Batch 290330, train_perplexity=18.653566, train_loss=2.9260373

Batch 290340, train_perplexity=18.653566, train_loss=2.9260373

Batch 290350, train_perplexity=18.653566, train_loss=2.9260373

Batch 290360, train_perplexity=18.65356, train_loss=2.926037

Batch 290370, train_perplexity=18.653566, train_loss=2.9260373

Batch 290380, train_perplexity=18.653566, train_loss=2.9260373

Batch 290390, train_perplexity=18.653566, train_loss=2.9260373

Batch 290400, train_perplexity=18.653557, train_loss=2.9260368

Batch 290410, train_perplexity=18.653566, train_loss=2.9260373

Batch 290420, train_perplexity=18.653566, train_loss=2.9260373

Batch 290430, train_perplexity=18.653566, train_loss=2.9260373

Batch 290440, train_perplexity=18.653566, train_loss=2.9260373

Batch 290450, train_perplexity=18.653557, train_loss=2.9260368

Batch 290460, train_perplexity=18.653566, train_loss=2.9260373

Batch 290470, train_perplexity=18.653557, train_loss=2.9260368

Batch 290480, train_perplexity=18.65356, train_loss=2.926037

Batch 290490, train_perplexity=18.653557, train_loss=2.9260368

Batch 290500, train_perplexity=18.653557, train_loss=2.9260368

Batch 290510, train_perplexity=18.653557, train_loss=2.9260368

Batch 290520, train_perplexity=18.653557, train_loss=2.9260368

Batch 290530, train_perplexity=18.653557, train_loss=2.9260368

Batch 290540, train_perplexity=18.653557, train_loss=2.9260368

Batch 290550, train_perplexity=18.653557, train_loss=2.9260368

Batch 290560, train_perplexity=18.653557, train_loss=2.9260368

Batch 290570, train_perplexity=18.653557, train_loss=2.9260368

Batch 290580, train_perplexity=18.653557, train_loss=2.9260368

Batch 290590, train_perplexity=18.653557, train_loss=2.9260368

Batch 290600, train_perplexity=18.653557, train_loss=2.9260368

Batch 290610, train_perplexity=18.65356, train_loss=2.926037

Batch 290620, train_perplexity=18.653557, train_loss=2.9260368

Batch 290630, train_perplexity=18.653557, train_loss=2.9260368

Batch 290640, train_perplexity=18.653557, train_loss=2.9260368

Batch 290650, train_perplexity=18.653557, train_loss=2.9260368

Batch 290660, train_perplexity=18.653557, train_loss=2.9260368

Batch 290670, train_perplexity=18.653557, train_loss=2.9260368

Batch 290680, train_perplexity=18.653557, train_loss=2.9260368

Batch 290690, train_perplexity=18.65356, train_loss=2.926037

Batch 290700, train_perplexity=18.653557, train_loss=2.9260368

Batch 290710, train_perplexity=18.653557, train_loss=2.9260368

Batch 290720, train_perplexity=18.653553, train_loss=2.9260366

Batch 290730, train_perplexity=18.653547, train_loss=2.9260364

Batch 290740, train_perplexity=18.653547, train_loss=2.9260364

Batch 290750, train_perplexity=18.653547, train_loss=2.9260364

Batch 290760, train_perplexity=18.653557, train_loss=2.9260368

Batch 290770, train_perplexity=18.653553, train_loss=2.9260366

Batch 290780, train_perplexity=18.653547, train_loss=2.9260364

Batch 290790, train_perplexity=18.653553, train_loss=2.9260366

Batch 290800, train_perplexity=18.653547, train_loss=2.9260364

Batch 290810, train_perplexity=18.653547, train_loss=2.9260364

Batch 290820, train_perplexity=18.653547, train_loss=2.9260364

Batch 290830, train_perplexity=18.653553, train_loss=2.9260366

Batch 290840, train_perplexity=18.653547, train_loss=2.9260364

Batch 290850, train_perplexity=18.653553, train_loss=2.9260366

Batch 290860, train_perplexity=18.653547, train_loss=2.9260364

Batch 290870, train_perplexity=18.653547, train_loss=2.9260364

Batch 290880, train_perplexity=18.653547, train_loss=2.9260364

Batch 290890, train_perplexity=18.653547, train_loss=2.9260364

Batch 290900, train_perplexity=18.653547, train_loss=2.9260364
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 290910, train_perplexity=18.653547, train_loss=2.9260364

Batch 290920, train_perplexity=18.653547, train_loss=2.9260364

Batch 290930, train_perplexity=18.653547, train_loss=2.9260364

Batch 290940, train_perplexity=18.653547, train_loss=2.9260364

Batch 290950, train_perplexity=18.653547, train_loss=2.9260364

Batch 290960, train_perplexity=18.653543, train_loss=2.9260361

Batch 290970, train_perplexity=18.653547, train_loss=2.9260364

Batch 290980, train_perplexity=18.653547, train_loss=2.9260364

Batch 290990, train_perplexity=18.653547, train_loss=2.9260364

Batch 291000, train_perplexity=18.653547, train_loss=2.9260364

Batch 291010, train_perplexity=18.65354, train_loss=2.926036

Batch 291020, train_perplexity=18.653547, train_loss=2.9260364

Batch 291030, train_perplexity=18.65354, train_loss=2.926036

Batch 291040, train_perplexity=18.653547, train_loss=2.9260364

Batch 291050, train_perplexity=18.65354, train_loss=2.926036

Batch 291060, train_perplexity=18.653547, train_loss=2.9260364

Batch 291070, train_perplexity=18.65354, train_loss=2.926036

Batch 291080, train_perplexity=18.65354, train_loss=2.926036

Batch 291090, train_perplexity=18.65354, train_loss=2.926036

Batch 291100, train_perplexity=18.65354, train_loss=2.926036

Batch 291110, train_perplexity=18.65354, train_loss=2.926036

Batch 291120, train_perplexity=18.65354, train_loss=2.926036

Batch 291130, train_perplexity=18.65354, train_loss=2.926036

Batch 291140, train_perplexity=18.65354, train_loss=2.926036

Batch 291150, train_perplexity=18.65354, train_loss=2.926036

Batch 291160, train_perplexity=18.65354, train_loss=2.926036

Batch 291170, train_perplexity=18.65354, train_loss=2.926036

Batch 291180, train_perplexity=18.65354, train_loss=2.926036

Batch 291190, train_perplexity=18.65354, train_loss=2.926036

Batch 291200, train_perplexity=18.65354, train_loss=2.926036

Batch 291210, train_perplexity=18.65354, train_loss=2.926036

Batch 291220, train_perplexity=18.65354, train_loss=2.926036

Batch 291230, train_perplexity=18.65354, train_loss=2.926036

Batch 291240, train_perplexity=18.65354, train_loss=2.926036

Batch 291250, train_perplexity=18.65354, train_loss=2.926036

Batch 291260, train_perplexity=18.65354, train_loss=2.926036

Batch 291270, train_perplexity=18.65354, train_loss=2.926036

Batch 291280, train_perplexity=18.65353, train_loss=2.9260354

Batch 291290, train_perplexity=18.65353, train_loss=2.9260354

Batch 291300, train_perplexity=18.653534, train_loss=2.9260356

Batch 291310, train_perplexity=18.65353, train_loss=2.9260354

Batch 291320, train_perplexity=18.65354, train_loss=2.926036

Batch 291330, train_perplexity=18.65354, train_loss=2.926036

Batch 291340, train_perplexity=18.653534, train_loss=2.9260356

Batch 291350, train_perplexity=18.653534, train_loss=2.9260356

Batch 291360, train_perplexity=18.65353, train_loss=2.9260354

Batch 291370, train_perplexity=18.653534, train_loss=2.9260356

Batch 291380, train_perplexity=18.653534, train_loss=2.9260356

Batch 291390, train_perplexity=18.65353, train_loss=2.9260354

Batch 291400, train_perplexity=18.653534, train_loss=2.9260356

Batch 291410, train_perplexity=18.653534, train_loss=2.9260356

Batch 291420, train_perplexity=18.65353, train_loss=2.9260354

Batch 291430, train_perplexity=18.65353, train_loss=2.9260354

Batch 291440, train_perplexity=18.65353, train_loss=2.9260354

Batch 291450, train_perplexity=18.65354, train_loss=2.926036

Batch 291460, train_perplexity=18.65353, train_loss=2.9260354

Batch 291470, train_perplexity=18.65353, train_loss=2.9260354

Batch 291480, train_perplexity=18.65353, train_loss=2.9260354

Batch 291490, train_perplexity=18.653534, train_loss=2.9260356

Batch 291500, train_perplexity=18.65353, train_loss=2.9260354

Batch 291510, train_perplexity=18.65353, train_loss=2.9260354

Batch 291520, train_perplexity=18.65353, train_loss=2.9260354

Batch 291530, train_perplexity=18.65353, train_loss=2.9260354

Batch 291540, train_perplexity=18.65353, train_loss=2.9260354

Batch 291550, train_perplexity=18.65353, train_loss=2.9260354

Batch 291560, train_perplexity=18.65353, train_loss=2.9260354

Batch 291570, train_perplexity=18.65353, train_loss=2.9260354

Batch 291580, train_perplexity=18.65353, train_loss=2.9260354

Batch 291590, train_perplexity=18.65353, train_loss=2.9260354

Batch 291600, train_perplexity=18.65353, train_loss=2.9260354

Batch 291610, train_perplexity=18.65353, train_loss=2.9260354

Batch 291620, train_perplexity=18.65353, train_loss=2.9260354

Batch 291630, train_perplexity=18.653526, train_loss=2.9260352

Batch 291640, train_perplexity=18.653526, train_loss=2.9260352

Batch 291650, train_perplexity=18.65352, train_loss=2.926035

Batch 291660, train_perplexity=18.65353, train_loss=2.9260354

Batch 291670, train_perplexity=18.653526, train_loss=2.9260352

Batch 291680, train_perplexity=18.65353, train_loss=2.9260354

Batch 291690, train_perplexity=18.65352, train_loss=2.926035

Batch 291700, train_perplexity=18.65353, train_loss=2.9260354

Batch 291710, train_perplexity=18.653526, train_loss=2.9260352

Batch 291720, train_perplexity=18.65353, train_loss=2.9260354

Batch 291730, train_perplexity=18.653526, train_loss=2.9260352

Batch 291740, train_perplexity=18.65352, train_loss=2.926035

Batch 291750, train_perplexity=18.65352, train_loss=2.926035

Batch 291760, train_perplexity=18.65352, train_loss=2.926035

Batch 291770, train_perplexity=18.65352, train_loss=2.926035

Batch 291780, train_perplexity=18.65352, train_loss=2.926035

Batch 291790, train_perplexity=18.65352, train_loss=2.926035

Batch 291800, train_perplexity=18.65352, train_loss=2.926035

Batch 291810, train_perplexity=18.653526, train_loss=2.9260352

Batch 291820, train_perplexity=18.65352, train_loss=2.926035

Batch 291830, train_perplexity=18.653517, train_loss=2.9260347

Batch 291840, train_perplexity=18.65352, train_loss=2.926035

Batch 291850, train_perplexity=18.65352, train_loss=2.926035

Batch 291860, train_perplexity=18.65352, train_loss=2.926035

Batch 291870, train_perplexity=18.65352, train_loss=2.926035

Batch 291880, train_perplexity=18.65352, train_loss=2.926035

Batch 291890, train_perplexity=18.65352, train_loss=2.926035

Batch 291900, train_perplexity=18.65352, train_loss=2.926035

Batch 291910, train_perplexity=18.653517, train_loss=2.9260347

Batch 291920, train_perplexity=18.65352, train_loss=2.926035

Batch 291930, train_perplexity=18.65352, train_loss=2.926035

Batch 291940, train_perplexity=18.653513, train_loss=2.9260345

Batch 291950, train_perplexity=18.65352, train_loss=2.926035

Batch 291960, train_perplexity=18.653517, train_loss=2.9260347

Batch 291970, train_perplexity=18.653517, train_loss=2.9260347

Batch 291980, train_perplexity=18.653517, train_loss=2.9260347

Batch 291990, train_perplexity=18.65352, train_loss=2.926035

Batch 292000, train_perplexity=18.653513, train_loss=2.9260345

Batch 292010, train_perplexity=18.653517, train_loss=2.9260347

Batch 292020, train_perplexity=18.653513, train_loss=2.9260345

Batch 292030, train_perplexity=18.653513, train_loss=2.9260345

Batch 292040, train_perplexity=18.653517, train_loss=2.9260347

Batch 292050, train_perplexity=18.653517, train_loss=2.9260347

Batch 292060, train_perplexity=18.653513, train_loss=2.9260345

Batch 292070, train_perplexity=18.653513, train_loss=2.9260345

Batch 292080, train_perplexity=18.653513, train_loss=2.9260345

Batch 292090, train_perplexity=18.653513, train_loss=2.9260345

Batch 292100, train_perplexity=18.653513, train_loss=2.9260345

Batch 292110, train_perplexity=18.653517, train_loss=2.9260347

Batch 292120, train_perplexity=18.653513, train_loss=2.9260345

Batch 292130, train_perplexity=18.653513, train_loss=2.9260345

Batch 292140, train_perplexity=18.653513, train_loss=2.9260345

Batch 292150, train_perplexity=18.653513, train_loss=2.9260345

Batch 292160, train_perplexity=18.653517, train_loss=2.9260347

Batch 292170, train_perplexity=18.653513, train_loss=2.9260345

Batch 292180, train_perplexity=18.653513, train_loss=2.9260345

Batch 292190, train_perplexity=18.653513, train_loss=2.9260345

Batch 292200, train_perplexity=18.653513, train_loss=2.9260345
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 292210, train_perplexity=18.653513, train_loss=2.9260345

Batch 292220, train_perplexity=18.653513, train_loss=2.9260345

Batch 292230, train_perplexity=18.653513, train_loss=2.9260345

Batch 292240, train_perplexity=18.653513, train_loss=2.9260345

Batch 292250, train_perplexity=18.653513, train_loss=2.9260345

Batch 292260, train_perplexity=18.653503, train_loss=2.926034

Batch 292270, train_perplexity=18.653513, train_loss=2.9260345

Batch 292280, train_perplexity=18.653517, train_loss=2.9260347

Batch 292290, train_perplexity=18.653507, train_loss=2.9260342

Batch 292300, train_perplexity=18.653513, train_loss=2.9260345

Batch 292310, train_perplexity=18.653507, train_loss=2.9260342

Batch 292320, train_perplexity=18.653513, train_loss=2.9260345

Batch 292330, train_perplexity=18.653513, train_loss=2.9260345

Batch 292340, train_perplexity=18.653503, train_loss=2.926034

Batch 292350, train_perplexity=18.653513, train_loss=2.9260345

Batch 292360, train_perplexity=18.653507, train_loss=2.9260342

Batch 292370, train_perplexity=18.653507, train_loss=2.9260342

Batch 292380, train_perplexity=18.653503, train_loss=2.926034

Batch 292390, train_perplexity=18.653503, train_loss=2.926034

Batch 292400, train_perplexity=18.653503, train_loss=2.926034

Batch 292410, train_perplexity=18.653513, train_loss=2.9260345

Batch 292420, train_perplexity=18.653503, train_loss=2.926034

Batch 292430, train_perplexity=18.653503, train_loss=2.926034

Batch 292440, train_perplexity=18.653503, train_loss=2.926034

Batch 292450, train_perplexity=18.653503, train_loss=2.926034

Batch 292460, train_perplexity=18.653503, train_loss=2.926034

Batch 292470, train_perplexity=18.653503, train_loss=2.926034

Batch 292480, train_perplexity=18.6535, train_loss=2.9260337

Batch 292490, train_perplexity=18.653503, train_loss=2.926034

Batch 292500, train_perplexity=18.653503, train_loss=2.926034

Batch 292510, train_perplexity=18.653503, train_loss=2.926034

Batch 292520, train_perplexity=18.6535, train_loss=2.9260337

Batch 292530, train_perplexity=18.653503, train_loss=2.926034

Batch 292540, train_perplexity=18.653503, train_loss=2.926034

Batch 292550, train_perplexity=18.653503, train_loss=2.926034

Batch 292560, train_perplexity=18.653503, train_loss=2.926034

Batch 292570, train_perplexity=18.653503, train_loss=2.926034

Batch 292580, train_perplexity=18.653503, train_loss=2.926034

Batch 292590, train_perplexity=18.6535, train_loss=2.9260337

Batch 292600, train_perplexity=18.653503, train_loss=2.926034

Batch 292610, train_perplexity=18.6535, train_loss=2.9260337

Batch 292620, train_perplexity=18.653503, train_loss=2.926034

Batch 292630, train_perplexity=18.6535, train_loss=2.9260337

Batch 292640, train_perplexity=18.653503, train_loss=2.926034

Batch 292650, train_perplexity=18.653503, train_loss=2.926034

Batch 292660, train_perplexity=18.6535, train_loss=2.9260337

Batch 292670, train_perplexity=18.6535, train_loss=2.9260337

Batch 292680, train_perplexity=18.6535, train_loss=2.9260337

Batch 292690, train_perplexity=18.653494, train_loss=2.9260335

Batch 292700, train_perplexity=18.653494, train_loss=2.9260335

Batch 292710, train_perplexity=18.6535, train_loss=2.9260337

Batch 292720, train_perplexity=18.6535, train_loss=2.9260337

Batch 292730, train_perplexity=18.653494, train_loss=2.9260335

Batch 292740, train_perplexity=18.653494, train_loss=2.9260335

Batch 292750, train_perplexity=18.653494, train_loss=2.9260335

Batch 292760, train_perplexity=18.653494, train_loss=2.9260335

Batch 292770, train_perplexity=18.653494, train_loss=2.9260335

Batch 292780, train_perplexity=18.653494, train_loss=2.9260335

Batch 292790, train_perplexity=18.653494, train_loss=2.9260335

Batch 292800, train_perplexity=18.653494, train_loss=2.9260335

Batch 292810, train_perplexity=18.653494, train_loss=2.9260335

Batch 292820, train_perplexity=18.653494, train_loss=2.9260335

Batch 292830, train_perplexity=18.653494, train_loss=2.9260335

Batch 292840, train_perplexity=18.65349, train_loss=2.9260333

Batch 292850, train_perplexity=18.65349, train_loss=2.9260333

Batch 292860, train_perplexity=18.653494, train_loss=2.9260335

Batch 292870, train_perplexity=18.653494, train_loss=2.9260335

Batch 292880, train_perplexity=18.653494, train_loss=2.9260335

Batch 292890, train_perplexity=18.653494, train_loss=2.9260335

Batch 292900, train_perplexity=18.653494, train_loss=2.9260335

Batch 292910, train_perplexity=18.653486, train_loss=2.926033

Batch 292920, train_perplexity=18.65349, train_loss=2.9260333

Batch 292930, train_perplexity=18.653494, train_loss=2.9260335

Batch 292940, train_perplexity=18.65349, train_loss=2.9260333

Batch 292950, train_perplexity=18.653486, train_loss=2.926033

Batch 292960, train_perplexity=18.65349, train_loss=2.9260333

Batch 292970, train_perplexity=18.653486, train_loss=2.926033

Batch 292980, train_perplexity=18.65349, train_loss=2.9260333

Batch 292990, train_perplexity=18.653486, train_loss=2.926033

Batch 293000, train_perplexity=18.653486, train_loss=2.926033

Batch 293010, train_perplexity=18.65349, train_loss=2.9260333

Batch 293020, train_perplexity=18.653486, train_loss=2.926033

Batch 293030, train_perplexity=18.653486, train_loss=2.926033

Batch 293040, train_perplexity=18.653486, train_loss=2.926033

Batch 293050, train_perplexity=18.653486, train_loss=2.926033

Batch 293060, train_perplexity=18.653486, train_loss=2.926033

Batch 293070, train_perplexity=18.653486, train_loss=2.926033

Batch 293080, train_perplexity=18.653486, train_loss=2.926033

Batch 293090, train_perplexity=18.653486, train_loss=2.926033

Batch 293100, train_perplexity=18.653486, train_loss=2.926033

Batch 293110, train_perplexity=18.653486, train_loss=2.926033

Batch 293120, train_perplexity=18.653486, train_loss=2.926033

Batch 293130, train_perplexity=18.653477, train_loss=2.9260325

Batch 293140, train_perplexity=18.653486, train_loss=2.926033

Batch 293150, train_perplexity=18.653486, train_loss=2.926033

Batch 293160, train_perplexity=18.653486, train_loss=2.926033

Batch 293170, train_perplexity=18.653486, train_loss=2.926033

Batch 293180, train_perplexity=18.65348, train_loss=2.9260328

Batch 293190, train_perplexity=18.653486, train_loss=2.926033

Batch 293200, train_perplexity=18.65348, train_loss=2.9260328

Batch 293210, train_perplexity=18.65348, train_loss=2.9260328

Batch 293220, train_perplexity=18.653486, train_loss=2.926033

Batch 293230, train_perplexity=18.653486, train_loss=2.926033

Batch 293240, train_perplexity=18.65348, train_loss=2.9260328

Batch 293250, train_perplexity=18.653486, train_loss=2.926033

Batch 293260, train_perplexity=18.653477, train_loss=2.9260325

Batch 293270, train_perplexity=18.65348, train_loss=2.9260328

Batch 293280, train_perplexity=18.65348, train_loss=2.9260328

Batch 293290, train_perplexity=18.653477, train_loss=2.9260325

Batch 293300, train_perplexity=18.65348, train_loss=2.9260328

Batch 293310, train_perplexity=18.653477, train_loss=2.9260325

Batch 293320, train_perplexity=18.653477, train_loss=2.9260325

Batch 293330, train_perplexity=18.65348, train_loss=2.9260328

Batch 293340, train_perplexity=18.653477, train_loss=2.9260325

Batch 293350, train_perplexity=18.653477, train_loss=2.9260325

Batch 293360, train_perplexity=18.65348, train_loss=2.9260328

Batch 293370, train_perplexity=18.65348, train_loss=2.9260328

Batch 293380, train_perplexity=18.653477, train_loss=2.9260325

Batch 293390, train_perplexity=18.653477, train_loss=2.9260325

Batch 293400, train_perplexity=18.653477, train_loss=2.9260325

Batch 293410, train_perplexity=18.653477, train_loss=2.9260325

Batch 293420, train_perplexity=18.653477, train_loss=2.9260325

Batch 293430, train_perplexity=18.653477, train_loss=2.9260325

Batch 293440, train_perplexity=18.653477, train_loss=2.9260325

Batch 293450, train_perplexity=18.653477, train_loss=2.9260325

Batch 293460, train_perplexity=18.653477, train_loss=2.9260325

Batch 293470, train_perplexity=18.653477, train_loss=2.9260325

Batch 293480, train_perplexity=18.653477, train_loss=2.9260325

Batch 293490, train_perplexity=18.653477, train_loss=2.9260325

Batch 293500, train_perplexity=18.653477, train_loss=2.9260325
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 293510, train_perplexity=18.653473, train_loss=2.9260323

Batch 293520, train_perplexity=18.653477, train_loss=2.9260325

Batch 293530, train_perplexity=18.653473, train_loss=2.9260323

Batch 293540, train_perplexity=18.653477, train_loss=2.9260325

Batch 293550, train_perplexity=18.653473, train_loss=2.9260323

Batch 293560, train_perplexity=18.653473, train_loss=2.9260323

Batch 293570, train_perplexity=18.653477, train_loss=2.9260325

Batch 293580, train_perplexity=18.653473, train_loss=2.9260323

Batch 293590, train_perplexity=18.653467, train_loss=2.926032

Batch 293600, train_perplexity=18.653467, train_loss=2.926032

Batch 293610, train_perplexity=18.653473, train_loss=2.9260323

Batch 293620, train_perplexity=18.653467, train_loss=2.926032

Batch 293630, train_perplexity=18.653467, train_loss=2.926032

Batch 293640, train_perplexity=18.653467, train_loss=2.926032

Batch 293650, train_perplexity=18.653467, train_loss=2.926032

Batch 293660, train_perplexity=18.653467, train_loss=2.926032

Batch 293670, train_perplexity=18.653467, train_loss=2.926032

Batch 293680, train_perplexity=18.653467, train_loss=2.926032

Batch 293690, train_perplexity=18.653467, train_loss=2.926032

Batch 293700, train_perplexity=18.653467, train_loss=2.926032

Batch 293710, train_perplexity=18.653467, train_loss=2.926032

Batch 293720, train_perplexity=18.653467, train_loss=2.926032

Batch 293730, train_perplexity=18.653467, train_loss=2.926032

Batch 293740, train_perplexity=18.653467, train_loss=2.926032

Batch 293750, train_perplexity=18.653467, train_loss=2.926032

Batch 293760, train_perplexity=18.653467, train_loss=2.926032

Batch 293770, train_perplexity=18.653467, train_loss=2.926032

Batch 293780, train_perplexity=18.653463, train_loss=2.9260318

Batch 293790, train_perplexity=18.653467, train_loss=2.926032

Batch 293800, train_perplexity=18.653467, train_loss=2.926032

Batch 293810, train_perplexity=18.653463, train_loss=2.9260318

Batch 293820, train_perplexity=18.653467, train_loss=2.926032

Batch 293830, train_perplexity=18.653467, train_loss=2.926032

Batch 293840, train_perplexity=18.653467, train_loss=2.926032

Batch 293850, train_perplexity=18.653467, train_loss=2.926032

Batch 293860, train_perplexity=18.653467, train_loss=2.926032

Batch 293870, train_perplexity=18.65346, train_loss=2.9260316

Batch 293880, train_perplexity=18.653467, train_loss=2.926032

Batch 293890, train_perplexity=18.653467, train_loss=2.926032

Batch 293900, train_perplexity=18.65346, train_loss=2.9260316

Batch 293910, train_perplexity=18.653463, train_loss=2.9260318

Batch 293920, train_perplexity=18.653463, train_loss=2.9260318

Batch 293930, train_perplexity=18.653463, train_loss=2.9260318

Batch 293940, train_perplexity=18.653463, train_loss=2.9260318

Batch 293950, train_perplexity=18.65346, train_loss=2.9260316

Batch 293960, train_perplexity=18.65346, train_loss=2.9260316

Batch 293970, train_perplexity=18.65346, train_loss=2.9260316

Batch 293980, train_perplexity=18.65346, train_loss=2.9260316

Batch 293990, train_perplexity=18.65346, train_loss=2.9260316

Batch 294000, train_perplexity=18.65346, train_loss=2.9260316

Batch 294010, train_perplexity=18.65346, train_loss=2.9260316

Batch 294020, train_perplexity=18.65346, train_loss=2.9260316

Batch 294030, train_perplexity=18.65346, train_loss=2.9260316

Batch 294040, train_perplexity=18.65346, train_loss=2.9260316

Batch 294050, train_perplexity=18.65346, train_loss=2.9260316

Batch 294060, train_perplexity=18.65346, train_loss=2.9260316

Batch 294070, train_perplexity=18.65346, train_loss=2.9260316

Batch 294080, train_perplexity=18.65346, train_loss=2.9260316

Batch 294090, train_perplexity=18.65346, train_loss=2.9260316

Batch 294100, train_perplexity=18.65346, train_loss=2.9260316

Batch 294110, train_perplexity=18.65346, train_loss=2.9260316

Batch 294120, train_perplexity=18.65346, train_loss=2.9260316

Batch 294130, train_perplexity=18.65346, train_loss=2.9260316

Batch 294140, train_perplexity=18.65346, train_loss=2.9260316

Batch 294150, train_perplexity=18.65346, train_loss=2.9260316

Batch 294160, train_perplexity=18.65346, train_loss=2.9260316

Batch 294170, train_perplexity=18.653454, train_loss=2.9260314

Batch 294180, train_perplexity=18.65346, train_loss=2.9260316

Batch 294190, train_perplexity=18.65346, train_loss=2.9260316

Batch 294200, train_perplexity=18.653454, train_loss=2.9260314

Batch 294210, train_perplexity=18.65346, train_loss=2.9260316

Batch 294220, train_perplexity=18.65345, train_loss=2.926031

Batch 294230, train_perplexity=18.653454, train_loss=2.9260314

Batch 294240, train_perplexity=18.65345, train_loss=2.926031

Batch 294250, train_perplexity=18.65345, train_loss=2.926031

Batch 294260, train_perplexity=18.65346, train_loss=2.9260316

Batch 294270, train_perplexity=18.653454, train_loss=2.9260314

Batch 294280, train_perplexity=18.653454, train_loss=2.9260314

Batch 294290, train_perplexity=18.65345, train_loss=2.926031

Batch 294300, train_perplexity=18.65345, train_loss=2.926031

Batch 294310, train_perplexity=18.65345, train_loss=2.926031

Batch 294320, train_perplexity=18.65345, train_loss=2.926031

Batch 294330, train_perplexity=18.65345, train_loss=2.926031

Batch 294340, train_perplexity=18.65345, train_loss=2.926031

Batch 294350, train_perplexity=18.65345, train_loss=2.926031

Batch 294360, train_perplexity=18.65345, train_loss=2.926031

Batch 294370, train_perplexity=18.65345, train_loss=2.926031

Batch 294380, train_perplexity=18.65345, train_loss=2.926031

Batch 294390, train_perplexity=18.65345, train_loss=2.926031

Batch 294400, train_perplexity=18.65345, train_loss=2.926031

Batch 294410, train_perplexity=18.65345, train_loss=2.926031

Batch 294420, train_perplexity=18.65345, train_loss=2.926031

Batch 294430, train_perplexity=18.65345, train_loss=2.926031

Batch 294440, train_perplexity=18.65344, train_loss=2.9260306

Batch 294450, train_perplexity=18.653446, train_loss=2.9260309

Batch 294460, train_perplexity=18.653446, train_loss=2.9260309

Batch 294470, train_perplexity=18.65345, train_loss=2.926031

Batch 294480, train_perplexity=18.65345, train_loss=2.926031

Batch 294490, train_perplexity=18.653446, train_loss=2.9260309

Batch 294500, train_perplexity=18.65345, train_loss=2.926031

Batch 294510, train_perplexity=18.65345, train_loss=2.926031

Batch 294520, train_perplexity=18.65344, train_loss=2.9260306

Batch 294530, train_perplexity=18.653446, train_loss=2.9260309

Batch 294540, train_perplexity=18.653446, train_loss=2.9260309

Batch 294550, train_perplexity=18.65344, train_loss=2.9260306

Batch 294560, train_perplexity=18.65344, train_loss=2.9260306

Batch 294570, train_perplexity=18.65344, train_loss=2.9260306

Batch 294580, train_perplexity=18.653446, train_loss=2.9260309

Batch 294590, train_perplexity=18.65344, train_loss=2.9260306

Batch 294600, train_perplexity=18.653446, train_loss=2.9260309

Batch 294610, train_perplexity=18.65344, train_loss=2.9260306

Batch 294620, train_perplexity=18.65344, train_loss=2.9260306

Batch 294630, train_perplexity=18.65344, train_loss=2.9260306

Batch 294640, train_perplexity=18.65344, train_loss=2.9260306

Batch 294650, train_perplexity=18.653446, train_loss=2.9260309

Batch 294660, train_perplexity=18.65344, train_loss=2.9260306

Batch 294670, train_perplexity=18.65344, train_loss=2.9260306

Batch 294680, train_perplexity=18.65344, train_loss=2.9260306

Batch 294690, train_perplexity=18.65344, train_loss=2.9260306

Batch 294700, train_perplexity=18.65344, train_loss=2.9260306

Batch 294710, train_perplexity=18.65344, train_loss=2.9260306

Batch 294720, train_perplexity=18.653437, train_loss=2.9260304

Batch 294730, train_perplexity=18.65344, train_loss=2.9260306

Batch 294740, train_perplexity=18.65344, train_loss=2.9260306

Batch 294750, train_perplexity=18.65344, train_loss=2.9260306

Batch 294760, train_perplexity=18.65344, train_loss=2.9260306

Batch 294770, train_perplexity=18.65344, train_loss=2.9260306

Batch 294780, train_perplexity=18.653437, train_loss=2.9260304

Batch 294790, train_perplexity=18.65344, train_loss=2.9260306

Batch 294800, train_perplexity=18.65344, train_loss=2.9260306
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 294810, train_perplexity=18.65344, train_loss=2.9260306

Batch 294820, train_perplexity=18.653433, train_loss=2.9260302

Batch 294830, train_perplexity=18.653433, train_loss=2.9260302

Batch 294840, train_perplexity=18.653437, train_loss=2.9260304

Batch 294850, train_perplexity=18.653437, train_loss=2.9260304

Batch 294860, train_perplexity=18.653433, train_loss=2.9260302

Batch 294870, train_perplexity=18.653433, train_loss=2.9260302

Batch 294880, train_perplexity=18.653433, train_loss=2.9260302

Batch 294890, train_perplexity=18.653437, train_loss=2.9260304

Batch 294900, train_perplexity=18.653433, train_loss=2.9260302

Batch 294910, train_perplexity=18.653433, train_loss=2.9260302

Batch 294920, train_perplexity=18.653433, train_loss=2.9260302

Batch 294930, train_perplexity=18.653437, train_loss=2.9260304

Batch 294940, train_perplexity=18.653433, train_loss=2.9260302

Batch 294950, train_perplexity=18.653433, train_loss=2.9260302

Batch 294960, train_perplexity=18.653433, train_loss=2.9260302

Batch 294970, train_perplexity=18.653433, train_loss=2.9260302

Batch 294980, train_perplexity=18.653433, train_loss=2.9260302

Batch 294990, train_perplexity=18.653433, train_loss=2.9260302

Batch 295000, train_perplexity=18.653433, train_loss=2.9260302

Batch 295010, train_perplexity=18.653433, train_loss=2.9260302

Batch 295020, train_perplexity=18.653433, train_loss=2.9260302

Batch 295030, train_perplexity=18.653427, train_loss=2.92603

Batch 295040, train_perplexity=18.653433, train_loss=2.9260302

Batch 295050, train_perplexity=18.653433, train_loss=2.9260302

Batch 295060, train_perplexity=18.653433, train_loss=2.9260302

Batch 295070, train_perplexity=18.653433, train_loss=2.9260302

Batch 295080, train_perplexity=18.653433, train_loss=2.9260302

Batch 295090, train_perplexity=18.653423, train_loss=2.9260297

Batch 295100, train_perplexity=18.653433, train_loss=2.9260302

Batch 295110, train_perplexity=18.653433, train_loss=2.9260302

Batch 295120, train_perplexity=18.653433, train_loss=2.9260302

Batch 295130, train_perplexity=18.653427, train_loss=2.92603

Batch 295140, train_perplexity=18.653427, train_loss=2.92603

Batch 295150, train_perplexity=18.653423, train_loss=2.9260297

Batch 295160, train_perplexity=18.653423, train_loss=2.9260297

Batch 295170, train_perplexity=18.653433, train_loss=2.9260302

Batch 295180, train_perplexity=18.653433, train_loss=2.9260302

Batch 295190, train_perplexity=18.653427, train_loss=2.92603

Batch 295200, train_perplexity=18.653423, train_loss=2.9260297

Batch 295210, train_perplexity=18.653423, train_loss=2.9260297

Batch 295220, train_perplexity=18.653427, train_loss=2.92603

Batch 295230, train_perplexity=18.653433, train_loss=2.9260302

Batch 295240, train_perplexity=18.653423, train_loss=2.9260297

Batch 295250, train_perplexity=18.653427, train_loss=2.92603

Batch 295260, train_perplexity=18.653427, train_loss=2.92603

Batch 295270, train_perplexity=18.653423, train_loss=2.9260297

Batch 295280, train_perplexity=18.653423, train_loss=2.9260297

Batch 295290, train_perplexity=18.653423, train_loss=2.9260297

Batch 295300, train_perplexity=18.653423, train_loss=2.9260297

Batch 295310, train_perplexity=18.653423, train_loss=2.9260297

Batch 295320, train_perplexity=18.653423, train_loss=2.9260297

Batch 295330, train_perplexity=18.653423, train_loss=2.9260297

Batch 295340, train_perplexity=18.653423, train_loss=2.9260297

Batch 295350, train_perplexity=18.653423, train_loss=2.9260297

Batch 295360, train_perplexity=18.653423, train_loss=2.9260297

Batch 295370, train_perplexity=18.653423, train_loss=2.9260297

Batch 295380, train_perplexity=18.653423, train_loss=2.9260297

Batch 295390, train_perplexity=18.653423, train_loss=2.9260297

Batch 295400, train_perplexity=18.653423, train_loss=2.9260297

Batch 295410, train_perplexity=18.653423, train_loss=2.9260297

Batch 295420, train_perplexity=18.653423, train_loss=2.9260297

Batch 295430, train_perplexity=18.653423, train_loss=2.9260297

Batch 295440, train_perplexity=18.653414, train_loss=2.9260292

Batch 295450, train_perplexity=18.653414, train_loss=2.9260292

Batch 295460, train_perplexity=18.653414, train_loss=2.9260292

Batch 295470, train_perplexity=18.653423, train_loss=2.9260297

Batch 295480, train_perplexity=18.653423, train_loss=2.9260297

Batch 295490, train_perplexity=18.653414, train_loss=2.9260292

Batch 295500, train_perplexity=18.653423, train_loss=2.9260297

Batch 295510, train_perplexity=18.653414, train_loss=2.9260292

Batch 295520, train_perplexity=18.653414, train_loss=2.9260292

Batch 295530, train_perplexity=18.653414, train_loss=2.9260292

Batch 295540, train_perplexity=18.653414, train_loss=2.9260292

Batch 295550, train_perplexity=18.653414, train_loss=2.9260292

Batch 295560, train_perplexity=18.653414, train_loss=2.9260292

Batch 295570, train_perplexity=18.653414, train_loss=2.9260292

Batch 295580, train_perplexity=18.653414, train_loss=2.9260292

Batch 295590, train_perplexity=18.653414, train_loss=2.9260292

Batch 295600, train_perplexity=18.653414, train_loss=2.9260292

Batch 295610, train_perplexity=18.653414, train_loss=2.9260292

Batch 295620, train_perplexity=18.653414, train_loss=2.9260292

Batch 295630, train_perplexity=18.653414, train_loss=2.9260292

Batch 295640, train_perplexity=18.653414, train_loss=2.9260292

Batch 295650, train_perplexity=18.65341, train_loss=2.926029

Batch 295660, train_perplexity=18.653414, train_loss=2.9260292

Batch 295670, train_perplexity=18.653414, train_loss=2.9260292

Batch 295680, train_perplexity=18.653414, train_loss=2.9260292

Batch 295690, train_perplexity=18.653414, train_loss=2.9260292

Batch 295700, train_perplexity=18.653414, train_loss=2.9260292

Batch 295710, train_perplexity=18.653414, train_loss=2.9260292

Batch 295720, train_perplexity=18.65341, train_loss=2.926029

Batch 295730, train_perplexity=18.653414, train_loss=2.9260292

Batch 295740, train_perplexity=18.653414, train_loss=2.9260292

Batch 295750, train_perplexity=18.65341, train_loss=2.926029

Batch 295760, train_perplexity=18.65341, train_loss=2.926029

Batch 295770, train_perplexity=18.653406, train_loss=2.9260287

Batch 295780, train_perplexity=18.653414, train_loss=2.9260292

Batch 295790, train_perplexity=18.65341, train_loss=2.926029

Batch 295800, train_perplexity=18.65341, train_loss=2.926029

Batch 295810, train_perplexity=18.65341, train_loss=2.926029

Batch 295820, train_perplexity=18.653406, train_loss=2.9260287

Batch 295830, train_perplexity=18.653406, train_loss=2.9260287

Batch 295840, train_perplexity=18.653406, train_loss=2.9260287

Batch 295850, train_perplexity=18.653406, train_loss=2.9260287

Batch 295860, train_perplexity=18.653414, train_loss=2.9260292

Batch 295870, train_perplexity=18.653406, train_loss=2.9260287

Batch 295880, train_perplexity=18.653406, train_loss=2.9260287

Batch 295890, train_perplexity=18.653414, train_loss=2.9260292

Batch 295900, train_perplexity=18.653406, train_loss=2.9260287

Batch 295910, train_perplexity=18.653406, train_loss=2.9260287

Batch 295920, train_perplexity=18.653406, train_loss=2.9260287

Batch 295930, train_perplexity=18.65341, train_loss=2.926029

Batch 295940, train_perplexity=18.653406, train_loss=2.9260287

Batch 295950, train_perplexity=18.653406, train_loss=2.9260287

Batch 295960, train_perplexity=18.653406, train_loss=2.9260287

Batch 295970, train_perplexity=18.653406, train_loss=2.9260287

Batch 295980, train_perplexity=18.653406, train_loss=2.9260287

Batch 295990, train_perplexity=18.653406, train_loss=2.9260287

Batch 296000, train_perplexity=18.6534, train_loss=2.9260285

Batch 296010, train_perplexity=18.653397, train_loss=2.9260283

Batch 296020, train_perplexity=18.653406, train_loss=2.9260287

Batch 296030, train_perplexity=18.653406, train_loss=2.9260287

Batch 296040, train_perplexity=18.653406, train_loss=2.9260287

Batch 296050, train_perplexity=18.653397, train_loss=2.9260283

Batch 296060, train_perplexity=18.653406, train_loss=2.9260287

Batch 296070, train_perplexity=18.653406, train_loss=2.9260287

Batch 296080, train_perplexity=18.6534, train_loss=2.9260285

Batch 296090, train_perplexity=18.6534, train_loss=2.9260285
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 296100, train_perplexity=18.6534, train_loss=2.9260285

Batch 296110, train_perplexity=18.6534, train_loss=2.9260285

Batch 296120, train_perplexity=18.653406, train_loss=2.9260287

Batch 296130, train_perplexity=18.653397, train_loss=2.9260283

Batch 296140, train_perplexity=18.653397, train_loss=2.9260283

Batch 296150, train_perplexity=18.6534, train_loss=2.9260285

Batch 296160, train_perplexity=18.653397, train_loss=2.9260283

Batch 296170, train_perplexity=18.653397, train_loss=2.9260283

Batch 296180, train_perplexity=18.653397, train_loss=2.9260283

Batch 296190, train_perplexity=18.653397, train_loss=2.9260283

Batch 296200, train_perplexity=18.653397, train_loss=2.9260283

Batch 296210, train_perplexity=18.653397, train_loss=2.9260283

Batch 296220, train_perplexity=18.653397, train_loss=2.9260283

Batch 296230, train_perplexity=18.6534, train_loss=2.9260285

Batch 296240, train_perplexity=18.653397, train_loss=2.9260283

Batch 296250, train_perplexity=18.653397, train_loss=2.9260283

Batch 296260, train_perplexity=18.653397, train_loss=2.9260283

Batch 296270, train_perplexity=18.653397, train_loss=2.9260283

Batch 296280, train_perplexity=18.653397, train_loss=2.9260283

Batch 296290, train_perplexity=18.653397, train_loss=2.9260283

Batch 296300, train_perplexity=18.653397, train_loss=2.9260283

Batch 296310, train_perplexity=18.653393, train_loss=2.926028

Batch 296320, train_perplexity=18.653393, train_loss=2.926028

Batch 296330, train_perplexity=18.653397, train_loss=2.9260283

Batch 296340, train_perplexity=18.653397, train_loss=2.9260283

Batch 296350, train_perplexity=18.653393, train_loss=2.926028

Batch 296360, train_perplexity=18.653393, train_loss=2.926028

Batch 296370, train_perplexity=18.653397, train_loss=2.9260283

Batch 296380, train_perplexity=18.653393, train_loss=2.926028

Batch 296390, train_perplexity=18.653397, train_loss=2.9260283

Batch 296400, train_perplexity=18.653393, train_loss=2.926028

Batch 296410, train_perplexity=18.653393, train_loss=2.926028

Batch 296420, train_perplexity=18.653387, train_loss=2.9260278

Batch 296430, train_perplexity=18.653393, train_loss=2.926028

Batch 296440, train_perplexity=18.653387, train_loss=2.9260278

Batch 296450, train_perplexity=18.653397, train_loss=2.9260283

Batch 296460, train_perplexity=18.653387, train_loss=2.9260278

Batch 296470, train_perplexity=18.653387, train_loss=2.9260278

Batch 296480, train_perplexity=18.653387, train_loss=2.9260278

Batch 296490, train_perplexity=18.653387, train_loss=2.9260278

Batch 296500, train_perplexity=18.653387, train_loss=2.9260278

Batch 296510, train_perplexity=18.653393, train_loss=2.926028

Batch 296520, train_perplexity=18.653387, train_loss=2.9260278

Batch 296530, train_perplexity=18.653387, train_loss=2.9260278

Batch 296540, train_perplexity=18.653387, train_loss=2.9260278

Batch 296550, train_perplexity=18.653387, train_loss=2.9260278

Batch 296560, train_perplexity=18.653387, train_loss=2.9260278

Batch 296570, train_perplexity=18.653383, train_loss=2.9260275

Batch 296580, train_perplexity=18.653387, train_loss=2.9260278

Batch 296590, train_perplexity=18.653387, train_loss=2.9260278

Batch 296600, train_perplexity=18.653387, train_loss=2.9260278

Batch 296610, train_perplexity=18.653387, train_loss=2.9260278

Batch 296620, train_perplexity=18.653383, train_loss=2.9260275

Batch 296630, train_perplexity=18.65338, train_loss=2.9260273

Batch 296640, train_perplexity=18.653387, train_loss=2.9260278

Batch 296650, train_perplexity=18.653387, train_loss=2.9260278

Batch 296660, train_perplexity=18.65338, train_loss=2.9260273

Batch 296670, train_perplexity=18.653387, train_loss=2.9260278

Batch 296680, train_perplexity=18.653387, train_loss=2.9260278

Batch 296690, train_perplexity=18.653387, train_loss=2.9260278

Batch 296700, train_perplexity=18.653387, train_loss=2.9260278

Batch 296710, train_perplexity=18.653383, train_loss=2.9260275

Batch 296720, train_perplexity=18.653383, train_loss=2.9260275

Batch 296730, train_perplexity=18.653383, train_loss=2.9260275

Batch 296740, train_perplexity=18.653383, train_loss=2.9260275

Batch 296750, train_perplexity=18.65338, train_loss=2.9260273

Batch 296760, train_perplexity=18.65338, train_loss=2.9260273

Batch 296770, train_perplexity=18.65338, train_loss=2.9260273

Batch 296780, train_perplexity=18.65338, train_loss=2.9260273

Batch 296790, train_perplexity=18.65338, train_loss=2.9260273

Batch 296800, train_perplexity=18.65338, train_loss=2.9260273

Batch 296810, train_perplexity=18.65338, train_loss=2.9260273

Batch 296820, train_perplexity=18.65338, train_loss=2.9260273

Batch 296830, train_perplexity=18.65338, train_loss=2.9260273

Batch 296840, train_perplexity=18.65338, train_loss=2.9260273

Batch 296850, train_perplexity=18.65338, train_loss=2.9260273

Batch 296860, train_perplexity=18.65338, train_loss=2.9260273

Batch 296870, train_perplexity=18.65338, train_loss=2.9260273

Batch 296880, train_perplexity=18.65338, train_loss=2.9260273

Batch 296890, train_perplexity=18.65338, train_loss=2.9260273

Batch 296900, train_perplexity=18.65338, train_loss=2.9260273

Batch 296910, train_perplexity=18.65338, train_loss=2.9260273

Batch 296920, train_perplexity=18.65338, train_loss=2.9260273

Batch 296930, train_perplexity=18.65338, train_loss=2.9260273

Batch 296940, train_perplexity=18.65338, train_loss=2.9260273

Batch 296950, train_perplexity=18.65338, train_loss=2.9260273

Batch 296960, train_perplexity=18.65338, train_loss=2.9260273

Batch 296970, train_perplexity=18.653374, train_loss=2.926027

Batch 296980, train_perplexity=18.653374, train_loss=2.926027

Batch 296990, train_perplexity=18.653374, train_loss=2.926027

Batch 297000, train_perplexity=18.65338, train_loss=2.9260273

Batch 297010, train_perplexity=18.653374, train_loss=2.926027

Batch 297020, train_perplexity=18.65337, train_loss=2.9260268

Batch 297030, train_perplexity=18.653374, train_loss=2.926027

Batch 297040, train_perplexity=18.653374, train_loss=2.926027

Batch 297050, train_perplexity=18.653374, train_loss=2.926027

Batch 297060, train_perplexity=18.653374, train_loss=2.926027

Batch 297070, train_perplexity=18.653374, train_loss=2.926027

Batch 297080, train_perplexity=18.65337, train_loss=2.9260268

Batch 297090, train_perplexity=18.65337, train_loss=2.9260268

Batch 297100, train_perplexity=18.65337, train_loss=2.9260268

Batch 297110, train_perplexity=18.65337, train_loss=2.9260268

Batch 297120, train_perplexity=18.653374, train_loss=2.926027

Batch 297130, train_perplexity=18.65337, train_loss=2.9260268

Batch 297140, train_perplexity=18.65337, train_loss=2.9260268

Batch 297150, train_perplexity=18.653374, train_loss=2.926027

Batch 297160, train_perplexity=18.65337, train_loss=2.9260268

Batch 297170, train_perplexity=18.653374, train_loss=2.926027

Batch 297180, train_perplexity=18.65337, train_loss=2.9260268

Batch 297190, train_perplexity=18.653374, train_loss=2.926027

Batch 297200, train_perplexity=18.65337, train_loss=2.9260268

Batch 297210, train_perplexity=18.65337, train_loss=2.9260268

Batch 297220, train_perplexity=18.65337, train_loss=2.9260268

Batch 297230, train_perplexity=18.65337, train_loss=2.9260268

Batch 297240, train_perplexity=18.65337, train_loss=2.9260268

Batch 297250, train_perplexity=18.65337, train_loss=2.9260268

Batch 297260, train_perplexity=18.65337, train_loss=2.9260268

Batch 297270, train_perplexity=18.65337, train_loss=2.9260268

Batch 297280, train_perplexity=18.65337, train_loss=2.9260268

Batch 297290, train_perplexity=18.65337, train_loss=2.9260268

Batch 297300, train_perplexity=18.653366, train_loss=2.9260266

Batch 297310, train_perplexity=18.65337, train_loss=2.9260268

Batch 297320, train_perplexity=18.653366, train_loss=2.9260266

Batch 297330, train_perplexity=18.65337, train_loss=2.9260268

Batch 297340, train_perplexity=18.653366, train_loss=2.9260266

Batch 297350, train_perplexity=18.65337, train_loss=2.9260268

Batch 297360, train_perplexity=18.65336, train_loss=2.9260263

Batch 297370, train_perplexity=18.65336, train_loss=2.9260263

Batch 297380, train_perplexity=18.65336, train_loss=2.9260263

Batch 297390, train_perplexity=18.653366, train_loss=2.9260266
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 297400, train_perplexity=18.65336, train_loss=2.9260263

Batch 297410, train_perplexity=18.653366, train_loss=2.9260266

Batch 297420, train_perplexity=18.653366, train_loss=2.9260266

Batch 297430, train_perplexity=18.65336, train_loss=2.9260263

Batch 297440, train_perplexity=18.65336, train_loss=2.9260263

Batch 297450, train_perplexity=18.653366, train_loss=2.9260266

Batch 297460, train_perplexity=18.65336, train_loss=2.9260263

Batch 297470, train_perplexity=18.65336, train_loss=2.9260263

Batch 297480, train_perplexity=18.65336, train_loss=2.9260263

Batch 297490, train_perplexity=18.65336, train_loss=2.9260263

Batch 297500, train_perplexity=18.65336, train_loss=2.9260263

Batch 297510, train_perplexity=18.65336, train_loss=2.9260263

Batch 297520, train_perplexity=18.65336, train_loss=2.9260263

Batch 297530, train_perplexity=18.65336, train_loss=2.9260263

Batch 297540, train_perplexity=18.65336, train_loss=2.9260263

Batch 297550, train_perplexity=18.65336, train_loss=2.9260263

Batch 297560, train_perplexity=18.65336, train_loss=2.9260263

Batch 297570, train_perplexity=18.65336, train_loss=2.9260263

Batch 297580, train_perplexity=18.65336, train_loss=2.9260263

Batch 297590, train_perplexity=18.65336, train_loss=2.9260263

Batch 297600, train_perplexity=18.653357, train_loss=2.926026

Batch 297610, train_perplexity=18.653357, train_loss=2.926026

Batch 297620, train_perplexity=18.65336, train_loss=2.9260263

Batch 297630, train_perplexity=18.65336, train_loss=2.9260263

Batch 297640, train_perplexity=18.653357, train_loss=2.926026

Batch 297650, train_perplexity=18.653357, train_loss=2.926026

Batch 297660, train_perplexity=18.653357, train_loss=2.926026

Batch 297670, train_perplexity=18.65336, train_loss=2.9260263

Batch 297680, train_perplexity=18.653353, train_loss=2.9260259

Batch 297690, train_perplexity=18.653353, train_loss=2.9260259

Batch 297700, train_perplexity=18.653357, train_loss=2.926026

Batch 297710, train_perplexity=18.653357, train_loss=2.926026

Batch 297720, train_perplexity=18.653353, train_loss=2.9260259

Batch 297730, train_perplexity=18.653353, train_loss=2.9260259

Batch 297740, train_perplexity=18.653353, train_loss=2.9260259

Batch 297750, train_perplexity=18.653353, train_loss=2.9260259

Batch 297760, train_perplexity=18.653353, train_loss=2.9260259

Batch 297770, train_perplexity=18.653353, train_loss=2.9260259

Batch 297780, train_perplexity=18.653353, train_loss=2.9260259

Batch 297790, train_perplexity=18.653353, train_loss=2.9260259

Batch 297800, train_perplexity=18.653353, train_loss=2.9260259

Batch 297810, train_perplexity=18.653353, train_loss=2.9260259

Batch 297820, train_perplexity=18.653353, train_loss=2.9260259

Batch 297830, train_perplexity=18.653353, train_loss=2.9260259

Batch 297840, train_perplexity=18.653353, train_loss=2.9260259

Batch 297850, train_perplexity=18.653353, train_loss=2.9260259

Batch 297860, train_perplexity=18.653353, train_loss=2.9260259

Batch 297870, train_perplexity=18.653353, train_loss=2.9260259

Batch 297880, train_perplexity=18.653347, train_loss=2.9260256

Batch 297890, train_perplexity=18.653353, train_loss=2.9260259

Batch 297900, train_perplexity=18.653353, train_loss=2.9260259

Batch 297910, train_perplexity=18.653353, train_loss=2.9260259

Batch 297920, train_perplexity=18.653347, train_loss=2.9260256

Batch 297930, train_perplexity=18.653347, train_loss=2.9260256

Batch 297940, train_perplexity=18.653353, train_loss=2.9260259

Batch 297950, train_perplexity=18.653347, train_loss=2.9260256

Batch 297960, train_perplexity=18.653353, train_loss=2.9260259

Batch 297970, train_perplexity=18.653353, train_loss=2.9260259

Batch 297980, train_perplexity=18.653353, train_loss=2.9260259

Batch 297990, train_perplexity=18.653343, train_loss=2.9260254

Batch 298000, train_perplexity=18.653347, train_loss=2.9260256

Batch 298010, train_perplexity=18.653343, train_loss=2.9260254

Batch 298020, train_perplexity=18.653353, train_loss=2.9260259

Batch 298030, train_perplexity=18.653347, train_loss=2.9260256

Batch 298040, train_perplexity=18.653353, train_loss=2.9260259

Batch 298050, train_perplexity=18.653343, train_loss=2.9260254

Batch 298060, train_perplexity=18.653343, train_loss=2.9260254

Batch 298070, train_perplexity=18.653343, train_loss=2.9260254

Batch 298080, train_perplexity=18.653343, train_loss=2.9260254

Batch 298090, train_perplexity=18.653343, train_loss=2.9260254

Batch 298100, train_perplexity=18.653343, train_loss=2.9260254

Batch 298110, train_perplexity=18.653343, train_loss=2.9260254

Batch 298120, train_perplexity=18.653343, train_loss=2.9260254

Batch 298130, train_perplexity=18.653343, train_loss=2.9260254

Batch 298140, train_perplexity=18.653343, train_loss=2.9260254

Batch 298150, train_perplexity=18.653343, train_loss=2.9260254

Batch 298160, train_perplexity=18.653343, train_loss=2.9260254

Batch 298170, train_perplexity=18.653343, train_loss=2.9260254

Batch 298180, train_perplexity=18.653343, train_loss=2.9260254

Batch 298190, train_perplexity=18.653343, train_loss=2.9260254

Batch 298200, train_perplexity=18.653343, train_loss=2.9260254

Batch 298210, train_perplexity=18.653343, train_loss=2.9260254

Batch 298220, train_perplexity=18.653343, train_loss=2.9260254

Batch 298230, train_perplexity=18.65334, train_loss=2.9260252

Batch 298240, train_perplexity=18.653343, train_loss=2.9260254

Batch 298250, train_perplexity=18.65334, train_loss=2.9260252

Batch 298260, train_perplexity=18.653343, train_loss=2.9260254

Batch 298270, train_perplexity=18.65334, train_loss=2.9260252

Batch 298280, train_perplexity=18.653334, train_loss=2.926025

Batch 298290, train_perplexity=18.65334, train_loss=2.9260252

Batch 298300, train_perplexity=18.653343, train_loss=2.9260254

Batch 298310, train_perplexity=18.65334, train_loss=2.9260252

Batch 298320, train_perplexity=18.653334, train_loss=2.926025

Batch 298330, train_perplexity=18.653334, train_loss=2.926025

Batch 298340, train_perplexity=18.65334, train_loss=2.9260252

Batch 298350, train_perplexity=18.653334, train_loss=2.926025

Batch 298360, train_perplexity=18.653334, train_loss=2.926025

Batch 298370, train_perplexity=18.653334, train_loss=2.926025

Batch 298380, train_perplexity=18.653334, train_loss=2.926025

Batch 298390, train_perplexity=18.65334, train_loss=2.9260252

Batch 298400, train_perplexity=18.653334, train_loss=2.926025

Batch 298410, train_perplexity=18.65334, train_loss=2.9260252

Batch 298420, train_perplexity=18.653334, train_loss=2.926025

Batch 298430, train_perplexity=18.65334, train_loss=2.9260252

Batch 298440, train_perplexity=18.653334, train_loss=2.926025

Batch 298450, train_perplexity=18.653334, train_loss=2.926025

Batch 298460, train_perplexity=18.653334, train_loss=2.926025

Batch 298470, train_perplexity=18.65333, train_loss=2.9260247

Batch 298480, train_perplexity=18.653334, train_loss=2.926025

Batch 298490, train_perplexity=18.653334, train_loss=2.926025

Batch 298500, train_perplexity=18.653334, train_loss=2.926025

Batch 298510, train_perplexity=18.653334, train_loss=2.926025

Batch 298520, train_perplexity=18.653334, train_loss=2.926025

Batch 298530, train_perplexity=18.653334, train_loss=2.926025

Batch 298540, train_perplexity=18.653334, train_loss=2.926025

Batch 298550, train_perplexity=18.653334, train_loss=2.926025

Batch 298560, train_perplexity=18.653334, train_loss=2.926025

Batch 298570, train_perplexity=18.653334, train_loss=2.926025

Batch 298580, train_perplexity=18.653334, train_loss=2.926025

Batch 298590, train_perplexity=18.653334, train_loss=2.926025

Batch 298600, train_perplexity=18.65333, train_loss=2.9260247

Batch 298610, train_perplexity=18.65333, train_loss=2.9260247

Batch 298620, train_perplexity=18.653334, train_loss=2.926025

Batch 298630, train_perplexity=18.65333, train_loss=2.9260247

Batch 298640, train_perplexity=18.653326, train_loss=2.9260244

Batch 298650, train_perplexity=18.653334, train_loss=2.926025

Batch 298660, train_perplexity=18.653334, train_loss=2.926025

Batch 298670, train_perplexity=18.653326, train_loss=2.9260244

Batch 298680, train_perplexity=18.653326, train_loss=2.9260244

Batch 298690, train_perplexity=18.653326, train_loss=2.9260244
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 298700, train_perplexity=18.653326, train_loss=2.9260244

Batch 298710, train_perplexity=18.653326, train_loss=2.9260244

Batch 298720, train_perplexity=18.653326, train_loss=2.9260244

Batch 298730, train_perplexity=18.653326, train_loss=2.9260244

Batch 298740, train_perplexity=18.653326, train_loss=2.9260244

Batch 298750, train_perplexity=18.653326, train_loss=2.9260244

Batch 298760, train_perplexity=18.653326, train_loss=2.9260244

Batch 298770, train_perplexity=18.653326, train_loss=2.9260244

Batch 298780, train_perplexity=18.653326, train_loss=2.9260244

Batch 298790, train_perplexity=18.653326, train_loss=2.9260244

Batch 298800, train_perplexity=18.653326, train_loss=2.9260244

Batch 298810, train_perplexity=18.653326, train_loss=2.9260244

Batch 298820, train_perplexity=18.65332, train_loss=2.9260242

Batch 298830, train_perplexity=18.653326, train_loss=2.9260244

Batch 298840, train_perplexity=18.653326, train_loss=2.9260244

Batch 298850, train_perplexity=18.653326, train_loss=2.9260244

Batch 298860, train_perplexity=18.653326, train_loss=2.9260244

Batch 298870, train_perplexity=18.653326, train_loss=2.9260244

Batch 298880, train_perplexity=18.65332, train_loss=2.9260242

Batch 298890, train_perplexity=18.65332, train_loss=2.9260242

Batch 298900, train_perplexity=18.653316, train_loss=2.926024

Batch 298910, train_perplexity=18.653326, train_loss=2.9260244

Batch 298920, train_perplexity=18.65332, train_loss=2.9260242

Batch 298930, train_perplexity=18.65332, train_loss=2.9260242

Batch 298940, train_perplexity=18.65332, train_loss=2.9260242

Batch 298950, train_perplexity=18.65332, train_loss=2.9260242

Batch 298960, train_perplexity=18.653316, train_loss=2.926024

Batch 298970, train_perplexity=18.653316, train_loss=2.926024

Batch 298980, train_perplexity=18.65332, train_loss=2.9260242

Batch 298990, train_perplexity=18.653316, train_loss=2.926024

Batch 299000, train_perplexity=18.653316, train_loss=2.926024

Batch 299010, train_perplexity=18.653316, train_loss=2.926024

Batch 299020, train_perplexity=18.653316, train_loss=2.926024

Batch 299030, train_perplexity=18.653316, train_loss=2.926024

Batch 299040, train_perplexity=18.653316, train_loss=2.926024

Batch 299050, train_perplexity=18.653316, train_loss=2.926024

Batch 299060, train_perplexity=18.653316, train_loss=2.926024

Batch 299070, train_perplexity=18.653316, train_loss=2.926024

Batch 299080, train_perplexity=18.653316, train_loss=2.926024

Batch 299090, train_perplexity=18.653316, train_loss=2.926024

Batch 299100, train_perplexity=18.653316, train_loss=2.926024

Batch 299110, train_perplexity=18.653316, train_loss=2.926024

Batch 299120, train_perplexity=18.653316, train_loss=2.926024

Batch 299130, train_perplexity=18.653316, train_loss=2.926024

Batch 299140, train_perplexity=18.653313, train_loss=2.9260237

Batch 299150, train_perplexity=18.653316, train_loss=2.926024

Batch 299160, train_perplexity=18.653316, train_loss=2.926024

Batch 299170, train_perplexity=18.653316, train_loss=2.926024

Batch 299180, train_perplexity=18.653316, train_loss=2.926024

Batch 299190, train_perplexity=18.653316, train_loss=2.926024

Batch 299200, train_perplexity=18.653313, train_loss=2.9260237

Batch 299210, train_perplexity=18.653316, train_loss=2.926024

Batch 299220, train_perplexity=18.653313, train_loss=2.9260237

Batch 299230, train_perplexity=18.653313, train_loss=2.9260237

Batch 299240, train_perplexity=18.653307, train_loss=2.9260235

Batch 299250, train_perplexity=18.653307, train_loss=2.9260235

Batch 299260, train_perplexity=18.653307, train_loss=2.9260235

Batch 299270, train_perplexity=18.653307, train_loss=2.9260235

Batch 299280, train_perplexity=18.653307, train_loss=2.9260235

Batch 299290, train_perplexity=18.653313, train_loss=2.9260237

Batch 299300, train_perplexity=18.653307, train_loss=2.9260235

Batch 299310, train_perplexity=18.653307, train_loss=2.9260235

Batch 299320, train_perplexity=18.653313, train_loss=2.9260237

Batch 299330, train_perplexity=18.653307, train_loss=2.9260235

Batch 299340, train_perplexity=18.653307, train_loss=2.9260235

Batch 299350, train_perplexity=18.653307, train_loss=2.9260235

Batch 299360, train_perplexity=18.653307, train_loss=2.9260235

Batch 299370, train_perplexity=18.653307, train_loss=2.9260235

Batch 299380, train_perplexity=18.653307, train_loss=2.9260235

Batch 299390, train_perplexity=18.653307, train_loss=2.9260235

Batch 299400, train_perplexity=18.653307, train_loss=2.9260235

Batch 299410, train_perplexity=18.653307, train_loss=2.9260235

Batch 299420, train_perplexity=18.653307, train_loss=2.9260235

Batch 299430, train_perplexity=18.653307, train_loss=2.9260235

Batch 299440, train_perplexity=18.653307, train_loss=2.9260235

Batch 299450, train_perplexity=18.653307, train_loss=2.9260235

Batch 299460, train_perplexity=18.653307, train_loss=2.9260235

Batch 299470, train_perplexity=18.653307, train_loss=2.9260235

Batch 299480, train_perplexity=18.653307, train_loss=2.9260235

Batch 299490, train_perplexity=18.653303, train_loss=2.9260232

Batch 299500, train_perplexity=18.653307, train_loss=2.9260235

Batch 299510, train_perplexity=18.653307, train_loss=2.9260235

Batch 299520, train_perplexity=18.653303, train_loss=2.9260232

Batch 299530, train_perplexity=18.653303, train_loss=2.9260232

Batch 299540, train_perplexity=18.6533, train_loss=2.926023

Batch 299550, train_perplexity=18.653303, train_loss=2.9260232

Batch 299560, train_perplexity=18.653303, train_loss=2.9260232

Batch 299570, train_perplexity=18.6533, train_loss=2.926023

Batch 299580, train_perplexity=18.653307, train_loss=2.9260235

Batch 299590, train_perplexity=18.6533, train_loss=2.926023

Batch 299600, train_perplexity=18.6533, train_loss=2.926023

Batch 299610, train_perplexity=18.6533, train_loss=2.926023

Batch 299620, train_perplexity=18.6533, train_loss=2.926023

Batch 299630, train_perplexity=18.6533, train_loss=2.926023

Batch 299640, train_perplexity=18.6533, train_loss=2.926023

Batch 299650, train_perplexity=18.6533, train_loss=2.926023

Batch 299660, train_perplexity=18.6533, train_loss=2.926023

Batch 299670, train_perplexity=18.6533, train_loss=2.926023

Batch 299680, train_perplexity=18.6533, train_loss=2.926023

Batch 299690, train_perplexity=18.653303, train_loss=2.9260232

Batch 299700, train_perplexity=18.6533, train_loss=2.926023

Batch 299710, train_perplexity=18.6533, train_loss=2.926023

Batch 299720, train_perplexity=18.6533, train_loss=2.926023

Batch 299730, train_perplexity=18.6533, train_loss=2.926023

Batch 299740, train_perplexity=18.6533, train_loss=2.926023

Batch 299750, train_perplexity=18.6533, train_loss=2.926023

Batch 299760, train_perplexity=18.6533, train_loss=2.926023

Batch 299770, train_perplexity=18.6533, train_loss=2.926023

Batch 299780, train_perplexity=18.6533, train_loss=2.926023

Batch 299790, train_perplexity=18.6533, train_loss=2.926023

Batch 299800, train_perplexity=18.6533, train_loss=2.926023

Batch 299810, train_perplexity=18.653294, train_loss=2.9260228

Batch 299820, train_perplexity=18.6533, train_loss=2.926023

Batch 299830, train_perplexity=18.6533, train_loss=2.926023

Batch 299840, train_perplexity=18.653294, train_loss=2.9260228

Batch 299850, train_perplexity=18.653294, train_loss=2.9260228

Batch 299860, train_perplexity=18.65329, train_loss=2.9260225

Batch 299870, train_perplexity=18.65329, train_loss=2.9260225

Batch 299880, train_perplexity=18.653294, train_loss=2.9260228

Batch 299890, train_perplexity=18.653294, train_loss=2.9260228

Batch 299900, train_perplexity=18.6533, train_loss=2.926023

Batch 299910, train_perplexity=18.65329, train_loss=2.9260225

Batch 299920, train_perplexity=18.65329, train_loss=2.9260225

Batch 299930, train_perplexity=18.653294, train_loss=2.9260228

Batch 299940, train_perplexity=18.65329, train_loss=2.9260225

Batch 299950, train_perplexity=18.6533, train_loss=2.926023

Batch 299960, train_perplexity=18.65329, train_loss=2.9260225

Batch 299970, train_perplexity=18.653294, train_loss=2.9260228

Batch 299980, train_perplexity=18.65329, train_loss=2.9260225

Batch 299990, train_perplexity=18.65329, train_loss=2.9260225
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 300000, train_perplexity=18.65329, train_loss=2.9260225

Batch 300010, train_perplexity=18.65329, train_loss=2.9260225

Batch 300020, train_perplexity=18.65329, train_loss=2.9260225

Batch 300030, train_perplexity=18.65329, train_loss=2.9260225

Batch 300040, train_perplexity=18.65329, train_loss=2.9260225

Batch 300050, train_perplexity=18.65329, train_loss=2.9260225

Batch 300060, train_perplexity=18.65329, train_loss=2.9260225

Batch 300070, train_perplexity=18.65329, train_loss=2.9260225

Batch 300080, train_perplexity=18.65329, train_loss=2.9260225

Batch 300090, train_perplexity=18.65329, train_loss=2.9260225

Batch 300100, train_perplexity=18.65329, train_loss=2.9260225

Batch 300110, train_perplexity=18.65329, train_loss=2.9260225

Batch 300120, train_perplexity=18.65329, train_loss=2.9260225

Batch 300130, train_perplexity=18.653286, train_loss=2.9260223

Batch 300140, train_perplexity=18.653286, train_loss=2.9260223

Batch 300150, train_perplexity=18.65329, train_loss=2.9260225

Batch 300160, train_perplexity=18.65329, train_loss=2.9260225

Batch 300170, train_perplexity=18.653286, train_loss=2.9260223

Batch 300180, train_perplexity=18.65328, train_loss=2.926022

Batch 300190, train_perplexity=18.65329, train_loss=2.9260225

Batch 300200, train_perplexity=18.65328, train_loss=2.926022

Batch 300210, train_perplexity=18.653286, train_loss=2.9260223

Batch 300220, train_perplexity=18.653286, train_loss=2.9260223

Batch 300230, train_perplexity=18.65328, train_loss=2.926022

Batch 300240, train_perplexity=18.65328, train_loss=2.926022

Batch 300250, train_perplexity=18.653286, train_loss=2.9260223

Batch 300260, train_perplexity=18.65328, train_loss=2.926022

Batch 300270, train_perplexity=18.65328, train_loss=2.926022

Batch 300280, train_perplexity=18.653286, train_loss=2.9260223

Batch 300290, train_perplexity=18.65328, train_loss=2.926022

Batch 300300, train_perplexity=18.65328, train_loss=2.926022

Batch 300310, train_perplexity=18.65328, train_loss=2.926022

Batch 300320, train_perplexity=18.65328, train_loss=2.926022

Batch 300330, train_perplexity=18.65328, train_loss=2.926022

Batch 300340, train_perplexity=18.65328, train_loss=2.926022

Batch 300350, train_perplexity=18.65328, train_loss=2.926022

Batch 300360, train_perplexity=18.65328, train_loss=2.926022

Batch 300370, train_perplexity=18.65328, train_loss=2.926022

Batch 300380, train_perplexity=18.65328, train_loss=2.926022

Batch 300390, train_perplexity=18.65328, train_loss=2.926022

Batch 300400, train_perplexity=18.65328, train_loss=2.926022

Batch 300410, train_perplexity=18.65328, train_loss=2.926022

Batch 300420, train_perplexity=18.65328, train_loss=2.926022

Batch 300430, train_perplexity=18.653276, train_loss=2.9260218

Batch 300440, train_perplexity=18.65328, train_loss=2.926022

Batch 300450, train_perplexity=18.65328, train_loss=2.926022

Batch 300460, train_perplexity=18.65328, train_loss=2.926022

Batch 300470, train_perplexity=18.65328, train_loss=2.926022

Batch 300480, train_perplexity=18.65328, train_loss=2.926022

Batch 300490, train_perplexity=18.65328, train_loss=2.926022

Batch 300500, train_perplexity=18.653273, train_loss=2.9260216

Batch 300510, train_perplexity=18.653273, train_loss=2.9260216

Batch 300520, train_perplexity=18.653273, train_loss=2.9260216

Batch 300530, train_perplexity=18.65328, train_loss=2.926022

Batch 300540, train_perplexity=18.653273, train_loss=2.9260216

Batch 300550, train_perplexity=18.653276, train_loss=2.9260218

Batch 300560, train_perplexity=18.653273, train_loss=2.9260216

Batch 300570, train_perplexity=18.653273, train_loss=2.9260216

Batch 300580, train_perplexity=18.653273, train_loss=2.9260216

Batch 300590, train_perplexity=18.653273, train_loss=2.9260216

Batch 300600, train_perplexity=18.653273, train_loss=2.9260216

Batch 300610, train_perplexity=18.653273, train_loss=2.9260216

Batch 300620, train_perplexity=18.653273, train_loss=2.9260216

Batch 300630, train_perplexity=18.653273, train_loss=2.9260216

Batch 300640, train_perplexity=18.653273, train_loss=2.9260216

Batch 300650, train_perplexity=18.653273, train_loss=2.9260216

Batch 300660, train_perplexity=18.653273, train_loss=2.9260216

Batch 300670, train_perplexity=18.653273, train_loss=2.9260216

Batch 300680, train_perplexity=18.653273, train_loss=2.9260216

Batch 300690, train_perplexity=18.653273, train_loss=2.9260216

Batch 300700, train_perplexity=18.653273, train_loss=2.9260216

Batch 300710, train_perplexity=18.653273, train_loss=2.9260216

Batch 300720, train_perplexity=18.653273, train_loss=2.9260216

Batch 300730, train_perplexity=18.653273, train_loss=2.9260216

Batch 300740, train_perplexity=18.653273, train_loss=2.9260216

Batch 300750, train_perplexity=18.653263, train_loss=2.926021

Batch 300760, train_perplexity=18.653273, train_loss=2.9260216

Batch 300770, train_perplexity=18.653273, train_loss=2.9260216

Batch 300780, train_perplexity=18.653273, train_loss=2.9260216

Batch 300790, train_perplexity=18.653267, train_loss=2.9260213

Batch 300800, train_perplexity=18.653263, train_loss=2.926021

Batch 300810, train_perplexity=18.653273, train_loss=2.9260216

Batch 300820, train_perplexity=18.653263, train_loss=2.926021

Batch 300830, train_perplexity=18.653267, train_loss=2.9260213

Batch 300840, train_perplexity=18.653267, train_loss=2.9260213

Batch 300850, train_perplexity=18.653267, train_loss=2.9260213

Batch 300860, train_perplexity=18.653273, train_loss=2.9260216

Batch 300870, train_perplexity=18.653267, train_loss=2.9260213

Batch 300880, train_perplexity=18.653263, train_loss=2.926021

Batch 300890, train_perplexity=18.653263, train_loss=2.926021

Batch 300900, train_perplexity=18.653267, train_loss=2.9260213

Batch 300910, train_perplexity=18.653263, train_loss=2.926021

Batch 300920, train_perplexity=18.653263, train_loss=2.926021

Batch 300930, train_perplexity=18.653263, train_loss=2.926021

Batch 300940, train_perplexity=18.653263, train_loss=2.926021

Batch 300950, train_perplexity=18.653263, train_loss=2.926021

Batch 300960, train_perplexity=18.653263, train_loss=2.926021

Batch 300970, train_perplexity=18.653263, train_loss=2.926021

Batch 300980, train_perplexity=18.653263, train_loss=2.926021

Batch 300990, train_perplexity=18.653263, train_loss=2.926021

Batch 301000, train_perplexity=18.653263, train_loss=2.926021

Batch 301010, train_perplexity=18.653263, train_loss=2.926021

Batch 301020, train_perplexity=18.653263, train_loss=2.926021

Batch 301030, train_perplexity=18.653263, train_loss=2.926021

Batch 301040, train_perplexity=18.653263, train_loss=2.926021

Batch 301050, train_perplexity=18.653263, train_loss=2.926021

Batch 301060, train_perplexity=18.65326, train_loss=2.9260209

Batch 301070, train_perplexity=18.653263, train_loss=2.926021

Batch 301080, train_perplexity=18.65326, train_loss=2.9260209

Batch 301090, train_perplexity=18.653263, train_loss=2.926021

Batch 301100, train_perplexity=18.653263, train_loss=2.926021

Batch 301110, train_perplexity=18.653263, train_loss=2.926021

Batch 301120, train_perplexity=18.653254, train_loss=2.9260206

Batch 301130, train_perplexity=18.653254, train_loss=2.9260206

Batch 301140, train_perplexity=18.653263, train_loss=2.926021

Batch 301150, train_perplexity=18.653254, train_loss=2.9260206

Batch 301160, train_perplexity=18.653254, train_loss=2.9260206

Batch 301170, train_perplexity=18.653263, train_loss=2.926021

Batch 301180, train_perplexity=18.653254, train_loss=2.9260206

Batch 301190, train_perplexity=18.653254, train_loss=2.9260206

Batch 301200, train_perplexity=18.65326, train_loss=2.9260209

Batch 301210, train_perplexity=18.653254, train_loss=2.9260206

Batch 301220, train_perplexity=18.653254, train_loss=2.9260206

Batch 301230, train_perplexity=18.653254, train_loss=2.9260206

Batch 301240, train_perplexity=18.653254, train_loss=2.9260206

Batch 301250, train_perplexity=18.653254, train_loss=2.9260206

Batch 301260, train_perplexity=18.653254, train_loss=2.9260206

Batch 301270, train_perplexity=18.653254, train_loss=2.9260206

Batch 301280, train_perplexity=18.653254, train_loss=2.9260206

Batch 301290, train_perplexity=18.653254, train_loss=2.9260206
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 301300, train_perplexity=18.65325, train_loss=2.9260204

Batch 301310, train_perplexity=18.653254, train_loss=2.9260206

Batch 301320, train_perplexity=18.653254, train_loss=2.9260206

Batch 301330, train_perplexity=18.653246, train_loss=2.9260201

Batch 301340, train_perplexity=18.65325, train_loss=2.9260204

Batch 301350, train_perplexity=18.653254, train_loss=2.9260206

Batch 301360, train_perplexity=18.653254, train_loss=2.9260206

Batch 301370, train_perplexity=18.653246, train_loss=2.9260201

Batch 301380, train_perplexity=18.65325, train_loss=2.9260204

Batch 301390, train_perplexity=18.653254, train_loss=2.9260206

Batch 301400, train_perplexity=18.65325, train_loss=2.9260204

Batch 301410, train_perplexity=18.65325, train_loss=2.9260204

Batch 301420, train_perplexity=18.65325, train_loss=2.9260204

Batch 301430, train_perplexity=18.653246, train_loss=2.9260201

Batch 301440, train_perplexity=18.653254, train_loss=2.9260206

Batch 301450, train_perplexity=18.65325, train_loss=2.9260204

Batch 301460, train_perplexity=18.653246, train_loss=2.9260201

Batch 301470, train_perplexity=18.653254, train_loss=2.9260206

Batch 301480, train_perplexity=18.653246, train_loss=2.9260201

Batch 301490, train_perplexity=18.653246, train_loss=2.9260201

Batch 301500, train_perplexity=18.65325, train_loss=2.9260204

Batch 301510, train_perplexity=18.65325, train_loss=2.9260204

Batch 301520, train_perplexity=18.653246, train_loss=2.9260201

Batch 301530, train_perplexity=18.653246, train_loss=2.9260201

Batch 301540, train_perplexity=18.653246, train_loss=2.9260201

Batch 301550, train_perplexity=18.65325, train_loss=2.9260204

Batch 301560, train_perplexity=18.653246, train_loss=2.9260201

Batch 301570, train_perplexity=18.653246, train_loss=2.9260201

Batch 301580, train_perplexity=18.653246, train_loss=2.9260201

Batch 301590, train_perplexity=18.653246, train_loss=2.9260201

Batch 301600, train_perplexity=18.653246, train_loss=2.9260201

Batch 301610, train_perplexity=18.65324, train_loss=2.92602

Batch 301620, train_perplexity=18.653246, train_loss=2.9260201

Batch 301630, train_perplexity=18.653246, train_loss=2.9260201

Batch 301640, train_perplexity=18.653246, train_loss=2.9260201

Batch 301650, train_perplexity=18.653246, train_loss=2.9260201

Batch 301660, train_perplexity=18.653246, train_loss=2.9260201

Batch 301670, train_perplexity=18.65324, train_loss=2.92602

Batch 301680, train_perplexity=18.653236, train_loss=2.9260197

Batch 301690, train_perplexity=18.653246, train_loss=2.9260201

Batch 301700, train_perplexity=18.653246, train_loss=2.9260201

Batch 301710, train_perplexity=18.653246, train_loss=2.9260201

Batch 301720, train_perplexity=18.653236, train_loss=2.9260197

Batch 301730, train_perplexity=18.65324, train_loss=2.92602

Batch 301740, train_perplexity=18.653246, train_loss=2.9260201

Batch 301750, train_perplexity=18.653246, train_loss=2.9260201

Batch 301760, train_perplexity=18.653236, train_loss=2.9260197

Batch 301770, train_perplexity=18.653236, train_loss=2.9260197

Batch 301780, train_perplexity=18.653246, train_loss=2.9260201

Batch 301790, train_perplexity=18.653236, train_loss=2.9260197

Batch 301800, train_perplexity=18.653236, train_loss=2.9260197

Batch 301810, train_perplexity=18.653246, train_loss=2.9260201

Batch 301820, train_perplexity=18.653246, train_loss=2.9260201

Batch 301830, train_perplexity=18.653236, train_loss=2.9260197

Batch 301840, train_perplexity=18.653236, train_loss=2.9260197

Batch 301850, train_perplexity=18.653236, train_loss=2.9260197

Batch 301860, train_perplexity=18.653236, train_loss=2.9260197

Batch 301870, train_perplexity=18.653236, train_loss=2.9260197

Batch 301880, train_perplexity=18.653236, train_loss=2.9260197

Batch 301890, train_perplexity=18.653236, train_loss=2.9260197

Batch 301900, train_perplexity=18.653236, train_loss=2.9260197

Batch 301910, train_perplexity=18.653236, train_loss=2.9260197

Batch 301920, train_perplexity=18.653233, train_loss=2.9260194

Batch 301930, train_perplexity=18.653236, train_loss=2.9260197

Batch 301940, train_perplexity=18.653227, train_loss=2.9260192

Batch 301950, train_perplexity=18.653236, train_loss=2.9260197

Batch 301960, train_perplexity=18.653233, train_loss=2.9260194

Batch 301970, train_perplexity=18.653236, train_loss=2.9260197

Batch 301980, train_perplexity=18.653233, train_loss=2.9260194

Batch 301990, train_perplexity=18.653236, train_loss=2.9260197

Batch 302000, train_perplexity=18.653233, train_loss=2.9260194

Batch 302010, train_perplexity=18.653236, train_loss=2.9260197

Batch 302020, train_perplexity=18.653233, train_loss=2.9260194

Batch 302030, train_perplexity=18.653233, train_loss=2.9260194

Batch 302040, train_perplexity=18.653227, train_loss=2.9260192

Batch 302050, train_perplexity=18.653236, train_loss=2.9260197

Batch 302060, train_perplexity=18.653227, train_loss=2.9260192

Batch 302070, train_perplexity=18.653227, train_loss=2.9260192

Batch 302080, train_perplexity=18.653233, train_loss=2.9260194

Batch 302090, train_perplexity=18.653233, train_loss=2.9260194

Batch 302100, train_perplexity=18.653227, train_loss=2.9260192

Batch 302110, train_perplexity=18.653236, train_loss=2.9260197

Batch 302120, train_perplexity=18.653233, train_loss=2.9260194

Batch 302130, train_perplexity=18.653233, train_loss=2.9260194

Batch 302140, train_perplexity=18.653233, train_loss=2.9260194

Batch 302150, train_perplexity=18.653233, train_loss=2.9260194

Batch 302160, train_perplexity=18.653227, train_loss=2.9260192

Batch 302170, train_perplexity=18.653233, train_loss=2.9260194

Batch 302180, train_perplexity=18.653227, train_loss=2.9260192

Batch 302190, train_perplexity=18.653227, train_loss=2.9260192

Batch 302200, train_perplexity=18.653233, train_loss=2.9260194

Batch 302210, train_perplexity=18.653227, train_loss=2.9260192

Batch 302220, train_perplexity=18.653227, train_loss=2.9260192

Batch 302230, train_perplexity=18.653227, train_loss=2.9260192

Batch 302240, train_perplexity=18.653227, train_loss=2.9260192

Batch 302250, train_perplexity=18.653227, train_loss=2.9260192

Batch 302260, train_perplexity=18.653227, train_loss=2.9260192

Batch 302270, train_perplexity=18.653227, train_loss=2.9260192

Batch 302280, train_perplexity=18.653227, train_loss=2.9260192

Batch 302290, train_perplexity=18.653227, train_loss=2.9260192

Batch 302300, train_perplexity=18.653227, train_loss=2.9260192

Batch 302310, train_perplexity=18.653223, train_loss=2.926019

Batch 302320, train_perplexity=18.65322, train_loss=2.9260187

Batch 302330, train_perplexity=18.653227, train_loss=2.9260192

Batch 302340, train_perplexity=18.653227, train_loss=2.9260192

Batch 302350, train_perplexity=18.653227, train_loss=2.9260192

Batch 302360, train_perplexity=18.653223, train_loss=2.926019

Batch 302370, train_perplexity=18.653227, train_loss=2.9260192

Batch 302380, train_perplexity=18.653227, train_loss=2.9260192

Batch 302390, train_perplexity=18.653227, train_loss=2.9260192

Batch 302400, train_perplexity=18.65322, train_loss=2.9260187

Batch 302410, train_perplexity=18.65322, train_loss=2.9260187

Batch 302420, train_perplexity=18.653227, train_loss=2.9260192

Batch 302430, train_perplexity=18.653227, train_loss=2.9260192

Batch 302440, train_perplexity=18.653223, train_loss=2.926019

Batch 302450, train_perplexity=18.65322, train_loss=2.9260187

Batch 302460, train_perplexity=18.65322, train_loss=2.9260187

Batch 302470, train_perplexity=18.65322, train_loss=2.9260187

Batch 302480, train_perplexity=18.65322, train_loss=2.9260187

Batch 302490, train_perplexity=18.65322, train_loss=2.9260187

Batch 302500, train_perplexity=18.653223, train_loss=2.926019

Batch 302510, train_perplexity=18.65322, train_loss=2.9260187

Batch 302520, train_perplexity=18.65322, train_loss=2.9260187

Batch 302530, train_perplexity=18.65322, train_loss=2.9260187

Batch 302540, train_perplexity=18.65322, train_loss=2.9260187

Batch 302550, train_perplexity=18.65322, train_loss=2.9260187

Batch 302560, train_perplexity=18.65322, train_loss=2.9260187

Batch 302570, train_perplexity=18.65322, train_loss=2.9260187

Batch 302580, train_perplexity=18.65322, train_loss=2.9260187
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 302590, train_perplexity=18.65322, train_loss=2.9260187

Batch 302600, train_perplexity=18.653214, train_loss=2.9260185

Batch 302610, train_perplexity=18.65322, train_loss=2.9260187

Batch 302620, train_perplexity=18.65322, train_loss=2.9260187

Batch 302630, train_perplexity=18.65321, train_loss=2.9260182

Batch 302640, train_perplexity=18.65321, train_loss=2.9260182

Batch 302650, train_perplexity=18.65322, train_loss=2.9260187

Batch 302660, train_perplexity=18.65322, train_loss=2.9260187

Batch 302670, train_perplexity=18.65322, train_loss=2.9260187

Batch 302680, train_perplexity=18.65322, train_loss=2.9260187

Batch 302690, train_perplexity=18.65322, train_loss=2.9260187

Batch 302700, train_perplexity=18.65322, train_loss=2.9260187

Batch 302710, train_perplexity=18.65322, train_loss=2.9260187

Batch 302720, train_perplexity=18.653214, train_loss=2.9260185

Batch 302730, train_perplexity=18.65321, train_loss=2.9260182

Batch 302740, train_perplexity=18.653214, train_loss=2.9260185

Batch 302750, train_perplexity=18.65321, train_loss=2.9260182

Batch 302760, train_perplexity=18.65321, train_loss=2.9260182

Batch 302770, train_perplexity=18.65321, train_loss=2.9260182

Batch 302780, train_perplexity=18.65321, train_loss=2.9260182

Batch 302790, train_perplexity=18.65321, train_loss=2.9260182

Batch 302800, train_perplexity=18.65321, train_loss=2.9260182

Batch 302810, train_perplexity=18.65321, train_loss=2.9260182

Batch 302820, train_perplexity=18.65321, train_loss=2.9260182

Batch 302830, train_perplexity=18.65321, train_loss=2.9260182

Batch 302840, train_perplexity=18.65321, train_loss=2.9260182

Batch 302850, train_perplexity=18.65321, train_loss=2.9260182

Batch 302860, train_perplexity=18.65321, train_loss=2.9260182

Batch 302870, train_perplexity=18.65321, train_loss=2.9260182

Batch 302880, train_perplexity=18.65321, train_loss=2.9260182

Batch 302890, train_perplexity=18.65321, train_loss=2.9260182

Batch 302900, train_perplexity=18.65321, train_loss=2.9260182

Batch 302910, train_perplexity=18.65321, train_loss=2.9260182

Batch 302920, train_perplexity=18.65321, train_loss=2.9260182

Batch 302930, train_perplexity=18.653206, train_loss=2.926018

Batch 302940, train_perplexity=18.65321, train_loss=2.9260182

Batch 302950, train_perplexity=18.653206, train_loss=2.926018

Batch 302960, train_perplexity=18.65321, train_loss=2.9260182

Batch 302970, train_perplexity=18.65321, train_loss=2.9260182

Batch 302980, train_perplexity=18.65321, train_loss=2.9260182

Batch 302990, train_perplexity=18.6532, train_loss=2.9260178

Batch 303000, train_perplexity=18.65321, train_loss=2.9260182

Batch 303010, train_perplexity=18.653206, train_loss=2.926018

Batch 303020, train_perplexity=18.6532, train_loss=2.9260178

Batch 303030, train_perplexity=18.6532, train_loss=2.9260178

Batch 303040, train_perplexity=18.6532, train_loss=2.9260178

Batch 303050, train_perplexity=18.6532, train_loss=2.9260178

Batch 303060, train_perplexity=18.6532, train_loss=2.9260178

Batch 303070, train_perplexity=18.6532, train_loss=2.9260178

Batch 303080, train_perplexity=18.6532, train_loss=2.9260178

Batch 303090, train_perplexity=18.6532, train_loss=2.9260178

Batch 303100, train_perplexity=18.6532, train_loss=2.9260178

Batch 303110, train_perplexity=18.6532, train_loss=2.9260178

Batch 303120, train_perplexity=18.6532, train_loss=2.9260178

Batch 303130, train_perplexity=18.6532, train_loss=2.9260178

Batch 303140, train_perplexity=18.6532, train_loss=2.9260178

Batch 303150, train_perplexity=18.6532, train_loss=2.9260178

Batch 303160, train_perplexity=18.6532, train_loss=2.9260178

Batch 303170, train_perplexity=18.6532, train_loss=2.9260178

Batch 303180, train_perplexity=18.6532, train_loss=2.9260178

Batch 303190, train_perplexity=18.6532, train_loss=2.9260178

Batch 303200, train_perplexity=18.6532, train_loss=2.9260178

Batch 303210, train_perplexity=18.6532, train_loss=2.9260178

Batch 303220, train_perplexity=18.6532, train_loss=2.9260178

Batch 303230, train_perplexity=18.6532, train_loss=2.9260178

Batch 303240, train_perplexity=18.653196, train_loss=2.9260175

Batch 303250, train_perplexity=18.653196, train_loss=2.9260175

Batch 303260, train_perplexity=18.6532, train_loss=2.9260178

Batch 303270, train_perplexity=18.653196, train_loss=2.9260175

Batch 303280, train_perplexity=18.6532, train_loss=2.9260178

Batch 303290, train_perplexity=18.653193, train_loss=2.9260173

Batch 303300, train_perplexity=18.653196, train_loss=2.9260175

Batch 303310, train_perplexity=18.653196, train_loss=2.9260175

Batch 303320, train_perplexity=18.653196, train_loss=2.9260175

Batch 303330, train_perplexity=18.653196, train_loss=2.9260175

Batch 303340, train_perplexity=18.653193, train_loss=2.9260173

Batch 303350, train_perplexity=18.653193, train_loss=2.9260173

Batch 303360, train_perplexity=18.653196, train_loss=2.9260175

Batch 303370, train_perplexity=18.653193, train_loss=2.9260173

Batch 303380, train_perplexity=18.653193, train_loss=2.9260173

Batch 303390, train_perplexity=18.653193, train_loss=2.9260173

Batch 303400, train_perplexity=18.653193, train_loss=2.9260173

Batch 303410, train_perplexity=18.653193, train_loss=2.9260173

Batch 303420, train_perplexity=18.653193, train_loss=2.9260173

Batch 303430, train_perplexity=18.653193, train_loss=2.9260173

Batch 303440, train_perplexity=18.653193, train_loss=2.9260173

Batch 303450, train_perplexity=18.653193, train_loss=2.9260173

Batch 303460, train_perplexity=18.653193, train_loss=2.9260173

Batch 303470, train_perplexity=18.653196, train_loss=2.9260175

Batch 303480, train_perplexity=18.653193, train_loss=2.9260173

Batch 303490, train_perplexity=18.653193, train_loss=2.9260173

Batch 303500, train_perplexity=18.653193, train_loss=2.9260173

Batch 303510, train_perplexity=18.653193, train_loss=2.9260173

Batch 303520, train_perplexity=18.653193, train_loss=2.9260173

Batch 303530, train_perplexity=18.653193, train_loss=2.9260173

Batch 303540, train_perplexity=18.653193, train_loss=2.9260173

Batch 303550, train_perplexity=18.653193, train_loss=2.9260173

Batch 303560, train_perplexity=18.653193, train_loss=2.9260173

Batch 303570, train_perplexity=18.653193, train_loss=2.9260173

Batch 303580, train_perplexity=18.653193, train_loss=2.9260173

Batch 303590, train_perplexity=18.653187, train_loss=2.926017

Batch 303600, train_perplexity=18.653187, train_loss=2.926017

Batch 303610, train_perplexity=18.653187, train_loss=2.926017

Batch 303620, train_perplexity=18.653193, train_loss=2.9260173

Batch 303630, train_perplexity=18.653193, train_loss=2.9260173

Batch 303640, train_perplexity=18.653187, train_loss=2.926017

Batch 303650, train_perplexity=18.653187, train_loss=2.926017

Batch 303660, train_perplexity=18.653193, train_loss=2.9260173

Batch 303670, train_perplexity=18.653193, train_loss=2.9260173

Batch 303680, train_perplexity=18.653183, train_loss=2.9260168

Batch 303690, train_perplexity=18.653187, train_loss=2.926017

Batch 303700, train_perplexity=18.653187, train_loss=2.926017

Batch 303710, train_perplexity=18.653183, train_loss=2.9260168

Batch 303720, train_perplexity=18.653187, train_loss=2.926017

Batch 303730, train_perplexity=18.653183, train_loss=2.9260168

Batch 303740, train_perplexity=18.653187, train_loss=2.926017

Batch 303750, train_perplexity=18.653187, train_loss=2.926017

Batch 303760, train_perplexity=18.653183, train_loss=2.9260168

Batch 303770, train_perplexity=18.653183, train_loss=2.9260168

Batch 303780, train_perplexity=18.653183, train_loss=2.9260168

Batch 303790, train_perplexity=18.653183, train_loss=2.9260168

Batch 303800, train_perplexity=18.65318, train_loss=2.9260166

Batch 303810, train_perplexity=18.653183, train_loss=2.9260168

Batch 303820, train_perplexity=18.653183, train_loss=2.9260168

Batch 303830, train_perplexity=18.653183, train_loss=2.9260168

Batch 303840, train_perplexity=18.653183, train_loss=2.9260168

Batch 303850, train_perplexity=18.653183, train_loss=2.9260168

Batch 303860, train_perplexity=18.65318, train_loss=2.9260166

Batch 303870, train_perplexity=18.653183, train_loss=2.9260168

Batch 303880, train_perplexity=18.65318, train_loss=2.9260166
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 303890, train_perplexity=18.653183, train_loss=2.9260168

Batch 303900, train_perplexity=18.653183, train_loss=2.9260168

Batch 303910, train_perplexity=18.653183, train_loss=2.9260168

Batch 303920, train_perplexity=18.653173, train_loss=2.9260163

Batch 303930, train_perplexity=18.65318, train_loss=2.9260166

Batch 303940, train_perplexity=18.65318, train_loss=2.9260166

Batch 303950, train_perplexity=18.65318, train_loss=2.9260166

Batch 303960, train_perplexity=18.653173, train_loss=2.9260163

Batch 303970, train_perplexity=18.65318, train_loss=2.9260166

Batch 303980, train_perplexity=18.65318, train_loss=2.9260166

Batch 303990, train_perplexity=18.653173, train_loss=2.9260163

Batch 304000, train_perplexity=18.653173, train_loss=2.9260163

Batch 304010, train_perplexity=18.65318, train_loss=2.9260166

Batch 304020, train_perplexity=18.653173, train_loss=2.9260163

Batch 304030, train_perplexity=18.653173, train_loss=2.9260163

Batch 304040, train_perplexity=18.653173, train_loss=2.9260163

Batch 304050, train_perplexity=18.65318, train_loss=2.9260166

Batch 304060, train_perplexity=18.653173, train_loss=2.9260163

Batch 304070, train_perplexity=18.653173, train_loss=2.9260163

Batch 304080, train_perplexity=18.653173, train_loss=2.9260163

Batch 304090, train_perplexity=18.653173, train_loss=2.9260163

Batch 304100, train_perplexity=18.653173, train_loss=2.9260163

Batch 304110, train_perplexity=18.653173, train_loss=2.9260163

Batch 304120, train_perplexity=18.65318, train_loss=2.9260166

Batch 304130, train_perplexity=18.653173, train_loss=2.9260163

Batch 304140, train_perplexity=18.653173, train_loss=2.9260163

Batch 304150, train_perplexity=18.653173, train_loss=2.9260163

Batch 304160, train_perplexity=18.65317, train_loss=2.926016

Batch 304170, train_perplexity=18.653173, train_loss=2.9260163

Batch 304180, train_perplexity=18.653173, train_loss=2.9260163

Batch 304190, train_perplexity=18.653173, train_loss=2.9260163

Batch 304200, train_perplexity=18.653173, train_loss=2.9260163

Batch 304210, train_perplexity=18.653173, train_loss=2.9260163

Batch 304220, train_perplexity=18.653173, train_loss=2.9260163

Batch 304230, train_perplexity=18.653173, train_loss=2.9260163

Batch 304240, train_perplexity=18.65317, train_loss=2.926016

Batch 304250, train_perplexity=18.653166, train_loss=2.9260159

Batch 304260, train_perplexity=18.653166, train_loss=2.9260159

Batch 304270, train_perplexity=18.653166, train_loss=2.9260159

Batch 304280, train_perplexity=18.653166, train_loss=2.9260159

Batch 304290, train_perplexity=18.653166, train_loss=2.9260159

Batch 304300, train_perplexity=18.65317, train_loss=2.926016

Batch 304310, train_perplexity=18.653173, train_loss=2.9260163

Batch 304320, train_perplexity=18.653166, train_loss=2.9260159

Batch 304330, train_perplexity=18.65317, train_loss=2.926016

Batch 304340, train_perplexity=18.653166, train_loss=2.9260159

Batch 304350, train_perplexity=18.653166, train_loss=2.9260159

Batch 304360, train_perplexity=18.653166, train_loss=2.9260159

Batch 304370, train_perplexity=18.653166, train_loss=2.9260159

Batch 304380, train_perplexity=18.653166, train_loss=2.9260159

Batch 304390, train_perplexity=18.653166, train_loss=2.9260159

Batch 304400, train_perplexity=18.653166, train_loss=2.9260159

Batch 304410, train_perplexity=18.653166, train_loss=2.9260159

Batch 304420, train_perplexity=18.653166, train_loss=2.9260159

Batch 304430, train_perplexity=18.653166, train_loss=2.9260159

Batch 304440, train_perplexity=18.653166, train_loss=2.9260159

Batch 304450, train_perplexity=18.653166, train_loss=2.9260159

Batch 304460, train_perplexity=18.653166, train_loss=2.9260159

Batch 304470, train_perplexity=18.653166, train_loss=2.9260159

Batch 304480, train_perplexity=18.653166, train_loss=2.9260159

Batch 304490, train_perplexity=18.653166, train_loss=2.9260159

Batch 304500, train_perplexity=18.653166, train_loss=2.9260159

Batch 304510, train_perplexity=18.653166, train_loss=2.9260159

Batch 304520, train_perplexity=18.653166, train_loss=2.9260159

Batch 304530, train_perplexity=18.653166, train_loss=2.9260159

Batch 304540, train_perplexity=18.653156, train_loss=2.9260154

Batch 304550, train_perplexity=18.653166, train_loss=2.9260159

Batch 304560, train_perplexity=18.65316, train_loss=2.9260156

Batch 304570, train_perplexity=18.653166, train_loss=2.9260159

Batch 304580, train_perplexity=18.653166, train_loss=2.9260159

Batch 304590, train_perplexity=18.65316, train_loss=2.9260156

Batch 304600, train_perplexity=18.65316, train_loss=2.9260156

Batch 304610, train_perplexity=18.65316, train_loss=2.9260156

Batch 304620, train_perplexity=18.65316, train_loss=2.9260156

Batch 304630, train_perplexity=18.65316, train_loss=2.9260156

Batch 304640, train_perplexity=18.65316, train_loss=2.9260156

Batch 304650, train_perplexity=18.653156, train_loss=2.9260154

Batch 304660, train_perplexity=18.653156, train_loss=2.9260154

Batch 304670, train_perplexity=18.653156, train_loss=2.9260154

Batch 304680, train_perplexity=18.653156, train_loss=2.9260154

Batch 304690, train_perplexity=18.653156, train_loss=2.9260154

Batch 304700, train_perplexity=18.65316, train_loss=2.9260156

Batch 304710, train_perplexity=18.653156, train_loss=2.9260154

Batch 304720, train_perplexity=18.653156, train_loss=2.9260154

Batch 304730, train_perplexity=18.653156, train_loss=2.9260154

Batch 304740, train_perplexity=18.653156, train_loss=2.9260154

Batch 304750, train_perplexity=18.653156, train_loss=2.9260154

Batch 304760, train_perplexity=18.653156, train_loss=2.9260154

Batch 304770, train_perplexity=18.653156, train_loss=2.9260154

Batch 304780, train_perplexity=18.653156, train_loss=2.9260154

Batch 304790, train_perplexity=18.653156, train_loss=2.9260154

Batch 304800, train_perplexity=18.653156, train_loss=2.9260154

Batch 304810, train_perplexity=18.653152, train_loss=2.9260151

Batch 304820, train_perplexity=18.653147, train_loss=2.926015

Batch 304830, train_perplexity=18.653156, train_loss=2.9260154

Batch 304840, train_perplexity=18.653156, train_loss=2.9260154

Batch 304850, train_perplexity=18.653156, train_loss=2.9260154

Batch 304860, train_perplexity=18.653156, train_loss=2.9260154

Batch 304870, train_perplexity=18.653156, train_loss=2.9260154

Batch 304880, train_perplexity=18.653147, train_loss=2.926015

Batch 304890, train_perplexity=18.653147, train_loss=2.926015

Batch 304900, train_perplexity=18.653156, train_loss=2.9260154

Batch 304910, train_perplexity=18.653152, train_loss=2.9260151

Batch 304920, train_perplexity=18.653147, train_loss=2.926015

Batch 304930, train_perplexity=18.653152, train_loss=2.9260151

Batch 304940, train_perplexity=18.653147, train_loss=2.926015

Batch 304950, train_perplexity=18.653152, train_loss=2.9260151

Batch 304960, train_perplexity=18.653152, train_loss=2.9260151

Batch 304970, train_perplexity=18.653147, train_loss=2.926015

Batch 304980, train_perplexity=18.653147, train_loss=2.926015

Batch 304990, train_perplexity=18.653147, train_loss=2.926015

Batch 305000, train_perplexity=18.653147, train_loss=2.926015

Batch 305010, train_perplexity=18.653147, train_loss=2.926015

Batch 305020, train_perplexity=18.653147, train_loss=2.926015

Batch 305030, train_perplexity=18.653147, train_loss=2.926015

Batch 305040, train_perplexity=18.653147, train_loss=2.926015

Batch 305050, train_perplexity=18.653147, train_loss=2.926015

Batch 305060, train_perplexity=18.653147, train_loss=2.926015

Batch 305070, train_perplexity=18.653147, train_loss=2.926015

Batch 305080, train_perplexity=18.653147, train_loss=2.926015

Batch 305090, train_perplexity=18.653147, train_loss=2.926015

Batch 305100, train_perplexity=18.653147, train_loss=2.926015

Batch 305110, train_perplexity=18.653147, train_loss=2.926015

Batch 305120, train_perplexity=18.653147, train_loss=2.926015

Batch 305130, train_perplexity=18.65314, train_loss=2.9260144

Batch 305140, train_perplexity=18.653147, train_loss=2.926015

Batch 305150, train_perplexity=18.653147, train_loss=2.926015

Batch 305160, train_perplexity=18.653147, train_loss=2.926015

Batch 305170, train_perplexity=18.653143, train_loss=2.9260147
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 305180, train_perplexity=18.653147, train_loss=2.926015

Batch 305190, train_perplexity=18.653147, train_loss=2.926015

Batch 305200, train_perplexity=18.65314, train_loss=2.9260144

Batch 305210, train_perplexity=18.653147, train_loss=2.926015

Batch 305220, train_perplexity=18.65314, train_loss=2.9260144

Batch 305230, train_perplexity=18.653143, train_loss=2.9260147

Batch 305240, train_perplexity=18.653143, train_loss=2.9260147

Batch 305250, train_perplexity=18.65314, train_loss=2.9260144

Batch 305260, train_perplexity=18.653143, train_loss=2.9260147

Batch 305270, train_perplexity=18.653147, train_loss=2.926015

Batch 305280, train_perplexity=18.653143, train_loss=2.9260147

Batch 305290, train_perplexity=18.653143, train_loss=2.9260147

Batch 305300, train_perplexity=18.65314, train_loss=2.9260144

Batch 305310, train_perplexity=18.653147, train_loss=2.926015

Batch 305320, train_perplexity=18.65314, train_loss=2.9260144

Batch 305330, train_perplexity=18.65314, train_loss=2.9260144

Batch 305340, train_perplexity=18.65314, train_loss=2.9260144

Batch 305350, train_perplexity=18.65314, train_loss=2.9260144

Batch 305360, train_perplexity=18.65314, train_loss=2.9260144

Batch 305370, train_perplexity=18.65314, train_loss=2.9260144

Batch 305380, train_perplexity=18.65314, train_loss=2.9260144

Batch 305390, train_perplexity=18.65314, train_loss=2.9260144

Batch 305400, train_perplexity=18.65314, train_loss=2.9260144

Batch 305410, train_perplexity=18.65314, train_loss=2.9260144

Batch 305420, train_perplexity=18.65314, train_loss=2.9260144

Batch 305430, train_perplexity=18.65314, train_loss=2.9260144

Batch 305440, train_perplexity=18.65314, train_loss=2.9260144

Batch 305450, train_perplexity=18.65314, train_loss=2.9260144

Batch 305460, train_perplexity=18.65314, train_loss=2.9260144

Batch 305470, train_perplexity=18.653133, train_loss=2.9260142

Batch 305480, train_perplexity=18.653133, train_loss=2.9260142

Batch 305490, train_perplexity=18.65314, train_loss=2.9260144

Batch 305500, train_perplexity=18.65314, train_loss=2.9260144

Batch 305510, train_perplexity=18.65314, train_loss=2.9260144

Batch 305520, train_perplexity=18.65314, train_loss=2.9260144

Batch 305530, train_perplexity=18.65313, train_loss=2.926014

Batch 305540, train_perplexity=18.65313, train_loss=2.926014

Batch 305550, train_perplexity=18.65313, train_loss=2.926014

Batch 305560, train_perplexity=18.65313, train_loss=2.926014

Batch 305570, train_perplexity=18.653133, train_loss=2.9260142

Batch 305580, train_perplexity=18.65313, train_loss=2.926014

Batch 305590, train_perplexity=18.653133, train_loss=2.9260142

Batch 305600, train_perplexity=18.65314, train_loss=2.9260144

Batch 305610, train_perplexity=18.65313, train_loss=2.926014

Batch 305620, train_perplexity=18.65313, train_loss=2.926014

Batch 305630, train_perplexity=18.65313, train_loss=2.926014

Batch 305640, train_perplexity=18.65313, train_loss=2.926014

Batch 305650, train_perplexity=18.65313, train_loss=2.926014

Batch 305660, train_perplexity=18.65313, train_loss=2.926014

Batch 305670, train_perplexity=18.65313, train_loss=2.926014

Batch 305680, train_perplexity=18.65313, train_loss=2.926014

Batch 305690, train_perplexity=18.65313, train_loss=2.926014

Batch 305700, train_perplexity=18.65313, train_loss=2.926014

Batch 305710, train_perplexity=18.65313, train_loss=2.926014

Batch 305720, train_perplexity=18.65313, train_loss=2.926014

Batch 305730, train_perplexity=18.653126, train_loss=2.9260137

Batch 305740, train_perplexity=18.65313, train_loss=2.926014

Batch 305750, train_perplexity=18.65313, train_loss=2.926014

Batch 305760, train_perplexity=18.65313, train_loss=2.926014

Batch 305770, train_perplexity=18.653126, train_loss=2.9260137

Batch 305780, train_perplexity=18.65313, train_loss=2.926014

Batch 305790, train_perplexity=18.653126, train_loss=2.9260137

Batch 305800, train_perplexity=18.65313, train_loss=2.926014

Batch 305810, train_perplexity=18.65313, train_loss=2.926014

Batch 305820, train_perplexity=18.653126, train_loss=2.9260137

Batch 305830, train_perplexity=18.65313, train_loss=2.926014

Batch 305840, train_perplexity=18.653126, train_loss=2.9260137

Batch 305850, train_perplexity=18.65312, train_loss=2.9260135

Batch 305860, train_perplexity=18.65313, train_loss=2.926014

Batch 305870, train_perplexity=18.65312, train_loss=2.9260135

Batch 305880, train_perplexity=18.65312, train_loss=2.9260135

Batch 305890, train_perplexity=18.653126, train_loss=2.9260137

Batch 305900, train_perplexity=18.653126, train_loss=2.9260137

Batch 305910, train_perplexity=18.65312, train_loss=2.9260135

Batch 305920, train_perplexity=18.65312, train_loss=2.9260135

Batch 305930, train_perplexity=18.65312, train_loss=2.9260135

Batch 305940, train_perplexity=18.653126, train_loss=2.9260137

Batch 305950, train_perplexity=18.65312, train_loss=2.9260135

Batch 305960, train_perplexity=18.653126, train_loss=2.9260137

Batch 305970, train_perplexity=18.65312, train_loss=2.9260135

Batch 305980, train_perplexity=18.65312, train_loss=2.9260135

Batch 305990, train_perplexity=18.65312, train_loss=2.9260135

Batch 306000, train_perplexity=18.65312, train_loss=2.9260135

Batch 306010, train_perplexity=18.65312, train_loss=2.9260135

Batch 306020, train_perplexity=18.65312, train_loss=2.9260135

Batch 306030, train_perplexity=18.65312, train_loss=2.9260135

Batch 306040, train_perplexity=18.65312, train_loss=2.9260135

Batch 306050, train_perplexity=18.65312, train_loss=2.9260135

Batch 306060, train_perplexity=18.65312, train_loss=2.9260135

Batch 306070, train_perplexity=18.65312, train_loss=2.9260135

Batch 306080, train_perplexity=18.653112, train_loss=2.926013

Batch 306090, train_perplexity=18.65312, train_loss=2.9260135

Batch 306100, train_perplexity=18.65312, train_loss=2.9260135

Batch 306110, train_perplexity=18.653116, train_loss=2.9260132

Batch 306120, train_perplexity=18.65312, train_loss=2.9260135

Batch 306130, train_perplexity=18.65312, train_loss=2.9260135

Batch 306140, train_perplexity=18.653116, train_loss=2.9260132

Batch 306150, train_perplexity=18.65312, train_loss=2.9260135

Batch 306160, train_perplexity=18.65312, train_loss=2.9260135

Batch 306170, train_perplexity=18.65312, train_loss=2.9260135

Batch 306180, train_perplexity=18.653112, train_loss=2.926013

Batch 306190, train_perplexity=18.65312, train_loss=2.9260135

Batch 306200, train_perplexity=18.653112, train_loss=2.926013

Batch 306210, train_perplexity=18.653112, train_loss=2.926013

Batch 306220, train_perplexity=18.653112, train_loss=2.926013

Batch 306230, train_perplexity=18.653112, train_loss=2.926013

Batch 306240, train_perplexity=18.653112, train_loss=2.926013

Batch 306250, train_perplexity=18.653112, train_loss=2.926013

Batch 306260, train_perplexity=18.653112, train_loss=2.926013

Batch 306270, train_perplexity=18.653112, train_loss=2.926013

Batch 306280, train_perplexity=18.653112, train_loss=2.926013

Batch 306290, train_perplexity=18.653112, train_loss=2.926013

Batch 306300, train_perplexity=18.653112, train_loss=2.926013

Batch 306310, train_perplexity=18.653112, train_loss=2.926013

Batch 306320, train_perplexity=18.653112, train_loss=2.926013

Batch 306330, train_perplexity=18.653112, train_loss=2.926013

Batch 306340, train_perplexity=18.653112, train_loss=2.926013

Batch 306350, train_perplexity=18.653112, train_loss=2.926013

Batch 306360, train_perplexity=18.653103, train_loss=2.9260125

Batch 306370, train_perplexity=18.653112, train_loss=2.926013

Batch 306380, train_perplexity=18.653112, train_loss=2.926013

Batch 306390, train_perplexity=18.653112, train_loss=2.926013

Batch 306400, train_perplexity=18.653112, train_loss=2.926013

Batch 306410, train_perplexity=18.653112, train_loss=2.926013

Batch 306420, train_perplexity=18.653103, train_loss=2.9260125

Batch 306430, train_perplexity=18.653103, train_loss=2.9260125

Batch 306440, train_perplexity=18.653112, train_loss=2.926013

Batch 306450, train_perplexity=18.653112, train_loss=2.926013

Batch 306460, train_perplexity=18.653112, train_loss=2.926013

Batch 306470, train_perplexity=18.653103, train_loss=2.9260125

Batch 306480, train_perplexity=18.653107, train_loss=2.9260128
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 306490, train_perplexity=18.653107, train_loss=2.9260128

Batch 306500, train_perplexity=18.653107, train_loss=2.9260128

Batch 306510, train_perplexity=18.653103, train_loss=2.9260125

Batch 306520, train_perplexity=18.653103, train_loss=2.9260125

Batch 306530, train_perplexity=18.653107, train_loss=2.9260128

Batch 306540, train_perplexity=18.653103, train_loss=2.9260125

Batch 306550, train_perplexity=18.653103, train_loss=2.9260125

Batch 306560, train_perplexity=18.653103, train_loss=2.9260125

Batch 306570, train_perplexity=18.653107, train_loss=2.9260128

Batch 306580, train_perplexity=18.653103, train_loss=2.9260125

Batch 306590, train_perplexity=18.653103, train_loss=2.9260125

Batch 306600, train_perplexity=18.653103, train_loss=2.9260125

Batch 306610, train_perplexity=18.653103, train_loss=2.9260125

Batch 306620, train_perplexity=18.653103, train_loss=2.9260125

Batch 306630, train_perplexity=18.653103, train_loss=2.9260125

Batch 306640, train_perplexity=18.653103, train_loss=2.9260125

Batch 306650, train_perplexity=18.653103, train_loss=2.9260125

Batch 306660, train_perplexity=18.653103, train_loss=2.9260125

Batch 306670, train_perplexity=18.653103, train_loss=2.9260125

Batch 306680, train_perplexity=18.653103, train_loss=2.9260125

Batch 306690, train_perplexity=18.6531, train_loss=2.9260123

Batch 306700, train_perplexity=18.653103, train_loss=2.9260125

Batch 306710, train_perplexity=18.653103, train_loss=2.9260125

Batch 306720, train_perplexity=18.653103, train_loss=2.9260125

Batch 306730, train_perplexity=18.653103, train_loss=2.9260125

Batch 306740, train_perplexity=18.653103, train_loss=2.9260125

Batch 306750, train_perplexity=18.653103, train_loss=2.9260125

Batch 306760, train_perplexity=18.653103, train_loss=2.9260125

Batch 306770, train_perplexity=18.653103, train_loss=2.9260125

Batch 306780, train_perplexity=18.653093, train_loss=2.926012

Batch 306790, train_perplexity=18.653093, train_loss=2.926012

Batch 306800, train_perplexity=18.653093, train_loss=2.926012

Batch 306810, train_perplexity=18.6531, train_loss=2.9260123

Batch 306820, train_perplexity=18.653093, train_loss=2.926012

Batch 306830, train_perplexity=18.653093, train_loss=2.926012

Batch 306840, train_perplexity=18.6531, train_loss=2.9260123

Batch 306850, train_perplexity=18.653093, train_loss=2.926012

Batch 306860, train_perplexity=18.653093, train_loss=2.926012

Batch 306870, train_perplexity=18.653093, train_loss=2.926012

Batch 306880, train_perplexity=18.653093, train_loss=2.926012

Batch 306890, train_perplexity=18.653093, train_loss=2.926012

Batch 306900, train_perplexity=18.653093, train_loss=2.926012

Batch 306910, train_perplexity=18.653103, train_loss=2.9260125

Batch 306920, train_perplexity=18.653093, train_loss=2.926012

Batch 306930, train_perplexity=18.653093, train_loss=2.926012

Batch 306940, train_perplexity=18.653093, train_loss=2.926012

Batch 306950, train_perplexity=18.653093, train_loss=2.926012

Batch 306960, train_perplexity=18.653093, train_loss=2.926012

Batch 306970, train_perplexity=18.653093, train_loss=2.926012

Batch 306980, train_perplexity=18.65309, train_loss=2.9260118

Batch 306990, train_perplexity=18.653093, train_loss=2.926012

Batch 307000, train_perplexity=18.65309, train_loss=2.9260118

Batch 307010, train_perplexity=18.653093, train_loss=2.926012

Batch 307020, train_perplexity=18.653093, train_loss=2.926012

Batch 307030, train_perplexity=18.653093, train_loss=2.926012

Batch 307040, train_perplexity=18.65309, train_loss=2.9260118

Batch 307050, train_perplexity=18.653093, train_loss=2.926012

Batch 307060, train_perplexity=18.653086, train_loss=2.9260116

Batch 307070, train_perplexity=18.653093, train_loss=2.926012

Batch 307080, train_perplexity=18.65309, train_loss=2.9260118

Batch 307090, train_perplexity=18.65309, train_loss=2.9260118

Batch 307100, train_perplexity=18.65309, train_loss=2.9260118

Batch 307110, train_perplexity=18.653086, train_loss=2.9260116

Batch 307120, train_perplexity=18.653086, train_loss=2.9260116

Batch 307130, train_perplexity=18.653086, train_loss=2.9260116

Batch 307140, train_perplexity=18.65309, train_loss=2.9260118

Batch 307150, train_perplexity=18.65309, train_loss=2.9260118

Batch 307160, train_perplexity=18.65309, train_loss=2.9260118

Batch 307170, train_perplexity=18.653086, train_loss=2.9260116

Batch 307180, train_perplexity=18.653086, train_loss=2.9260116

Batch 307190, train_perplexity=18.653086, train_loss=2.9260116

Batch 307200, train_perplexity=18.653086, train_loss=2.9260116

Batch 307210, train_perplexity=18.65309, train_loss=2.9260118

Batch 307220, train_perplexity=18.653093, train_loss=2.926012

Batch 307230, train_perplexity=18.65309, train_loss=2.9260118

Batch 307240, train_perplexity=18.653086, train_loss=2.9260116

Batch 307250, train_perplexity=18.653086, train_loss=2.9260116

Batch 307260, train_perplexity=18.653086, train_loss=2.9260116

Batch 307270, train_perplexity=18.653086, train_loss=2.9260116

Batch 307280, train_perplexity=18.65308, train_loss=2.9260113

Batch 307290, train_perplexity=18.653086, train_loss=2.9260116

Batch 307300, train_perplexity=18.653086, train_loss=2.9260116

Batch 307310, train_perplexity=18.653086, train_loss=2.9260116

Batch 307320, train_perplexity=18.653086, train_loss=2.9260116

Batch 307330, train_perplexity=18.653086, train_loss=2.9260116

Batch 307340, train_perplexity=18.653086, train_loss=2.9260116

Batch 307350, train_perplexity=18.653086, train_loss=2.9260116

Batch 307360, train_perplexity=18.65308, train_loss=2.9260113

Batch 307370, train_perplexity=18.653086, train_loss=2.9260116

Batch 307380, train_perplexity=18.653086, train_loss=2.9260116

Batch 307390, train_perplexity=18.653086, train_loss=2.9260116

Batch 307400, train_perplexity=18.653086, train_loss=2.9260116

Batch 307410, train_perplexity=18.65308, train_loss=2.9260113

Batch 307420, train_perplexity=18.65308, train_loss=2.9260113

Batch 307430, train_perplexity=18.65308, train_loss=2.9260113

Batch 307440, train_perplexity=18.65308, train_loss=2.9260113

Batch 307450, train_perplexity=18.653076, train_loss=2.926011

Batch 307460, train_perplexity=18.65308, train_loss=2.9260113

Batch 307470, train_perplexity=18.653086, train_loss=2.9260116

Batch 307480, train_perplexity=18.653076, train_loss=2.926011

Batch 307490, train_perplexity=18.653076, train_loss=2.926011

Batch 307500, train_perplexity=18.653076, train_loss=2.926011

Batch 307510, train_perplexity=18.65308, train_loss=2.9260113

Batch 307520, train_perplexity=18.65308, train_loss=2.9260113

Batch 307530, train_perplexity=18.653076, train_loss=2.926011

Batch 307540, train_perplexity=18.653076, train_loss=2.926011

Batch 307550, train_perplexity=18.653076, train_loss=2.926011

Batch 307560, train_perplexity=18.653076, train_loss=2.926011

Batch 307570, train_perplexity=18.653076, train_loss=2.926011

Batch 307580, train_perplexity=18.653076, train_loss=2.926011

Batch 307590, train_perplexity=18.653076, train_loss=2.926011

Batch 307600, train_perplexity=18.653076, train_loss=2.926011

Batch 307610, train_perplexity=18.653076, train_loss=2.926011

Batch 307620, train_perplexity=18.653072, train_loss=2.9260108

Batch 307630, train_perplexity=18.653076, train_loss=2.926011

Batch 307640, train_perplexity=18.653076, train_loss=2.926011

Batch 307650, train_perplexity=18.653076, train_loss=2.926011

Batch 307660, train_perplexity=18.653076, train_loss=2.926011

Batch 307670, train_perplexity=18.653076, train_loss=2.926011

Batch 307680, train_perplexity=18.653076, train_loss=2.926011

Batch 307690, train_perplexity=18.653076, train_loss=2.926011

Batch 307700, train_perplexity=18.653072, train_loss=2.9260108

Batch 307710, train_perplexity=18.653076, train_loss=2.926011

Batch 307720, train_perplexity=18.653072, train_loss=2.9260108

Batch 307730, train_perplexity=18.653072, train_loss=2.9260108

Batch 307740, train_perplexity=18.653072, train_loss=2.9260108

Batch 307750, train_perplexity=18.653067, train_loss=2.9260106

Batch 307760, train_perplexity=18.653067, train_loss=2.9260106

Batch 307770, train_perplexity=18.653067, train_loss=2.9260106

Batch 307780, train_perplexity=18.653072, train_loss=2.9260108
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 307790, train_perplexity=18.653067, train_loss=2.9260106

Batch 307800, train_perplexity=18.653072, train_loss=2.9260108

Batch 307810, train_perplexity=18.653067, train_loss=2.9260106

Batch 307820, train_perplexity=18.653067, train_loss=2.9260106

Batch 307830, train_perplexity=18.653067, train_loss=2.9260106

Batch 307840, train_perplexity=18.653067, train_loss=2.9260106

Batch 307850, train_perplexity=18.653067, train_loss=2.9260106

Batch 307860, train_perplexity=18.653067, train_loss=2.9260106

Batch 307870, train_perplexity=18.653067, train_loss=2.9260106

Batch 307880, train_perplexity=18.653067, train_loss=2.9260106

Batch 307890, train_perplexity=18.653067, train_loss=2.9260106

Batch 307900, train_perplexity=18.653067, train_loss=2.9260106

Batch 307910, train_perplexity=18.653067, train_loss=2.9260106

Batch 307920, train_perplexity=18.653067, train_loss=2.9260106

Batch 307930, train_perplexity=18.653067, train_loss=2.9260106

Batch 307940, train_perplexity=18.653067, train_loss=2.9260106

Batch 307950, train_perplexity=18.653067, train_loss=2.9260106

Batch 307960, train_perplexity=18.653067, train_loss=2.9260106

Batch 307970, train_perplexity=18.653067, train_loss=2.9260106

Batch 307980, train_perplexity=18.653067, train_loss=2.9260106

Batch 307990, train_perplexity=18.653067, train_loss=2.9260106

Batch 308000, train_perplexity=18.653067, train_loss=2.9260106

Batch 308010, train_perplexity=18.653067, train_loss=2.9260106

Batch 308020, train_perplexity=18.653067, train_loss=2.9260106

Batch 308030, train_perplexity=18.653067, train_loss=2.9260106

Batch 308040, train_perplexity=18.653059, train_loss=2.9260101

Batch 308050, train_perplexity=18.653059, train_loss=2.9260101

Batch 308060, train_perplexity=18.653063, train_loss=2.9260104

Batch 308070, train_perplexity=18.653067, train_loss=2.9260106

Batch 308080, train_perplexity=18.653059, train_loss=2.9260101

Batch 308090, train_perplexity=18.653063, train_loss=2.9260104

Batch 308100, train_perplexity=18.653059, train_loss=2.9260101

Batch 308110, train_perplexity=18.653059, train_loss=2.9260101

Batch 308120, train_perplexity=18.653059, train_loss=2.9260101

Batch 308130, train_perplexity=18.653059, train_loss=2.9260101

Batch 308140, train_perplexity=18.653059, train_loss=2.9260101

Batch 308150, train_perplexity=18.653059, train_loss=2.9260101

Batch 308160, train_perplexity=18.653059, train_loss=2.9260101

Batch 308170, train_perplexity=18.653059, train_loss=2.9260101

Batch 308180, train_perplexity=18.653059, train_loss=2.9260101

Batch 308190, train_perplexity=18.653059, train_loss=2.9260101

Batch 308200, train_perplexity=18.653059, train_loss=2.9260101

Batch 308210, train_perplexity=18.653059, train_loss=2.9260101

Batch 308220, train_perplexity=18.653059, train_loss=2.9260101

Batch 308230, train_perplexity=18.653059, train_loss=2.9260101

Batch 308240, train_perplexity=18.653059, train_loss=2.9260101

Batch 308250, train_perplexity=18.653059, train_loss=2.9260101

Batch 308260, train_perplexity=18.653053, train_loss=2.92601

Batch 308270, train_perplexity=18.653053, train_loss=2.92601

Batch 308280, train_perplexity=18.653053, train_loss=2.92601

Batch 308290, train_perplexity=18.653059, train_loss=2.9260101

Batch 308300, train_perplexity=18.653053, train_loss=2.92601

Batch 308310, train_perplexity=18.653059, train_loss=2.9260101

Batch 308320, train_perplexity=18.653053, train_loss=2.92601

Batch 308330, train_perplexity=18.653053, train_loss=2.92601

Batch 308340, train_perplexity=18.653059, train_loss=2.9260101

Batch 308350, train_perplexity=18.65305, train_loss=2.9260097

Batch 308360, train_perplexity=18.653059, train_loss=2.9260101

Batch 308370, train_perplexity=18.653053, train_loss=2.92601

Batch 308380, train_perplexity=18.653053, train_loss=2.92601

Batch 308390, train_perplexity=18.653053, train_loss=2.92601

Batch 308400, train_perplexity=18.653059, train_loss=2.9260101

Batch 308410, train_perplexity=18.65305, train_loss=2.9260097

Batch 308420, train_perplexity=18.65305, train_loss=2.9260097

Batch 308430, train_perplexity=18.65305, train_loss=2.9260097

Batch 308440, train_perplexity=18.653053, train_loss=2.92601

Batch 308450, train_perplexity=18.65305, train_loss=2.9260097

Batch 308460, train_perplexity=18.65305, train_loss=2.9260097

Batch 308470, train_perplexity=18.65305, train_loss=2.9260097

Batch 308480, train_perplexity=18.65305, train_loss=2.9260097

Batch 308490, train_perplexity=18.65305, train_loss=2.9260097

Batch 308500, train_perplexity=18.65305, train_loss=2.9260097

Batch 308510, train_perplexity=18.65305, train_loss=2.9260097

Batch 308520, train_perplexity=18.65305, train_loss=2.9260097

Batch 308530, train_perplexity=18.65305, train_loss=2.9260097

Batch 308540, train_perplexity=18.65305, train_loss=2.9260097

Batch 308550, train_perplexity=18.65305, train_loss=2.9260097

Batch 308560, train_perplexity=18.65305, train_loss=2.9260097

Batch 308570, train_perplexity=18.65305, train_loss=2.9260097

Batch 308580, train_perplexity=18.65305, train_loss=2.9260097

Batch 308590, train_perplexity=18.65305, train_loss=2.9260097

Batch 308600, train_perplexity=18.65305, train_loss=2.9260097

Batch 308610, train_perplexity=18.65305, train_loss=2.9260097

Batch 308620, train_perplexity=18.65305, train_loss=2.9260097

Batch 308630, train_perplexity=18.653046, train_loss=2.9260094

Batch 308640, train_perplexity=18.653046, train_loss=2.9260094

Batch 308650, train_perplexity=18.65305, train_loss=2.9260097

Batch 308660, train_perplexity=18.653046, train_loss=2.9260094

Batch 308670, train_perplexity=18.653046, train_loss=2.9260094

Batch 308680, train_perplexity=18.65304, train_loss=2.9260092

Batch 308690, train_perplexity=18.653046, train_loss=2.9260094

Batch 308700, train_perplexity=18.65304, train_loss=2.9260092

Batch 308710, train_perplexity=18.65304, train_loss=2.9260092

Batch 308720, train_perplexity=18.65305, train_loss=2.9260097

Batch 308730, train_perplexity=18.653046, train_loss=2.9260094

Batch 308740, train_perplexity=18.653046, train_loss=2.9260094

Batch 308750, train_perplexity=18.65304, train_loss=2.9260092

Batch 308760, train_perplexity=18.65304, train_loss=2.9260092

Batch 308770, train_perplexity=18.653046, train_loss=2.9260094

Batch 308780, train_perplexity=18.65304, train_loss=2.9260092

Batch 308790, train_perplexity=18.653046, train_loss=2.9260094

Batch 308800, train_perplexity=18.65304, train_loss=2.9260092

Batch 308810, train_perplexity=18.65304, train_loss=2.9260092

Batch 308820, train_perplexity=18.65304, train_loss=2.9260092

Batch 308830, train_perplexity=18.65304, train_loss=2.9260092

Batch 308840, train_perplexity=18.65304, train_loss=2.9260092

Batch 308850, train_perplexity=18.65304, train_loss=2.9260092

Batch 308860, train_perplexity=18.65304, train_loss=2.9260092

Batch 308870, train_perplexity=18.65304, train_loss=2.9260092

Batch 308880, train_perplexity=18.65304, train_loss=2.9260092

Batch 308890, train_perplexity=18.65304, train_loss=2.9260092

Batch 308900, train_perplexity=18.65304, train_loss=2.9260092

Batch 308910, train_perplexity=18.65304, train_loss=2.9260092

Batch 308920, train_perplexity=18.65304, train_loss=2.9260092

Batch 308930, train_perplexity=18.653032, train_loss=2.9260087

Batch 308940, train_perplexity=18.65304, train_loss=2.9260092

Batch 308950, train_perplexity=18.65304, train_loss=2.9260092

Batch 308960, train_perplexity=18.65304, train_loss=2.9260092

Batch 308970, train_perplexity=18.65304, train_loss=2.9260092

Batch 308980, train_perplexity=18.65304, train_loss=2.9260092

Batch 308990, train_perplexity=18.653036, train_loss=2.926009

Batch 309000, train_perplexity=18.653036, train_loss=2.926009

Batch 309010, train_perplexity=18.65304, train_loss=2.9260092

Batch 309020, train_perplexity=18.653032, train_loss=2.9260087

Batch 309030, train_perplexity=18.653036, train_loss=2.926009

Batch 309040, train_perplexity=18.653036, train_loss=2.926009

Batch 309050, train_perplexity=18.653032, train_loss=2.9260087

Batch 309060, train_perplexity=18.653036, train_loss=2.926009

Batch 309070, train_perplexity=18.653032, train_loss=2.9260087

Batch 309080, train_perplexity=18.653032, train_loss=2.9260087
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 309090, train_perplexity=18.653032, train_loss=2.9260087

Batch 309100, train_perplexity=18.653032, train_loss=2.9260087

Batch 309110, train_perplexity=18.653032, train_loss=2.9260087

Batch 309120, train_perplexity=18.653032, train_loss=2.9260087

Batch 309130, train_perplexity=18.653032, train_loss=2.9260087

Batch 309140, train_perplexity=18.653032, train_loss=2.9260087

Batch 309150, train_perplexity=18.653032, train_loss=2.9260087

Batch 309160, train_perplexity=18.653027, train_loss=2.9260085

Batch 309170, train_perplexity=18.653032, train_loss=2.9260087

Batch 309180, train_perplexity=18.653032, train_loss=2.9260087

Batch 309190, train_perplexity=18.653032, train_loss=2.9260087

Batch 309200, train_perplexity=18.653032, train_loss=2.9260087

Batch 309210, train_perplexity=18.653032, train_loss=2.9260087

Batch 309220, train_perplexity=18.653027, train_loss=2.9260085

Batch 309230, train_perplexity=18.653032, train_loss=2.9260087

Batch 309240, train_perplexity=18.653032, train_loss=2.9260087

Batch 309250, train_perplexity=18.653032, train_loss=2.9260087

Batch 309260, train_perplexity=18.653023, train_loss=2.9260082

Batch 309270, train_perplexity=18.653032, train_loss=2.9260087

Batch 309280, train_perplexity=18.653032, train_loss=2.9260087

Batch 309290, train_perplexity=18.653032, train_loss=2.9260087

Batch 309300, train_perplexity=18.653023, train_loss=2.9260082

Batch 309310, train_perplexity=18.653023, train_loss=2.9260082

Batch 309320, train_perplexity=18.653027, train_loss=2.9260085

Batch 309330, train_perplexity=18.653027, train_loss=2.9260085

Batch 309340, train_perplexity=18.653023, train_loss=2.9260082

Batch 309350, train_perplexity=18.653027, train_loss=2.9260085

Batch 309360, train_perplexity=18.653023, train_loss=2.9260082

Batch 309370, train_perplexity=18.653027, train_loss=2.9260085

Batch 309380, train_perplexity=18.653023, train_loss=2.9260082

Batch 309390, train_perplexity=18.653023, train_loss=2.9260082

Batch 309400, train_perplexity=18.653023, train_loss=2.9260082

Batch 309410, train_perplexity=18.653023, train_loss=2.9260082

Batch 309420, train_perplexity=18.653023, train_loss=2.9260082

Batch 309430, train_perplexity=18.653023, train_loss=2.9260082

Batch 309440, train_perplexity=18.653023, train_loss=2.9260082

Batch 309450, train_perplexity=18.653023, train_loss=2.9260082

Batch 309460, train_perplexity=18.653023, train_loss=2.9260082

Batch 309470, train_perplexity=18.653023, train_loss=2.9260082

Batch 309480, train_perplexity=18.653023, train_loss=2.9260082

Batch 309490, train_perplexity=18.653023, train_loss=2.9260082

Batch 309500, train_perplexity=18.653023, train_loss=2.9260082

Batch 309510, train_perplexity=18.653023, train_loss=2.9260082

Batch 309520, train_perplexity=18.653023, train_loss=2.9260082

Batch 309530, train_perplexity=18.653023, train_loss=2.9260082

Batch 309540, train_perplexity=18.653023, train_loss=2.9260082

Batch 309550, train_perplexity=18.653023, train_loss=2.9260082

Batch 309560, train_perplexity=18.653023, train_loss=2.9260082

Batch 309570, train_perplexity=18.653023, train_loss=2.9260082

Batch 309580, train_perplexity=18.653019, train_loss=2.926008

Batch 309590, train_perplexity=18.653019, train_loss=2.926008

Batch 309600, train_perplexity=18.653023, train_loss=2.9260082

Batch 309610, train_perplexity=18.653013, train_loss=2.9260077

Batch 309620, train_perplexity=18.653013, train_loss=2.9260077

Batch 309630, train_perplexity=18.653013, train_loss=2.9260077

Batch 309640, train_perplexity=18.653023, train_loss=2.9260082

Batch 309650, train_perplexity=18.653019, train_loss=2.926008

Batch 309660, train_perplexity=18.653019, train_loss=2.926008

Batch 309670, train_perplexity=18.653019, train_loss=2.926008

Batch 309680, train_perplexity=18.653013, train_loss=2.9260077

Batch 309690, train_perplexity=18.653019, train_loss=2.926008

Batch 309700, train_perplexity=18.653013, train_loss=2.9260077

Batch 309710, train_perplexity=18.653013, train_loss=2.9260077

Batch 309720, train_perplexity=18.653013, train_loss=2.9260077

Batch 309730, train_perplexity=18.653013, train_loss=2.9260077

Batch 309740, train_perplexity=18.653013, train_loss=2.9260077

Batch 309750, train_perplexity=18.653013, train_loss=2.9260077

Batch 309760, train_perplexity=18.653019, train_loss=2.926008

Batch 309770, train_perplexity=18.653013, train_loss=2.9260077

Batch 309780, train_perplexity=18.653013, train_loss=2.9260077

Batch 309790, train_perplexity=18.653013, train_loss=2.9260077

Batch 309800, train_perplexity=18.653013, train_loss=2.9260077

Batch 309810, train_perplexity=18.65301, train_loss=2.9260075

Batch 309820, train_perplexity=18.653013, train_loss=2.9260077

Batch 309830, train_perplexity=18.65301, train_loss=2.9260075

Batch 309840, train_perplexity=18.653013, train_loss=2.9260077

Batch 309850, train_perplexity=18.653013, train_loss=2.9260077

Batch 309860, train_perplexity=18.65301, train_loss=2.9260075

Batch 309870, train_perplexity=18.653013, train_loss=2.9260077

Batch 309880, train_perplexity=18.653013, train_loss=2.9260077

Batch 309890, train_perplexity=18.653013, train_loss=2.9260077

Batch 309900, train_perplexity=18.653013, train_loss=2.9260077

Batch 309910, train_perplexity=18.653013, train_loss=2.9260077

Batch 309920, train_perplexity=18.653006, train_loss=2.9260073

Batch 309930, train_perplexity=18.653013, train_loss=2.9260077

Batch 309940, train_perplexity=18.653013, train_loss=2.9260077

Batch 309950, train_perplexity=18.653006, train_loss=2.9260073

Batch 309960, train_perplexity=18.653006, train_loss=2.9260073

Batch 309970, train_perplexity=18.653006, train_loss=2.9260073

Batch 309980, train_perplexity=18.653006, train_loss=2.9260073

Batch 309990, train_perplexity=18.653006, train_loss=2.9260073

Batch 310000, train_perplexity=18.653013, train_loss=2.9260077

Batch 310010, train_perplexity=18.653006, train_loss=2.9260073

Batch 310020, train_perplexity=18.653006, train_loss=2.9260073

Batch 310030, train_perplexity=18.653006, train_loss=2.9260073

Batch 310040, train_perplexity=18.65301, train_loss=2.9260075

Batch 310050, train_perplexity=18.653006, train_loss=2.9260073

Batch 310060, train_perplexity=18.653006, train_loss=2.9260073

Batch 310070, train_perplexity=18.653006, train_loss=2.9260073

Batch 310080, train_perplexity=18.653006, train_loss=2.9260073

Batch 310090, train_perplexity=18.653006, train_loss=2.9260073

Batch 310100, train_perplexity=18.653006, train_loss=2.9260073

Batch 310110, train_perplexity=18.653006, train_loss=2.9260073

Batch 310120, train_perplexity=18.653006, train_loss=2.9260073

Batch 310130, train_perplexity=18.653006, train_loss=2.9260073

Batch 310140, train_perplexity=18.653006, train_loss=2.9260073

Batch 310150, train_perplexity=18.653006, train_loss=2.9260073

Batch 310160, train_perplexity=18.653006, train_loss=2.9260073

Batch 310170, train_perplexity=18.653006, train_loss=2.9260073

Batch 310180, train_perplexity=18.653006, train_loss=2.9260073

Batch 310190, train_perplexity=18.652996, train_loss=2.9260068

Batch 310200, train_perplexity=18.653, train_loss=2.926007

Batch 310210, train_perplexity=18.653006, train_loss=2.9260073

Batch 310220, train_perplexity=18.653, train_loss=2.926007

Batch 310230, train_perplexity=18.653, train_loss=2.926007

Batch 310240, train_perplexity=18.653, train_loss=2.926007

Batch 310250, train_perplexity=18.653006, train_loss=2.9260073

Batch 310260, train_perplexity=18.653, train_loss=2.926007

Batch 310270, train_perplexity=18.652996, train_loss=2.9260068

Batch 310280, train_perplexity=18.653, train_loss=2.926007

Batch 310290, train_perplexity=18.653, train_loss=2.926007

Batch 310300, train_perplexity=18.652996, train_loss=2.9260068

Batch 310310, train_perplexity=18.653, train_loss=2.926007

Batch 310320, train_perplexity=18.652996, train_loss=2.9260068

Batch 310330, train_perplexity=18.652996, train_loss=2.9260068

Batch 310340, train_perplexity=18.652996, train_loss=2.9260068

Batch 310350, train_perplexity=18.652996, train_loss=2.9260068

Batch 310360, train_perplexity=18.652996, train_loss=2.9260068

Batch 310370, train_perplexity=18.652996, train_loss=2.9260068
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 310380, train_perplexity=18.652996, train_loss=2.9260068

Batch 310390, train_perplexity=18.652996, train_loss=2.9260068

Batch 310400, train_perplexity=18.652996, train_loss=2.9260068

Batch 310410, train_perplexity=18.652996, train_loss=2.9260068

Batch 310420, train_perplexity=18.652996, train_loss=2.9260068

Batch 310430, train_perplexity=18.652996, train_loss=2.9260068

Batch 310440, train_perplexity=18.652996, train_loss=2.9260068

Batch 310450, train_perplexity=18.652996, train_loss=2.9260068

Batch 310460, train_perplexity=18.652996, train_loss=2.9260068

Batch 310470, train_perplexity=18.652996, train_loss=2.9260068

Batch 310480, train_perplexity=18.652996, train_loss=2.9260068

Batch 310490, train_perplexity=18.652996, train_loss=2.9260068

Batch 310500, train_perplexity=18.652996, train_loss=2.9260068

Batch 310510, train_perplexity=18.652996, train_loss=2.9260068

Batch 310520, train_perplexity=18.652992, train_loss=2.9260066

Batch 310530, train_perplexity=18.652996, train_loss=2.9260068

Batch 310540, train_perplexity=18.652996, train_loss=2.9260068

Batch 310550, train_perplexity=18.652996, train_loss=2.9260068

Batch 310560, train_perplexity=18.652992, train_loss=2.9260066

Batch 310570, train_perplexity=18.652992, train_loss=2.9260066

Batch 310580, train_perplexity=18.652992, train_loss=2.9260066

Batch 310590, train_perplexity=18.652987, train_loss=2.9260063

Batch 310600, train_perplexity=18.652996, train_loss=2.9260068

Batch 310610, train_perplexity=18.652987, train_loss=2.9260063

Batch 310620, train_perplexity=18.652987, train_loss=2.9260063

Batch 310630, train_perplexity=18.652987, train_loss=2.9260063

Batch 310640, train_perplexity=18.652987, train_loss=2.9260063

Batch 310650, train_perplexity=18.652987, train_loss=2.9260063

Batch 310660, train_perplexity=18.652996, train_loss=2.9260068

Batch 310670, train_perplexity=18.652987, train_loss=2.9260063

Batch 310680, train_perplexity=18.652987, train_loss=2.9260063

Batch 310690, train_perplexity=18.652987, train_loss=2.9260063

Batch 310700, train_perplexity=18.652987, train_loss=2.9260063

Batch 310710, train_perplexity=18.652987, train_loss=2.9260063

Batch 310720, train_perplexity=18.652987, train_loss=2.9260063

Batch 310730, train_perplexity=18.652983, train_loss=2.926006

Batch 310740, train_perplexity=18.652987, train_loss=2.9260063

Batch 310750, train_perplexity=18.652987, train_loss=2.9260063

Batch 310760, train_perplexity=18.652987, train_loss=2.9260063

Batch 310770, train_perplexity=18.652987, train_loss=2.9260063

Batch 310780, train_perplexity=18.652987, train_loss=2.9260063

Batch 310790, train_perplexity=18.652987, train_loss=2.9260063

Batch 310800, train_perplexity=18.652987, train_loss=2.9260063

Batch 310810, train_perplexity=18.652987, train_loss=2.9260063

Batch 310820, train_perplexity=18.652987, train_loss=2.9260063

Batch 310830, train_perplexity=18.652983, train_loss=2.926006

Batch 310840, train_perplexity=18.652987, train_loss=2.9260063

Batch 310850, train_perplexity=18.652987, train_loss=2.9260063

Batch 310860, train_perplexity=18.652983, train_loss=2.926006

Batch 310870, train_perplexity=18.652983, train_loss=2.926006

Batch 310880, train_perplexity=18.652983, train_loss=2.926006

Batch 310890, train_perplexity=18.652979, train_loss=2.9260058

Batch 310900, train_perplexity=18.652987, train_loss=2.9260063

Batch 310910, train_perplexity=18.652979, train_loss=2.9260058

Batch 310920, train_perplexity=18.652983, train_loss=2.926006

Batch 310930, train_perplexity=18.652987, train_loss=2.9260063

Batch 310940, train_perplexity=18.652979, train_loss=2.9260058

Batch 310950, train_perplexity=18.652983, train_loss=2.926006

Batch 310960, train_perplexity=18.652979, train_loss=2.9260058

Batch 310970, train_perplexity=18.652979, train_loss=2.9260058

Batch 310980, train_perplexity=18.652979, train_loss=2.9260058

Batch 310990, train_perplexity=18.652979, train_loss=2.9260058

Batch 311000, train_perplexity=18.652979, train_loss=2.9260058

Batch 311010, train_perplexity=18.652979, train_loss=2.9260058

Batch 311020, train_perplexity=18.652979, train_loss=2.9260058

Batch 311030, train_perplexity=18.652979, train_loss=2.9260058

Batch 311040, train_perplexity=18.652979, train_loss=2.9260058

Batch 311050, train_perplexity=18.652979, train_loss=2.9260058

Batch 311060, train_perplexity=18.652979, train_loss=2.9260058

Batch 311070, train_perplexity=18.652979, train_loss=2.9260058

Batch 311080, train_perplexity=18.652979, train_loss=2.9260058

Batch 311090, train_perplexity=18.652979, train_loss=2.9260058

Batch 311100, train_perplexity=18.652979, train_loss=2.9260058

Batch 311110, train_perplexity=18.652979, train_loss=2.9260058

Batch 311120, train_perplexity=18.652979, train_loss=2.9260058

Batch 311130, train_perplexity=18.652979, train_loss=2.9260058

Batch 311140, train_perplexity=18.652979, train_loss=2.9260058

Batch 311150, train_perplexity=18.652979, train_loss=2.9260058

Batch 311160, train_perplexity=18.652979, train_loss=2.9260058

Batch 311170, train_perplexity=18.652973, train_loss=2.9260056

Batch 311180, train_perplexity=18.652973, train_loss=2.9260056

Batch 311190, train_perplexity=18.65297, train_loss=2.9260054

Batch 311200, train_perplexity=18.652979, train_loss=2.9260058

Batch 311210, train_perplexity=18.65297, train_loss=2.9260054

Batch 311220, train_perplexity=18.652979, train_loss=2.9260058

Batch 311230, train_perplexity=18.652973, train_loss=2.9260056

Batch 311240, train_perplexity=18.65297, train_loss=2.9260054

Batch 311250, train_perplexity=18.65297, train_loss=2.9260054

Batch 311260, train_perplexity=18.65297, train_loss=2.9260054

Batch 311270, train_perplexity=18.65297, train_loss=2.9260054

Batch 311280, train_perplexity=18.652979, train_loss=2.9260058

Batch 311290, train_perplexity=18.65297, train_loss=2.9260054

Batch 311300, train_perplexity=18.65297, train_loss=2.9260054

Batch 311310, train_perplexity=18.65297, train_loss=2.9260054

Batch 311320, train_perplexity=18.65297, train_loss=2.9260054

Batch 311330, train_perplexity=18.65297, train_loss=2.9260054

Batch 311340, train_perplexity=18.65297, train_loss=2.9260054

Batch 311350, train_perplexity=18.65297, train_loss=2.9260054

Batch 311360, train_perplexity=18.65297, train_loss=2.9260054

Batch 311370, train_perplexity=18.65297, train_loss=2.9260054

Batch 311380, train_perplexity=18.65297, train_loss=2.9260054

Batch 311390, train_perplexity=18.65297, train_loss=2.9260054

Batch 311400, train_perplexity=18.65297, train_loss=2.9260054

Batch 311410, train_perplexity=18.652966, train_loss=2.9260051

Batch 311420, train_perplexity=18.65297, train_loss=2.9260054

Batch 311430, train_perplexity=18.65297, train_loss=2.9260054

Batch 311440, train_perplexity=18.65297, train_loss=2.9260054

Batch 311450, train_perplexity=18.65297, train_loss=2.9260054

Batch 311460, train_perplexity=18.65297, train_loss=2.9260054

Batch 311470, train_perplexity=18.65297, train_loss=2.9260054

Batch 311480, train_perplexity=18.65297, train_loss=2.9260054

Batch 311490, train_perplexity=18.65297, train_loss=2.9260054

Batch 311500, train_perplexity=18.65296, train_loss=2.926005

Batch 311510, train_perplexity=18.65296, train_loss=2.926005

Batch 311520, train_perplexity=18.65297, train_loss=2.9260054

Batch 311530, train_perplexity=18.652966, train_loss=2.9260051

Batch 311540, train_perplexity=18.65296, train_loss=2.926005

Batch 311550, train_perplexity=18.652966, train_loss=2.9260051

Batch 311560, train_perplexity=18.65296, train_loss=2.926005

Batch 311570, train_perplexity=18.65296, train_loss=2.926005

Batch 311580, train_perplexity=18.65296, train_loss=2.926005

Batch 311590, train_perplexity=18.65296, train_loss=2.926005

Batch 311600, train_perplexity=18.652966, train_loss=2.9260051

Batch 311610, train_perplexity=18.65296, train_loss=2.926005

Batch 311620, train_perplexity=18.65296, train_loss=2.926005

Batch 311630, train_perplexity=18.65296, train_loss=2.926005

Batch 311640, train_perplexity=18.65296, train_loss=2.926005

Batch 311650, train_perplexity=18.65296, train_loss=2.926005

Batch 311660, train_perplexity=18.65296, train_loss=2.926005
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 311670, train_perplexity=18.65296, train_loss=2.926005

Batch 311680, train_perplexity=18.65296, train_loss=2.926005

Batch 311690, train_perplexity=18.65296, train_loss=2.926005

Batch 311700, train_perplexity=18.65296, train_loss=2.926005

Batch 311710, train_perplexity=18.65296, train_loss=2.926005

Batch 311720, train_perplexity=18.65296, train_loss=2.926005

Batch 311730, train_perplexity=18.65296, train_loss=2.926005

Batch 311740, train_perplexity=18.65296, train_loss=2.926005

Batch 311750, train_perplexity=18.65296, train_loss=2.926005

Batch 311760, train_perplexity=18.65296, train_loss=2.926005

Batch 311770, train_perplexity=18.65296, train_loss=2.926005

Batch 311780, train_perplexity=18.65296, train_loss=2.926005

Batch 311790, train_perplexity=18.65296, train_loss=2.926005

Batch 311800, train_perplexity=18.652952, train_loss=2.9260044

Batch 311810, train_perplexity=18.65296, train_loss=2.926005

Batch 311820, train_perplexity=18.65296, train_loss=2.926005

Batch 311830, train_perplexity=18.65296, train_loss=2.926005

Batch 311840, train_perplexity=18.652956, train_loss=2.9260046

Batch 311850, train_perplexity=18.652956, train_loss=2.9260046

Batch 311860, train_perplexity=18.652956, train_loss=2.9260046

Batch 311870, train_perplexity=18.652952, train_loss=2.9260044

Batch 311880, train_perplexity=18.65296, train_loss=2.926005

Batch 311890, train_perplexity=18.652952, train_loss=2.9260044

Batch 311900, train_perplexity=18.652952, train_loss=2.9260044

Batch 311910, train_perplexity=18.652952, train_loss=2.9260044

Batch 311920, train_perplexity=18.652952, train_loss=2.9260044

Batch 311930, train_perplexity=18.652952, train_loss=2.9260044

Batch 311940, train_perplexity=18.652952, train_loss=2.9260044

Batch 311950, train_perplexity=18.652952, train_loss=2.9260044

Batch 311960, train_perplexity=18.652952, train_loss=2.9260044

Batch 311970, train_perplexity=18.652952, train_loss=2.9260044

Batch 311980, train_perplexity=18.652952, train_loss=2.9260044

Batch 311990, train_perplexity=18.652952, train_loss=2.9260044

Batch 312000, train_perplexity=18.652952, train_loss=2.9260044

Batch 312010, train_perplexity=18.652952, train_loss=2.9260044

Batch 312020, train_perplexity=18.652956, train_loss=2.9260046

Batch 312030, train_perplexity=18.652952, train_loss=2.9260044

Batch 312040, train_perplexity=18.652952, train_loss=2.9260044

Batch 312050, train_perplexity=18.652952, train_loss=2.9260044

Batch 312060, train_perplexity=18.652943, train_loss=2.926004

Batch 312070, train_perplexity=18.652946, train_loss=2.9260042

Batch 312080, train_perplexity=18.652952, train_loss=2.9260044

Batch 312090, train_perplexity=18.652946, train_loss=2.9260042

Batch 312100, train_perplexity=18.652952, train_loss=2.9260044

Batch 312110, train_perplexity=18.652952, train_loss=2.9260044

Batch 312120, train_perplexity=18.652952, train_loss=2.9260044

Batch 312130, train_perplexity=18.652946, train_loss=2.9260042

Batch 312140, train_perplexity=18.652943, train_loss=2.926004

Batch 312150, train_perplexity=18.652946, train_loss=2.9260042

Batch 312160, train_perplexity=18.652946, train_loss=2.9260042

Batch 312170, train_perplexity=18.652943, train_loss=2.926004

Batch 312180, train_perplexity=18.652952, train_loss=2.9260044

Batch 312190, train_perplexity=18.652946, train_loss=2.9260042

Batch 312200, train_perplexity=18.652943, train_loss=2.926004

Batch 312210, train_perplexity=18.652946, train_loss=2.9260042

Batch 312220, train_perplexity=18.652943, train_loss=2.926004

Batch 312230, train_perplexity=18.652943, train_loss=2.926004

Batch 312240, train_perplexity=18.652943, train_loss=2.926004

Batch 312250, train_perplexity=18.652943, train_loss=2.926004

Batch 312260, train_perplexity=18.652943, train_loss=2.926004

Batch 312270, train_perplexity=18.652943, train_loss=2.926004

Batch 312280, train_perplexity=18.652943, train_loss=2.926004

Batch 312290, train_perplexity=18.652943, train_loss=2.926004

Batch 312300, train_perplexity=18.652943, train_loss=2.926004

Batch 312310, train_perplexity=18.652943, train_loss=2.926004

Batch 312320, train_perplexity=18.652943, train_loss=2.926004

Batch 312330, train_perplexity=18.652943, train_loss=2.926004

Batch 312340, train_perplexity=18.652943, train_loss=2.926004

Batch 312350, train_perplexity=18.652943, train_loss=2.926004

Batch 312360, train_perplexity=18.652939, train_loss=2.9260037

Batch 312370, train_perplexity=18.652943, train_loss=2.926004

Batch 312380, train_perplexity=18.652943, train_loss=2.926004

Batch 312390, train_perplexity=18.652933, train_loss=2.9260035

Batch 312400, train_perplexity=18.652939, train_loss=2.9260037

Batch 312410, train_perplexity=18.652933, train_loss=2.9260035

Batch 312420, train_perplexity=18.652943, train_loss=2.926004

Batch 312430, train_perplexity=18.652943, train_loss=2.926004

Batch 312440, train_perplexity=18.652939, train_loss=2.9260037

Batch 312450, train_perplexity=18.652933, train_loss=2.9260035

Batch 312460, train_perplexity=18.652939, train_loss=2.9260037

Batch 312470, train_perplexity=18.652933, train_loss=2.9260035

Batch 312480, train_perplexity=18.652933, train_loss=2.9260035

Batch 312490, train_perplexity=18.652933, train_loss=2.9260035

Batch 312500, train_perplexity=18.652933, train_loss=2.9260035

Batch 312510, train_perplexity=18.652943, train_loss=2.926004

Batch 312520, train_perplexity=18.652943, train_loss=2.926004

Batch 312530, train_perplexity=18.652933, train_loss=2.9260035

Batch 312540, train_perplexity=18.652933, train_loss=2.9260035

Batch 312550, train_perplexity=18.652933, train_loss=2.9260035

Batch 312560, train_perplexity=18.652933, train_loss=2.9260035

Batch 312570, train_perplexity=18.652933, train_loss=2.9260035

Batch 312580, train_perplexity=18.652939, train_loss=2.9260037

Batch 312590, train_perplexity=18.652933, train_loss=2.9260035

Batch 312600, train_perplexity=18.652933, train_loss=2.9260035

Batch 312610, train_perplexity=18.652933, train_loss=2.9260035

Batch 312620, train_perplexity=18.652933, train_loss=2.9260035

Batch 312630, train_perplexity=18.652933, train_loss=2.9260035

Batch 312640, train_perplexity=18.652933, train_loss=2.9260035

Batch 312650, train_perplexity=18.652933, train_loss=2.9260035

Batch 312660, train_perplexity=18.652933, train_loss=2.9260035

Batch 312670, train_perplexity=18.652933, train_loss=2.9260035

Batch 312680, train_perplexity=18.652933, train_loss=2.9260035

Batch 312690, train_perplexity=18.652933, train_loss=2.9260035

Batch 312700, train_perplexity=18.65293, train_loss=2.9260032

Batch 312710, train_perplexity=18.652933, train_loss=2.9260035

Batch 312720, train_perplexity=18.65293, train_loss=2.9260032

Batch 312730, train_perplexity=18.652933, train_loss=2.9260035

Batch 312740, train_perplexity=18.65293, train_loss=2.9260032

Batch 312750, train_perplexity=18.65293, train_loss=2.9260032

Batch 312760, train_perplexity=18.652925, train_loss=2.926003

Batch 312770, train_perplexity=18.65293, train_loss=2.9260032

Batch 312780, train_perplexity=18.65293, train_loss=2.9260032

Batch 312790, train_perplexity=18.65293, train_loss=2.9260032

Batch 312800, train_perplexity=18.65293, train_loss=2.9260032

Batch 312810, train_perplexity=18.65293, train_loss=2.9260032

Batch 312820, train_perplexity=18.652925, train_loss=2.926003

Batch 312830, train_perplexity=18.65293, train_loss=2.9260032

Batch 312840, train_perplexity=18.65293, train_loss=2.9260032

Batch 312850, train_perplexity=18.652925, train_loss=2.926003

Batch 312860, train_perplexity=18.65293, train_loss=2.9260032

Batch 312870, train_perplexity=18.652925, train_loss=2.926003

Batch 312880, train_perplexity=18.65293, train_loss=2.9260032

Batch 312890, train_perplexity=18.652925, train_loss=2.926003

Batch 312900, train_perplexity=18.652925, train_loss=2.926003

Batch 312910, train_perplexity=18.652925, train_loss=2.926003

Batch 312920, train_perplexity=18.652925, train_loss=2.926003

Batch 312930, train_perplexity=18.652925, train_loss=2.926003

Batch 312940, train_perplexity=18.652925, train_loss=2.926003

Batch 312950, train_perplexity=18.652925, train_loss=2.926003

Batch 312960, train_perplexity=18.652925, train_loss=2.926003
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 312970, train_perplexity=18.652925, train_loss=2.926003

Batch 312980, train_perplexity=18.652925, train_loss=2.926003

Batch 312990, train_perplexity=18.652925, train_loss=2.926003

Batch 313000, train_perplexity=18.65292, train_loss=2.9260027

Batch 313010, train_perplexity=18.652925, train_loss=2.926003

Batch 313020, train_perplexity=18.652925, train_loss=2.926003

Batch 313030, train_perplexity=18.652925, train_loss=2.926003

Batch 313040, train_perplexity=18.652925, train_loss=2.926003

Batch 313050, train_perplexity=18.652925, train_loss=2.926003

Batch 313060, train_perplexity=18.652925, train_loss=2.926003

Batch 313070, train_perplexity=18.65292, train_loss=2.9260027

Batch 313080, train_perplexity=18.652916, train_loss=2.9260025

Batch 313090, train_perplexity=18.65292, train_loss=2.9260027

Batch 313100, train_perplexity=18.652916, train_loss=2.9260025

Batch 313110, train_perplexity=18.652916, train_loss=2.9260025

Batch 313120, train_perplexity=18.652916, train_loss=2.9260025

Batch 313130, train_perplexity=18.652916, train_loss=2.9260025

Batch 313140, train_perplexity=18.65292, train_loss=2.9260027

Batch 313150, train_perplexity=18.652916, train_loss=2.9260025

Batch 313160, train_perplexity=18.652916, train_loss=2.9260025

Batch 313170, train_perplexity=18.652916, train_loss=2.9260025

Batch 313180, train_perplexity=18.652916, train_loss=2.9260025

Batch 313190, train_perplexity=18.652916, train_loss=2.9260025

Batch 313200, train_perplexity=18.652916, train_loss=2.9260025

Batch 313210, train_perplexity=18.652916, train_loss=2.9260025

Batch 313220, train_perplexity=18.652916, train_loss=2.9260025

Batch 313230, train_perplexity=18.652916, train_loss=2.9260025

Batch 313240, train_perplexity=18.652916, train_loss=2.9260025

Batch 313250, train_perplexity=18.652916, train_loss=2.9260025

Batch 313260, train_perplexity=18.652916, train_loss=2.9260025

Batch 313270, train_perplexity=18.652916, train_loss=2.9260025

Batch 313280, train_perplexity=18.652916, train_loss=2.9260025

Batch 313290, train_perplexity=18.652916, train_loss=2.9260025

Batch 313300, train_perplexity=18.652916, train_loss=2.9260025

Batch 313310, train_perplexity=18.652916, train_loss=2.9260025

Batch 313320, train_perplexity=18.652916, train_loss=2.9260025

Batch 313330, train_perplexity=18.652912, train_loss=2.9260023

Batch 313340, train_perplexity=18.652916, train_loss=2.9260025

Batch 313350, train_perplexity=18.652912, train_loss=2.9260023

Batch 313360, train_perplexity=18.652912, train_loss=2.9260023

Batch 313370, train_perplexity=18.652916, train_loss=2.9260025

Batch 313380, train_perplexity=18.652912, train_loss=2.9260023

Batch 313390, train_perplexity=18.652916, train_loss=2.9260025

Batch 313400, train_perplexity=18.652916, train_loss=2.9260025

Batch 313410, train_perplexity=18.652908, train_loss=2.926002

Batch 313420, train_perplexity=18.652908, train_loss=2.926002

Batch 313430, train_perplexity=18.652908, train_loss=2.926002

Batch 313440, train_perplexity=18.652908, train_loss=2.926002

Batch 313450, train_perplexity=18.652908, train_loss=2.926002

Batch 313460, train_perplexity=18.652912, train_loss=2.9260023

Batch 313470, train_perplexity=18.652912, train_loss=2.9260023

Batch 313480, train_perplexity=18.652908, train_loss=2.926002

Batch 313490, train_perplexity=18.652912, train_loss=2.9260023

Batch 313500, train_perplexity=18.652908, train_loss=2.926002

Batch 313510, train_perplexity=18.652908, train_loss=2.926002

Batch 313520, train_perplexity=18.652912, train_loss=2.9260023

Batch 313530, train_perplexity=18.652908, train_loss=2.926002

Batch 313540, train_perplexity=18.652908, train_loss=2.926002

Batch 313550, train_perplexity=18.652908, train_loss=2.926002

Batch 313560, train_perplexity=18.652908, train_loss=2.926002

Batch 313570, train_perplexity=18.652908, train_loss=2.926002

Batch 313580, train_perplexity=18.652908, train_loss=2.926002

Batch 313590, train_perplexity=18.652908, train_loss=2.926002

Batch 313600, train_perplexity=18.652908, train_loss=2.926002

Batch 313610, train_perplexity=18.652903, train_loss=2.9260018

Batch 313620, train_perplexity=18.652908, train_loss=2.926002

Batch 313630, train_perplexity=18.652908, train_loss=2.926002

Batch 313640, train_perplexity=18.652908, train_loss=2.926002

Batch 313650, train_perplexity=18.652903, train_loss=2.9260018

Batch 313660, train_perplexity=18.652908, train_loss=2.926002

Batch 313670, train_perplexity=18.652903, train_loss=2.9260018

Batch 313680, train_perplexity=18.652908, train_loss=2.926002

Batch 313690, train_perplexity=18.652903, train_loss=2.9260018

Batch 313700, train_perplexity=18.652908, train_loss=2.926002

Batch 313710, train_perplexity=18.652908, train_loss=2.926002

Batch 313720, train_perplexity=18.652899, train_loss=2.9260015

Batch 313730, train_perplexity=18.652908, train_loss=2.926002

Batch 313740, train_perplexity=18.652903, train_loss=2.9260018

Batch 313750, train_perplexity=18.652899, train_loss=2.9260015

Batch 313760, train_perplexity=18.652903, train_loss=2.9260018

Batch 313770, train_perplexity=18.652899, train_loss=2.9260015

Batch 313780, train_perplexity=18.652899, train_loss=2.9260015

Batch 313790, train_perplexity=18.652899, train_loss=2.9260015

Batch 313800, train_perplexity=18.652908, train_loss=2.926002

Batch 313810, train_perplexity=18.652899, train_loss=2.9260015

Batch 313820, train_perplexity=18.652899, train_loss=2.9260015

Batch 313830, train_perplexity=18.652899, train_loss=2.9260015

Batch 313840, train_perplexity=18.652899, train_loss=2.9260015

Batch 313850, train_perplexity=18.652899, train_loss=2.9260015

Batch 313860, train_perplexity=18.652899, train_loss=2.9260015

Batch 313870, train_perplexity=18.652899, train_loss=2.9260015

Batch 313880, train_perplexity=18.652899, train_loss=2.9260015

Batch 313890, train_perplexity=18.652899, train_loss=2.9260015

Batch 313900, train_perplexity=18.652899, train_loss=2.9260015

Batch 313910, train_perplexity=18.652895, train_loss=2.9260013

Batch 313920, train_perplexity=18.652899, train_loss=2.9260015

Batch 313930, train_perplexity=18.652899, train_loss=2.9260015

Batch 313940, train_perplexity=18.652899, train_loss=2.9260015

Batch 313950, train_perplexity=18.65289, train_loss=2.926001

Batch 313960, train_perplexity=18.652899, train_loss=2.9260015

Batch 313970, train_perplexity=18.652899, train_loss=2.9260015

Batch 313980, train_perplexity=18.652899, train_loss=2.9260015

Batch 313990, train_perplexity=18.652895, train_loss=2.9260013

Batch 314000, train_perplexity=18.652899, train_loss=2.9260015

Batch 314010, train_perplexity=18.652895, train_loss=2.9260013

Batch 314020, train_perplexity=18.652895, train_loss=2.9260013

Batch 314030, train_perplexity=18.652895, train_loss=2.9260013

Batch 314040, train_perplexity=18.652895, train_loss=2.9260013

Batch 314050, train_perplexity=18.652895, train_loss=2.9260013

Batch 314060, train_perplexity=18.652895, train_loss=2.9260013

Batch 314070, train_perplexity=18.652899, train_loss=2.9260015

Batch 314080, train_perplexity=18.652895, train_loss=2.9260013

Batch 314090, train_perplexity=18.65289, train_loss=2.926001

Batch 314100, train_perplexity=18.65289, train_loss=2.926001

Batch 314110, train_perplexity=18.65289, train_loss=2.926001

Batch 314120, train_perplexity=18.65289, train_loss=2.926001

Batch 314130, train_perplexity=18.65289, train_loss=2.926001

Batch 314140, train_perplexity=18.65289, train_loss=2.926001

Batch 314150, train_perplexity=18.65289, train_loss=2.926001

Batch 314160, train_perplexity=18.65289, train_loss=2.926001

Batch 314170, train_perplexity=18.652899, train_loss=2.9260015

Batch 314180, train_perplexity=18.65289, train_loss=2.926001

Batch 314190, train_perplexity=18.65289, train_loss=2.926001

Batch 314200, train_perplexity=18.652895, train_loss=2.9260013

Batch 314210, train_perplexity=18.65289, train_loss=2.926001

Batch 314220, train_perplexity=18.65289, train_loss=2.926001

Batch 314230, train_perplexity=18.65289, train_loss=2.926001

Batch 314240, train_perplexity=18.65289, train_loss=2.926001

Batch 314250, train_perplexity=18.652885, train_loss=2.9260008

Batch 314260, train_perplexity=18.65289, train_loss=2.926001
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 314270, train_perplexity=18.65289, train_loss=2.926001

Batch 314280, train_perplexity=18.65289, train_loss=2.926001

Batch 314290, train_perplexity=18.65289, train_loss=2.926001

Batch 314300, train_perplexity=18.65289, train_loss=2.926001

Batch 314310, train_perplexity=18.65289, train_loss=2.926001

Batch 314320, train_perplexity=18.652885, train_loss=2.9260008

Batch 314330, train_perplexity=18.652885, train_loss=2.9260008

Batch 314340, train_perplexity=18.652885, train_loss=2.9260008

Batch 314350, train_perplexity=18.652882, train_loss=2.9260006

Batch 314360, train_perplexity=18.65289, train_loss=2.926001

Batch 314370, train_perplexity=18.652882, train_loss=2.9260006

Batch 314380, train_perplexity=18.65289, train_loss=2.926001

Batch 314390, train_perplexity=18.652882, train_loss=2.9260006

Batch 314400, train_perplexity=18.652882, train_loss=2.9260006

Batch 314410, train_perplexity=18.652882, train_loss=2.9260006

Batch 314420, train_perplexity=18.652882, train_loss=2.9260006

Batch 314430, train_perplexity=18.652882, train_loss=2.9260006

Batch 314440, train_perplexity=18.652882, train_loss=2.9260006

Batch 314450, train_perplexity=18.652882, train_loss=2.9260006

Batch 314460, train_perplexity=18.652882, train_loss=2.9260006

Batch 314470, train_perplexity=18.652882, train_loss=2.9260006

Batch 314480, train_perplexity=18.652882, train_loss=2.9260006

Batch 314490, train_perplexity=18.652882, train_loss=2.9260006

Batch 314500, train_perplexity=18.652882, train_loss=2.9260006

Batch 314510, train_perplexity=18.652882, train_loss=2.9260006

Batch 314520, train_perplexity=18.652876, train_loss=2.9260004

Batch 314530, train_perplexity=18.652882, train_loss=2.9260006

Batch 314540, train_perplexity=18.652876, train_loss=2.9260004

Batch 314550, train_perplexity=18.652882, train_loss=2.9260006

Batch 314560, train_perplexity=18.652876, train_loss=2.9260004

Batch 314570, train_perplexity=18.652876, train_loss=2.9260004

Batch 314580, train_perplexity=18.652882, train_loss=2.9260006

Batch 314590, train_perplexity=18.652872, train_loss=2.926

Batch 314600, train_perplexity=18.652882, train_loss=2.9260006

Batch 314610, train_perplexity=18.652882, train_loss=2.9260006

Batch 314620, train_perplexity=18.652882, train_loss=2.9260006

Batch 314630, train_perplexity=18.652876, train_loss=2.9260004

Batch 314640, train_perplexity=18.652876, train_loss=2.9260004

Batch 314650, train_perplexity=18.652882, train_loss=2.9260006

Batch 314660, train_perplexity=18.652872, train_loss=2.926

Batch 314670, train_perplexity=18.652872, train_loss=2.926

Batch 314680, train_perplexity=18.652872, train_loss=2.926

Batch 314690, train_perplexity=18.652876, train_loss=2.9260004

Batch 314700, train_perplexity=18.652876, train_loss=2.9260004

Batch 314710, train_perplexity=18.652872, train_loss=2.926

Batch 314720, train_perplexity=18.652876, train_loss=2.9260004

Batch 314730, train_perplexity=18.652876, train_loss=2.9260004

Batch 314740, train_perplexity=18.652872, train_loss=2.926

Batch 314750, train_perplexity=18.652872, train_loss=2.926

Batch 314760, train_perplexity=18.652872, train_loss=2.926

Batch 314770, train_perplexity=18.652876, train_loss=2.9260004

Batch 314780, train_perplexity=18.652872, train_loss=2.926

Batch 314790, train_perplexity=18.652872, train_loss=2.926

Batch 314800, train_perplexity=18.652872, train_loss=2.926

Batch 314810, train_perplexity=18.652872, train_loss=2.926

Batch 314820, train_perplexity=18.652872, train_loss=2.926

Batch 314830, train_perplexity=18.652872, train_loss=2.926

Batch 314840, train_perplexity=18.652872, train_loss=2.926

Batch 314850, train_perplexity=18.652872, train_loss=2.926

Batch 314860, train_perplexity=18.652872, train_loss=2.926

Batch 314870, train_perplexity=18.652872, train_loss=2.926

Batch 314880, train_perplexity=18.652868, train_loss=2.9259999

Batch 314890, train_perplexity=18.652872, train_loss=2.926

Batch 314900, train_perplexity=18.652872, train_loss=2.926

Batch 314910, train_perplexity=18.652872, train_loss=2.926

Batch 314920, train_perplexity=18.652872, train_loss=2.926

Batch 314930, train_perplexity=18.652872, train_loss=2.926

Batch 314940, train_perplexity=18.652868, train_loss=2.9259999

Batch 314950, train_perplexity=18.652872, train_loss=2.926

Batch 314960, train_perplexity=18.652872, train_loss=2.926

Batch 314970, train_perplexity=18.652872, train_loss=2.926

Batch 314980, train_perplexity=18.652872, train_loss=2.926

Batch 314990, train_perplexity=18.652863, train_loss=2.9259996

Batch 315000, train_perplexity=18.652868, train_loss=2.9259999

Batch 315010, train_perplexity=18.652863, train_loss=2.9259996

Batch 315020, train_perplexity=18.652863, train_loss=2.9259996

Batch 315030, train_perplexity=18.652863, train_loss=2.9259996

Batch 315040, train_perplexity=18.652863, train_loss=2.9259996

Batch 315050, train_perplexity=18.652863, train_loss=2.9259996

Batch 315060, train_perplexity=18.652863, train_loss=2.9259996

Batch 315070, train_perplexity=18.652863, train_loss=2.9259996

Batch 315080, train_perplexity=18.652863, train_loss=2.9259996

Batch 315090, train_perplexity=18.652863, train_loss=2.9259996

Batch 315100, train_perplexity=18.652863, train_loss=2.9259996

Batch 315110, train_perplexity=18.652863, train_loss=2.9259996

Batch 315120, train_perplexity=18.652863, train_loss=2.9259996

Batch 315130, train_perplexity=18.652859, train_loss=2.9259994

Batch 315140, train_perplexity=18.652863, train_loss=2.9259996

Batch 315150, train_perplexity=18.652863, train_loss=2.9259996

Batch 315160, train_perplexity=18.652863, train_loss=2.9259996

Batch 315170, train_perplexity=18.652863, train_loss=2.9259996

Batch 315180, train_perplexity=18.652863, train_loss=2.9259996

Batch 315190, train_perplexity=18.652859, train_loss=2.9259994

Batch 315200, train_perplexity=18.652863, train_loss=2.9259996

Batch 315210, train_perplexity=18.652859, train_loss=2.9259994

Batch 315220, train_perplexity=18.652863, train_loss=2.9259996

Batch 315230, train_perplexity=18.652863, train_loss=2.9259996

Batch 315240, train_perplexity=18.652859, train_loss=2.9259994

Batch 315250, train_perplexity=18.652863, train_loss=2.9259996

Batch 315260, train_perplexity=18.652859, train_loss=2.9259994

Batch 315270, train_perplexity=18.652855, train_loss=2.9259992

Batch 315280, train_perplexity=18.652855, train_loss=2.9259992

Batch 315290, train_perplexity=18.652863, train_loss=2.9259996

Batch 315300, train_perplexity=18.652855, train_loss=2.9259992

Batch 315310, train_perplexity=18.652863, train_loss=2.9259996

Batch 315320, train_perplexity=18.652859, train_loss=2.9259994

Batch 315330, train_perplexity=18.652859, train_loss=2.9259994

Batch 315340, train_perplexity=18.652859, train_loss=2.9259994

Batch 315350, train_perplexity=18.652863, train_loss=2.9259996

Batch 315360, train_perplexity=18.652859, train_loss=2.9259994

Batch 315370, train_perplexity=18.652855, train_loss=2.9259992

Batch 315380, train_perplexity=18.652855, train_loss=2.9259992

Batch 315390, train_perplexity=18.652855, train_loss=2.9259992

Batch 315400, train_perplexity=18.652855, train_loss=2.9259992

Batch 315410, train_perplexity=18.652855, train_loss=2.9259992

Batch 315420, train_perplexity=18.652859, train_loss=2.9259994

Batch 315430, train_perplexity=18.652859, train_loss=2.9259994

Batch 315440, train_perplexity=18.652855, train_loss=2.9259992

Batch 315450, train_perplexity=18.652855, train_loss=2.9259992

Batch 315460, train_perplexity=18.652855, train_loss=2.9259992

Batch 315470, train_perplexity=18.652855, train_loss=2.9259992

Batch 315480, train_perplexity=18.652855, train_loss=2.9259992

Batch 315490, train_perplexity=18.652855, train_loss=2.9259992

Batch 315500, train_perplexity=18.652855, train_loss=2.9259992

Batch 315510, train_perplexity=18.652855, train_loss=2.9259992

Batch 315520, train_perplexity=18.65285, train_loss=2.925999

Batch 315530, train_perplexity=18.652855, train_loss=2.9259992

Batch 315540, train_perplexity=18.652855, train_loss=2.9259992

Batch 315550, train_perplexity=18.652845, train_loss=2.9259987

Batch 315560, train_perplexity=18.652855, train_loss=2.9259992
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 315570, train_perplexity=18.652855, train_loss=2.9259992

Batch 315580, train_perplexity=18.652855, train_loss=2.9259992

Batch 315590, train_perplexity=18.652855, train_loss=2.9259992

Batch 315600, train_perplexity=18.652845, train_loss=2.9259987

Batch 315610, train_perplexity=18.652855, train_loss=2.9259992

Batch 315620, train_perplexity=18.652855, train_loss=2.9259992

Batch 315630, train_perplexity=18.65285, train_loss=2.925999

Batch 315640, train_perplexity=18.652845, train_loss=2.9259987

Batch 315650, train_perplexity=18.652845, train_loss=2.9259987

Batch 315660, train_perplexity=18.65285, train_loss=2.925999

Batch 315670, train_perplexity=18.65285, train_loss=2.925999

Batch 315680, train_perplexity=18.652845, train_loss=2.9259987

Batch 315690, train_perplexity=18.652845, train_loss=2.9259987

Batch 315700, train_perplexity=18.65285, train_loss=2.925999

Batch 315710, train_perplexity=18.652845, train_loss=2.9259987

Batch 315720, train_perplexity=18.652845, train_loss=2.9259987

Batch 315730, train_perplexity=18.652845, train_loss=2.9259987

Batch 315740, train_perplexity=18.652845, train_loss=2.9259987

Batch 315750, train_perplexity=18.652845, train_loss=2.9259987

Batch 315760, train_perplexity=18.652845, train_loss=2.9259987

Batch 315770, train_perplexity=18.652845, train_loss=2.9259987

Batch 315780, train_perplexity=18.652842, train_loss=2.9259984

Batch 315790, train_perplexity=18.652845, train_loss=2.9259987

Batch 315800, train_perplexity=18.652845, train_loss=2.9259987

Batch 315810, train_perplexity=18.652845, train_loss=2.9259987

Batch 315820, train_perplexity=18.652845, train_loss=2.9259987

Batch 315830, train_perplexity=18.652845, train_loss=2.9259987

Batch 315840, train_perplexity=18.652845, train_loss=2.9259987

Batch 315850, train_perplexity=18.652842, train_loss=2.9259984

Batch 315860, train_perplexity=18.652845, train_loss=2.9259987

Batch 315870, train_perplexity=18.652836, train_loss=2.9259982

Batch 315880, train_perplexity=18.652845, train_loss=2.9259987

Batch 315890, train_perplexity=18.652845, train_loss=2.9259987

Batch 315900, train_perplexity=18.652836, train_loss=2.9259982

Batch 315910, train_perplexity=18.652842, train_loss=2.9259984

Batch 315920, train_perplexity=18.652845, train_loss=2.9259987

Batch 315930, train_perplexity=18.652836, train_loss=2.9259982

Batch 315940, train_perplexity=18.652845, train_loss=2.9259987

Batch 315950, train_perplexity=18.652842, train_loss=2.9259984

Batch 315960, train_perplexity=18.652836, train_loss=2.9259982

Batch 315970, train_perplexity=18.652836, train_loss=2.9259982

Batch 315980, train_perplexity=18.652836, train_loss=2.9259982

Batch 315990, train_perplexity=18.652836, train_loss=2.9259982

Batch 316000, train_perplexity=18.652836, train_loss=2.9259982

Batch 316010, train_perplexity=18.652836, train_loss=2.9259982

Batch 316020, train_perplexity=18.652836, train_loss=2.9259982

Batch 316030, train_perplexity=18.652836, train_loss=2.9259982

Batch 316040, train_perplexity=18.652842, train_loss=2.9259984

Batch 316050, train_perplexity=18.652836, train_loss=2.9259982

Batch 316060, train_perplexity=18.652836, train_loss=2.9259982

Batch 316070, train_perplexity=18.652836, train_loss=2.9259982

Batch 316080, train_perplexity=18.652836, train_loss=2.9259982

Batch 316090, train_perplexity=18.652836, train_loss=2.9259982

Batch 316100, train_perplexity=18.652836, train_loss=2.9259982

Batch 316110, train_perplexity=18.652832, train_loss=2.925998

Batch 316120, train_perplexity=18.652836, train_loss=2.9259982

Batch 316130, train_perplexity=18.652836, train_loss=2.9259982

Batch 316140, train_perplexity=18.652836, train_loss=2.9259982

Batch 316150, train_perplexity=18.652836, train_loss=2.9259982

Batch 316160, train_perplexity=18.652836, train_loss=2.9259982

Batch 316170, train_perplexity=18.652836, train_loss=2.9259982

Batch 316180, train_perplexity=18.652836, train_loss=2.9259982

Batch 316190, train_perplexity=18.652836, train_loss=2.9259982

Batch 316200, train_perplexity=18.652836, train_loss=2.9259982

Batch 316210, train_perplexity=18.652828, train_loss=2.9259977

Batch 316220, train_perplexity=18.652836, train_loss=2.9259982

Batch 316230, train_perplexity=18.652832, train_loss=2.925998

Batch 316240, train_perplexity=18.652836, train_loss=2.9259982

Batch 316250, train_perplexity=18.652828, train_loss=2.9259977

Batch 316260, train_perplexity=18.652836, train_loss=2.9259982

Batch 316270, train_perplexity=18.652828, train_loss=2.9259977

Batch 316280, train_perplexity=18.652828, train_loss=2.9259977

Batch 316290, train_perplexity=18.652828, train_loss=2.9259977

Batch 316300, train_perplexity=18.652828, train_loss=2.9259977

Batch 316310, train_perplexity=18.652828, train_loss=2.9259977

Batch 316320, train_perplexity=18.652828, train_loss=2.9259977

Batch 316330, train_perplexity=18.652832, train_loss=2.925998

Batch 316340, train_perplexity=18.652828, train_loss=2.9259977

Batch 316350, train_perplexity=18.652828, train_loss=2.9259977

Batch 316360, train_perplexity=18.652828, train_loss=2.9259977

Batch 316370, train_perplexity=18.652828, train_loss=2.9259977

Batch 316380, train_perplexity=18.652828, train_loss=2.9259977

Batch 316390, train_perplexity=18.652828, train_loss=2.9259977

Batch 316400, train_perplexity=18.652828, train_loss=2.9259977

Batch 316410, train_perplexity=18.652828, train_loss=2.9259977

Batch 316420, train_perplexity=18.652828, train_loss=2.9259977

Batch 316430, train_perplexity=18.652828, train_loss=2.9259977

Batch 316440, train_perplexity=18.652828, train_loss=2.9259977

Batch 316450, train_perplexity=18.652828, train_loss=2.9259977

Batch 316460, train_perplexity=18.652828, train_loss=2.9259977

Batch 316470, train_perplexity=18.652819, train_loss=2.9259973

Batch 316480, train_perplexity=18.652828, train_loss=2.9259977

Batch 316490, train_perplexity=18.652828, train_loss=2.9259977

Batch 316500, train_perplexity=18.652822, train_loss=2.9259975

Batch 316510, train_perplexity=18.652819, train_loss=2.9259973

Batch 316520, train_perplexity=18.652828, train_loss=2.9259977

Batch 316530, train_perplexity=18.652822, train_loss=2.9259975

Batch 316540, train_perplexity=18.652828, train_loss=2.9259977

Batch 316550, train_perplexity=18.652828, train_loss=2.9259977

Batch 316560, train_perplexity=18.652828, train_loss=2.9259977

Batch 316570, train_perplexity=18.652819, train_loss=2.9259973

Batch 316580, train_perplexity=18.652819, train_loss=2.9259973

Batch 316590, train_perplexity=18.652822, train_loss=2.9259975

Batch 316600, train_perplexity=18.652819, train_loss=2.9259973

Batch 316610, train_perplexity=18.652819, train_loss=2.9259973

Batch 316620, train_perplexity=18.652819, train_loss=2.9259973

Batch 316630, train_perplexity=18.652819, train_loss=2.9259973

Batch 316640, train_perplexity=18.652819, train_loss=2.9259973

Batch 316650, train_perplexity=18.652819, train_loss=2.9259973

Batch 316660, train_perplexity=18.652819, train_loss=2.9259973

Batch 316670, train_perplexity=18.652819, train_loss=2.9259973

Batch 316680, train_perplexity=18.652819, train_loss=2.9259973

Batch 316690, train_perplexity=18.652819, train_loss=2.9259973

Batch 316700, train_perplexity=18.652815, train_loss=2.925997

Batch 316710, train_perplexity=18.652819, train_loss=2.9259973

Batch 316720, train_perplexity=18.652815, train_loss=2.925997

Batch 316730, train_perplexity=18.652819, train_loss=2.9259973

Batch 316740, train_perplexity=18.652819, train_loss=2.9259973

Batch 316750, train_perplexity=18.652819, train_loss=2.9259973

Batch 316760, train_perplexity=18.652819, train_loss=2.9259973

Batch 316770, train_perplexity=18.652819, train_loss=2.9259973

Batch 316780, train_perplexity=18.652815, train_loss=2.925997

Batch 316790, train_perplexity=18.652819, train_loss=2.9259973

Batch 316800, train_perplexity=18.65281, train_loss=2.9259968

Batch 316810, train_perplexity=18.652815, train_loss=2.925997

Batch 316820, train_perplexity=18.652815, train_loss=2.925997

Batch 316830, train_perplexity=18.652819, train_loss=2.9259973

Batch 316840, train_perplexity=18.65281, train_loss=2.9259968

Batch 316850, train_perplexity=18.65281, train_loss=2.9259968
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 316860, train_perplexity=18.652815, train_loss=2.925997

Batch 316870, train_perplexity=18.652815, train_loss=2.925997

Batch 316880, train_perplexity=18.65281, train_loss=2.9259968

Batch 316890, train_perplexity=18.652819, train_loss=2.9259973

Batch 316900, train_perplexity=18.652815, train_loss=2.925997

Batch 316910, train_perplexity=18.65281, train_loss=2.9259968

Batch 316920, train_perplexity=18.65281, train_loss=2.9259968

Batch 316930, train_perplexity=18.65281, train_loss=2.9259968

Batch 316940, train_perplexity=18.65281, train_loss=2.9259968

Batch 316950, train_perplexity=18.65281, train_loss=2.9259968

Batch 316960, train_perplexity=18.65281, train_loss=2.9259968

Batch 316970, train_perplexity=18.65281, train_loss=2.9259968

Batch 316980, train_perplexity=18.65281, train_loss=2.9259968

Batch 316990, train_perplexity=18.65281, train_loss=2.9259968

Batch 317000, train_perplexity=18.65281, train_loss=2.9259968

Batch 317010, train_perplexity=18.65281, train_loss=2.9259968

Batch 317020, train_perplexity=18.65281, train_loss=2.9259968

Batch 317030, train_perplexity=18.65281, train_loss=2.9259968

Batch 317040, train_perplexity=18.65281, train_loss=2.9259968

Batch 317050, train_perplexity=18.65281, train_loss=2.9259968

Batch 317060, train_perplexity=18.65281, train_loss=2.9259968

Batch 317070, train_perplexity=18.65281, train_loss=2.9259968

Batch 317080, train_perplexity=18.65281, train_loss=2.9259968

Batch 317090, train_perplexity=18.65281, train_loss=2.9259968

Batch 317100, train_perplexity=18.65281, train_loss=2.9259968

Batch 317110, train_perplexity=18.65281, train_loss=2.9259968

Batch 317120, train_perplexity=18.65281, train_loss=2.9259968

Batch 317130, train_perplexity=18.65281, train_loss=2.9259968

Batch 317140, train_perplexity=18.65281, train_loss=2.9259968

Batch 317150, train_perplexity=18.65281, train_loss=2.9259968

Batch 317160, train_perplexity=18.652802, train_loss=2.9259963

Batch 317170, train_perplexity=18.652802, train_loss=2.9259963

Batch 317180, train_perplexity=18.652802, train_loss=2.9259963

Batch 317190, train_perplexity=18.652802, train_loss=2.9259963

Batch 317200, train_perplexity=18.652802, train_loss=2.9259963

Batch 317210, train_perplexity=18.652805, train_loss=2.9259965

Batch 317220, train_perplexity=18.652802, train_loss=2.9259963

Batch 317230, train_perplexity=18.652805, train_loss=2.9259965

Batch 317240, train_perplexity=18.65281, train_loss=2.9259968

Batch 317250, train_perplexity=18.652802, train_loss=2.9259963

Batch 317260, train_perplexity=18.652802, train_loss=2.9259963

Batch 317270, train_perplexity=18.652802, train_loss=2.9259963

Batch 317280, train_perplexity=18.652802, train_loss=2.9259963

Batch 317290, train_perplexity=18.652802, train_loss=2.9259963

Batch 317300, train_perplexity=18.652802, train_loss=2.9259963

Batch 317310, train_perplexity=18.65281, train_loss=2.9259968

Batch 317320, train_perplexity=18.652802, train_loss=2.9259963

Batch 317330, train_perplexity=18.652802, train_loss=2.9259963

Batch 317340, train_perplexity=18.652802, train_loss=2.9259963

Batch 317350, train_perplexity=18.652802, train_loss=2.9259963

Batch 317360, train_perplexity=18.652802, train_loss=2.9259963

Batch 317370, train_perplexity=18.652802, train_loss=2.9259963

Batch 317380, train_perplexity=18.652802, train_loss=2.9259963

Batch 317390, train_perplexity=18.652802, train_loss=2.9259963

Batch 317400, train_perplexity=18.652802, train_loss=2.9259963

Batch 317410, train_perplexity=18.652802, train_loss=2.9259963

Batch 317420, train_perplexity=18.652792, train_loss=2.9259958

Batch 317430, train_perplexity=18.652802, train_loss=2.9259963

Batch 317440, train_perplexity=18.652802, train_loss=2.9259963

Batch 317450, train_perplexity=18.652802, train_loss=2.9259963

Batch 317460, train_perplexity=18.652796, train_loss=2.925996

Batch 317470, train_perplexity=18.652792, train_loss=2.9259958

Batch 317480, train_perplexity=18.652792, train_loss=2.9259958

Batch 317490, train_perplexity=18.652802, train_loss=2.9259963

Batch 317500, train_perplexity=18.652802, train_loss=2.9259963

Batch 317510, train_perplexity=18.652802, train_loss=2.9259963

Batch 317520, train_perplexity=18.652802, train_loss=2.9259963

Batch 317530, train_perplexity=18.652792, train_loss=2.9259958

Batch 317540, train_perplexity=18.652796, train_loss=2.925996

Batch 317550, train_perplexity=18.652792, train_loss=2.9259958

Batch 317560, train_perplexity=18.652792, train_loss=2.9259958

Batch 317570, train_perplexity=18.652792, train_loss=2.9259958

Batch 317580, train_perplexity=18.652792, train_loss=2.9259958

Batch 317590, train_perplexity=18.652792, train_loss=2.9259958

Batch 317600, train_perplexity=18.652792, train_loss=2.9259958

Batch 317610, train_perplexity=18.652792, train_loss=2.9259958

Batch 317620, train_perplexity=18.652792, train_loss=2.9259958

Batch 317630, train_perplexity=18.652792, train_loss=2.9259958

Batch 317640, train_perplexity=18.652792, train_loss=2.9259958

Batch 317650, train_perplexity=18.652796, train_loss=2.925996

Batch 317660, train_perplexity=18.652792, train_loss=2.9259958

Batch 317670, train_perplexity=18.652792, train_loss=2.9259958

Batch 317680, train_perplexity=18.652792, train_loss=2.9259958

Batch 317690, train_perplexity=18.652792, train_loss=2.9259958

Batch 317700, train_perplexity=18.652792, train_loss=2.9259958

Batch 317710, train_perplexity=18.652788, train_loss=2.9259956

Batch 317720, train_perplexity=18.652792, train_loss=2.9259958

Batch 317730, train_perplexity=18.652788, train_loss=2.9259956

Batch 317740, train_perplexity=18.652792, train_loss=2.9259958

Batch 317750, train_perplexity=18.652788, train_loss=2.9259956

Batch 317760, train_perplexity=18.652788, train_loss=2.9259956

Batch 317770, train_perplexity=18.652788, train_loss=2.9259956

Batch 317780, train_perplexity=18.652792, train_loss=2.9259958

Batch 317790, train_perplexity=18.652788, train_loss=2.9259956

Batch 317800, train_perplexity=18.652788, train_loss=2.9259956

Batch 317810, train_perplexity=18.652782, train_loss=2.9259953

Batch 317820, train_perplexity=18.652782, train_loss=2.9259953

Batch 317830, train_perplexity=18.652792, train_loss=2.9259958

Batch 317840, train_perplexity=18.652782, train_loss=2.9259953

Batch 317850, train_perplexity=18.652782, train_loss=2.9259953

Batch 317860, train_perplexity=18.652782, train_loss=2.9259953

Batch 317870, train_perplexity=18.652788, train_loss=2.9259956

Batch 317880, train_perplexity=18.652782, train_loss=2.9259953

Batch 317890, train_perplexity=18.652782, train_loss=2.9259953

Batch 317900, train_perplexity=18.652782, train_loss=2.9259953

Batch 317910, train_perplexity=18.652788, train_loss=2.9259956

Batch 317920, train_perplexity=18.652782, train_loss=2.9259953

Batch 317930, train_perplexity=18.652782, train_loss=2.9259953

Batch 317940, train_perplexity=18.652788, train_loss=2.9259956

Batch 317950, train_perplexity=18.652782, train_loss=2.9259953

Batch 317960, train_perplexity=18.652782, train_loss=2.9259953

Batch 317970, train_perplexity=18.652782, train_loss=2.9259953

Batch 317980, train_perplexity=18.652782, train_loss=2.9259953

Batch 317990, train_perplexity=18.652782, train_loss=2.9259953

Batch 318000, train_perplexity=18.652782, train_loss=2.9259953

Batch 318010, train_perplexity=18.652782, train_loss=2.9259953

Batch 318020, train_perplexity=18.652782, train_loss=2.9259953

Batch 318030, train_perplexity=18.652782, train_loss=2.9259953

Batch 318040, train_perplexity=18.652782, train_loss=2.9259953

Batch 318050, train_perplexity=18.652779, train_loss=2.925995

Batch 318060, train_perplexity=18.652782, train_loss=2.9259953

Batch 318070, train_perplexity=18.652782, train_loss=2.9259953

Batch 318080, train_perplexity=18.652782, train_loss=2.9259953

Batch 318090, train_perplexity=18.652782, train_loss=2.9259953

Batch 318100, train_perplexity=18.652775, train_loss=2.9259949

Batch 318110, train_perplexity=18.652775, train_loss=2.9259949

Batch 318120, train_perplexity=18.652775, train_loss=2.9259949

Batch 318130, train_perplexity=18.652779, train_loss=2.925995

Batch 318140, train_perplexity=18.652775, train_loss=2.9259949
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 318150, train_perplexity=18.652775, train_loss=2.9259949

Batch 318160, train_perplexity=18.652782, train_loss=2.9259953

Batch 318170, train_perplexity=18.652782, train_loss=2.9259953

Batch 318180, train_perplexity=18.652775, train_loss=2.9259949

Batch 318190, train_perplexity=18.652782, train_loss=2.9259953

Batch 318200, train_perplexity=18.652775, train_loss=2.9259949

Batch 318210, train_perplexity=18.652775, train_loss=2.9259949

Batch 318220, train_perplexity=18.652779, train_loss=2.925995

Batch 318230, train_perplexity=18.652775, train_loss=2.9259949

Batch 318240, train_perplexity=18.652775, train_loss=2.9259949

Batch 318250, train_perplexity=18.652775, train_loss=2.9259949

Batch 318260, train_perplexity=18.652775, train_loss=2.9259949

Batch 318270, train_perplexity=18.652775, train_loss=2.9259949

Batch 318280, train_perplexity=18.652775, train_loss=2.9259949

Batch 318290, train_perplexity=18.652775, train_loss=2.9259949

Batch 318300, train_perplexity=18.65277, train_loss=2.9259946

Batch 318310, train_perplexity=18.652775, train_loss=2.9259949

Batch 318320, train_perplexity=18.652775, train_loss=2.9259949

Batch 318330, train_perplexity=18.65277, train_loss=2.9259946

Batch 318340, train_perplexity=18.652775, train_loss=2.9259949

Batch 318350, train_perplexity=18.652775, train_loss=2.9259949

Batch 318360, train_perplexity=18.652775, train_loss=2.9259949

Batch 318370, train_perplexity=18.652775, train_loss=2.9259949

Batch 318380, train_perplexity=18.652775, train_loss=2.9259949

Batch 318390, train_perplexity=18.652775, train_loss=2.9259949

Batch 318400, train_perplexity=18.65277, train_loss=2.9259946

Batch 318410, train_perplexity=18.652775, train_loss=2.9259949

Batch 318420, train_perplexity=18.652765, train_loss=2.9259944

Batch 318430, train_perplexity=18.65277, train_loss=2.9259946

Batch 318440, train_perplexity=18.652775, train_loss=2.9259949

Batch 318450, train_perplexity=18.652775, train_loss=2.9259949

Batch 318460, train_perplexity=18.652765, train_loss=2.9259944

Batch 318470, train_perplexity=18.65277, train_loss=2.9259946

Batch 318480, train_perplexity=18.65277, train_loss=2.9259946

Batch 318490, train_perplexity=18.652775, train_loss=2.9259949

Batch 318500, train_perplexity=18.65277, train_loss=2.9259946

Batch 318510, train_perplexity=18.652765, train_loss=2.9259944

Batch 318520, train_perplexity=18.652765, train_loss=2.9259944

Batch 318530, train_perplexity=18.65277, train_loss=2.9259946

Batch 318540, train_perplexity=18.652765, train_loss=2.9259944

Batch 318550, train_perplexity=18.65277, train_loss=2.9259946

Batch 318560, train_perplexity=18.652765, train_loss=2.9259944

Batch 318570, train_perplexity=18.652765, train_loss=2.9259944

Batch 318580, train_perplexity=18.652765, train_loss=2.9259944

Batch 318590, train_perplexity=18.652765, train_loss=2.9259944

Batch 318600, train_perplexity=18.65277, train_loss=2.9259946

Batch 318610, train_perplexity=18.652761, train_loss=2.9259942

Batch 318620, train_perplexity=18.652761, train_loss=2.9259942

Batch 318630, train_perplexity=18.652765, train_loss=2.9259944

Batch 318640, train_perplexity=18.652765, train_loss=2.9259944

Batch 318650, train_perplexity=18.652761, train_loss=2.9259942

Batch 318660, train_perplexity=18.652765, train_loss=2.9259944

Batch 318670, train_perplexity=18.652761, train_loss=2.9259942

Batch 318680, train_perplexity=18.652765, train_loss=2.9259944

Batch 318690, train_perplexity=18.652765, train_loss=2.9259944

Batch 318700, train_perplexity=18.652765, train_loss=2.9259944

Batch 318710, train_perplexity=18.652765, train_loss=2.9259944

Batch 318720, train_perplexity=18.652765, train_loss=2.9259944

Batch 318730, train_perplexity=18.652765, train_loss=2.9259944

Batch 318740, train_perplexity=18.652756, train_loss=2.925994

Batch 318750, train_perplexity=18.652765, train_loss=2.9259944

Batch 318760, train_perplexity=18.652761, train_loss=2.9259942

Batch 318770, train_perplexity=18.652761, train_loss=2.9259942

Batch 318780, train_perplexity=18.652761, train_loss=2.9259942

Batch 318790, train_perplexity=18.652756, train_loss=2.925994

Batch 318800, train_perplexity=18.652761, train_loss=2.9259942

Batch 318810, train_perplexity=18.652756, train_loss=2.925994

Batch 318820, train_perplexity=18.652756, train_loss=2.925994

Batch 318830, train_perplexity=18.652756, train_loss=2.925994

Batch 318840, train_perplexity=18.652756, train_loss=2.925994

Batch 318850, train_perplexity=18.652756, train_loss=2.925994

Batch 318860, train_perplexity=18.652761, train_loss=2.9259942

Batch 318870, train_perplexity=18.652756, train_loss=2.925994

Batch 318880, train_perplexity=18.652756, train_loss=2.925994

Batch 318890, train_perplexity=18.652756, train_loss=2.925994

Batch 318900, train_perplexity=18.652756, train_loss=2.925994

Batch 318910, train_perplexity=18.652756, train_loss=2.925994

Batch 318920, train_perplexity=18.652756, train_loss=2.925994

Batch 318930, train_perplexity=18.652756, train_loss=2.925994

Batch 318940, train_perplexity=18.652752, train_loss=2.9259937

Batch 318950, train_perplexity=18.652756, train_loss=2.925994

Batch 318960, train_perplexity=18.652756, train_loss=2.925994

Batch 318970, train_perplexity=18.652756, train_loss=2.925994

Batch 318980, train_perplexity=18.652756, train_loss=2.925994

Batch 318990, train_perplexity=18.652756, train_loss=2.925994

Batch 319000, train_perplexity=18.652748, train_loss=2.9259934

Batch 319010, train_perplexity=18.652756, train_loss=2.925994

Batch 319020, train_perplexity=18.652752, train_loss=2.9259937

Batch 319030, train_perplexity=18.652748, train_loss=2.9259934

Batch 319040, train_perplexity=18.652756, train_loss=2.925994

Batch 319050, train_perplexity=18.652752, train_loss=2.9259937

Batch 319060, train_perplexity=18.652748, train_loss=2.9259934

Batch 319070, train_perplexity=18.652748, train_loss=2.9259934

Batch 319080, train_perplexity=18.652756, train_loss=2.925994

Batch 319090, train_perplexity=18.652756, train_loss=2.925994

Batch 319100, train_perplexity=18.652752, train_loss=2.9259937

Batch 319110, train_perplexity=18.652752, train_loss=2.9259937

Batch 319120, train_perplexity=18.652748, train_loss=2.9259934

Batch 319130, train_perplexity=18.652748, train_loss=2.9259934

Batch 319140, train_perplexity=18.652748, train_loss=2.9259934

Batch 319150, train_perplexity=18.652748, train_loss=2.9259934

Batch 319160, train_perplexity=18.652748, train_loss=2.9259934

Batch 319170, train_perplexity=18.652748, train_loss=2.9259934

Batch 319180, train_perplexity=18.652748, train_loss=2.9259934

Batch 319190, train_perplexity=18.652748, train_loss=2.9259934

Batch 319200, train_perplexity=18.652748, train_loss=2.9259934

Batch 319210, train_perplexity=18.652748, train_loss=2.9259934

Batch 319220, train_perplexity=18.652748, train_loss=2.9259934

Batch 319230, train_perplexity=18.652748, train_loss=2.9259934

Batch 319240, train_perplexity=18.652748, train_loss=2.9259934

Batch 319250, train_perplexity=18.652748, train_loss=2.9259934

Batch 319260, train_perplexity=18.652748, train_loss=2.9259934

Batch 319270, train_perplexity=18.652748, train_loss=2.9259934

Batch 319280, train_perplexity=18.652748, train_loss=2.9259934

Batch 319290, train_perplexity=18.652742, train_loss=2.9259932

Batch 319300, train_perplexity=18.652742, train_loss=2.9259932

Batch 319310, train_perplexity=18.652748, train_loss=2.9259934

Batch 319320, train_perplexity=18.652742, train_loss=2.9259932

Batch 319330, train_perplexity=18.652748, train_loss=2.9259934

Batch 319340, train_perplexity=18.652748, train_loss=2.9259934

Batch 319350, train_perplexity=18.652739, train_loss=2.925993

Batch 319360, train_perplexity=18.652742, train_loss=2.9259932

Batch 319370, train_perplexity=18.652739, train_loss=2.925993

Batch 319380, train_perplexity=18.652739, train_loss=2.925993

Batch 319390, train_perplexity=18.652739, train_loss=2.925993

Batch 319400, train_perplexity=18.652742, train_loss=2.9259932

Batch 319410, train_perplexity=18.652739, train_loss=2.925993

Batch 319420, train_perplexity=18.652739, train_loss=2.925993

Batch 319430, train_perplexity=18.652739, train_loss=2.925993
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 319440, train_perplexity=18.652739, train_loss=2.925993

Batch 319450, train_perplexity=18.652739, train_loss=2.925993

Batch 319460, train_perplexity=18.652739, train_loss=2.925993

Batch 319470, train_perplexity=18.652739, train_loss=2.925993

Batch 319480, train_perplexity=18.652739, train_loss=2.925993

Batch 319490, train_perplexity=18.652739, train_loss=2.925993

Batch 319500, train_perplexity=18.652739, train_loss=2.925993

Batch 319510, train_perplexity=18.652735, train_loss=2.9259927

Batch 319520, train_perplexity=18.652739, train_loss=2.925993

Batch 319530, train_perplexity=18.652735, train_loss=2.9259927

Batch 319540, train_perplexity=18.652739, train_loss=2.925993

Batch 319550, train_perplexity=18.652735, train_loss=2.9259927

Batch 319560, train_perplexity=18.652735, train_loss=2.9259927

Batch 319570, train_perplexity=18.652739, train_loss=2.925993

Batch 319580, train_perplexity=18.652735, train_loss=2.9259927

Batch 319590, train_perplexity=18.652739, train_loss=2.925993

Batch 319600, train_perplexity=18.652735, train_loss=2.9259927

Batch 319610, train_perplexity=18.652739, train_loss=2.925993

Batch 319620, train_perplexity=18.652729, train_loss=2.9259925

Batch 319630, train_perplexity=18.652739, train_loss=2.925993

Batch 319640, train_perplexity=18.652729, train_loss=2.9259925

Batch 319650, train_perplexity=18.652735, train_loss=2.9259927

Batch 319660, train_perplexity=18.652729, train_loss=2.9259925

Batch 319670, train_perplexity=18.652739, train_loss=2.925993

Batch 319680, train_perplexity=18.652735, train_loss=2.9259927

Batch 319690, train_perplexity=18.652729, train_loss=2.9259925

Batch 319700, train_perplexity=18.652735, train_loss=2.9259927

Batch 319710, train_perplexity=18.652729, train_loss=2.9259925

Batch 319720, train_perplexity=18.652729, train_loss=2.9259925

Batch 319730, train_perplexity=18.652729, train_loss=2.9259925

Batch 319740, train_perplexity=18.652729, train_loss=2.9259925

Batch 319750, train_perplexity=18.652729, train_loss=2.9259925

Batch 319760, train_perplexity=18.652729, train_loss=2.9259925

Batch 319770, train_perplexity=18.652729, train_loss=2.9259925

Batch 319780, train_perplexity=18.652729, train_loss=2.9259925

Batch 319790, train_perplexity=18.652729, train_loss=2.9259925

Batch 319800, train_perplexity=18.652729, train_loss=2.9259925

Batch 319810, train_perplexity=18.652729, train_loss=2.9259925

Batch 319820, train_perplexity=18.652725, train_loss=2.9259923

Batch 319830, train_perplexity=18.652729, train_loss=2.9259925

Batch 319840, train_perplexity=18.652729, train_loss=2.9259925

Batch 319850, train_perplexity=18.652729, train_loss=2.9259925

Batch 319860, train_perplexity=18.652729, train_loss=2.9259925

Batch 319870, train_perplexity=18.652729, train_loss=2.9259925

Batch 319880, train_perplexity=18.652725, train_loss=2.9259923

Batch 319890, train_perplexity=18.652729, train_loss=2.9259925

Batch 319900, train_perplexity=18.652729, train_loss=2.9259925

Batch 319910, train_perplexity=18.652725, train_loss=2.9259923

Batch 319920, train_perplexity=18.652729, train_loss=2.9259925

Batch 319930, train_perplexity=18.652725, train_loss=2.9259923

Batch 319940, train_perplexity=18.652725, train_loss=2.9259923

Batch 319950, train_perplexity=18.652725, train_loss=2.9259923

Batch 319960, train_perplexity=18.652725, train_loss=2.9259923

Batch 319970, train_perplexity=18.652721, train_loss=2.925992

Batch 319980, train_perplexity=18.652721, train_loss=2.925992

Batch 319990, train_perplexity=18.652725, train_loss=2.9259923

Batch 320000, train_perplexity=18.652721, train_loss=2.925992

Batch 320010, train_perplexity=18.652725, train_loss=2.9259923

Batch 320020, train_perplexity=18.652721, train_loss=2.925992

Batch 320030, train_perplexity=18.652721, train_loss=2.925992

Batch 320040, train_perplexity=18.652716, train_loss=2.9259918

Batch 320050, train_perplexity=18.652721, train_loss=2.925992

Batch 320060, train_perplexity=18.652721, train_loss=2.925992

Batch 320070, train_perplexity=18.652721, train_loss=2.925992

Batch 320080, train_perplexity=18.652716, train_loss=2.9259918

Batch 320090, train_perplexity=18.652721, train_loss=2.925992

Batch 320100, train_perplexity=18.652721, train_loss=2.925992

Batch 320110, train_perplexity=18.652721, train_loss=2.925992

Batch 320120, train_perplexity=18.652716, train_loss=2.9259918

Batch 320130, train_perplexity=18.652721, train_loss=2.925992

Batch 320140, train_perplexity=18.652721, train_loss=2.925992

Batch 320150, train_perplexity=18.652721, train_loss=2.925992

Batch 320160, train_perplexity=18.652721, train_loss=2.925992

Batch 320170, train_perplexity=18.652721, train_loss=2.925992

Batch 320180, train_perplexity=18.652721, train_loss=2.925992

Batch 320190, train_perplexity=18.652712, train_loss=2.9259915

Batch 320200, train_perplexity=18.652716, train_loss=2.9259918

Batch 320210, train_perplexity=18.652716, train_loss=2.9259918

Batch 320220, train_perplexity=18.652721, train_loss=2.925992

Batch 320230, train_perplexity=18.652721, train_loss=2.925992

Batch 320240, train_perplexity=18.652716, train_loss=2.9259918

Batch 320250, train_perplexity=18.652712, train_loss=2.9259915

Batch 320260, train_perplexity=18.652716, train_loss=2.9259918

Batch 320270, train_perplexity=18.652712, train_loss=2.9259915

Batch 320280, train_perplexity=18.652708, train_loss=2.9259913

Batch 320290, train_perplexity=18.652712, train_loss=2.9259915

Batch 320300, train_perplexity=18.652712, train_loss=2.9259915

Batch 320310, train_perplexity=18.652712, train_loss=2.9259915

Batch 320320, train_perplexity=18.652721, train_loss=2.925992

Batch 320330, train_perplexity=18.652712, train_loss=2.9259915

Batch 320340, train_perplexity=18.652712, train_loss=2.9259915

Batch 320350, train_perplexity=18.652712, train_loss=2.9259915

Batch 320360, train_perplexity=18.652712, train_loss=2.9259915

Batch 320370, train_perplexity=18.652712, train_loss=2.9259915

Batch 320380, train_perplexity=18.652708, train_loss=2.9259913

Batch 320390, train_perplexity=18.652712, train_loss=2.9259915

Batch 320400, train_perplexity=18.652702, train_loss=2.925991

Batch 320410, train_perplexity=18.652708, train_loss=2.9259913

Batch 320420, train_perplexity=18.652712, train_loss=2.9259915

Batch 320430, train_perplexity=18.652712, train_loss=2.9259915

Batch 320440, train_perplexity=18.652712, train_loss=2.9259915

Batch 320450, train_perplexity=18.652712, train_loss=2.9259915

Batch 320460, train_perplexity=18.652712, train_loss=2.9259915

Batch 320470, train_perplexity=18.652712, train_loss=2.9259915

Batch 320480, train_perplexity=18.652712, train_loss=2.9259915

Batch 320490, train_perplexity=18.652712, train_loss=2.9259915

Batch 320500, train_perplexity=18.652708, train_loss=2.9259913

Batch 320510, train_perplexity=18.652708, train_loss=2.9259913

Batch 320520, train_perplexity=18.652712, train_loss=2.9259915

Batch 320530, train_perplexity=18.652712, train_loss=2.9259915

Batch 320540, train_perplexity=18.652708, train_loss=2.9259913

Batch 320550, train_perplexity=18.652712, train_loss=2.9259915

Batch 320560, train_perplexity=18.652708, train_loss=2.9259913

Batch 320570, train_perplexity=18.652708, train_loss=2.9259913

Batch 320580, train_perplexity=18.652702, train_loss=2.925991

Batch 320590, train_perplexity=18.652708, train_loss=2.9259913

Batch 320600, train_perplexity=18.652702, train_loss=2.925991

Batch 320610, train_perplexity=18.652712, train_loss=2.9259915

Batch 320620, train_perplexity=18.652702, train_loss=2.925991

Batch 320630, train_perplexity=18.652702, train_loss=2.925991

Batch 320640, train_perplexity=18.652708, train_loss=2.9259913

Batch 320650, train_perplexity=18.652702, train_loss=2.925991

Batch 320660, train_perplexity=18.652699, train_loss=2.9259908

Batch 320670, train_perplexity=18.652702, train_loss=2.925991

Batch 320680, train_perplexity=18.652702, train_loss=2.925991

Batch 320690, train_perplexity=18.652702, train_loss=2.925991

Batch 320700, train_perplexity=18.652699, train_loss=2.9259908

Batch 320710, train_perplexity=18.652702, train_loss=2.925991

Batch 320720, train_perplexity=18.652702, train_loss=2.925991
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 320730, train_perplexity=18.652702, train_loss=2.925991

Batch 320740, train_perplexity=18.652702, train_loss=2.925991

Batch 320750, train_perplexity=18.652699, train_loss=2.9259908

Batch 320760, train_perplexity=18.652702, train_loss=2.925991

Batch 320770, train_perplexity=18.652702, train_loss=2.925991

Batch 320780, train_perplexity=18.652702, train_loss=2.925991

Batch 320790, train_perplexity=18.652699, train_loss=2.9259908

Batch 320800, train_perplexity=18.652695, train_loss=2.9259906

Batch 320810, train_perplexity=18.652699, train_loss=2.9259908

Batch 320820, train_perplexity=18.652699, train_loss=2.9259908

Batch 320830, train_perplexity=18.652699, train_loss=2.9259908

Batch 320840, train_perplexity=18.652695, train_loss=2.9259906

Batch 320850, train_perplexity=18.652695, train_loss=2.9259906

Batch 320860, train_perplexity=18.652702, train_loss=2.925991

Batch 320870, train_perplexity=18.652695, train_loss=2.9259906

Batch 320880, train_perplexity=18.652695, train_loss=2.9259906

Batch 320890, train_perplexity=18.652695, train_loss=2.9259906

Batch 320900, train_perplexity=18.652695, train_loss=2.9259906

Batch 320910, train_perplexity=18.652695, train_loss=2.9259906

Batch 320920, train_perplexity=18.652695, train_loss=2.9259906

Batch 320930, train_perplexity=18.652695, train_loss=2.9259906

Batch 320940, train_perplexity=18.652695, train_loss=2.9259906

Batch 320950, train_perplexity=18.652695, train_loss=2.9259906

Batch 320960, train_perplexity=18.652702, train_loss=2.925991

Batch 320970, train_perplexity=18.652695, train_loss=2.9259906

Batch 320980, train_perplexity=18.652695, train_loss=2.9259906

Batch 320990, train_perplexity=18.652695, train_loss=2.9259906

Batch 321000, train_perplexity=18.652695, train_loss=2.9259906

Batch 321010, train_perplexity=18.652695, train_loss=2.9259906

Batch 321020, train_perplexity=18.652685, train_loss=2.92599

Batch 321030, train_perplexity=18.652695, train_loss=2.9259906

Batch 321040, train_perplexity=18.652695, train_loss=2.9259906

Batch 321050, train_perplexity=18.652689, train_loss=2.9259903

Batch 321060, train_perplexity=18.652695, train_loss=2.9259906

Batch 321070, train_perplexity=18.652695, train_loss=2.9259906

Batch 321080, train_perplexity=18.652685, train_loss=2.92599

Batch 321090, train_perplexity=18.652689, train_loss=2.9259903

Batch 321100, train_perplexity=18.652695, train_loss=2.9259906

Batch 321110, train_perplexity=18.652685, train_loss=2.92599

Batch 321120, train_perplexity=18.652689, train_loss=2.9259903

Batch 321130, train_perplexity=18.652685, train_loss=2.92599

Batch 321140, train_perplexity=18.652689, train_loss=2.9259903

Batch 321150, train_perplexity=18.652685, train_loss=2.92599

Batch 321160, train_perplexity=18.652685, train_loss=2.92599

Batch 321170, train_perplexity=18.652695, train_loss=2.9259906

Batch 321180, train_perplexity=18.652685, train_loss=2.92599

Batch 321190, train_perplexity=18.652685, train_loss=2.92599

Batch 321200, train_perplexity=18.652685, train_loss=2.92599

Batch 321210, train_perplexity=18.652695, train_loss=2.9259906

Batch 321220, train_perplexity=18.652689, train_loss=2.9259903

Batch 321230, train_perplexity=18.652681, train_loss=2.9259899

Batch 321240, train_perplexity=18.652685, train_loss=2.92599

Batch 321250, train_perplexity=18.652685, train_loss=2.92599

Batch 321260, train_perplexity=18.652685, train_loss=2.92599

Batch 321270, train_perplexity=18.652685, train_loss=2.92599

Batch 321280, train_perplexity=18.652685, train_loss=2.92599

Batch 321290, train_perplexity=18.652685, train_loss=2.92599

Batch 321300, train_perplexity=18.652685, train_loss=2.92599

Batch 321310, train_perplexity=18.652676, train_loss=2.9259896

Batch 321320, train_perplexity=18.652685, train_loss=2.92599

Batch 321330, train_perplexity=18.652685, train_loss=2.92599

Batch 321340, train_perplexity=18.652681, train_loss=2.9259899

Batch 321350, train_perplexity=18.652676, train_loss=2.9259896

Batch 321360, train_perplexity=18.652685, train_loss=2.92599

Batch 321370, train_perplexity=18.652681, train_loss=2.9259899

Batch 321380, train_perplexity=18.652685, train_loss=2.92599

Batch 321390, train_perplexity=18.652685, train_loss=2.92599

Batch 321400, train_perplexity=18.652681, train_loss=2.9259899

Batch 321410, train_perplexity=18.652676, train_loss=2.9259896

Batch 321420, train_perplexity=18.652681, train_loss=2.9259899

Batch 321430, train_perplexity=18.652685, train_loss=2.92599

Batch 321440, train_perplexity=18.652681, train_loss=2.9259899

Batch 321450, train_perplexity=18.652681, train_loss=2.9259899

Batch 321460, train_perplexity=18.652676, train_loss=2.9259896

Batch 321470, train_perplexity=18.652676, train_loss=2.9259896

Batch 321480, train_perplexity=18.652676, train_loss=2.9259896

Batch 321490, train_perplexity=18.652676, train_loss=2.9259896

Batch 321500, train_perplexity=18.652676, train_loss=2.9259896

Batch 321510, train_perplexity=18.652681, train_loss=2.9259899

Batch 321520, train_perplexity=18.652676, train_loss=2.9259896

Batch 321530, train_perplexity=18.652676, train_loss=2.9259896

Batch 321540, train_perplexity=18.652676, train_loss=2.9259896

Batch 321550, train_perplexity=18.652676, train_loss=2.9259896

Batch 321560, train_perplexity=18.652685, train_loss=2.92599

Batch 321570, train_perplexity=18.652676, train_loss=2.9259896

Batch 321580, train_perplexity=18.652676, train_loss=2.9259896

Batch 321590, train_perplexity=18.652676, train_loss=2.9259896

Batch 321600, train_perplexity=18.652676, train_loss=2.9259896

Batch 321610, train_perplexity=18.652668, train_loss=2.9259892

Batch 321620, train_perplexity=18.652676, train_loss=2.9259896

Batch 321630, train_perplexity=18.652676, train_loss=2.9259896

Batch 321640, train_perplexity=18.652676, train_loss=2.9259896

Batch 321650, train_perplexity=18.652676, train_loss=2.9259896

Batch 321660, train_perplexity=18.652676, train_loss=2.9259896

Batch 321670, train_perplexity=18.652676, train_loss=2.9259896

Batch 321680, train_perplexity=18.652668, train_loss=2.9259892

Batch 321690, train_perplexity=18.652668, train_loss=2.9259892

Batch 321700, train_perplexity=18.652672, train_loss=2.9259894

Batch 321710, train_perplexity=18.652668, train_loss=2.9259892

Batch 321720, train_perplexity=18.652668, train_loss=2.9259892

Batch 321730, train_perplexity=18.652672, train_loss=2.9259894

Batch 321740, train_perplexity=18.652672, train_loss=2.9259894

Batch 321750, train_perplexity=18.652668, train_loss=2.9259892

Batch 321760, train_perplexity=18.652668, train_loss=2.9259892

Batch 321770, train_perplexity=18.652668, train_loss=2.9259892

Batch 321780, train_perplexity=18.652668, train_loss=2.9259892

Batch 321790, train_perplexity=18.652668, train_loss=2.9259892

Batch 321800, train_perplexity=18.652668, train_loss=2.9259892

Batch 321810, train_perplexity=18.652668, train_loss=2.9259892

Batch 321820, train_perplexity=18.652668, train_loss=2.9259892

Batch 321830, train_perplexity=18.652668, train_loss=2.9259892

Batch 321840, train_perplexity=18.652668, train_loss=2.9259892

Batch 321850, train_perplexity=18.652668, train_loss=2.9259892

Batch 321860, train_perplexity=18.652662, train_loss=2.925989

Batch 321870, train_perplexity=18.652668, train_loss=2.9259892

Batch 321880, train_perplexity=18.652668, train_loss=2.9259892

Batch 321890, train_perplexity=18.652662, train_loss=2.925989

Batch 321900, train_perplexity=18.652668, train_loss=2.9259892

Batch 321910, train_perplexity=18.652668, train_loss=2.9259892

Batch 321920, train_perplexity=18.652668, train_loss=2.9259892

Batch 321930, train_perplexity=18.652662, train_loss=2.925989

Batch 321940, train_perplexity=18.652668, train_loss=2.9259892

Batch 321950, train_perplexity=18.652662, train_loss=2.925989

Batch 321960, train_perplexity=18.652662, train_loss=2.925989

Batch 321970, train_perplexity=18.652658, train_loss=2.9259887

Batch 321980, train_perplexity=18.652668, train_loss=2.9259892

Batch 321990, train_perplexity=18.652658, train_loss=2.9259887

Batch 322000, train_perplexity=18.652658, train_loss=2.9259887

Batch 322010, train_perplexity=18.652662, train_loss=2.925989
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 322020, train_perplexity=18.652662, train_loss=2.925989

Batch 322030, train_perplexity=18.652662, train_loss=2.925989

Batch 322040, train_perplexity=18.652662, train_loss=2.925989

Batch 322050, train_perplexity=18.652668, train_loss=2.9259892

Batch 322060, train_perplexity=18.652658, train_loss=2.9259887

Batch 322070, train_perplexity=18.652662, train_loss=2.925989

Batch 322080, train_perplexity=18.652658, train_loss=2.9259887

Batch 322090, train_perplexity=18.652658, train_loss=2.9259887

Batch 322100, train_perplexity=18.652658, train_loss=2.9259887

Batch 322110, train_perplexity=18.652658, train_loss=2.9259887

Batch 322120, train_perplexity=18.652658, train_loss=2.9259887

Batch 322130, train_perplexity=18.652658, train_loss=2.9259887

Batch 322140, train_perplexity=18.652658, train_loss=2.9259887

Batch 322150, train_perplexity=18.652662, train_loss=2.925989

Batch 322160, train_perplexity=18.652658, train_loss=2.9259887

Batch 322170, train_perplexity=18.652658, train_loss=2.9259887

Batch 322180, train_perplexity=18.652658, train_loss=2.9259887

Batch 322190, train_perplexity=18.652658, train_loss=2.9259887

Batch 322200, train_perplexity=18.652658, train_loss=2.9259887

Batch 322210, train_perplexity=18.652658, train_loss=2.9259887

Batch 322220, train_perplexity=18.652658, train_loss=2.9259887

Batch 322230, train_perplexity=18.652658, train_loss=2.9259887

Batch 322240, train_perplexity=18.652649, train_loss=2.9259882

Batch 322250, train_perplexity=18.652655, train_loss=2.9259884

Batch 322260, train_perplexity=18.652658, train_loss=2.9259887

Batch 322270, train_perplexity=18.652658, train_loss=2.9259887

Batch 322280, train_perplexity=18.652655, train_loss=2.9259884

Batch 322290, train_perplexity=18.652655, train_loss=2.9259884

Batch 322300, train_perplexity=18.652649, train_loss=2.9259882

Batch 322310, train_perplexity=18.652655, train_loss=2.9259884

Batch 322320, train_perplexity=18.652658, train_loss=2.9259887

Batch 322330, train_perplexity=18.652649, train_loss=2.9259882

Batch 322340, train_perplexity=18.652649, train_loss=2.9259882

Batch 322350, train_perplexity=18.652655, train_loss=2.9259884

Batch 322360, train_perplexity=18.652649, train_loss=2.9259882

Batch 322370, train_perplexity=18.652655, train_loss=2.9259884

Batch 322380, train_perplexity=18.652655, train_loss=2.9259884

Batch 322390, train_perplexity=18.652649, train_loss=2.9259882

Batch 322400, train_perplexity=18.652649, train_loss=2.9259882

Batch 322410, train_perplexity=18.652649, train_loss=2.9259882

Batch 322420, train_perplexity=18.652649, train_loss=2.9259882

Batch 322430, train_perplexity=18.652649, train_loss=2.9259882

Batch 322440, train_perplexity=18.652649, train_loss=2.9259882

Batch 322450, train_perplexity=18.652649, train_loss=2.9259882

Batch 322460, train_perplexity=18.652649, train_loss=2.9259882

Batch 322470, train_perplexity=18.652649, train_loss=2.9259882

Batch 322480, train_perplexity=18.652649, train_loss=2.9259882

Batch 322490, train_perplexity=18.652649, train_loss=2.9259882

Batch 322500, train_perplexity=18.652649, train_loss=2.9259882

Batch 322510, train_perplexity=18.652649, train_loss=2.9259882

Batch 322520, train_perplexity=18.652645, train_loss=2.925988

Batch 322530, train_perplexity=18.652645, train_loss=2.925988

Batch 322540, train_perplexity=18.652649, train_loss=2.9259882

Batch 322550, train_perplexity=18.652649, train_loss=2.9259882

Batch 322560, train_perplexity=18.652649, train_loss=2.9259882

Batch 322570, train_perplexity=18.652645, train_loss=2.925988

Batch 322580, train_perplexity=18.652649, train_loss=2.9259882

Batch 322590, train_perplexity=18.652649, train_loss=2.9259882

Batch 322600, train_perplexity=18.652641, train_loss=2.9259877

Batch 322610, train_perplexity=18.652645, train_loss=2.925988

Batch 322620, train_perplexity=18.652645, train_loss=2.925988

Batch 322630, train_perplexity=18.652645, train_loss=2.925988

Batch 322640, train_perplexity=18.652641, train_loss=2.9259877

Batch 322650, train_perplexity=18.652641, train_loss=2.9259877

Batch 322660, train_perplexity=18.652641, train_loss=2.9259877

Batch 322670, train_perplexity=18.652641, train_loss=2.9259877

Batch 322680, train_perplexity=18.652645, train_loss=2.925988

Batch 322690, train_perplexity=18.652645, train_loss=2.925988

Batch 322700, train_perplexity=18.652641, train_loss=2.9259877

Batch 322710, train_perplexity=18.652641, train_loss=2.9259877

Batch 322720, train_perplexity=18.652641, train_loss=2.9259877

Batch 322730, train_perplexity=18.652645, train_loss=2.925988

Batch 322740, train_perplexity=18.652636, train_loss=2.9259875

Batch 322750, train_perplexity=18.652641, train_loss=2.9259877

Batch 322760, train_perplexity=18.652641, train_loss=2.9259877

Batch 322770, train_perplexity=18.652641, train_loss=2.9259877

Batch 322780, train_perplexity=18.652641, train_loss=2.9259877

Batch 322790, train_perplexity=18.652641, train_loss=2.9259877

Batch 322800, train_perplexity=18.652641, train_loss=2.9259877

Batch 322810, train_perplexity=18.652641, train_loss=2.9259877

Batch 322820, train_perplexity=18.652641, train_loss=2.9259877

Batch 322830, train_perplexity=18.652641, train_loss=2.9259877

Batch 322840, train_perplexity=18.652641, train_loss=2.9259877

Batch 322850, train_perplexity=18.652641, train_loss=2.9259877

Batch 322860, train_perplexity=18.652641, train_loss=2.9259877

Batch 322870, train_perplexity=18.652641, train_loss=2.9259877

Batch 322880, train_perplexity=18.652641, train_loss=2.9259877

Batch 322890, train_perplexity=18.652641, train_loss=2.9259877

Batch 322900, train_perplexity=18.652641, train_loss=2.9259877

Batch 322910, train_perplexity=18.652636, train_loss=2.9259875

Batch 322920, train_perplexity=18.652632, train_loss=2.9259872

Batch 322930, train_perplexity=18.652641, train_loss=2.9259877

Batch 322940, train_perplexity=18.652632, train_loss=2.9259872

Batch 322950, train_perplexity=18.652632, train_loss=2.9259872

Batch 322960, train_perplexity=18.652632, train_loss=2.9259872

Batch 322970, train_perplexity=18.652632, train_loss=2.9259872

Batch 322980, train_perplexity=18.652632, train_loss=2.9259872

Batch 322990, train_perplexity=18.652632, train_loss=2.9259872

Batch 323000, train_perplexity=18.652632, train_loss=2.9259872

Batch 323010, train_perplexity=18.652632, train_loss=2.9259872

Batch 323020, train_perplexity=18.652632, train_loss=2.9259872

Batch 323030, train_perplexity=18.652632, train_loss=2.9259872

Batch 323040, train_perplexity=18.652632, train_loss=2.9259872

Batch 323050, train_perplexity=18.652632, train_loss=2.9259872

Batch 323060, train_perplexity=18.652632, train_loss=2.9259872

Batch 323070, train_perplexity=18.652632, train_loss=2.9259872

Batch 323080, train_perplexity=18.652632, train_loss=2.9259872

Batch 323090, train_perplexity=18.652632, train_loss=2.9259872

Batch 323100, train_perplexity=18.652632, train_loss=2.9259872

Batch 323110, train_perplexity=18.652632, train_loss=2.9259872

Batch 323120, train_perplexity=18.652632, train_loss=2.9259872

Batch 323130, train_perplexity=18.652632, train_loss=2.9259872

Batch 323140, train_perplexity=18.652632, train_loss=2.9259872

Batch 323150, train_perplexity=18.652632, train_loss=2.9259872

Batch 323160, train_perplexity=18.652632, train_loss=2.9259872

Batch 323170, train_perplexity=18.652628, train_loss=2.925987

Batch 323180, train_perplexity=18.652632, train_loss=2.9259872

Batch 323190, train_perplexity=18.652632, train_loss=2.9259872

Batch 323200, train_perplexity=18.652628, train_loss=2.925987

Batch 323210, train_perplexity=18.652622, train_loss=2.9259868

Batch 323220, train_perplexity=18.652622, train_loss=2.9259868

Batch 323230, train_perplexity=18.652622, train_loss=2.9259868

Batch 323240, train_perplexity=18.652628, train_loss=2.925987

Batch 323250, train_perplexity=18.652632, train_loss=2.9259872

Batch 323260, train_perplexity=18.652622, train_loss=2.9259868

Batch 323270, train_perplexity=18.652622, train_loss=2.9259868

Batch 323280, train_perplexity=18.652622, train_loss=2.9259868

Batch 323290, train_perplexity=18.652622, train_loss=2.9259868

Batch 323300, train_perplexity=18.652622, train_loss=2.9259868
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 323310, train_perplexity=18.652622, train_loss=2.9259868

Batch 323320, train_perplexity=18.652622, train_loss=2.9259868

Batch 323330, train_perplexity=18.652622, train_loss=2.9259868

Batch 323340, train_perplexity=18.652622, train_loss=2.9259868

Batch 323350, train_perplexity=18.652622, train_loss=2.9259868

Batch 323360, train_perplexity=18.652622, train_loss=2.9259868

Batch 323370, train_perplexity=18.652622, train_loss=2.9259868

Batch 323380, train_perplexity=18.652622, train_loss=2.9259868

Batch 323390, train_perplexity=18.652622, train_loss=2.9259868

Batch 323400, train_perplexity=18.652622, train_loss=2.9259868

Batch 323410, train_perplexity=18.652622, train_loss=2.9259868

Batch 323420, train_perplexity=18.652622, train_loss=2.9259868

Batch 323430, train_perplexity=18.652622, train_loss=2.9259868

Batch 323440, train_perplexity=18.652622, train_loss=2.9259868

Batch 323450, train_perplexity=18.652618, train_loss=2.9259865

Batch 323460, train_perplexity=18.652622, train_loss=2.9259868

Batch 323470, train_perplexity=18.652622, train_loss=2.9259868

Batch 323480, train_perplexity=18.652618, train_loss=2.9259865

Batch 323490, train_perplexity=18.652622, train_loss=2.9259868

Batch 323500, train_perplexity=18.652618, train_loss=2.9259865

Batch 323510, train_perplexity=18.652618, train_loss=2.9259865

Batch 323520, train_perplexity=18.652615, train_loss=2.9259863

Batch 323530, train_perplexity=18.652618, train_loss=2.9259865

Batch 323540, train_perplexity=18.652615, train_loss=2.9259863

Batch 323550, train_perplexity=18.652618, train_loss=2.9259865

Batch 323560, train_perplexity=18.652618, train_loss=2.9259865

Batch 323570, train_perplexity=18.652615, train_loss=2.9259863

Batch 323580, train_perplexity=18.652615, train_loss=2.9259863

Batch 323590, train_perplexity=18.652615, train_loss=2.9259863

Batch 323600, train_perplexity=18.652615, train_loss=2.9259863

Batch 323610, train_perplexity=18.652615, train_loss=2.9259863

Batch 323620, train_perplexity=18.652622, train_loss=2.9259868

Batch 323630, train_perplexity=18.652622, train_loss=2.9259868

Batch 323640, train_perplexity=18.652615, train_loss=2.9259863

Batch 323650, train_perplexity=18.652615, train_loss=2.9259863

Batch 323660, train_perplexity=18.652615, train_loss=2.9259863

Batch 323670, train_perplexity=18.652615, train_loss=2.9259863

Batch 323680, train_perplexity=18.652615, train_loss=2.9259863

Batch 323690, train_perplexity=18.652615, train_loss=2.9259863

Batch 323700, train_perplexity=18.652615, train_loss=2.9259863

Batch 323710, train_perplexity=18.652615, train_loss=2.9259863

Batch 323720, train_perplexity=18.652615, train_loss=2.9259863

Batch 323730, train_perplexity=18.652615, train_loss=2.9259863

Batch 323740, train_perplexity=18.652615, train_loss=2.9259863

Batch 323750, train_perplexity=18.652615, train_loss=2.9259863

Batch 323760, train_perplexity=18.652615, train_loss=2.9259863

Batch 323770, train_perplexity=18.652615, train_loss=2.9259863

Batch 323780, train_perplexity=18.652615, train_loss=2.9259863

Batch 323790, train_perplexity=18.652615, train_loss=2.9259863

Batch 323800, train_perplexity=18.652615, train_loss=2.9259863

Batch 323810, train_perplexity=18.652615, train_loss=2.9259863

Batch 323820, train_perplexity=18.652615, train_loss=2.9259863

Batch 323830, train_perplexity=18.652615, train_loss=2.9259863

Batch 323840, train_perplexity=18.652605, train_loss=2.9259858

Batch 323850, train_perplexity=18.652605, train_loss=2.9259858

Batch 323860, train_perplexity=18.652609, train_loss=2.925986

Batch 323870, train_perplexity=18.652605, train_loss=2.9259858

Batch 323880, train_perplexity=18.652615, train_loss=2.9259863

Batch 323890, train_perplexity=18.652605, train_loss=2.9259858

Batch 323900, train_perplexity=18.652609, train_loss=2.925986

Batch 323910, train_perplexity=18.652605, train_loss=2.9259858

Batch 323920, train_perplexity=18.652605, train_loss=2.9259858

Batch 323930, train_perplexity=18.652605, train_loss=2.9259858

Batch 323940, train_perplexity=18.652605, train_loss=2.9259858

Batch 323950, train_perplexity=18.652605, train_loss=2.9259858

Batch 323960, train_perplexity=18.652605, train_loss=2.9259858

Batch 323970, train_perplexity=18.652605, train_loss=2.9259858

Batch 323980, train_perplexity=18.652605, train_loss=2.9259858

Batch 323990, train_perplexity=18.652605, train_loss=2.9259858

Batch 324000, train_perplexity=18.652605, train_loss=2.9259858

Batch 324010, train_perplexity=18.652605, train_loss=2.9259858

Batch 324020, train_perplexity=18.652605, train_loss=2.9259858

Batch 324030, train_perplexity=18.652605, train_loss=2.9259858

Batch 324040, train_perplexity=18.652605, train_loss=2.9259858

Batch 324050, train_perplexity=18.652605, train_loss=2.9259858

Batch 324060, train_perplexity=18.652605, train_loss=2.9259858

Batch 324070, train_perplexity=18.652601, train_loss=2.9259856

Batch 324080, train_perplexity=18.652605, train_loss=2.9259858

Batch 324090, train_perplexity=18.652605, train_loss=2.9259858

Batch 324100, train_perplexity=18.652605, train_loss=2.9259858

Batch 324110, train_perplexity=18.652605, train_loss=2.9259858

Batch 324120, train_perplexity=18.652601, train_loss=2.9259856

Batch 324130, train_perplexity=18.652605, train_loss=2.9259858

Batch 324140, train_perplexity=18.652601, train_loss=2.9259856

Batch 324150, train_perplexity=18.652596, train_loss=2.9259853

Batch 324160, train_perplexity=18.652596, train_loss=2.9259853

Batch 324170, train_perplexity=18.652605, train_loss=2.9259858

Batch 324180, train_perplexity=18.652596, train_loss=2.9259853

Batch 324190, train_perplexity=18.652596, train_loss=2.9259853

Batch 324200, train_perplexity=18.652605, train_loss=2.9259858

Batch 324210, train_perplexity=18.652601, train_loss=2.9259856

Batch 324220, train_perplexity=18.652601, train_loss=2.9259856

Batch 324230, train_perplexity=18.652601, train_loss=2.9259856

Batch 324240, train_perplexity=18.652596, train_loss=2.9259853

Batch 324250, train_perplexity=18.652596, train_loss=2.9259853

Batch 324260, train_perplexity=18.652596, train_loss=2.9259853

Batch 324270, train_perplexity=18.652596, train_loss=2.9259853

Batch 324280, train_perplexity=18.652596, train_loss=2.9259853

Batch 324290, train_perplexity=18.652596, train_loss=2.9259853

Batch 324300, train_perplexity=18.652596, train_loss=2.9259853

Batch 324310, train_perplexity=18.652596, train_loss=2.9259853

Batch 324320, train_perplexity=18.652596, train_loss=2.9259853

Batch 324330, train_perplexity=18.652596, train_loss=2.9259853

Batch 324340, train_perplexity=18.652596, train_loss=2.9259853

Batch 324350, train_perplexity=18.652596, train_loss=2.9259853

Batch 324360, train_perplexity=18.652596, train_loss=2.9259853

Batch 324370, train_perplexity=18.652596, train_loss=2.9259853

Batch 324380, train_perplexity=18.652596, train_loss=2.9259853

Batch 324390, train_perplexity=18.652596, train_loss=2.9259853

Batch 324400, train_perplexity=18.652596, train_loss=2.9259853

Batch 324410, train_perplexity=18.652596, train_loss=2.9259853

Batch 324420, train_perplexity=18.652596, train_loss=2.9259853

Batch 324430, train_perplexity=18.652596, train_loss=2.9259853

Batch 324440, train_perplexity=18.652596, train_loss=2.9259853

Batch 324450, train_perplexity=18.652596, train_loss=2.9259853

Batch 324460, train_perplexity=18.652592, train_loss=2.925985

Batch 324470, train_perplexity=18.652592, train_loss=2.925985

Batch 324480, train_perplexity=18.652588, train_loss=2.9259849

Batch 324490, train_perplexity=18.652588, train_loss=2.9259849

Batch 324500, train_perplexity=18.652588, train_loss=2.9259849

Batch 324510, train_perplexity=18.652596, train_loss=2.9259853

Batch 324520, train_perplexity=18.652596, train_loss=2.9259853

Batch 324530, train_perplexity=18.652596, train_loss=2.9259853

Batch 324540, train_perplexity=18.652588, train_loss=2.9259849

Batch 324550, train_perplexity=18.652592, train_loss=2.925985

Batch 324560, train_perplexity=18.652588, train_loss=2.9259849

Batch 324570, train_perplexity=18.652588, train_loss=2.9259849

Batch 324580, train_perplexity=18.652588, train_loss=2.9259849

Batch 324590, train_perplexity=18.652588, train_loss=2.9259849
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 324600, train_perplexity=18.652592, train_loss=2.925985

Batch 324610, train_perplexity=18.652588, train_loss=2.9259849

Batch 324620, train_perplexity=18.652588, train_loss=2.9259849

Batch 324630, train_perplexity=18.652588, train_loss=2.9259849

Batch 324640, train_perplexity=18.652588, train_loss=2.9259849

Batch 324650, train_perplexity=18.652588, train_loss=2.9259849

Batch 324660, train_perplexity=18.652588, train_loss=2.9259849

Batch 324670, train_perplexity=18.652588, train_loss=2.9259849

Batch 324680, train_perplexity=18.652588, train_loss=2.9259849

Batch 324690, train_perplexity=18.652582, train_loss=2.9259846

Batch 324700, train_perplexity=18.652588, train_loss=2.9259849

Batch 324710, train_perplexity=18.652588, train_loss=2.9259849

Batch 324720, train_perplexity=18.652588, train_loss=2.9259849

Batch 324730, train_perplexity=18.652582, train_loss=2.9259846

Batch 324740, train_perplexity=18.652578, train_loss=2.9259844

Batch 324750, train_perplexity=18.652582, train_loss=2.9259846

Batch 324760, train_perplexity=18.652582, train_loss=2.9259846

Batch 324770, train_perplexity=18.652578, train_loss=2.9259844

Batch 324780, train_perplexity=18.652578, train_loss=2.9259844

Batch 324790, train_perplexity=18.652582, train_loss=2.9259846

Batch 324800, train_perplexity=18.652582, train_loss=2.9259846

Batch 324810, train_perplexity=18.652588, train_loss=2.9259849

Batch 324820, train_perplexity=18.652578, train_loss=2.9259844

Batch 324830, train_perplexity=18.652578, train_loss=2.9259844

Batch 324840, train_perplexity=18.652578, train_loss=2.9259844

Batch 324850, train_perplexity=18.652578, train_loss=2.9259844

Batch 324860, train_perplexity=18.652578, train_loss=2.9259844

Batch 324870, train_perplexity=18.652578, train_loss=2.9259844

Batch 324880, train_perplexity=18.652578, train_loss=2.9259844

Batch 324890, train_perplexity=18.652578, train_loss=2.9259844

Batch 324900, train_perplexity=18.652578, train_loss=2.9259844

Batch 324910, train_perplexity=18.652578, train_loss=2.9259844

Batch 324920, train_perplexity=18.652578, train_loss=2.9259844

Batch 324930, train_perplexity=18.652578, train_loss=2.9259844

Batch 324940, train_perplexity=18.652578, train_loss=2.9259844

Batch 324950, train_perplexity=18.652578, train_loss=2.9259844

Batch 324960, train_perplexity=18.652578, train_loss=2.9259844

Batch 324970, train_perplexity=18.652578, train_loss=2.9259844

Batch 324980, train_perplexity=18.652578, train_loss=2.9259844

Batch 324990, train_perplexity=18.652578, train_loss=2.9259844

Batch 325000, train_perplexity=18.652578, train_loss=2.9259844

Batch 325010, train_perplexity=18.652575, train_loss=2.9259841

Batch 325020, train_perplexity=18.652575, train_loss=2.9259841

Batch 325030, train_perplexity=18.652578, train_loss=2.9259844

Batch 325040, train_perplexity=18.652578, train_loss=2.9259844

Batch 325050, train_perplexity=18.652578, train_loss=2.9259844

Batch 325060, train_perplexity=18.652578, train_loss=2.9259844

Batch 325070, train_perplexity=18.652569, train_loss=2.925984

Batch 325080, train_perplexity=18.652575, train_loss=2.9259841

Batch 325090, train_perplexity=18.652569, train_loss=2.925984

Batch 325100, train_perplexity=18.652569, train_loss=2.925984

Batch 325110, train_perplexity=18.652569, train_loss=2.925984

Batch 325120, train_perplexity=18.652569, train_loss=2.925984

Batch 325130, train_perplexity=18.652569, train_loss=2.925984

Batch 325140, train_perplexity=18.652575, train_loss=2.9259841

Batch 325150, train_perplexity=18.652578, train_loss=2.9259844

Batch 325160, train_perplexity=18.652569, train_loss=2.925984

Batch 325170, train_perplexity=18.652569, train_loss=2.925984

Batch 325180, train_perplexity=18.652569, train_loss=2.925984

Batch 325190, train_perplexity=18.652575, train_loss=2.9259841

Batch 325200, train_perplexity=18.652569, train_loss=2.925984

Batch 325210, train_perplexity=18.652569, train_loss=2.925984

Batch 325220, train_perplexity=18.652569, train_loss=2.925984

Batch 325230, train_perplexity=18.652569, train_loss=2.925984

Batch 325240, train_perplexity=18.652569, train_loss=2.925984

Batch 325250, train_perplexity=18.652569, train_loss=2.925984

Batch 325260, train_perplexity=18.652569, train_loss=2.925984

Batch 325270, train_perplexity=18.652569, train_loss=2.925984

Batch 325280, train_perplexity=18.652569, train_loss=2.925984

Batch 325290, train_perplexity=18.652575, train_loss=2.9259841

Batch 325300, train_perplexity=18.652565, train_loss=2.9259837

Batch 325310, train_perplexity=18.652569, train_loss=2.925984

Batch 325320, train_perplexity=18.652565, train_loss=2.9259837

Batch 325330, train_perplexity=18.652565, train_loss=2.9259837

Batch 325340, train_perplexity=18.652569, train_loss=2.925984

Batch 325350, train_perplexity=18.652569, train_loss=2.925984

Batch 325360, train_perplexity=18.652565, train_loss=2.9259837

Batch 325370, train_perplexity=18.652569, train_loss=2.925984

Batch 325380, train_perplexity=18.652565, train_loss=2.9259837

Batch 325390, train_perplexity=18.652561, train_loss=2.9259834

Batch 325400, train_perplexity=18.652569, train_loss=2.925984

Batch 325410, train_perplexity=18.652569, train_loss=2.925984

Batch 325420, train_perplexity=18.652565, train_loss=2.9259837

Batch 325430, train_perplexity=18.652565, train_loss=2.9259837

Batch 325440, train_perplexity=18.652565, train_loss=2.9259837

Batch 325450, train_perplexity=18.652565, train_loss=2.9259837

Batch 325460, train_perplexity=18.652569, train_loss=2.925984

Batch 325470, train_perplexity=18.652561, train_loss=2.9259834

Batch 325480, train_perplexity=18.652569, train_loss=2.925984

Batch 325490, train_perplexity=18.652561, train_loss=2.9259834

Batch 325500, train_perplexity=18.652561, train_loss=2.9259834

Batch 325510, train_perplexity=18.652569, train_loss=2.925984

Batch 325520, train_perplexity=18.652561, train_loss=2.9259834

Batch 325530, train_perplexity=18.652561, train_loss=2.9259834

Batch 325540, train_perplexity=18.652561, train_loss=2.9259834

Batch 325550, train_perplexity=18.652561, train_loss=2.9259834

Batch 325560, train_perplexity=18.652561, train_loss=2.9259834

Batch 325570, train_perplexity=18.652555, train_loss=2.9259832

Batch 325580, train_perplexity=18.652561, train_loss=2.9259834

Batch 325590, train_perplexity=18.652561, train_loss=2.9259834

Batch 325600, train_perplexity=18.652561, train_loss=2.9259834

Batch 325610, train_perplexity=18.652561, train_loss=2.9259834

Batch 325620, train_perplexity=18.652561, train_loss=2.9259834

Batch 325630, train_perplexity=18.652561, train_loss=2.9259834

Batch 325640, train_perplexity=18.652552, train_loss=2.925983

Batch 325650, train_perplexity=18.652561, train_loss=2.9259834

Batch 325660, train_perplexity=18.652561, train_loss=2.9259834

Batch 325670, train_perplexity=18.652561, train_loss=2.9259834

Batch 325680, train_perplexity=18.652561, train_loss=2.9259834

Batch 325690, train_perplexity=18.652561, train_loss=2.9259834

Batch 325700, train_perplexity=18.652555, train_loss=2.9259832

Batch 325710, train_perplexity=18.652561, train_loss=2.9259834

Batch 325720, train_perplexity=18.652555, train_loss=2.9259832

Batch 325730, train_perplexity=18.652552, train_loss=2.925983

Batch 325740, train_perplexity=18.652555, train_loss=2.9259832

Batch 325750, train_perplexity=18.652552, train_loss=2.925983

Batch 325760, train_perplexity=18.652552, train_loss=2.925983

Batch 325770, train_perplexity=18.652555, train_loss=2.9259832

Batch 325780, train_perplexity=18.652552, train_loss=2.925983

Batch 325790, train_perplexity=18.652552, train_loss=2.925983

Batch 325800, train_perplexity=18.652552, train_loss=2.925983

Batch 325810, train_perplexity=18.652552, train_loss=2.925983

Batch 325820, train_perplexity=18.652561, train_loss=2.9259834

Batch 325830, train_perplexity=18.652552, train_loss=2.925983

Batch 325840, train_perplexity=18.652555, train_loss=2.9259832

Batch 325850, train_perplexity=18.652552, train_loss=2.925983

Batch 325860, train_perplexity=18.652552, train_loss=2.925983

Batch 325870, train_perplexity=18.652552, train_loss=2.925983

Batch 325880, train_perplexity=18.652552, train_loss=2.925983
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 325890, train_perplexity=18.652552, train_loss=2.925983

Batch 325900, train_perplexity=18.652552, train_loss=2.925983

Batch 325910, train_perplexity=18.652552, train_loss=2.925983

Batch 325920, train_perplexity=18.652552, train_loss=2.925983

Batch 325930, train_perplexity=18.652548, train_loss=2.9259827

Batch 325940, train_perplexity=18.652552, train_loss=2.925983

Batch 325950, train_perplexity=18.652552, train_loss=2.925983

Batch 325960, train_perplexity=18.652552, train_loss=2.925983

Batch 325970, train_perplexity=18.652548, train_loss=2.9259827

Batch 325980, train_perplexity=18.652548, train_loss=2.9259827

Batch 325990, train_perplexity=18.652552, train_loss=2.925983

Batch 326000, train_perplexity=18.652548, train_loss=2.9259827

Batch 326010, train_perplexity=18.652552, train_loss=2.925983

Batch 326020, train_perplexity=18.652552, train_loss=2.925983

Batch 326030, train_perplexity=18.652548, train_loss=2.9259827

Batch 326040, train_perplexity=18.652552, train_loss=2.925983

Batch 326050, train_perplexity=18.652548, train_loss=2.9259827

Batch 326060, train_perplexity=18.652542, train_loss=2.9259825

Batch 326070, train_perplexity=18.652542, train_loss=2.9259825

Batch 326080, train_perplexity=18.652542, train_loss=2.9259825

Batch 326090, train_perplexity=18.652552, train_loss=2.925983

Batch 326100, train_perplexity=18.652542, train_loss=2.9259825

Batch 326110, train_perplexity=18.652542, train_loss=2.9259825

Batch 326120, train_perplexity=18.652542, train_loss=2.9259825

Batch 326130, train_perplexity=18.652542, train_loss=2.9259825

Batch 326140, train_perplexity=18.652542, train_loss=2.9259825

Batch 326150, train_perplexity=18.652542, train_loss=2.9259825

Batch 326160, train_perplexity=18.652542, train_loss=2.9259825

Batch 326170, train_perplexity=18.652542, train_loss=2.9259825

Batch 326180, train_perplexity=18.652542, train_loss=2.9259825

Batch 326190, train_perplexity=18.652542, train_loss=2.9259825

Batch 326200, train_perplexity=18.652542, train_loss=2.9259825

Batch 326210, train_perplexity=18.652542, train_loss=2.9259825

Batch 326220, train_perplexity=18.652542, train_loss=2.9259825

Batch 326230, train_perplexity=18.652542, train_loss=2.9259825

Batch 326240, train_perplexity=18.652542, train_loss=2.9259825

Batch 326250, train_perplexity=18.652542, train_loss=2.9259825

Batch 326260, train_perplexity=18.652542, train_loss=2.9259825

Batch 326270, train_perplexity=18.652534, train_loss=2.925982

Batch 326280, train_perplexity=18.652538, train_loss=2.9259822

Batch 326290, train_perplexity=18.652542, train_loss=2.9259825

Batch 326300, train_perplexity=18.652542, train_loss=2.9259825

Batch 326310, train_perplexity=18.652538, train_loss=2.9259822

Batch 326320, train_perplexity=18.652534, train_loss=2.925982

Batch 326330, train_perplexity=18.652538, train_loss=2.9259822

Batch 326340, train_perplexity=18.652542, train_loss=2.9259825

Batch 326350, train_perplexity=18.652542, train_loss=2.9259825

Batch 326360, train_perplexity=18.652534, train_loss=2.925982

Batch 326370, train_perplexity=18.652538, train_loss=2.9259822

Batch 326380, train_perplexity=18.652538, train_loss=2.9259822

Batch 326390, train_perplexity=18.652534, train_loss=2.925982

Batch 326400, train_perplexity=18.652534, train_loss=2.925982

Batch 326410, train_perplexity=18.652534, train_loss=2.925982

Batch 326420, train_perplexity=18.652534, train_loss=2.925982

Batch 326430, train_perplexity=18.652534, train_loss=2.925982

Batch 326440, train_perplexity=18.652534, train_loss=2.925982

Batch 326450, train_perplexity=18.652534, train_loss=2.925982

Batch 326460, train_perplexity=18.652534, train_loss=2.925982

Batch 326470, train_perplexity=18.652538, train_loss=2.9259822

Batch 326480, train_perplexity=18.652534, train_loss=2.925982

Batch 326490, train_perplexity=18.652534, train_loss=2.925982

Batch 326500, train_perplexity=18.652538, train_loss=2.9259822

Batch 326510, train_perplexity=18.652534, train_loss=2.925982

Batch 326520, train_perplexity=18.652534, train_loss=2.925982

Batch 326530, train_perplexity=18.652534, train_loss=2.925982

Batch 326540, train_perplexity=18.652534, train_loss=2.925982

Batch 326550, train_perplexity=18.652529, train_loss=2.9259818

Batch 326560, train_perplexity=18.652534, train_loss=2.925982

Batch 326570, train_perplexity=18.652534, train_loss=2.925982

Batch 326580, train_perplexity=18.652534, train_loss=2.925982

Batch 326590, train_perplexity=18.652534, train_loss=2.925982

Batch 326600, train_perplexity=18.652534, train_loss=2.925982

Batch 326610, train_perplexity=18.652529, train_loss=2.9259818

Batch 326620, train_perplexity=18.652534, train_loss=2.925982

Batch 326630, train_perplexity=18.652525, train_loss=2.9259815

Batch 326640, train_perplexity=18.652529, train_loss=2.9259818

Batch 326650, train_perplexity=18.652529, train_loss=2.9259818

Batch 326660, train_perplexity=18.652529, train_loss=2.9259818

Batch 326670, train_perplexity=18.652525, train_loss=2.9259815

Batch 326680, train_perplexity=18.652529, train_loss=2.9259818

Batch 326690, train_perplexity=18.652525, train_loss=2.9259815

Batch 326700, train_perplexity=18.652534, train_loss=2.925982

Batch 326710, train_perplexity=18.652525, train_loss=2.9259815

Batch 326720, train_perplexity=18.652525, train_loss=2.9259815

Batch 326730, train_perplexity=18.652529, train_loss=2.9259818

Batch 326740, train_perplexity=18.652525, train_loss=2.9259815

Batch 326750, train_perplexity=18.652525, train_loss=2.9259815

Batch 326760, train_perplexity=18.652525, train_loss=2.9259815

Batch 326770, train_perplexity=18.652525, train_loss=2.9259815

Batch 326780, train_perplexity=18.652525, train_loss=2.9259815

Batch 326790, train_perplexity=18.652525, train_loss=2.9259815

Batch 326800, train_perplexity=18.652525, train_loss=2.9259815

Batch 326810, train_perplexity=18.652525, train_loss=2.9259815

Batch 326820, train_perplexity=18.652525, train_loss=2.9259815

Batch 326830, train_perplexity=18.652525, train_loss=2.9259815

Batch 326840, train_perplexity=18.652525, train_loss=2.9259815

Batch 326850, train_perplexity=18.652525, train_loss=2.9259815

Batch 326860, train_perplexity=18.652525, train_loss=2.9259815

Batch 326870, train_perplexity=18.652525, train_loss=2.9259815

Batch 326880, train_perplexity=18.652525, train_loss=2.9259815

Batch 326890, train_perplexity=18.652525, train_loss=2.9259815

Batch 326900, train_perplexity=18.652525, train_loss=2.9259815

Batch 326910, train_perplexity=18.652525, train_loss=2.9259815

Batch 326920, train_perplexity=18.652515, train_loss=2.925981

Batch 326930, train_perplexity=18.652525, train_loss=2.9259815

Batch 326940, train_perplexity=18.652525, train_loss=2.9259815

Batch 326950, train_perplexity=18.652515, train_loss=2.925981

Batch 326960, train_perplexity=18.652515, train_loss=2.925981

Batch 326970, train_perplexity=18.652515, train_loss=2.925981

Batch 326980, train_perplexity=18.652515, train_loss=2.925981

Batch 326990, train_perplexity=18.652515, train_loss=2.925981

Batch 327000, train_perplexity=18.652521, train_loss=2.9259813

Batch 327010, train_perplexity=18.652515, train_loss=2.925981

Batch 327020, train_perplexity=18.652525, train_loss=2.9259815

Batch 327030, train_perplexity=18.652515, train_loss=2.925981

Batch 327040, train_perplexity=18.652515, train_loss=2.925981

Batch 327050, train_perplexity=18.652515, train_loss=2.925981

Batch 327060, train_perplexity=18.652515, train_loss=2.925981

Batch 327070, train_perplexity=18.652515, train_loss=2.925981

Batch 327080, train_perplexity=18.652515, train_loss=2.925981

Batch 327090, train_perplexity=18.652512, train_loss=2.9259808

Batch 327100, train_perplexity=18.652515, train_loss=2.925981

Batch 327110, train_perplexity=18.652515, train_loss=2.925981

Batch 327120, train_perplexity=18.652515, train_loss=2.925981

Batch 327130, train_perplexity=18.652515, train_loss=2.925981

Batch 327140, train_perplexity=18.652515, train_loss=2.925981

Batch 327150, train_perplexity=18.652515, train_loss=2.925981

Batch 327160, train_perplexity=18.652515, train_loss=2.925981

Batch 327170, train_perplexity=18.652515, train_loss=2.925981
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 327180, train_perplexity=18.652512, train_loss=2.9259808

Batch 327190, train_perplexity=18.652515, train_loss=2.925981

Batch 327200, train_perplexity=18.652512, train_loss=2.9259808

Batch 327210, train_perplexity=18.652515, train_loss=2.925981

Batch 327220, train_perplexity=18.652515, train_loss=2.925981

Batch 327230, train_perplexity=18.652515, train_loss=2.925981

Batch 327240, train_perplexity=18.652512, train_loss=2.9259808

Batch 327250, train_perplexity=18.652515, train_loss=2.925981

Batch 327260, train_perplexity=18.652515, train_loss=2.925981

Batch 327270, train_perplexity=18.652515, train_loss=2.925981

Batch 327280, train_perplexity=18.652508, train_loss=2.9259806

Batch 327290, train_perplexity=18.652508, train_loss=2.9259806

Batch 327300, train_perplexity=18.652508, train_loss=2.9259806

Batch 327310, train_perplexity=18.652515, train_loss=2.925981

Batch 327320, train_perplexity=18.652512, train_loss=2.9259808

Batch 327330, train_perplexity=18.652515, train_loss=2.925981

Batch 327340, train_perplexity=18.652508, train_loss=2.9259806

Batch 327350, train_perplexity=18.652508, train_loss=2.9259806

Batch 327360, train_perplexity=18.652512, train_loss=2.9259808

Batch 327370, train_perplexity=18.652508, train_loss=2.9259806

Batch 327380, train_perplexity=18.652512, train_loss=2.9259808

Batch 327390, train_perplexity=18.652508, train_loss=2.9259806

Batch 327400, train_perplexity=18.652508, train_loss=2.9259806

Batch 327410, train_perplexity=18.652508, train_loss=2.9259806

Batch 327420, train_perplexity=18.652508, train_loss=2.9259806

Batch 327430, train_perplexity=18.652508, train_loss=2.9259806

Batch 327440, train_perplexity=18.652508, train_loss=2.9259806

Batch 327450, train_perplexity=18.652508, train_loss=2.9259806

Batch 327460, train_perplexity=18.652508, train_loss=2.9259806

Batch 327470, train_perplexity=18.652508, train_loss=2.9259806

Batch 327480, train_perplexity=18.652508, train_loss=2.9259806

Batch 327490, train_perplexity=18.652508, train_loss=2.9259806

Batch 327500, train_perplexity=18.652508, train_loss=2.9259806

Batch 327510, train_perplexity=18.652508, train_loss=2.9259806

Batch 327520, train_perplexity=18.652508, train_loss=2.9259806

Batch 327530, train_perplexity=18.652502, train_loss=2.9259803

Batch 327540, train_perplexity=18.652508, train_loss=2.9259806

Batch 327550, train_perplexity=18.652502, train_loss=2.9259803

Batch 327560, train_perplexity=18.652498, train_loss=2.92598

Batch 327570, train_perplexity=18.652502, train_loss=2.9259803

Batch 327580, train_perplexity=18.652508, train_loss=2.9259806

Batch 327590, train_perplexity=18.652508, train_loss=2.9259806

Batch 327600, train_perplexity=18.652508, train_loss=2.9259806

Batch 327610, train_perplexity=18.652498, train_loss=2.92598

Batch 327620, train_perplexity=18.652502, train_loss=2.9259803

Batch 327630, train_perplexity=18.652498, train_loss=2.92598

Batch 327640, train_perplexity=18.652502, train_loss=2.9259803

Batch 327650, train_perplexity=18.652502, train_loss=2.9259803

Batch 327660, train_perplexity=18.652498, train_loss=2.92598

Batch 327670, train_perplexity=18.652502, train_loss=2.9259803

Batch 327680, train_perplexity=18.652508, train_loss=2.9259806

Batch 327690, train_perplexity=18.652498, train_loss=2.92598

Batch 327700, train_perplexity=18.652498, train_loss=2.92598

Batch 327710, train_perplexity=18.652498, train_loss=2.92598

Batch 327720, train_perplexity=18.652498, train_loss=2.92598

Batch 327730, train_perplexity=18.652498, train_loss=2.92598

Batch 327740, train_perplexity=18.652498, train_loss=2.92598

Batch 327750, train_perplexity=18.652498, train_loss=2.92598

Batch 327760, train_perplexity=18.652498, train_loss=2.92598

Batch 327770, train_perplexity=18.652498, train_loss=2.92598

Batch 327780, train_perplexity=18.652498, train_loss=2.92598

Batch 327790, train_perplexity=18.652498, train_loss=2.92598

Batch 327800, train_perplexity=18.652498, train_loss=2.92598

Batch 327810, train_perplexity=18.652498, train_loss=2.92598

Batch 327820, train_perplexity=18.652498, train_loss=2.92598

Batch 327830, train_perplexity=18.652498, train_loss=2.92598

Batch 327840, train_perplexity=18.652498, train_loss=2.92598

Batch 327850, train_perplexity=18.652498, train_loss=2.92598

Batch 327860, train_perplexity=18.652494, train_loss=2.9259799

Batch 327870, train_perplexity=18.652498, train_loss=2.92598

Batch 327880, train_perplexity=18.652498, train_loss=2.92598

Batch 327890, train_perplexity=18.652498, train_loss=2.92598

Batch 327900, train_perplexity=18.652494, train_loss=2.9259799

Batch 327910, train_perplexity=18.652494, train_loss=2.9259799

Batch 327920, train_perplexity=18.652489, train_loss=2.9259796

Batch 327930, train_perplexity=18.652489, train_loss=2.9259796

Batch 327940, train_perplexity=18.652489, train_loss=2.9259796

Batch 327950, train_perplexity=18.652494, train_loss=2.9259799

Batch 327960, train_perplexity=18.652489, train_loss=2.9259796

Batch 327970, train_perplexity=18.652489, train_loss=2.9259796

Batch 327980, train_perplexity=18.652494, train_loss=2.9259799

Batch 327990, train_perplexity=18.652494, train_loss=2.9259799

Batch 328000, train_perplexity=18.652489, train_loss=2.9259796

Batch 328010, train_perplexity=18.652489, train_loss=2.9259796

Batch 328020, train_perplexity=18.652489, train_loss=2.9259796

Batch 328030, train_perplexity=18.652489, train_loss=2.9259796

Batch 328040, train_perplexity=18.652489, train_loss=2.9259796

Batch 328050, train_perplexity=18.652489, train_loss=2.9259796

Batch 328060, train_perplexity=18.652489, train_loss=2.9259796

Batch 328070, train_perplexity=18.652489, train_loss=2.9259796

Batch 328080, train_perplexity=18.652489, train_loss=2.9259796

Batch 328090, train_perplexity=18.652489, train_loss=2.9259796

Batch 328100, train_perplexity=18.652489, train_loss=2.9259796

Batch 328110, train_perplexity=18.652489, train_loss=2.9259796

Batch 328120, train_perplexity=18.652489, train_loss=2.9259796

Batch 328130, train_perplexity=18.652489, train_loss=2.9259796

Batch 328140, train_perplexity=18.652489, train_loss=2.9259796

Batch 328150, train_perplexity=18.652489, train_loss=2.9259796

Batch 328160, train_perplexity=18.652489, train_loss=2.9259796

Batch 328170, train_perplexity=18.652489, train_loss=2.9259796

Batch 328180, train_perplexity=18.652485, train_loss=2.9259794

Batch 328190, train_perplexity=18.652489, train_loss=2.9259796

Batch 328200, train_perplexity=18.652489, train_loss=2.9259796

Batch 328210, train_perplexity=18.652489, train_loss=2.9259796

Batch 328220, train_perplexity=18.652481, train_loss=2.9259791

Batch 328230, train_perplexity=18.652485, train_loss=2.9259794

Batch 328240, train_perplexity=18.652489, train_loss=2.9259796

Batch 328250, train_perplexity=18.652481, train_loss=2.9259791

Batch 328260, train_perplexity=18.652481, train_loss=2.9259791

Batch 328270, train_perplexity=18.652485, train_loss=2.9259794

Batch 328280, train_perplexity=18.652481, train_loss=2.9259791

Batch 328290, train_perplexity=18.652481, train_loss=2.9259791

Batch 328300, train_perplexity=18.652481, train_loss=2.9259791

Batch 328310, train_perplexity=18.652481, train_loss=2.9259791

Batch 328320, train_perplexity=18.652481, train_loss=2.9259791

Batch 328330, train_perplexity=18.652481, train_loss=2.9259791

Batch 328340, train_perplexity=18.652481, train_loss=2.9259791

Batch 328350, train_perplexity=18.652485, train_loss=2.9259794

Batch 328360, train_perplexity=18.652481, train_loss=2.9259791

Batch 328370, train_perplexity=18.652481, train_loss=2.9259791

Batch 328380, train_perplexity=18.652481, train_loss=2.9259791

Batch 328390, train_perplexity=18.652481, train_loss=2.9259791

Batch 328400, train_perplexity=18.652481, train_loss=2.9259791

Batch 328410, train_perplexity=18.652481, train_loss=2.9259791

Batch 328420, train_perplexity=18.652481, train_loss=2.9259791

Batch 328430, train_perplexity=18.652481, train_loss=2.9259791

Batch 328440, train_perplexity=18.652481, train_loss=2.9259791

Batch 328450, train_perplexity=18.652475, train_loss=2.925979

Batch 328460, train_perplexity=18.652472, train_loss=2.9259787
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 328470, train_perplexity=18.652481, train_loss=2.9259791

Batch 328480, train_perplexity=18.652475, train_loss=2.925979

Batch 328490, train_perplexity=18.652481, train_loss=2.9259791

Batch 328500, train_perplexity=18.652481, train_loss=2.9259791

Batch 328510, train_perplexity=18.652481, train_loss=2.9259791

Batch 328520, train_perplexity=18.652481, train_loss=2.9259791

Batch 328530, train_perplexity=18.652475, train_loss=2.925979

Batch 328540, train_perplexity=18.652472, train_loss=2.9259787

Batch 328550, train_perplexity=18.652481, train_loss=2.9259791

Batch 328560, train_perplexity=18.652475, train_loss=2.925979

Batch 328570, train_perplexity=18.652472, train_loss=2.9259787

Batch 328580, train_perplexity=18.652472, train_loss=2.9259787

Batch 328590, train_perplexity=18.652472, train_loss=2.9259787

Batch 328600, train_perplexity=18.652475, train_loss=2.925979

Batch 328610, train_perplexity=18.652475, train_loss=2.925979

Batch 328620, train_perplexity=18.652481, train_loss=2.9259791

Batch 328630, train_perplexity=18.652472, train_loss=2.9259787

Batch 328640, train_perplexity=18.652472, train_loss=2.9259787

Batch 328650, train_perplexity=18.652475, train_loss=2.925979

Batch 328660, train_perplexity=18.652472, train_loss=2.9259787

Batch 328670, train_perplexity=18.652472, train_loss=2.9259787

Batch 328680, train_perplexity=18.652472, train_loss=2.9259787

Batch 328690, train_perplexity=18.652472, train_loss=2.9259787

Batch 328700, train_perplexity=18.652472, train_loss=2.9259787

Batch 328710, train_perplexity=18.652472, train_loss=2.9259787

Batch 328720, train_perplexity=18.652472, train_loss=2.9259787

Batch 328730, train_perplexity=18.652472, train_loss=2.9259787

Batch 328740, train_perplexity=18.652472, train_loss=2.9259787

Batch 328750, train_perplexity=18.652472, train_loss=2.9259787

Batch 328760, train_perplexity=18.652472, train_loss=2.9259787

Batch 328770, train_perplexity=18.652468, train_loss=2.9259784

Batch 328780, train_perplexity=18.652472, train_loss=2.9259787

Batch 328790, train_perplexity=18.652472, train_loss=2.9259787

Batch 328800, train_perplexity=18.652472, train_loss=2.9259787

Batch 328810, train_perplexity=18.652468, train_loss=2.9259784

Batch 328820, train_perplexity=18.652468, train_loss=2.9259784

Batch 328830, train_perplexity=18.652472, train_loss=2.9259787

Batch 328840, train_perplexity=18.652472, train_loss=2.9259787

Batch 328850, train_perplexity=18.652468, train_loss=2.9259784

Batch 328860, train_perplexity=18.652472, train_loss=2.9259787

Batch 328870, train_perplexity=18.652472, train_loss=2.9259787

Batch 328880, train_perplexity=18.652462, train_loss=2.9259782

Batch 328890, train_perplexity=18.652468, train_loss=2.9259784

Batch 328900, train_perplexity=18.652462, train_loss=2.9259782

Batch 328910, train_perplexity=18.652462, train_loss=2.9259782

Batch 328920, train_perplexity=18.652468, train_loss=2.9259784

Batch 328930, train_perplexity=18.652462, train_loss=2.9259782

Batch 328940, train_perplexity=18.652468, train_loss=2.9259784

Batch 328950, train_perplexity=18.652462, train_loss=2.9259782

Batch 328960, train_perplexity=18.652462, train_loss=2.9259782

Batch 328970, train_perplexity=18.652462, train_loss=2.9259782

Batch 328980, train_perplexity=18.652462, train_loss=2.9259782

Batch 328990, train_perplexity=18.652462, train_loss=2.9259782

Batch 329000, train_perplexity=18.652468, train_loss=2.9259784

Batch 329010, train_perplexity=18.652468, train_loss=2.9259784

Batch 329020, train_perplexity=18.652462, train_loss=2.9259782

Batch 329030, train_perplexity=18.652462, train_loss=2.9259782

Batch 329040, train_perplexity=18.652462, train_loss=2.9259782

Batch 329050, train_perplexity=18.652462, train_loss=2.9259782

Batch 329060, train_perplexity=18.652462, train_loss=2.9259782

Batch 329070, train_perplexity=18.652462, train_loss=2.9259782

Batch 329080, train_perplexity=18.652462, train_loss=2.9259782

Batch 329090, train_perplexity=18.652458, train_loss=2.925978

Batch 329100, train_perplexity=18.652462, train_loss=2.9259782

Batch 329110, train_perplexity=18.652458, train_loss=2.925978

Batch 329120, train_perplexity=18.652458, train_loss=2.925978

Batch 329130, train_perplexity=18.652458, train_loss=2.925978

Batch 329140, train_perplexity=18.652462, train_loss=2.9259782

Batch 329150, train_perplexity=18.652454, train_loss=2.9259777

Batch 329160, train_perplexity=18.652462, train_loss=2.9259782

Batch 329170, train_perplexity=18.652458, train_loss=2.925978

Batch 329180, train_perplexity=18.652458, train_loss=2.925978

Batch 329190, train_perplexity=18.652454, train_loss=2.9259777

Batch 329200, train_perplexity=18.652458, train_loss=2.925978

Batch 329210, train_perplexity=18.652454, train_loss=2.9259777

Batch 329220, train_perplexity=18.652458, train_loss=2.925978

Batch 329230, train_perplexity=18.652454, train_loss=2.9259777

Batch 329240, train_perplexity=18.652458, train_loss=2.925978

Batch 329250, train_perplexity=18.652458, train_loss=2.925978

Batch 329260, train_perplexity=18.652454, train_loss=2.9259777

Batch 329270, train_perplexity=18.652454, train_loss=2.9259777

Batch 329280, train_perplexity=18.652454, train_loss=2.9259777

Batch 329290, train_perplexity=18.652454, train_loss=2.9259777

Batch 329300, train_perplexity=18.652454, train_loss=2.9259777

Batch 329310, train_perplexity=18.652454, train_loss=2.9259777

Batch 329320, train_perplexity=18.652458, train_loss=2.925978

Batch 329330, train_perplexity=18.652454, train_loss=2.9259777

Batch 329340, train_perplexity=18.652454, train_loss=2.9259777

Batch 329350, train_perplexity=18.652454, train_loss=2.9259777

Batch 329360, train_perplexity=18.652454, train_loss=2.9259777

Batch 329370, train_perplexity=18.652454, train_loss=2.9259777

Batch 329380, train_perplexity=18.652454, train_loss=2.9259777

Batch 329390, train_perplexity=18.652454, train_loss=2.9259777

Batch 329400, train_perplexity=18.652454, train_loss=2.9259777

Batch 329410, train_perplexity=18.652454, train_loss=2.9259777

Batch 329420, train_perplexity=18.652454, train_loss=2.9259777

Batch 329430, train_perplexity=18.652449, train_loss=2.9259775

Batch 329440, train_perplexity=18.652454, train_loss=2.9259777

Batch 329450, train_perplexity=18.652454, train_loss=2.9259777

Batch 329460, train_perplexity=18.652454, train_loss=2.9259777

Batch 329470, train_perplexity=18.652445, train_loss=2.9259772

Batch 329480, train_perplexity=18.652449, train_loss=2.9259775

Batch 329490, train_perplexity=18.652454, train_loss=2.9259777

Batch 329500, train_perplexity=18.652445, train_loss=2.9259772

Batch 329510, train_perplexity=18.652449, train_loss=2.9259775

Batch 329520, train_perplexity=18.652445, train_loss=2.9259772

Batch 329530, train_perplexity=18.652445, train_loss=2.9259772

Batch 329540, train_perplexity=18.652449, train_loss=2.9259775

Batch 329550, train_perplexity=18.652454, train_loss=2.9259777

Batch 329560, train_perplexity=18.652445, train_loss=2.9259772

Batch 329570, train_perplexity=18.652445, train_loss=2.9259772

Batch 329580, train_perplexity=18.652445, train_loss=2.9259772

Batch 329590, train_perplexity=18.652449, train_loss=2.9259775

Batch 329600, train_perplexity=18.652441, train_loss=2.925977

Batch 329610, train_perplexity=18.652445, train_loss=2.9259772

Batch 329620, train_perplexity=18.652445, train_loss=2.9259772

Batch 329630, train_perplexity=18.652445, train_loss=2.9259772

Batch 329640, train_perplexity=18.652445, train_loss=2.9259772

Batch 329650, train_perplexity=18.652445, train_loss=2.9259772

Batch 329660, train_perplexity=18.652445, train_loss=2.9259772

Batch 329670, train_perplexity=18.652445, train_loss=2.9259772

Batch 329680, train_perplexity=18.652441, train_loss=2.925977

Batch 329690, train_perplexity=18.652445, train_loss=2.9259772

Batch 329700, train_perplexity=18.652441, train_loss=2.925977

Batch 329710, train_perplexity=18.652445, train_loss=2.9259772

Batch 329720, train_perplexity=18.652445, train_loss=2.9259772

Batch 329730, train_perplexity=18.652435, train_loss=2.9259768

Batch 329740, train_perplexity=18.652445, train_loss=2.9259772

Batch 329750, train_perplexity=18.652445, train_loss=2.9259772
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 329760, train_perplexity=18.652445, train_loss=2.9259772

Batch 329770, train_perplexity=18.652445, train_loss=2.9259772

Batch 329780, train_perplexity=18.652441, train_loss=2.925977

Batch 329790, train_perplexity=18.652435, train_loss=2.9259768

Batch 329800, train_perplexity=18.652445, train_loss=2.9259772

Batch 329810, train_perplexity=18.652441, train_loss=2.925977

Batch 329820, train_perplexity=18.652445, train_loss=2.9259772

Batch 329830, train_perplexity=18.652441, train_loss=2.925977

Batch 329840, train_perplexity=18.652441, train_loss=2.925977

Batch 329850, train_perplexity=18.652435, train_loss=2.9259768

Batch 329860, train_perplexity=18.652441, train_loss=2.925977

Batch 329870, train_perplexity=18.652435, train_loss=2.9259768

Batch 329880, train_perplexity=18.652445, train_loss=2.9259772

Batch 329890, train_perplexity=18.652435, train_loss=2.9259768

Batch 329900, train_perplexity=18.652435, train_loss=2.9259768

Batch 329910, train_perplexity=18.652435, train_loss=2.9259768

Batch 329920, train_perplexity=18.652435, train_loss=2.9259768

Batch 329930, train_perplexity=18.652435, train_loss=2.9259768

Batch 329940, train_perplexity=18.652431, train_loss=2.9259765

Batch 329950, train_perplexity=18.652435, train_loss=2.9259768

Batch 329960, train_perplexity=18.652435, train_loss=2.9259768

Batch 329970, train_perplexity=18.652435, train_loss=2.9259768

Batch 329980, train_perplexity=18.652435, train_loss=2.9259768

Batch 329990, train_perplexity=18.652435, train_loss=2.9259768

Batch 330000, train_perplexity=18.652435, train_loss=2.9259768

Batch 330010, train_perplexity=18.652435, train_loss=2.9259768

Batch 330020, train_perplexity=18.652435, train_loss=2.9259768

Batch 330030, train_perplexity=18.652435, train_loss=2.9259768

Batch 330040, train_perplexity=18.652435, train_loss=2.9259768

Batch 330050, train_perplexity=18.652435, train_loss=2.9259768

Batch 330060, train_perplexity=18.652435, train_loss=2.9259768

Batch 330070, train_perplexity=18.652435, train_loss=2.9259768

Batch 330080, train_perplexity=18.652435, train_loss=2.9259768

Batch 330090, train_perplexity=18.652435, train_loss=2.9259768

Batch 330100, train_perplexity=18.652428, train_loss=2.9259763

Batch 330110, train_perplexity=18.652435, train_loss=2.9259768

Batch 330120, train_perplexity=18.652435, train_loss=2.9259768

Batch 330130, train_perplexity=18.652431, train_loss=2.9259765

Batch 330140, train_perplexity=18.652428, train_loss=2.9259763

Batch 330150, train_perplexity=18.652431, train_loss=2.9259765

Batch 330160, train_perplexity=18.652435, train_loss=2.9259768

Batch 330170, train_perplexity=18.652435, train_loss=2.9259768

Batch 330180, train_perplexity=18.652428, train_loss=2.9259763

Batch 330190, train_perplexity=18.652428, train_loss=2.9259763

Batch 330200, train_perplexity=18.652428, train_loss=2.9259763

Batch 330210, train_perplexity=18.652428, train_loss=2.9259763

Batch 330220, train_perplexity=18.652428, train_loss=2.9259763

Batch 330230, train_perplexity=18.652428, train_loss=2.9259763

Batch 330240, train_perplexity=18.652428, train_loss=2.9259763

Batch 330250, train_perplexity=18.652428, train_loss=2.9259763

Batch 330260, train_perplexity=18.652428, train_loss=2.9259763

Batch 330270, train_perplexity=18.652428, train_loss=2.9259763

Batch 330280, train_perplexity=18.652428, train_loss=2.9259763

Batch 330290, train_perplexity=18.652428, train_loss=2.9259763

Batch 330300, train_perplexity=18.652428, train_loss=2.9259763

Batch 330310, train_perplexity=18.652428, train_loss=2.9259763

Batch 330320, train_perplexity=18.652428, train_loss=2.9259763

Batch 330330, train_perplexity=18.652422, train_loss=2.925976

Batch 330340, train_perplexity=18.652428, train_loss=2.9259763

Batch 330350, train_perplexity=18.652428, train_loss=2.9259763

Batch 330360, train_perplexity=18.652428, train_loss=2.9259763

Batch 330370, train_perplexity=18.652428, train_loss=2.9259763

Batch 330380, train_perplexity=18.652428, train_loss=2.9259763

Batch 330390, train_perplexity=18.652428, train_loss=2.9259763

Batch 330400, train_perplexity=18.652422, train_loss=2.925976

Batch 330410, train_perplexity=18.652422, train_loss=2.925976

Batch 330420, train_perplexity=18.652422, train_loss=2.925976

Batch 330430, train_perplexity=18.652422, train_loss=2.925976

Batch 330440, train_perplexity=18.652422, train_loss=2.925976

Batch 330450, train_perplexity=18.652418, train_loss=2.9259758

Batch 330460, train_perplexity=18.652422, train_loss=2.925976

Batch 330470, train_perplexity=18.652428, train_loss=2.9259763

Batch 330480, train_perplexity=18.652418, train_loss=2.9259758

Batch 330490, train_perplexity=18.652422, train_loss=2.925976

Batch 330500, train_perplexity=18.652422, train_loss=2.925976

Batch 330510, train_perplexity=18.652418, train_loss=2.9259758

Batch 330520, train_perplexity=18.652422, train_loss=2.925976

Batch 330530, train_perplexity=18.652418, train_loss=2.9259758

Batch 330540, train_perplexity=18.652418, train_loss=2.9259758

Batch 330550, train_perplexity=18.652418, train_loss=2.9259758

Batch 330560, train_perplexity=18.652418, train_loss=2.9259758

Batch 330570, train_perplexity=18.652418, train_loss=2.9259758

Batch 330580, train_perplexity=18.652418, train_loss=2.9259758

Batch 330590, train_perplexity=18.652418, train_loss=2.9259758

Batch 330600, train_perplexity=18.652418, train_loss=2.9259758

Batch 330610, train_perplexity=18.652418, train_loss=2.9259758

Batch 330620, train_perplexity=18.652414, train_loss=2.9259756

Batch 330630, train_perplexity=18.652418, train_loss=2.9259758

Batch 330640, train_perplexity=18.652418, train_loss=2.9259758

Batch 330650, train_perplexity=18.652418, train_loss=2.9259758

Batch 330660, train_perplexity=18.652418, train_loss=2.9259758

Batch 330670, train_perplexity=18.652409, train_loss=2.9259753

Batch 330680, train_perplexity=18.652418, train_loss=2.9259758

Batch 330690, train_perplexity=18.652418, train_loss=2.9259758

Batch 330700, train_perplexity=18.652418, train_loss=2.9259758

Batch 330710, train_perplexity=18.652418, train_loss=2.9259758

Batch 330720, train_perplexity=18.652418, train_loss=2.9259758

Batch 330730, train_perplexity=18.652418, train_loss=2.9259758

Batch 330740, train_perplexity=18.652414, train_loss=2.9259756

Batch 330750, train_perplexity=18.652409, train_loss=2.9259753

Batch 330760, train_perplexity=18.652409, train_loss=2.9259753

Batch 330770, train_perplexity=18.652414, train_loss=2.9259756

Batch 330780, train_perplexity=18.652414, train_loss=2.9259756

Batch 330790, train_perplexity=18.652409, train_loss=2.9259753

Batch 330800, train_perplexity=18.652409, train_loss=2.9259753

Batch 330810, train_perplexity=18.652409, train_loss=2.9259753

Batch 330820, train_perplexity=18.652409, train_loss=2.9259753

Batch 330830, train_perplexity=18.652409, train_loss=2.9259753

Batch 330840, train_perplexity=18.652418, train_loss=2.9259758

Batch 330850, train_perplexity=18.652409, train_loss=2.9259753

Batch 330860, train_perplexity=18.652409, train_loss=2.9259753

Batch 330870, train_perplexity=18.652409, train_loss=2.9259753

Batch 330880, train_perplexity=18.652409, train_loss=2.9259753

Batch 330890, train_perplexity=18.652409, train_loss=2.9259753

Batch 330900, train_perplexity=18.652409, train_loss=2.9259753

Batch 330910, train_perplexity=18.652409, train_loss=2.9259753

Batch 330920, train_perplexity=18.652405, train_loss=2.925975

Batch 330930, train_perplexity=18.652409, train_loss=2.9259753

Batch 330940, train_perplexity=18.652409, train_loss=2.9259753

Batch 330950, train_perplexity=18.652409, train_loss=2.9259753

Batch 330960, train_perplexity=18.652409, train_loss=2.9259753

Batch 330970, train_perplexity=18.652409, train_loss=2.9259753

Batch 330980, train_perplexity=18.652409, train_loss=2.9259753

Batch 330990, train_perplexity=18.652409, train_loss=2.9259753

Batch 331000, train_perplexity=18.652405, train_loss=2.925975

Batch 331010, train_perplexity=18.652409, train_loss=2.9259753

Batch 331020, train_perplexity=18.652405, train_loss=2.925975

Batch 331030, train_perplexity=18.652409, train_loss=2.9259753

Batch 331040, train_perplexity=18.652405, train_loss=2.925975
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 331050, train_perplexity=18.652401, train_loss=2.9259748

Batch 331060, train_perplexity=18.652401, train_loss=2.9259748

Batch 331070, train_perplexity=18.652405, train_loss=2.925975

Batch 331080, train_perplexity=18.652401, train_loss=2.9259748

Batch 331090, train_perplexity=18.652401, train_loss=2.9259748

Batch 331100, train_perplexity=18.652401, train_loss=2.9259748

Batch 331110, train_perplexity=18.652405, train_loss=2.925975

Batch 331120, train_perplexity=18.652409, train_loss=2.9259753

Batch 331130, train_perplexity=18.652409, train_loss=2.9259753

Batch 331140, train_perplexity=18.652401, train_loss=2.9259748

Batch 331150, train_perplexity=18.652401, train_loss=2.9259748

Batch 331160, train_perplexity=18.652405, train_loss=2.925975

Batch 331170, train_perplexity=18.652401, train_loss=2.9259748

Batch 331180, train_perplexity=18.652401, train_loss=2.9259748

Batch 331190, train_perplexity=18.652401, train_loss=2.9259748

Batch 331200, train_perplexity=18.652401, train_loss=2.9259748

Batch 331210, train_perplexity=18.652401, train_loss=2.9259748

Batch 331220, train_perplexity=18.652395, train_loss=2.9259746

Batch 331230, train_perplexity=18.652401, train_loss=2.9259748

Batch 331240, train_perplexity=18.652401, train_loss=2.9259748

Batch 331250, train_perplexity=18.652401, train_loss=2.9259748

Batch 331260, train_perplexity=18.652395, train_loss=2.9259746

Batch 331270, train_perplexity=18.652401, train_loss=2.9259748

Batch 331280, train_perplexity=18.652401, train_loss=2.9259748

Batch 331290, train_perplexity=18.652401, train_loss=2.9259748

Batch 331300, train_perplexity=18.652401, train_loss=2.9259748

Batch 331310, train_perplexity=18.652401, train_loss=2.9259748

Batch 331320, train_perplexity=18.652401, train_loss=2.9259748

Batch 331330, train_perplexity=18.652395, train_loss=2.9259746

Batch 331340, train_perplexity=18.652401, train_loss=2.9259748

Batch 331350, train_perplexity=18.652391, train_loss=2.9259744

Batch 331360, train_perplexity=18.652401, train_loss=2.9259748

Batch 331370, train_perplexity=18.652401, train_loss=2.9259748

Batch 331380, train_perplexity=18.652401, train_loss=2.9259748

Batch 331390, train_perplexity=18.652401, train_loss=2.9259748

Batch 331400, train_perplexity=18.652395, train_loss=2.9259746

Batch 331410, train_perplexity=18.652395, train_loss=2.9259746

Batch 331420, train_perplexity=18.652391, train_loss=2.9259744

Batch 331430, train_perplexity=18.652391, train_loss=2.9259744

Batch 331440, train_perplexity=18.652391, train_loss=2.9259744

Batch 331450, train_perplexity=18.652395, train_loss=2.9259746

Batch 331460, train_perplexity=18.652391, train_loss=2.9259744

Batch 331470, train_perplexity=18.652391, train_loss=2.9259744

Batch 331480, train_perplexity=18.652391, train_loss=2.9259744

Batch 331490, train_perplexity=18.652391, train_loss=2.9259744

Batch 331500, train_perplexity=18.652391, train_loss=2.9259744

Batch 331510, train_perplexity=18.652391, train_loss=2.9259744

Batch 331520, train_perplexity=18.652391, train_loss=2.9259744

Batch 331530, train_perplexity=18.652391, train_loss=2.9259744

Batch 331540, train_perplexity=18.652391, train_loss=2.9259744

Batch 331550, train_perplexity=18.652391, train_loss=2.9259744

Batch 331560, train_perplexity=18.652391, train_loss=2.9259744

Batch 331570, train_perplexity=18.652391, train_loss=2.9259744

Batch 331580, train_perplexity=18.652391, train_loss=2.9259744

Batch 331590, train_perplexity=18.652388, train_loss=2.9259741

Batch 331600, train_perplexity=18.652391, train_loss=2.9259744

Batch 331610, train_perplexity=18.652391, train_loss=2.9259744

Batch 331620, train_perplexity=18.652391, train_loss=2.9259744

Batch 331630, train_perplexity=18.652391, train_loss=2.9259744

Batch 331640, train_perplexity=18.652388, train_loss=2.9259741

Batch 331650, train_perplexity=18.652388, train_loss=2.9259741

Batch 331660, train_perplexity=18.652391, train_loss=2.9259744

Batch 331670, train_perplexity=18.652382, train_loss=2.925974

Batch 331680, train_perplexity=18.652388, train_loss=2.9259741

Batch 331690, train_perplexity=18.652391, train_loss=2.9259744

Batch 331700, train_perplexity=18.652388, train_loss=2.9259741

Batch 331710, train_perplexity=18.652382, train_loss=2.925974

Batch 331720, train_perplexity=18.652388, train_loss=2.9259741

Batch 331730, train_perplexity=18.652391, train_loss=2.9259744

Batch 331740, train_perplexity=18.652382, train_loss=2.925974

Batch 331750, train_perplexity=18.652391, train_loss=2.9259744

Batch 331760, train_perplexity=18.652382, train_loss=2.925974

Batch 331770, train_perplexity=18.652388, train_loss=2.9259741

Batch 331780, train_perplexity=18.652382, train_loss=2.925974

Batch 331790, train_perplexity=18.652382, train_loss=2.925974

Batch 331800, train_perplexity=18.652388, train_loss=2.9259741

Batch 331810, train_perplexity=18.652382, train_loss=2.925974

Batch 331820, train_perplexity=18.652382, train_loss=2.925974

Batch 331830, train_perplexity=18.652382, train_loss=2.925974

Batch 331840, train_perplexity=18.652382, train_loss=2.925974

Batch 331850, train_perplexity=18.652382, train_loss=2.925974

Batch 331860, train_perplexity=18.652382, train_loss=2.925974

Batch 331870, train_perplexity=18.652382, train_loss=2.925974

Batch 331880, train_perplexity=18.652382, train_loss=2.925974

Batch 331890, train_perplexity=18.652382, train_loss=2.925974

Batch 331900, train_perplexity=18.652382, train_loss=2.925974

Batch 331910, train_perplexity=18.652382, train_loss=2.925974

Batch 331920, train_perplexity=18.652382, train_loss=2.925974

Batch 331930, train_perplexity=18.652382, train_loss=2.925974

Batch 331940, train_perplexity=18.652382, train_loss=2.925974

Batch 331950, train_perplexity=18.652378, train_loss=2.9259737

Batch 331960, train_perplexity=18.652382, train_loss=2.925974

Batch 331970, train_perplexity=18.652378, train_loss=2.9259737

Batch 331980, train_perplexity=18.652382, train_loss=2.925974

Batch 331990, train_perplexity=18.652382, train_loss=2.925974

Batch 332000, train_perplexity=18.652374, train_loss=2.9259734

Batch 332010, train_perplexity=18.652374, train_loss=2.9259734

Batch 332020, train_perplexity=18.652374, train_loss=2.9259734

Batch 332030, train_perplexity=18.652374, train_loss=2.9259734

Batch 332040, train_perplexity=18.652382, train_loss=2.925974

Batch 332050, train_perplexity=18.652374, train_loss=2.9259734

Batch 332060, train_perplexity=18.652374, train_loss=2.9259734

Batch 332070, train_perplexity=18.652374, train_loss=2.9259734

Batch 332080, train_perplexity=18.652374, train_loss=2.9259734

Batch 332090, train_perplexity=18.652374, train_loss=2.9259734

Batch 332100, train_perplexity=18.652374, train_loss=2.9259734

Batch 332110, train_perplexity=18.652374, train_loss=2.9259734

Batch 332120, train_perplexity=18.652378, train_loss=2.9259737

Batch 332130, train_perplexity=18.652374, train_loss=2.9259734

Batch 332140, train_perplexity=18.652374, train_loss=2.9259734

Batch 332150, train_perplexity=18.652374, train_loss=2.9259734

Batch 332160, train_perplexity=18.652374, train_loss=2.9259734

Batch 332170, train_perplexity=18.652374, train_loss=2.9259734

Batch 332180, train_perplexity=18.652374, train_loss=2.9259734

Batch 332190, train_perplexity=18.652374, train_loss=2.9259734

Batch 332200, train_perplexity=18.652369, train_loss=2.9259732

Batch 332210, train_perplexity=18.652374, train_loss=2.9259734

Batch 332220, train_perplexity=18.652374, train_loss=2.9259734

Batch 332230, train_perplexity=18.652374, train_loss=2.9259734

Batch 332240, train_perplexity=18.652374, train_loss=2.9259734

Batch 332250, train_perplexity=18.652374, train_loss=2.9259734

Batch 332260, train_perplexity=18.652374, train_loss=2.9259734

Batch 332270, train_perplexity=18.652374, train_loss=2.9259734

Batch 332280, train_perplexity=18.652369, train_loss=2.9259732

Batch 332290, train_perplexity=18.652369, train_loss=2.9259732

Batch 332300, train_perplexity=18.652369, train_loss=2.9259732

Batch 332310, train_perplexity=18.652374, train_loss=2.9259734

Batch 332320, train_perplexity=18.652369, train_loss=2.9259732

Batch 332330, train_perplexity=18.652369, train_loss=2.9259732
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 332340, train_perplexity=18.652365, train_loss=2.925973

Batch 332350, train_perplexity=18.652369, train_loss=2.9259732

Batch 332360, train_perplexity=18.652365, train_loss=2.925973

Batch 332370, train_perplexity=18.652365, train_loss=2.925973

Batch 332380, train_perplexity=18.652365, train_loss=2.925973

Batch 332390, train_perplexity=18.652369, train_loss=2.9259732

Batch 332400, train_perplexity=18.652369, train_loss=2.9259732

Batch 332410, train_perplexity=18.652374, train_loss=2.9259734

Batch 332420, train_perplexity=18.652365, train_loss=2.925973

Batch 332430, train_perplexity=18.652365, train_loss=2.925973

Batch 332440, train_perplexity=18.652365, train_loss=2.925973

Batch 332450, train_perplexity=18.652365, train_loss=2.925973

Batch 332460, train_perplexity=18.652365, train_loss=2.925973

Batch 332470, train_perplexity=18.652365, train_loss=2.925973

Batch 332480, train_perplexity=18.652365, train_loss=2.925973

Batch 332490, train_perplexity=18.652365, train_loss=2.925973

Batch 332500, train_perplexity=18.652365, train_loss=2.925973

Batch 332510, train_perplexity=18.652365, train_loss=2.925973

Batch 332520, train_perplexity=18.652365, train_loss=2.925973

Batch 332530, train_perplexity=18.65236, train_loss=2.9259727

Batch 332540, train_perplexity=18.652365, train_loss=2.925973

Batch 332550, train_perplexity=18.652365, train_loss=2.925973

Batch 332560, train_perplexity=18.652365, train_loss=2.925973

Batch 332570, train_perplexity=18.652355, train_loss=2.9259725

Batch 332580, train_perplexity=18.652365, train_loss=2.925973

Batch 332590, train_perplexity=18.65236, train_loss=2.9259727

Batch 332600, train_perplexity=18.65236, train_loss=2.9259727

Batch 332610, train_perplexity=18.652365, train_loss=2.925973

Batch 332620, train_perplexity=18.652355, train_loss=2.9259725

Batch 332630, train_perplexity=18.652365, train_loss=2.925973

Batch 332640, train_perplexity=18.652355, train_loss=2.9259725

Batch 332650, train_perplexity=18.652355, train_loss=2.9259725

Batch 332660, train_perplexity=18.652365, train_loss=2.925973

Batch 332670, train_perplexity=18.65236, train_loss=2.9259727

Batch 332680, train_perplexity=18.652355, train_loss=2.9259725

Batch 332690, train_perplexity=18.652355, train_loss=2.9259725

Batch 332700, train_perplexity=18.652365, train_loss=2.925973

Batch 332710, train_perplexity=18.652355, train_loss=2.9259725

Batch 332720, train_perplexity=18.652355, train_loss=2.9259725

Batch 332730, train_perplexity=18.652355, train_loss=2.9259725

Batch 332740, train_perplexity=18.65236, train_loss=2.9259727

Batch 332750, train_perplexity=18.652355, train_loss=2.9259725

Batch 332760, train_perplexity=18.652355, train_loss=2.9259725

Batch 332770, train_perplexity=18.652355, train_loss=2.9259725

Batch 332780, train_perplexity=18.652355, train_loss=2.9259725

Batch 332790, train_perplexity=18.652355, train_loss=2.9259725

Batch 332800, train_perplexity=18.652351, train_loss=2.9259722

Batch 332810, train_perplexity=18.652355, train_loss=2.9259725

Batch 332820, train_perplexity=18.652355, train_loss=2.9259725

Batch 332830, train_perplexity=18.65236, train_loss=2.9259727

Batch 332840, train_perplexity=18.652355, train_loss=2.9259725

Batch 332850, train_perplexity=18.652355, train_loss=2.9259725

Batch 332860, train_perplexity=18.652348, train_loss=2.925972

Batch 332870, train_perplexity=18.652355, train_loss=2.9259725

Batch 332880, train_perplexity=18.652355, train_loss=2.9259725

Batch 332890, train_perplexity=18.652355, train_loss=2.9259725

Batch 332900, train_perplexity=18.652351, train_loss=2.9259722

Batch 332910, train_perplexity=18.652355, train_loss=2.9259725

Batch 332920, train_perplexity=18.652351, train_loss=2.9259722

Batch 332930, train_perplexity=18.652351, train_loss=2.9259722

Batch 332940, train_perplexity=18.652351, train_loss=2.9259722

Batch 332950, train_perplexity=18.652351, train_loss=2.9259722

Batch 332960, train_perplexity=18.652348, train_loss=2.925972

Batch 332970, train_perplexity=18.652348, train_loss=2.925972

Batch 332980, train_perplexity=18.652348, train_loss=2.925972

Batch 332990, train_perplexity=18.652351, train_loss=2.9259722

Batch 333000, train_perplexity=18.652351, train_loss=2.9259722

Batch 333010, train_perplexity=18.652351, train_loss=2.9259722

Batch 333020, train_perplexity=18.652348, train_loss=2.925972

Batch 333030, train_perplexity=18.652348, train_loss=2.925972

Batch 333040, train_perplexity=18.652348, train_loss=2.925972

Batch 333050, train_perplexity=18.652348, train_loss=2.925972

Batch 333060, train_perplexity=18.652348, train_loss=2.925972

Batch 333070, train_perplexity=18.652348, train_loss=2.925972

Batch 333080, train_perplexity=18.652348, train_loss=2.925972

Batch 333090, train_perplexity=18.652348, train_loss=2.925972

Batch 333100, train_perplexity=18.652348, train_loss=2.925972

Batch 333110, train_perplexity=18.652348, train_loss=2.925972

Batch 333120, train_perplexity=18.652348, train_loss=2.925972

Batch 333130, train_perplexity=18.652348, train_loss=2.925972

Batch 333140, train_perplexity=18.652348, train_loss=2.925972

Batch 333150, train_perplexity=18.652348, train_loss=2.925972

Batch 333160, train_perplexity=18.652348, train_loss=2.925972

Batch 333170, train_perplexity=18.652348, train_loss=2.925972

Batch 333180, train_perplexity=18.652348, train_loss=2.925972

Batch 333190, train_perplexity=18.652348, train_loss=2.925972

Batch 333200, train_perplexity=18.652348, train_loss=2.925972

Batch 333210, train_perplexity=18.652348, train_loss=2.925972

Batch 333220, train_perplexity=18.652348, train_loss=2.925972

Batch 333230, train_perplexity=18.652348, train_loss=2.925972

Batch 333240, train_perplexity=18.652348, train_loss=2.925972

Batch 333250, train_perplexity=18.652348, train_loss=2.925972

Batch 333260, train_perplexity=18.652342, train_loss=2.9259717

Batch 333270, train_perplexity=18.652348, train_loss=2.925972

Batch 333280, train_perplexity=18.652338, train_loss=2.9259715

Batch 333290, train_perplexity=18.652342, train_loss=2.9259717

Batch 333300, train_perplexity=18.652342, train_loss=2.9259717

Batch 333310, train_perplexity=18.652348, train_loss=2.925972

Batch 333320, train_perplexity=18.652338, train_loss=2.9259715

Batch 333330, train_perplexity=18.652342, train_loss=2.9259717

Batch 333340, train_perplexity=18.652338, train_loss=2.9259715

Batch 333350, train_perplexity=18.652338, train_loss=2.9259715

Batch 333360, train_perplexity=18.652338, train_loss=2.9259715

Batch 333370, train_perplexity=18.652338, train_loss=2.9259715

Batch 333380, train_perplexity=18.652338, train_loss=2.9259715

Batch 333390, train_perplexity=18.652338, train_loss=2.9259715

Batch 333400, train_perplexity=18.652338, train_loss=2.9259715

Batch 333410, train_perplexity=18.652342, train_loss=2.9259717

Batch 333420, train_perplexity=18.652338, train_loss=2.9259715

Batch 333430, train_perplexity=18.652338, train_loss=2.9259715

Batch 333440, train_perplexity=18.652338, train_loss=2.9259715

Batch 333450, train_perplexity=18.652338, train_loss=2.9259715

Batch 333460, train_perplexity=18.652338, train_loss=2.9259715

Batch 333470, train_perplexity=18.652338, train_loss=2.9259715

Batch 333480, train_perplexity=18.652342, train_loss=2.9259717

Batch 333490, train_perplexity=18.652338, train_loss=2.9259715

Batch 333500, train_perplexity=18.652338, train_loss=2.9259715

Batch 333510, train_perplexity=18.652338, train_loss=2.9259715

Batch 333520, train_perplexity=18.652338, train_loss=2.9259715

Batch 333530, train_perplexity=18.652334, train_loss=2.9259713

Batch 333540, train_perplexity=18.652338, train_loss=2.9259715

Batch 333550, train_perplexity=18.652334, train_loss=2.9259713

Batch 333560, train_perplexity=18.652328, train_loss=2.925971

Batch 333570, train_perplexity=18.652338, train_loss=2.9259715

Batch 333580, train_perplexity=18.652328, train_loss=2.925971

Batch 333590, train_perplexity=18.652328, train_loss=2.925971

Batch 333600, train_perplexity=18.652328, train_loss=2.925971

Batch 333610, train_perplexity=18.652328, train_loss=2.925971

Batch 333620, train_perplexity=18.652334, train_loss=2.9259713

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 333630, train_perplexity=18.652328, train_loss=2.925971

Batch 333640, train_perplexity=18.652328, train_loss=2.925971

Batch 333650, train_perplexity=18.652328, train_loss=2.925971

Batch 333660, train_perplexity=18.652328, train_loss=2.925971

Batch 333670, train_perplexity=18.652328, train_loss=2.925971

Batch 333680, train_perplexity=18.652328, train_loss=2.925971

Batch 333690, train_perplexity=18.652328, train_loss=2.925971

Batch 333700, train_perplexity=18.652328, train_loss=2.925971

Batch 333710, train_perplexity=18.652328, train_loss=2.925971

Batch 333720, train_perplexity=18.652328, train_loss=2.925971

Batch 333730, train_perplexity=18.652328, train_loss=2.925971

Batch 333740, train_perplexity=18.652328, train_loss=2.925971

Batch 333750, train_perplexity=18.652328, train_loss=2.925971

Batch 333760, train_perplexity=18.652328, train_loss=2.925971

Batch 333770, train_perplexity=18.652328, train_loss=2.925971

Batch 333780, train_perplexity=18.652328, train_loss=2.925971

Batch 333790, train_perplexity=18.652328, train_loss=2.925971

Batch 333800, train_perplexity=18.652328, train_loss=2.925971

Batch 333810, train_perplexity=18.652328, train_loss=2.925971

Batch 333820, train_perplexity=18.652328, train_loss=2.925971

Batch 333830, train_perplexity=18.652325, train_loss=2.9259708

Batch 333840, train_perplexity=18.652328, train_loss=2.925971

Batch 333850, train_perplexity=18.652328, train_loss=2.925971

Batch 333860, train_perplexity=18.65232, train_loss=2.9259706

Batch 333870, train_perplexity=18.652328, train_loss=2.925971

Batch 333880, train_perplexity=18.652325, train_loss=2.9259708

Batch 333890, train_perplexity=18.65232, train_loss=2.9259706

Batch 333900, train_perplexity=18.652325, train_loss=2.9259708

Batch 333910, train_perplexity=18.65232, train_loss=2.9259706

Batch 333920, train_perplexity=18.652325, train_loss=2.9259708

Batch 333930, train_perplexity=18.65232, train_loss=2.9259706

Batch 333940, train_perplexity=18.65232, train_loss=2.9259706

Batch 333950, train_perplexity=18.652325, train_loss=2.9259708

Batch 333960, train_perplexity=18.65232, train_loss=2.9259706

Batch 333970, train_perplexity=18.65232, train_loss=2.9259706

Batch 333980, train_perplexity=18.65232, train_loss=2.9259706

Batch 333990, train_perplexity=18.65232, train_loss=2.9259706

Batch 334000, train_perplexity=18.65232, train_loss=2.9259706

Batch 334010, train_perplexity=18.652325, train_loss=2.9259708

Batch 334020, train_perplexity=18.65232, train_loss=2.9259706

Batch 334030, train_perplexity=18.65232, train_loss=2.9259706

Batch 334040, train_perplexity=18.652325, train_loss=2.9259708

Batch 334050, train_perplexity=18.65232, train_loss=2.9259706

Batch 334060, train_perplexity=18.65232, train_loss=2.9259706

Batch 334070, train_perplexity=18.65232, train_loss=2.9259706

Batch 334080, train_perplexity=18.65232, train_loss=2.9259706

Batch 334090, train_perplexity=18.65232, train_loss=2.9259706

Batch 334100, train_perplexity=18.65232, train_loss=2.9259706

Batch 334110, train_perplexity=18.65232, train_loss=2.9259706

Batch 334120, train_perplexity=18.65232, train_loss=2.9259706

Batch 334130, train_perplexity=18.652315, train_loss=2.9259703

Batch 334140, train_perplexity=18.65232, train_loss=2.9259706

Batch 334150, train_perplexity=18.65232, train_loss=2.9259706

Batch 334160, train_perplexity=18.65232, train_loss=2.9259706

Batch 334170, train_perplexity=18.65232, train_loss=2.9259706

Batch 334180, train_perplexity=18.652315, train_loss=2.9259703

Batch 334190, train_perplexity=18.65232, train_loss=2.9259706

Batch 334200, train_perplexity=18.652315, train_loss=2.9259703

Batch 334210, train_perplexity=18.65232, train_loss=2.9259706

Batch 334220, train_perplexity=18.65232, train_loss=2.9259706

Batch 334230, train_perplexity=18.652315, train_loss=2.9259703

Batch 334240, train_perplexity=18.652315, train_loss=2.9259703

Batch 334250, train_perplexity=18.652315, train_loss=2.9259703

Batch 334260, train_perplexity=18.652311, train_loss=2.92597

Batch 334270, train_perplexity=18.652315, train_loss=2.9259703

Batch 334280, train_perplexity=18.652311, train_loss=2.92597

Batch 334290, train_perplexity=18.652311, train_loss=2.92597

Batch 334300, train_perplexity=18.652311, train_loss=2.92597

Batch 334310, train_perplexity=18.652311, train_loss=2.92597

Batch 334320, train_perplexity=18.652311, train_loss=2.92597

Batch 334330, train_perplexity=18.652311, train_loss=2.92597

Batch 334340, train_perplexity=18.652311, train_loss=2.92597

Batch 334350, train_perplexity=18.652311, train_loss=2.92597

Batch 334360, train_perplexity=18.652311, train_loss=2.92597

Batch 334370, train_perplexity=18.652311, train_loss=2.92597

Batch 334380, train_perplexity=18.652311, train_loss=2.92597

Batch 334390, train_perplexity=18.652311, train_loss=2.92597

Batch 334400, train_perplexity=18.652311, train_loss=2.92597

Batch 334410, train_perplexity=18.652311, train_loss=2.92597

Batch 334420, train_perplexity=18.652302, train_loss=2.9259696

Batch 334430, train_perplexity=18.652311, train_loss=2.92597

Batch 334440, train_perplexity=18.652311, train_loss=2.92597

Batch 334450, train_perplexity=18.652311, train_loss=2.92597

Batch 334460, train_perplexity=18.652311, train_loss=2.92597

Batch 334470, train_perplexity=18.652311, train_loss=2.92597

Batch 334480, train_perplexity=18.652308, train_loss=2.9259698

Batch 334490, train_perplexity=18.652311, train_loss=2.92597

Batch 334500, train_perplexity=18.652311, train_loss=2.92597

Batch 334510, train_perplexity=18.652311, train_loss=2.92597

Batch 334520, train_perplexity=18.652302, train_loss=2.9259696

Batch 334530, train_perplexity=18.652308, train_loss=2.9259698

Batch 334540, train_perplexity=18.652308, train_loss=2.9259698

Batch 334550, train_perplexity=18.652311, train_loss=2.92597

Batch 334560, train_perplexity=18.652302, train_loss=2.9259696

Batch 334570, train_perplexity=18.652302, train_loss=2.9259696

Batch 334580, train_perplexity=18.652302, train_loss=2.9259696

Batch 334590, train_perplexity=18.652302, train_loss=2.9259696

Batch 334600, train_perplexity=18.652302, train_loss=2.9259696

Batch 334610, train_perplexity=18.652302, train_loss=2.9259696

Batch 334620, train_perplexity=18.652302, train_loss=2.9259696

Batch 334630, train_perplexity=18.652302, train_loss=2.9259696

Batch 334640, train_perplexity=18.652298, train_loss=2.9259694

Batch 334650, train_perplexity=18.652302, train_loss=2.9259696

Batch 334660, train_perplexity=18.652302, train_loss=2.9259696

Batch 334670, train_perplexity=18.652302, train_loss=2.9259696

Batch 334680, train_perplexity=18.652302, train_loss=2.9259696

Batch 334690, train_perplexity=18.652302, train_loss=2.9259696

Batch 334700, train_perplexity=18.652302, train_loss=2.9259696

Batch 334710, train_perplexity=18.652302, train_loss=2.9259696

Batch 334720, train_perplexity=18.652302, train_loss=2.9259696

Batch 334730, train_perplexity=18.652302, train_loss=2.9259696

Batch 334740, train_perplexity=18.652302, train_loss=2.9259696

Batch 334750, train_perplexity=18.652302, train_loss=2.9259696

Batch 334760, train_perplexity=18.652298, train_loss=2.9259694

Batch 334770, train_perplexity=18.652302, train_loss=2.9259696

Batch 334780, train_perplexity=18.652298, train_loss=2.9259694

Batch 334790, train_perplexity=18.652298, train_loss=2.9259694

Batch 334800, train_perplexity=18.652298, train_loss=2.9259694

Batch 334810, train_perplexity=18.652302, train_loss=2.9259696

Batch 334820, train_perplexity=18.652294, train_loss=2.9259691

Batch 334830, train_perplexity=18.652298, train_loss=2.9259694

Batch 334840, train_perplexity=18.652298, train_loss=2.9259694

Batch 334850, train_perplexity=18.652298, train_loss=2.9259694

Batch 334860, train_perplexity=18.652294, train_loss=2.9259691

Batch 334870, train_perplexity=18.652298, train_loss=2.9259694

Batch 334880, train_perplexity=18.652294, train_loss=2.9259691

Batch 334890, train_perplexity=18.652302, train_loss=2.9259696

Batch 334900, train_perplexity=18.652294, train_loss=2.9259691

Batch 334910, train_perplexity=18.652294, train_loss=2.9259691

Batch 334920, train_perplexity=18.652294, train_loss=2.9259691
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 334930, train_perplexity=18.652294, train_loss=2.9259691

Batch 334940, train_perplexity=18.652298, train_loss=2.9259694

Batch 334950, train_perplexity=18.652294, train_loss=2.9259691

Batch 334960, train_perplexity=18.652294, train_loss=2.9259691

Batch 334970, train_perplexity=18.652294, train_loss=2.9259691

Batch 334980, train_perplexity=18.652294, train_loss=2.9259691

Batch 334990, train_perplexity=18.652294, train_loss=2.9259691

Batch 335000, train_perplexity=18.652294, train_loss=2.9259691

Batch 335010, train_perplexity=18.652294, train_loss=2.9259691

Batch 335020, train_perplexity=18.652294, train_loss=2.9259691

Batch 335030, train_perplexity=18.652294, train_loss=2.9259691

Batch 335040, train_perplexity=18.652294, train_loss=2.9259691

Batch 335050, train_perplexity=18.652294, train_loss=2.9259691

Batch 335060, train_perplexity=18.652294, train_loss=2.9259691

Batch 335070, train_perplexity=18.652294, train_loss=2.9259691

Batch 335080, train_perplexity=18.652285, train_loss=2.9259686

Batch 335090, train_perplexity=18.652294, train_loss=2.9259691

Batch 335100, train_perplexity=18.652294, train_loss=2.9259691

Batch 335110, train_perplexity=18.652294, train_loss=2.9259691

Batch 335120, train_perplexity=18.652294, train_loss=2.9259691

Batch 335130, train_perplexity=18.652294, train_loss=2.9259691

Batch 335140, train_perplexity=18.652285, train_loss=2.9259686

Batch 335150, train_perplexity=18.652285, train_loss=2.9259686

Batch 335160, train_perplexity=18.652294, train_loss=2.9259691

Batch 335170, train_perplexity=18.652285, train_loss=2.9259686

Batch 335180, train_perplexity=18.652285, train_loss=2.9259686

Batch 335190, train_perplexity=18.652294, train_loss=2.9259691

Batch 335200, train_perplexity=18.652288, train_loss=2.925969

Batch 335210, train_perplexity=18.652285, train_loss=2.9259686

Batch 335220, train_perplexity=18.652285, train_loss=2.9259686

Batch 335230, train_perplexity=18.652288, train_loss=2.925969

Batch 335240, train_perplexity=18.652285, train_loss=2.9259686

Batch 335250, train_perplexity=18.652285, train_loss=2.9259686

Batch 335260, train_perplexity=18.652285, train_loss=2.9259686

Batch 335270, train_perplexity=18.652285, train_loss=2.9259686

Batch 335280, train_perplexity=18.652285, train_loss=2.9259686

Batch 335290, train_perplexity=18.652285, train_loss=2.9259686

Batch 335300, train_perplexity=18.652285, train_loss=2.9259686

Batch 335310, train_perplexity=18.652285, train_loss=2.9259686

Batch 335320, train_perplexity=18.652285, train_loss=2.9259686

Batch 335330, train_perplexity=18.652285, train_loss=2.9259686

Batch 335340, train_perplexity=18.652285, train_loss=2.9259686

Batch 335350, train_perplexity=18.652275, train_loss=2.9259682

Batch 335360, train_perplexity=18.652285, train_loss=2.9259686

Batch 335370, train_perplexity=18.652285, train_loss=2.9259686

Batch 335380, train_perplexity=18.65228, train_loss=2.9259684

Batch 335390, train_perplexity=18.652285, train_loss=2.9259686

Batch 335400, train_perplexity=18.652285, train_loss=2.9259686

Batch 335410, train_perplexity=18.652285, train_loss=2.9259686

Batch 335420, train_perplexity=18.652285, train_loss=2.9259686

Batch 335430, train_perplexity=18.652285, train_loss=2.9259686

Batch 335440, train_perplexity=18.652285, train_loss=2.9259686

Batch 335450, train_perplexity=18.652285, train_loss=2.9259686

Batch 335460, train_perplexity=18.65228, train_loss=2.9259684

Batch 335470, train_perplexity=18.652275, train_loss=2.9259682

Batch 335480, train_perplexity=18.652275, train_loss=2.9259682

Batch 335490, train_perplexity=18.652275, train_loss=2.9259682

Batch 335500, train_perplexity=18.65228, train_loss=2.9259684

Batch 335510, train_perplexity=18.652285, train_loss=2.9259686

Batch 335520, train_perplexity=18.652275, train_loss=2.9259682

Batch 335530, train_perplexity=18.652275, train_loss=2.9259682

Batch 335540, train_perplexity=18.652275, train_loss=2.9259682

Batch 335550, train_perplexity=18.65228, train_loss=2.9259684

Batch 335560, train_perplexity=18.652275, train_loss=2.9259682

Batch 335570, train_perplexity=18.652275, train_loss=2.9259682

Batch 335580, train_perplexity=18.652275, train_loss=2.9259682

Batch 335590, train_perplexity=18.652275, train_loss=2.9259682

Batch 335600, train_perplexity=18.65228, train_loss=2.9259684

Batch 335610, train_perplexity=18.652275, train_loss=2.9259682

Batch 335620, train_perplexity=18.652275, train_loss=2.9259682

Batch 335630, train_perplexity=18.652275, train_loss=2.9259682

Batch 335640, train_perplexity=18.652275, train_loss=2.9259682

Batch 335650, train_perplexity=18.652275, train_loss=2.9259682

Batch 335660, train_perplexity=18.652275, train_loss=2.9259682

Batch 335670, train_perplexity=18.652275, train_loss=2.9259682

Batch 335680, train_perplexity=18.652275, train_loss=2.9259682

Batch 335690, train_perplexity=18.652275, train_loss=2.9259682

Batch 335700, train_perplexity=18.652275, train_loss=2.9259682

Batch 335710, train_perplexity=18.652275, train_loss=2.9259682

Batch 335720, train_perplexity=18.652275, train_loss=2.9259682

Batch 335730, train_perplexity=18.652271, train_loss=2.925968

Batch 335740, train_perplexity=18.652267, train_loss=2.9259677

Batch 335750, train_perplexity=18.652271, train_loss=2.925968

Batch 335760, train_perplexity=18.652267, train_loss=2.9259677

Batch 335770, train_perplexity=18.652275, train_loss=2.9259682

Batch 335780, train_perplexity=18.652275, train_loss=2.9259682

Batch 335790, train_perplexity=18.652275, train_loss=2.9259682

Batch 335800, train_perplexity=18.652275, train_loss=2.9259682

Batch 335810, train_perplexity=18.652275, train_loss=2.9259682

Batch 335820, train_perplexity=18.652267, train_loss=2.9259677

Batch 335830, train_perplexity=18.652271, train_loss=2.925968

Batch 335840, train_perplexity=18.652267, train_loss=2.9259677

Batch 335850, train_perplexity=18.652267, train_loss=2.9259677

Batch 335860, train_perplexity=18.652267, train_loss=2.9259677

Batch 335870, train_perplexity=18.652267, train_loss=2.9259677

Batch 335880, train_perplexity=18.652267, train_loss=2.9259677

Batch 335890, train_perplexity=18.652267, train_loss=2.9259677

Batch 335900, train_perplexity=18.652267, train_loss=2.9259677

Batch 335910, train_perplexity=18.652267, train_loss=2.9259677

Batch 335920, train_perplexity=18.652267, train_loss=2.9259677

Batch 335930, train_perplexity=18.652267, train_loss=2.9259677

Batch 335940, train_perplexity=18.652267, train_loss=2.9259677

Batch 335950, train_perplexity=18.652267, train_loss=2.9259677

Batch 335960, train_perplexity=18.652267, train_loss=2.9259677

Batch 335970, train_perplexity=18.652267, train_loss=2.9259677

Batch 335980, train_perplexity=18.652267, train_loss=2.9259677

Batch 335990, train_perplexity=18.652267, train_loss=2.9259677

Batch 336000, train_perplexity=18.652267, train_loss=2.9259677

Batch 336010, train_perplexity=18.652267, train_loss=2.9259677

Batch 336020, train_perplexity=18.652267, train_loss=2.9259677

Batch 336030, train_perplexity=18.652267, train_loss=2.9259677

Batch 336040, train_perplexity=18.652262, train_loss=2.9259675

Batch 336050, train_perplexity=18.652262, train_loss=2.9259675

Batch 336060, train_perplexity=18.652262, train_loss=2.9259675

Batch 336070, train_perplexity=18.652262, train_loss=2.9259675

Batch 336080, train_perplexity=18.652258, train_loss=2.9259672

Batch 336090, train_perplexity=18.652262, train_loss=2.9259675

Batch 336100, train_perplexity=18.652262, train_loss=2.9259675

Batch 336110, train_perplexity=18.652267, train_loss=2.9259677

Batch 336120, train_perplexity=18.652262, train_loss=2.9259675

Batch 336130, train_perplexity=18.652258, train_loss=2.9259672

Batch 336140, train_perplexity=18.652258, train_loss=2.9259672

Batch 336150, train_perplexity=18.652262, train_loss=2.9259675

Batch 336160, train_perplexity=18.652262, train_loss=2.9259675

Batch 336170, train_perplexity=18.652258, train_loss=2.9259672

Batch 336180, train_perplexity=18.652258, train_loss=2.9259672

Batch 336190, train_perplexity=18.652258, train_loss=2.9259672

Batch 336200, train_perplexity=18.652258, train_loss=2.9259672

Batch 336210, train_perplexity=18.652262, train_loss=2.9259675
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 336220, train_perplexity=18.652258, train_loss=2.9259672

Batch 336230, train_perplexity=18.652258, train_loss=2.9259672

Batch 336240, train_perplexity=18.652258, train_loss=2.9259672

Batch 336250, train_perplexity=18.652258, train_loss=2.9259672

Batch 336260, train_perplexity=18.652258, train_loss=2.9259672

Batch 336270, train_perplexity=18.652258, train_loss=2.9259672

Batch 336280, train_perplexity=18.652258, train_loss=2.9259672

Batch 336290, train_perplexity=18.652258, train_loss=2.9259672

Batch 336300, train_perplexity=18.652258, train_loss=2.9259672

Batch 336310, train_perplexity=18.652258, train_loss=2.9259672

Batch 336320, train_perplexity=18.652258, train_loss=2.9259672

Batch 336330, train_perplexity=18.652258, train_loss=2.9259672

Batch 336340, train_perplexity=18.652258, train_loss=2.9259672

Batch 336350, train_perplexity=18.652258, train_loss=2.9259672

Batch 336360, train_perplexity=18.652258, train_loss=2.9259672

Batch 336370, train_perplexity=18.652258, train_loss=2.9259672

Batch 336380, train_perplexity=18.652258, train_loss=2.9259672

Batch 336390, train_perplexity=18.652254, train_loss=2.925967

Batch 336400, train_perplexity=18.652254, train_loss=2.925967

Batch 336410, train_perplexity=18.652258, train_loss=2.9259672

Batch 336420, train_perplexity=18.652248, train_loss=2.9259667

Batch 336430, train_perplexity=18.652258, train_loss=2.9259672

Batch 336440, train_perplexity=18.652248, train_loss=2.9259667

Batch 336450, train_perplexity=18.652254, train_loss=2.925967

Batch 336460, train_perplexity=18.652248, train_loss=2.9259667

Batch 336470, train_perplexity=18.652254, train_loss=2.925967

Batch 336480, train_perplexity=18.652248, train_loss=2.9259667

Batch 336490, train_perplexity=18.652248, train_loss=2.9259667

Batch 336500, train_perplexity=18.652254, train_loss=2.925967

Batch 336510, train_perplexity=18.652254, train_loss=2.925967

Batch 336520, train_perplexity=18.652248, train_loss=2.9259667

Batch 336530, train_perplexity=18.652248, train_loss=2.9259667

Batch 336540, train_perplexity=18.652248, train_loss=2.9259667

Batch 336550, train_perplexity=18.652248, train_loss=2.9259667

Batch 336560, train_perplexity=18.652248, train_loss=2.9259667

Batch 336570, train_perplexity=18.652248, train_loss=2.9259667

Batch 336580, train_perplexity=18.652248, train_loss=2.9259667

Batch 336590, train_perplexity=18.652248, train_loss=2.9259667

Batch 336600, train_perplexity=18.652248, train_loss=2.9259667

Batch 336610, train_perplexity=18.652248, train_loss=2.9259667

Batch 336620, train_perplexity=18.652248, train_loss=2.9259667

Batch 336630, train_perplexity=18.652248, train_loss=2.9259667

Batch 336640, train_perplexity=18.652245, train_loss=2.9259665

Batch 336650, train_perplexity=18.652248, train_loss=2.9259667

Batch 336660, train_perplexity=18.652248, train_loss=2.9259667

Batch 336670, train_perplexity=18.652245, train_loss=2.9259665

Batch 336680, train_perplexity=18.652248, train_loss=2.9259667

Batch 336690, train_perplexity=18.652248, train_loss=2.9259667

Batch 336700, train_perplexity=18.652245, train_loss=2.9259665

Batch 336710, train_perplexity=18.652248, train_loss=2.9259667

Batch 336720, train_perplexity=18.652248, train_loss=2.9259667

Batch 336730, train_perplexity=18.65224, train_loss=2.9259663

Batch 336740, train_perplexity=18.65224, train_loss=2.9259663

Batch 336750, train_perplexity=18.652248, train_loss=2.9259667

Batch 336760, train_perplexity=18.652245, train_loss=2.9259665

Batch 336770, train_perplexity=18.652245, train_loss=2.9259665

Batch 336780, train_perplexity=18.65224, train_loss=2.9259663

Batch 336790, train_perplexity=18.65224, train_loss=2.9259663

Batch 336800, train_perplexity=18.652245, train_loss=2.9259665

Batch 336810, train_perplexity=18.65224, train_loss=2.9259663

Batch 336820, train_perplexity=18.65224, train_loss=2.9259663

Batch 336830, train_perplexity=18.65224, train_loss=2.9259663

Batch 336840, train_perplexity=18.65224, train_loss=2.9259663

Batch 336850, train_perplexity=18.65224, train_loss=2.9259663

Batch 336860, train_perplexity=18.65224, train_loss=2.9259663

Batch 336870, train_perplexity=18.65224, train_loss=2.9259663

Batch 336880, train_perplexity=18.652245, train_loss=2.9259665

Batch 336890, train_perplexity=18.65224, train_loss=2.9259663

Batch 336900, train_perplexity=18.65224, train_loss=2.9259663

Batch 336910, train_perplexity=18.65224, train_loss=2.9259663

Batch 336920, train_perplexity=18.65224, train_loss=2.9259663

Batch 336930, train_perplexity=18.65224, train_loss=2.9259663

Batch 336940, train_perplexity=18.65224, train_loss=2.9259663

Batch 336950, train_perplexity=18.65224, train_loss=2.9259663

Batch 336960, train_perplexity=18.652231, train_loss=2.9259658

Batch 336970, train_perplexity=18.65224, train_loss=2.9259663

Batch 336980, train_perplexity=18.65224, train_loss=2.9259663

Batch 336990, train_perplexity=18.652235, train_loss=2.925966

Batch 337000, train_perplexity=18.65224, train_loss=2.9259663

Batch 337010, train_perplexity=18.652231, train_loss=2.9259658

Batch 337020, train_perplexity=18.65224, train_loss=2.9259663

Batch 337030, train_perplexity=18.65224, train_loss=2.9259663

Batch 337040, train_perplexity=18.65224, train_loss=2.9259663

Batch 337050, train_perplexity=18.652235, train_loss=2.925966

Batch 337060, train_perplexity=18.652231, train_loss=2.9259658

Batch 337070, train_perplexity=18.652231, train_loss=2.9259658

Batch 337080, train_perplexity=18.652231, train_loss=2.9259658

Batch 337090, train_perplexity=18.652231, train_loss=2.9259658

Batch 337100, train_perplexity=18.652231, train_loss=2.9259658

Batch 337110, train_perplexity=18.652231, train_loss=2.9259658

Batch 337120, train_perplexity=18.652231, train_loss=2.9259658

Batch 337130, train_perplexity=18.652231, train_loss=2.9259658

Batch 337140, train_perplexity=18.652231, train_loss=2.9259658

Batch 337150, train_perplexity=18.652231, train_loss=2.9259658

Batch 337160, train_perplexity=18.652231, train_loss=2.9259658

Batch 337170, train_perplexity=18.652231, train_loss=2.9259658

Batch 337180, train_perplexity=18.652231, train_loss=2.9259658

Batch 337190, train_perplexity=18.652231, train_loss=2.9259658

Batch 337200, train_perplexity=18.652231, train_loss=2.9259658

Batch 337210, train_perplexity=18.652231, train_loss=2.9259658

Batch 337220, train_perplexity=18.652231, train_loss=2.9259658

Batch 337230, train_perplexity=18.652231, train_loss=2.9259658

Batch 337240, train_perplexity=18.652231, train_loss=2.9259658

Batch 337250, train_perplexity=18.652231, train_loss=2.9259658

Batch 337260, train_perplexity=18.652231, train_loss=2.9259658

Batch 337270, train_perplexity=18.652231, train_loss=2.9259658

Batch 337280, train_perplexity=18.652227, train_loss=2.9259655

Batch 337290, train_perplexity=18.652231, train_loss=2.9259658

Batch 337300, train_perplexity=18.652227, train_loss=2.9259655

Batch 337310, train_perplexity=18.652231, train_loss=2.9259658

Batch 337320, train_perplexity=18.652227, train_loss=2.9259655

Batch 337330, train_perplexity=18.652227, train_loss=2.9259655

Batch 337340, train_perplexity=18.652227, train_loss=2.9259655

Batch 337350, train_perplexity=18.652222, train_loss=2.9259653

Batch 337360, train_perplexity=18.652222, train_loss=2.9259653

Batch 337370, train_perplexity=18.652227, train_loss=2.9259655

Batch 337380, train_perplexity=18.652222, train_loss=2.9259653

Batch 337390, train_perplexity=18.652222, train_loss=2.9259653

Batch 337400, train_perplexity=18.652222, train_loss=2.9259653

Batch 337410, train_perplexity=18.652231, train_loss=2.9259658

Batch 337420, train_perplexity=18.652222, train_loss=2.9259653

Batch 337430, train_perplexity=18.652222, train_loss=2.9259653

Batch 337440, train_perplexity=18.652222, train_loss=2.9259653

Batch 337450, train_perplexity=18.652222, train_loss=2.9259653

Batch 337460, train_perplexity=18.652222, train_loss=2.9259653

Batch 337470, train_perplexity=18.652222, train_loss=2.9259653

Batch 337480, train_perplexity=18.652222, train_loss=2.9259653

Batch 337490, train_perplexity=18.652222, train_loss=2.9259653

Batch 337500, train_perplexity=18.652222, train_loss=2.9259653
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 337510, train_perplexity=18.652222, train_loss=2.9259653

Batch 337520, train_perplexity=18.652222, train_loss=2.9259653

Batch 337530, train_perplexity=18.652222, train_loss=2.9259653

Batch 337540, train_perplexity=18.652222, train_loss=2.9259653

Batch 337550, train_perplexity=18.652222, train_loss=2.9259653

Batch 337560, train_perplexity=18.652222, train_loss=2.9259653

Batch 337570, train_perplexity=18.652222, train_loss=2.9259653

Batch 337580, train_perplexity=18.652222, train_loss=2.9259653

Batch 337590, train_perplexity=18.652222, train_loss=2.9259653

Batch 337600, train_perplexity=18.652222, train_loss=2.9259653

Batch 337610, train_perplexity=18.652222, train_loss=2.9259653

Batch 337620, train_perplexity=18.652218, train_loss=2.925965

Batch 337630, train_perplexity=18.652222, train_loss=2.9259653

Batch 337640, train_perplexity=18.652222, train_loss=2.9259653

Batch 337650, train_perplexity=18.652222, train_loss=2.9259653

Batch 337660, train_perplexity=18.652214, train_loss=2.9259648

Batch 337670, train_perplexity=18.652214, train_loss=2.9259648

Batch 337680, train_perplexity=18.652222, train_loss=2.9259653

Batch 337690, train_perplexity=18.652214, train_loss=2.9259648

Batch 337700, train_perplexity=18.652214, train_loss=2.9259648

Batch 337710, train_perplexity=18.652222, train_loss=2.9259653

Batch 337720, train_perplexity=18.652214, train_loss=2.9259648

Batch 337730, train_perplexity=18.652218, train_loss=2.925965

Batch 337740, train_perplexity=18.652218, train_loss=2.925965

Batch 337750, train_perplexity=18.652214, train_loss=2.9259648

Batch 337760, train_perplexity=18.652214, train_loss=2.9259648

Batch 337770, train_perplexity=18.652214, train_loss=2.9259648

Batch 337780, train_perplexity=18.652214, train_loss=2.9259648

Batch 337790, train_perplexity=18.652218, train_loss=2.925965

Batch 337800, train_perplexity=18.652214, train_loss=2.9259648

Batch 337810, train_perplexity=18.652214, train_loss=2.9259648

Batch 337820, train_perplexity=18.652214, train_loss=2.9259648

Batch 337830, train_perplexity=18.652214, train_loss=2.9259648

Batch 337840, train_perplexity=18.652214, train_loss=2.9259648

Batch 337850, train_perplexity=18.652214, train_loss=2.9259648

Batch 337860, train_perplexity=18.652208, train_loss=2.9259646

Batch 337870, train_perplexity=18.652214, train_loss=2.9259648

Batch 337880, train_perplexity=18.652214, train_loss=2.9259648

Batch 337890, train_perplexity=18.652214, train_loss=2.9259648

Batch 337900, train_perplexity=18.652208, train_loss=2.9259646

Batch 337910, train_perplexity=18.652214, train_loss=2.9259648

Batch 337920, train_perplexity=18.652208, train_loss=2.9259646

Batch 337930, train_perplexity=18.652214, train_loss=2.9259648

Batch 337940, train_perplexity=18.652214, train_loss=2.9259648

Batch 337950, train_perplexity=18.652214, train_loss=2.9259648

Batch 337960, train_perplexity=18.652205, train_loss=2.9259644

Batch 337970, train_perplexity=18.652214, train_loss=2.9259648

Batch 337980, train_perplexity=18.652208, train_loss=2.9259646

Batch 337990, train_perplexity=18.652208, train_loss=2.9259646

Batch 338000, train_perplexity=18.652205, train_loss=2.9259644

Batch 338010, train_perplexity=18.652214, train_loss=2.9259648

Batch 338020, train_perplexity=18.652205, train_loss=2.9259644

Batch 338030, train_perplexity=18.652205, train_loss=2.9259644

Batch 338040, train_perplexity=18.652205, train_loss=2.9259644

Batch 338050, train_perplexity=18.652208, train_loss=2.9259646

Batch 338060, train_perplexity=18.652205, train_loss=2.9259644

Batch 338070, train_perplexity=18.652205, train_loss=2.9259644

Batch 338080, train_perplexity=18.652208, train_loss=2.9259646

Batch 338090, train_perplexity=18.652205, train_loss=2.9259644

Batch 338100, train_perplexity=18.652205, train_loss=2.9259644

Batch 338110, train_perplexity=18.652205, train_loss=2.9259644

Batch 338120, train_perplexity=18.652205, train_loss=2.9259644

Batch 338130, train_perplexity=18.652205, train_loss=2.9259644

Batch 338140, train_perplexity=18.6522, train_loss=2.925964

Batch 338150, train_perplexity=18.6522, train_loss=2.925964

Batch 338160, train_perplexity=18.652205, train_loss=2.9259644

Batch 338170, train_perplexity=18.652205, train_loss=2.9259644

Batch 338180, train_perplexity=18.652205, train_loss=2.9259644

Batch 338190, train_perplexity=18.652205, train_loss=2.9259644

Batch 338200, train_perplexity=18.652205, train_loss=2.9259644

Batch 338210, train_perplexity=18.652205, train_loss=2.9259644

Batch 338220, train_perplexity=18.652205, train_loss=2.9259644

Batch 338230, train_perplexity=18.6522, train_loss=2.925964

Batch 338240, train_perplexity=18.652205, train_loss=2.9259644

Batch 338250, train_perplexity=18.652205, train_loss=2.9259644

Batch 338260, train_perplexity=18.6522, train_loss=2.925964

Batch 338270, train_perplexity=18.652205, train_loss=2.9259644

Batch 338280, train_perplexity=18.652205, train_loss=2.9259644

Batch 338290, train_perplexity=18.652195, train_loss=2.9259639

Batch 338300, train_perplexity=18.652195, train_loss=2.9259639

Batch 338310, train_perplexity=18.652195, train_loss=2.9259639

Batch 338320, train_perplexity=18.6522, train_loss=2.925964

Batch 338330, train_perplexity=18.6522, train_loss=2.925964

Batch 338340, train_perplexity=18.652195, train_loss=2.9259639

Batch 338350, train_perplexity=18.652195, train_loss=2.9259639

Batch 338360, train_perplexity=18.652195, train_loss=2.9259639

Batch 338370, train_perplexity=18.652195, train_loss=2.9259639

Batch 338380, train_perplexity=18.652195, train_loss=2.9259639

Batch 338390, train_perplexity=18.652195, train_loss=2.9259639

Batch 338400, train_perplexity=18.652195, train_loss=2.9259639

Batch 338410, train_perplexity=18.652195, train_loss=2.9259639

Batch 338420, train_perplexity=18.652195, train_loss=2.9259639

Batch 338430, train_perplexity=18.6522, train_loss=2.925964

Batch 338440, train_perplexity=18.652195, train_loss=2.9259639

Batch 338450, train_perplexity=18.652195, train_loss=2.9259639

Batch 338460, train_perplexity=18.652195, train_loss=2.9259639

Batch 338470, train_perplexity=18.652195, train_loss=2.9259639

Batch 338480, train_perplexity=18.652195, train_loss=2.9259639

Batch 338490, train_perplexity=18.652195, train_loss=2.9259639

Batch 338500, train_perplexity=18.652195, train_loss=2.9259639

Batch 338510, train_perplexity=18.652195, train_loss=2.9259639

Batch 338520, train_perplexity=18.652195, train_loss=2.9259639

Batch 338530, train_perplexity=18.652195, train_loss=2.9259639

Batch 338540, train_perplexity=18.652195, train_loss=2.9259639

Batch 338550, train_perplexity=18.652195, train_loss=2.9259639

Batch 338560, train_perplexity=18.652195, train_loss=2.9259639

Batch 338570, train_perplexity=18.652191, train_loss=2.9259636

Batch 338580, train_perplexity=18.652191, train_loss=2.9259636

Batch 338590, train_perplexity=18.652195, train_loss=2.9259639

Batch 338600, train_perplexity=18.652195, train_loss=2.9259639

Batch 338610, train_perplexity=18.652187, train_loss=2.9259634

Batch 338620, train_perplexity=18.652187, train_loss=2.9259634

Batch 338630, train_perplexity=18.652187, train_loss=2.9259634

Batch 338640, train_perplexity=18.652187, train_loss=2.9259634

Batch 338650, train_perplexity=18.652191, train_loss=2.9259636

Batch 338660, train_perplexity=18.652191, train_loss=2.9259636

Batch 338670, train_perplexity=18.652187, train_loss=2.9259634

Batch 338680, train_perplexity=18.652191, train_loss=2.9259636

Batch 338690, train_perplexity=18.652187, train_loss=2.9259634

Batch 338700, train_perplexity=18.652187, train_loss=2.9259634

Batch 338710, train_perplexity=18.652187, train_loss=2.9259634

Batch 338720, train_perplexity=18.652191, train_loss=2.9259636

Batch 338730, train_perplexity=18.652187, train_loss=2.9259634

Batch 338740, train_perplexity=18.652187, train_loss=2.9259634

Batch 338750, train_perplexity=18.652187, train_loss=2.9259634

Batch 338760, train_perplexity=18.652187, train_loss=2.9259634

Batch 338770, train_perplexity=18.652187, train_loss=2.9259634

Batch 338780, train_perplexity=18.652187, train_loss=2.9259634

Batch 338790, train_perplexity=18.652187, train_loss=2.9259634
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 338800, train_perplexity=18.652187, train_loss=2.9259634

Batch 338810, train_perplexity=18.652187, train_loss=2.9259634

Batch 338820, train_perplexity=18.652187, train_loss=2.9259634

Batch 338830, train_perplexity=18.652187, train_loss=2.9259634

Batch 338840, train_perplexity=18.652187, train_loss=2.9259634

Batch 338850, train_perplexity=18.652182, train_loss=2.9259632

Batch 338860, train_perplexity=18.652178, train_loss=2.925963

Batch 338870, train_perplexity=18.652187, train_loss=2.9259634

Batch 338880, train_perplexity=18.652178, train_loss=2.925963

Batch 338890, train_perplexity=18.652182, train_loss=2.9259632

Batch 338900, train_perplexity=18.652187, train_loss=2.9259634

Batch 338910, train_perplexity=18.652187, train_loss=2.9259634

Batch 338920, train_perplexity=18.652187, train_loss=2.9259634

Batch 338930, train_perplexity=18.652187, train_loss=2.9259634

Batch 338940, train_perplexity=18.652182, train_loss=2.9259632

Batch 338950, train_perplexity=18.652178, train_loss=2.925963

Batch 338960, train_perplexity=18.652182, train_loss=2.9259632

Batch 338970, train_perplexity=18.652187, train_loss=2.9259634

Batch 338980, train_perplexity=18.652182, train_loss=2.9259632

Batch 338990, train_perplexity=18.652178, train_loss=2.925963

Batch 339000, train_perplexity=18.652178, train_loss=2.925963

Batch 339010, train_perplexity=18.652178, train_loss=2.925963

Batch 339020, train_perplexity=18.652182, train_loss=2.9259632

Batch 339030, train_perplexity=18.652178, train_loss=2.925963

Batch 339040, train_perplexity=18.652178, train_loss=2.925963

Batch 339050, train_perplexity=18.652178, train_loss=2.925963

Batch 339060, train_perplexity=18.652182, train_loss=2.9259632

Batch 339070, train_perplexity=18.652178, train_loss=2.925963

Batch 339080, train_perplexity=18.652178, train_loss=2.925963

Batch 339090, train_perplexity=18.652178, train_loss=2.925963

Batch 339100, train_perplexity=18.652174, train_loss=2.9259627

Batch 339110, train_perplexity=18.652178, train_loss=2.925963

Batch 339120, train_perplexity=18.652178, train_loss=2.925963

Batch 339130, train_perplexity=18.652178, train_loss=2.925963

Batch 339140, train_perplexity=18.652174, train_loss=2.9259627

Batch 339150, train_perplexity=18.652178, train_loss=2.925963

Batch 339160, train_perplexity=18.652178, train_loss=2.925963

Batch 339170, train_perplexity=18.652178, train_loss=2.925963

Batch 339180, train_perplexity=18.652178, train_loss=2.925963

Batch 339190, train_perplexity=18.652178, train_loss=2.925963

Batch 339200, train_perplexity=18.652178, train_loss=2.925963

Batch 339210, train_perplexity=18.652168, train_loss=2.9259624

Batch 339220, train_perplexity=18.652174, train_loss=2.9259627

Batch 339230, train_perplexity=18.652178, train_loss=2.925963

Batch 339240, train_perplexity=18.652168, train_loss=2.9259624

Batch 339250, train_perplexity=18.652174, train_loss=2.9259627

Batch 339260, train_perplexity=18.652174, train_loss=2.9259627

Batch 339270, train_perplexity=18.652174, train_loss=2.9259627

Batch 339280, train_perplexity=18.652174, train_loss=2.9259627

Batch 339290, train_perplexity=18.652174, train_loss=2.9259627

Batch 339300, train_perplexity=18.652174, train_loss=2.9259627

Batch 339310, train_perplexity=18.652168, train_loss=2.9259624

Batch 339320, train_perplexity=18.652168, train_loss=2.9259624

Batch 339330, train_perplexity=18.652168, train_loss=2.9259624

Batch 339340, train_perplexity=18.652168, train_loss=2.9259624

Batch 339350, train_perplexity=18.652168, train_loss=2.9259624

Batch 339360, train_perplexity=18.652164, train_loss=2.9259622

Batch 339370, train_perplexity=18.652174, train_loss=2.9259627

Batch 339380, train_perplexity=18.652168, train_loss=2.9259624

Batch 339390, train_perplexity=18.652168, train_loss=2.9259624

Batch 339400, train_perplexity=18.652168, train_loss=2.9259624

Batch 339410, train_perplexity=18.652168, train_loss=2.9259624

Batch 339420, train_perplexity=18.652168, train_loss=2.9259624

Batch 339430, train_perplexity=18.652168, train_loss=2.9259624

Batch 339440, train_perplexity=18.652164, train_loss=2.9259622

Batch 339450, train_perplexity=18.652168, train_loss=2.9259624

Batch 339460, train_perplexity=18.652168, train_loss=2.9259624

Batch 339470, train_perplexity=18.652168, train_loss=2.9259624

Batch 339480, train_perplexity=18.652168, train_loss=2.9259624

Batch 339490, train_perplexity=18.652168, train_loss=2.9259624

Batch 339500, train_perplexity=18.652164, train_loss=2.9259622

Batch 339510, train_perplexity=18.65216, train_loss=2.925962

Batch 339520, train_perplexity=18.652168, train_loss=2.9259624

Batch 339530, train_perplexity=18.652164, train_loss=2.9259622

Batch 339540, train_perplexity=18.652168, train_loss=2.9259624

Batch 339550, train_perplexity=18.65216, train_loss=2.925962

Batch 339560, train_perplexity=18.652164, train_loss=2.9259622

Batch 339570, train_perplexity=18.65216, train_loss=2.925962

Batch 339580, train_perplexity=18.652168, train_loss=2.9259624

Batch 339590, train_perplexity=18.65216, train_loss=2.925962

Batch 339600, train_perplexity=18.652168, train_loss=2.9259624

Batch 339610, train_perplexity=18.652168, train_loss=2.9259624

Batch 339620, train_perplexity=18.65216, train_loss=2.925962

Batch 339630, train_perplexity=18.65216, train_loss=2.925962

Batch 339640, train_perplexity=18.652164, train_loss=2.9259622

Batch 339650, train_perplexity=18.65216, train_loss=2.925962

Batch 339660, train_perplexity=18.65216, train_loss=2.925962

Batch 339670, train_perplexity=18.652164, train_loss=2.9259622

Batch 339680, train_perplexity=18.65216, train_loss=2.925962

Batch 339690, train_perplexity=18.65216, train_loss=2.925962

Batch 339700, train_perplexity=18.65216, train_loss=2.925962

Batch 339710, train_perplexity=18.65216, train_loss=2.925962

Batch 339720, train_perplexity=18.65216, train_loss=2.925962

Batch 339730, train_perplexity=18.65216, train_loss=2.925962

Batch 339740, train_perplexity=18.65216, train_loss=2.925962

Batch 339750, train_perplexity=18.65216, train_loss=2.925962

Batch 339760, train_perplexity=18.65216, train_loss=2.925962

Batch 339770, train_perplexity=18.65216, train_loss=2.925962

Batch 339780, train_perplexity=18.65216, train_loss=2.925962

Batch 339790, train_perplexity=18.65216, train_loss=2.925962

Batch 339800, train_perplexity=18.65216, train_loss=2.925962

Batch 339810, train_perplexity=18.65216, train_loss=2.925962

Batch 339820, train_perplexity=18.65216, train_loss=2.925962

Batch 339830, train_perplexity=18.652155, train_loss=2.9259617

Batch 339840, train_perplexity=18.652151, train_loss=2.9259615

Batch 339850, train_perplexity=18.652155, train_loss=2.9259617

Batch 339860, train_perplexity=18.652155, train_loss=2.9259617

Batch 339870, train_perplexity=18.652155, train_loss=2.9259617

Batch 339880, train_perplexity=18.65216, train_loss=2.925962

Batch 339890, train_perplexity=18.652155, train_loss=2.9259617

Batch 339900, train_perplexity=18.65216, train_loss=2.925962

Batch 339910, train_perplexity=18.652155, train_loss=2.9259617

Batch 339920, train_perplexity=18.652155, train_loss=2.9259617

Batch 339930, train_perplexity=18.652151, train_loss=2.9259615

Batch 339940, train_perplexity=18.652151, train_loss=2.9259615

Batch 339950, train_perplexity=18.652151, train_loss=2.9259615

Batch 339960, train_perplexity=18.652151, train_loss=2.9259615

Batch 339970, train_perplexity=18.652151, train_loss=2.9259615

Batch 339980, train_perplexity=18.652151, train_loss=2.9259615

Batch 339990, train_perplexity=18.652151, train_loss=2.9259615

Batch 340000, train_perplexity=18.652151, train_loss=2.9259615

Batch 340010, train_perplexity=18.652147, train_loss=2.9259613

Batch 340020, train_perplexity=18.652151, train_loss=2.9259615

Batch 340030, train_perplexity=18.652151, train_loss=2.9259615

Batch 340040, train_perplexity=18.652151, train_loss=2.9259615

Batch 340050, train_perplexity=18.652151, train_loss=2.9259615

Batch 340060, train_perplexity=18.652151, train_loss=2.9259615

Batch 340070, train_perplexity=18.652151, train_loss=2.9259615

Batch 340080, train_perplexity=18.652151, train_loss=2.9259615

Batch 340090, train_perplexity=18.652151, train_loss=2.9259615
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 340100, train_perplexity=18.652151, train_loss=2.9259615

Batch 340110, train_perplexity=18.652151, train_loss=2.9259615

Batch 340120, train_perplexity=18.652147, train_loss=2.9259613

Batch 340130, train_perplexity=18.652151, train_loss=2.9259615

Batch 340140, train_perplexity=18.652142, train_loss=2.925961

Batch 340150, train_perplexity=18.652151, train_loss=2.9259615

Batch 340160, train_perplexity=18.652151, train_loss=2.9259615

Batch 340170, train_perplexity=18.652142, train_loss=2.925961

Batch 340180, train_perplexity=18.652147, train_loss=2.9259613

Batch 340190, train_perplexity=18.652142, train_loss=2.925961

Batch 340200, train_perplexity=18.652142, train_loss=2.925961

Batch 340210, train_perplexity=18.652142, train_loss=2.925961

Batch 340220, train_perplexity=18.652142, train_loss=2.925961

Batch 340230, train_perplexity=18.652151, train_loss=2.9259615

Batch 340240, train_perplexity=18.652147, train_loss=2.9259613

Batch 340250, train_perplexity=18.652147, train_loss=2.9259613

Batch 340260, train_perplexity=18.652142, train_loss=2.925961

Batch 340270, train_perplexity=18.652142, train_loss=2.925961

Batch 340280, train_perplexity=18.652142, train_loss=2.925961

Batch 340290, train_perplexity=18.652142, train_loss=2.925961

Batch 340300, train_perplexity=18.652142, train_loss=2.925961

Batch 340310, train_perplexity=18.652142, train_loss=2.925961

Batch 340320, train_perplexity=18.652142, train_loss=2.925961

Batch 340330, train_perplexity=18.652142, train_loss=2.925961

Batch 340340, train_perplexity=18.652142, train_loss=2.925961

Batch 340350, train_perplexity=18.652142, train_loss=2.925961

Batch 340360, train_perplexity=18.652142, train_loss=2.925961

Batch 340370, train_perplexity=18.652142, train_loss=2.925961

Batch 340380, train_perplexity=18.652142, train_loss=2.925961

Batch 340390, train_perplexity=18.652142, train_loss=2.925961

Batch 340400, train_perplexity=18.652142, train_loss=2.925961

Batch 340410, train_perplexity=18.652138, train_loss=2.9259608

Batch 340420, train_perplexity=18.652142, train_loss=2.925961

Batch 340430, train_perplexity=18.652142, train_loss=2.925961

Batch 340440, train_perplexity=18.652142, train_loss=2.925961

Batch 340450, train_perplexity=18.652142, train_loss=2.925961

Batch 340460, train_perplexity=18.652142, train_loss=2.925961

Batch 340470, train_perplexity=18.652138, train_loss=2.9259608

Batch 340480, train_perplexity=18.652138, train_loss=2.9259608

Batch 340490, train_perplexity=18.652134, train_loss=2.9259605

Batch 340500, train_perplexity=18.652134, train_loss=2.9259605

Batch 340510, train_perplexity=18.652138, train_loss=2.9259608

Batch 340520, train_perplexity=18.652138, train_loss=2.9259608

Batch 340530, train_perplexity=18.652138, train_loss=2.9259608

Batch 340540, train_perplexity=18.652134, train_loss=2.9259605

Batch 340550, train_perplexity=18.652138, train_loss=2.9259608

Batch 340560, train_perplexity=18.652138, train_loss=2.9259608

Batch 340570, train_perplexity=18.652134, train_loss=2.9259605

Batch 340580, train_perplexity=18.652138, train_loss=2.9259608

Batch 340590, train_perplexity=18.652138, train_loss=2.9259608

Batch 340600, train_perplexity=18.652134, train_loss=2.9259605

Batch 340610, train_perplexity=18.652134, train_loss=2.9259605

Batch 340620, train_perplexity=18.652134, train_loss=2.9259605

Batch 340630, train_perplexity=18.652134, train_loss=2.9259605

Batch 340640, train_perplexity=18.652134, train_loss=2.9259605

Batch 340650, train_perplexity=18.652134, train_loss=2.9259605

Batch 340660, train_perplexity=18.652134, train_loss=2.9259605

Batch 340670, train_perplexity=18.652134, train_loss=2.9259605

Batch 340680, train_perplexity=18.652134, train_loss=2.9259605

Batch 340690, train_perplexity=18.652134, train_loss=2.9259605

Batch 340700, train_perplexity=18.652134, train_loss=2.9259605

Batch 340710, train_perplexity=18.652134, train_loss=2.9259605

Batch 340720, train_perplexity=18.652134, train_loss=2.9259605

Batch 340730, train_perplexity=18.652134, train_loss=2.9259605

Batch 340740, train_perplexity=18.652134, train_loss=2.9259605

Batch 340750, train_perplexity=18.652134, train_loss=2.9259605

Batch 340760, train_perplexity=18.652134, train_loss=2.9259605

Batch 340770, train_perplexity=18.652134, train_loss=2.9259605

Batch 340780, train_perplexity=18.652134, train_loss=2.9259605

Batch 340790, train_perplexity=18.652134, train_loss=2.9259605

Batch 340800, train_perplexity=18.652134, train_loss=2.9259605

Batch 340810, train_perplexity=18.652128, train_loss=2.9259603

Batch 340820, train_perplexity=18.652134, train_loss=2.9259605

Batch 340830, train_perplexity=18.652134, train_loss=2.9259605

Batch 340840, train_perplexity=18.652128, train_loss=2.9259603

Batch 340850, train_perplexity=18.652128, train_loss=2.9259603

Batch 340860, train_perplexity=18.652134, train_loss=2.9259605

Batch 340870, train_perplexity=18.652124, train_loss=2.92596

Batch 340880, train_perplexity=18.652124, train_loss=2.92596

Batch 340890, train_perplexity=18.652128, train_loss=2.9259603

Batch 340900, train_perplexity=18.652124, train_loss=2.92596

Batch 340910, train_perplexity=18.652124, train_loss=2.92596

Batch 340920, train_perplexity=18.652124, train_loss=2.92596

Batch 340930, train_perplexity=18.652128, train_loss=2.9259603

Batch 340940, train_perplexity=18.652124, train_loss=2.92596

Batch 340950, train_perplexity=18.652124, train_loss=2.92596

Batch 340960, train_perplexity=18.652124, train_loss=2.92596

Batch 340970, train_perplexity=18.652124, train_loss=2.92596

Batch 340980, train_perplexity=18.652124, train_loss=2.92596

Batch 340990, train_perplexity=18.652124, train_loss=2.92596

Batch 341000, train_perplexity=18.652124, train_loss=2.92596

Batch 341010, train_perplexity=18.652124, train_loss=2.92596

Batch 341020, train_perplexity=18.652124, train_loss=2.92596

Batch 341030, train_perplexity=18.652124, train_loss=2.92596

Batch 341040, train_perplexity=18.652124, train_loss=2.92596

Batch 341050, train_perplexity=18.65212, train_loss=2.9259598

Batch 341060, train_perplexity=18.652124, train_loss=2.92596

Batch 341070, train_perplexity=18.65212, train_loss=2.9259598

Batch 341080, train_perplexity=18.652124, train_loss=2.92596

Batch 341090, train_perplexity=18.652115, train_loss=2.9259596

Batch 341100, train_perplexity=18.652124, train_loss=2.92596

Batch 341110, train_perplexity=18.65212, train_loss=2.9259598

Batch 341120, train_perplexity=18.65212, train_loss=2.9259598

Batch 341130, train_perplexity=18.652115, train_loss=2.9259596

Batch 341140, train_perplexity=18.652124, train_loss=2.92596

Batch 341150, train_perplexity=18.652115, train_loss=2.9259596

Batch 341160, train_perplexity=18.652115, train_loss=2.9259596

Batch 341170, train_perplexity=18.652115, train_loss=2.9259596

Batch 341180, train_perplexity=18.652115, train_loss=2.9259596

Batch 341190, train_perplexity=18.65212, train_loss=2.9259598

Batch 341200, train_perplexity=18.652115, train_loss=2.9259596

Batch 341210, train_perplexity=18.652115, train_loss=2.9259596

Batch 341220, train_perplexity=18.652115, train_loss=2.9259596

Batch 341230, train_perplexity=18.652115, train_loss=2.9259596

Batch 341240, train_perplexity=18.652115, train_loss=2.9259596

Batch 341250, train_perplexity=18.652115, train_loss=2.9259596

Batch 341260, train_perplexity=18.652115, train_loss=2.9259596

Batch 341270, train_perplexity=18.652115, train_loss=2.9259596

Batch 341280, train_perplexity=18.652115, train_loss=2.9259596

Batch 341290, train_perplexity=18.652115, train_loss=2.9259596

Batch 341300, train_perplexity=18.652115, train_loss=2.9259596

Batch 341310, train_perplexity=18.652115, train_loss=2.9259596

Batch 341320, train_perplexity=18.652115, train_loss=2.9259596

Batch 341330, train_perplexity=18.652115, train_loss=2.9259596

Batch 341340, train_perplexity=18.652115, train_loss=2.9259596

Batch 341350, train_perplexity=18.652115, train_loss=2.9259596

Batch 341360, train_perplexity=18.652107, train_loss=2.925959

Batch 341370, train_perplexity=18.652115, train_loss=2.9259596

Batch 341380, train_perplexity=18.652115, train_loss=2.9259596

Batch 341390, train_perplexity=18.652115, train_loss=2.9259596
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 341400, train_perplexity=18.652107, train_loss=2.925959

Batch 341410, train_perplexity=18.652115, train_loss=2.9259596

Batch 341420, train_perplexity=18.652107, train_loss=2.925959

Batch 341430, train_perplexity=18.652111, train_loss=2.9259593

Batch 341440, train_perplexity=18.652111, train_loss=2.9259593

Batch 341450, train_perplexity=18.652111, train_loss=2.9259593

Batch 341460, train_perplexity=18.652107, train_loss=2.925959

Batch 341470, train_perplexity=18.652115, train_loss=2.9259596

Batch 341480, train_perplexity=18.652115, train_loss=2.9259596

Batch 341490, train_perplexity=18.652107, train_loss=2.925959

Batch 341500, train_perplexity=18.652107, train_loss=2.925959

Batch 341510, train_perplexity=18.652107, train_loss=2.925959

Batch 341520, train_perplexity=18.652107, train_loss=2.925959

Batch 341530, train_perplexity=18.652115, train_loss=2.9259596

Batch 341540, train_perplexity=18.652107, train_loss=2.925959

Batch 341550, train_perplexity=18.652107, train_loss=2.925959

Batch 341560, train_perplexity=18.652107, train_loss=2.925959

Batch 341570, train_perplexity=18.652107, train_loss=2.925959

Batch 341580, train_perplexity=18.652107, train_loss=2.925959

Batch 341590, train_perplexity=18.652107, train_loss=2.925959

Batch 341600, train_perplexity=18.652107, train_loss=2.925959

Batch 341610, train_perplexity=18.652107, train_loss=2.925959

Batch 341620, train_perplexity=18.652107, train_loss=2.925959

Batch 341630, train_perplexity=18.652107, train_loss=2.925959

Batch 341640, train_perplexity=18.652102, train_loss=2.9259589

Batch 341650, train_perplexity=18.652107, train_loss=2.925959

Batch 341660, train_perplexity=18.652107, train_loss=2.925959

Batch 341670, train_perplexity=18.652107, train_loss=2.925959

Batch 341680, train_perplexity=18.652107, train_loss=2.925959

Batch 341690, train_perplexity=18.652107, train_loss=2.925959

Batch 341700, train_perplexity=18.652107, train_loss=2.925959

Batch 341710, train_perplexity=18.652107, train_loss=2.925959

Batch 341720, train_perplexity=18.652107, train_loss=2.925959

Batch 341730, train_perplexity=18.652107, train_loss=2.925959

Batch 341740, train_perplexity=18.652102, train_loss=2.9259589

Batch 341750, train_perplexity=18.652107, train_loss=2.925959

Batch 341760, train_perplexity=18.652098, train_loss=2.9259586

Batch 341770, train_perplexity=18.652098, train_loss=2.9259586

Batch 341780, train_perplexity=18.652107, train_loss=2.925959

Batch 341790, train_perplexity=18.652107, train_loss=2.925959

Batch 341800, train_perplexity=18.652107, train_loss=2.925959

Batch 341810, train_perplexity=18.652102, train_loss=2.9259589

Batch 341820, train_perplexity=18.652098, train_loss=2.9259586

Batch 341830, train_perplexity=18.652098, train_loss=2.9259586

Batch 341840, train_perplexity=18.652102, train_loss=2.9259589

Batch 341850, train_perplexity=18.652098, train_loss=2.9259586

Batch 341860, train_perplexity=18.652102, train_loss=2.9259589

Batch 341870, train_perplexity=18.652098, train_loss=2.9259586

Batch 341880, train_perplexity=18.652098, train_loss=2.9259586

Batch 341890, train_perplexity=18.652098, train_loss=2.9259586

Batch 341900, train_perplexity=18.652098, train_loss=2.9259586

Batch 341910, train_perplexity=18.652098, train_loss=2.9259586

Batch 341920, train_perplexity=18.652098, train_loss=2.9259586

Batch 341930, train_perplexity=18.652098, train_loss=2.9259586

Batch 341940, train_perplexity=18.652098, train_loss=2.9259586

Batch 341950, train_perplexity=18.652098, train_loss=2.9259586

Batch 341960, train_perplexity=18.652098, train_loss=2.9259586

Batch 341970, train_perplexity=18.652098, train_loss=2.9259586

Batch 341980, train_perplexity=18.652098, train_loss=2.9259586

Batch 341990, train_perplexity=18.652098, train_loss=2.9259586

Batch 342000, train_perplexity=18.652098, train_loss=2.9259586

Batch 342010, train_perplexity=18.652098, train_loss=2.9259586

Batch 342020, train_perplexity=18.652098, train_loss=2.9259586

Batch 342030, train_perplexity=18.652098, train_loss=2.9259586

Batch 342040, train_perplexity=18.652098, train_loss=2.9259586

Batch 342050, train_perplexity=18.652098, train_loss=2.9259586

Batch 342060, train_perplexity=18.652088, train_loss=2.9259582

Batch 342070, train_perplexity=18.652088, train_loss=2.9259582

Batch 342080, train_perplexity=18.652088, train_loss=2.9259582

Batch 342090, train_perplexity=18.652088, train_loss=2.9259582

Batch 342100, train_perplexity=18.652094, train_loss=2.9259584

Batch 342110, train_perplexity=18.652088, train_loss=2.9259582

Batch 342120, train_perplexity=18.652088, train_loss=2.9259582

Batch 342130, train_perplexity=18.652094, train_loss=2.9259584

Batch 342140, train_perplexity=18.652088, train_loss=2.9259582

Batch 342150, train_perplexity=18.652094, train_loss=2.9259584

Batch 342160, train_perplexity=18.652088, train_loss=2.9259582

Batch 342170, train_perplexity=18.652088, train_loss=2.9259582

Batch 342180, train_perplexity=18.652088, train_loss=2.9259582

Batch 342190, train_perplexity=18.652088, train_loss=2.9259582

Batch 342200, train_perplexity=18.652088, train_loss=2.9259582

Batch 342210, train_perplexity=18.652088, train_loss=2.9259582

Batch 342220, train_perplexity=18.652088, train_loss=2.9259582

Batch 342230, train_perplexity=18.652088, train_loss=2.9259582

Batch 342240, train_perplexity=18.652088, train_loss=2.9259582

Batch 342250, train_perplexity=18.652088, train_loss=2.9259582

Batch 342260, train_perplexity=18.652088, train_loss=2.9259582

Batch 342270, train_perplexity=18.652088, train_loss=2.9259582

Batch 342280, train_perplexity=18.652088, train_loss=2.9259582

Batch 342290, train_perplexity=18.652088, train_loss=2.9259582

Batch 342300, train_perplexity=18.65208, train_loss=2.9259577

Batch 342310, train_perplexity=18.652088, train_loss=2.9259582

Batch 342320, train_perplexity=18.652088, train_loss=2.9259582

Batch 342330, train_perplexity=18.652084, train_loss=2.925958

Batch 342340, train_perplexity=18.652088, train_loss=2.9259582

Batch 342350, train_perplexity=18.652088, train_loss=2.9259582

Batch 342360, train_perplexity=18.652088, train_loss=2.9259582

Batch 342370, train_perplexity=18.652084, train_loss=2.925958

Batch 342380, train_perplexity=18.652084, train_loss=2.925958

Batch 342390, train_perplexity=18.652084, train_loss=2.925958

Batch 342400, train_perplexity=18.65208, train_loss=2.9259577

Batch 342410, train_perplexity=18.652088, train_loss=2.9259582

Batch 342420, train_perplexity=18.652084, train_loss=2.925958

Batch 342430, train_perplexity=18.65208, train_loss=2.9259577

Batch 342440, train_perplexity=18.652084, train_loss=2.925958

Batch 342450, train_perplexity=18.652084, train_loss=2.925958

Batch 342460, train_perplexity=18.65208, train_loss=2.9259577

Batch 342470, train_perplexity=18.65208, train_loss=2.9259577

Batch 342480, train_perplexity=18.65208, train_loss=2.9259577

Batch 342490, train_perplexity=18.652088, train_loss=2.9259582

Batch 342500, train_perplexity=18.65208, train_loss=2.9259577

Batch 342510, train_perplexity=18.65208, train_loss=2.9259577

Batch 342520, train_perplexity=18.65208, train_loss=2.9259577

Batch 342530, train_perplexity=18.65208, train_loss=2.9259577

Batch 342540, train_perplexity=18.65208, train_loss=2.9259577

Batch 342550, train_perplexity=18.65208, train_loss=2.9259577

Batch 342560, train_perplexity=18.65208, train_loss=2.9259577

Batch 342570, train_perplexity=18.65208, train_loss=2.9259577

Batch 342580, train_perplexity=18.65208, train_loss=2.9259577

Batch 342590, train_perplexity=18.65208, train_loss=2.9259577

Batch 342600, train_perplexity=18.65208, train_loss=2.9259577

Batch 342610, train_perplexity=18.65208, train_loss=2.9259577

Batch 342620, train_perplexity=18.65208, train_loss=2.9259577

Batch 342630, train_perplexity=18.65208, train_loss=2.9259577

Batch 342640, train_perplexity=18.65208, train_loss=2.9259577

Batch 342650, train_perplexity=18.65208, train_loss=2.9259577

Batch 342660, train_perplexity=18.65208, train_loss=2.9259577

Batch 342670, train_perplexity=18.65208, train_loss=2.9259577

Batch 342680, train_perplexity=18.65208, train_loss=2.9259577
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 342690, train_perplexity=18.652077, train_loss=2.9259574

Batch 342700, train_perplexity=18.652077, train_loss=2.9259574

Batch 342710, train_perplexity=18.65208, train_loss=2.9259577

Batch 342720, train_perplexity=18.65208, train_loss=2.9259577

Batch 342730, train_perplexity=18.652077, train_loss=2.9259574

Batch 342740, train_perplexity=18.652077, train_loss=2.9259574

Batch 342750, train_perplexity=18.652077, train_loss=2.9259574

Batch 342760, train_perplexity=18.652077, train_loss=2.9259574

Batch 342770, train_perplexity=18.65208, train_loss=2.9259577

Batch 342780, train_perplexity=18.652077, train_loss=2.9259574

Batch 342790, train_perplexity=18.652077, train_loss=2.9259574

Batch 342800, train_perplexity=18.652071, train_loss=2.9259572

Batch 342810, train_perplexity=18.652077, train_loss=2.9259574

Batch 342820, train_perplexity=18.65208, train_loss=2.9259577

Batch 342830, train_perplexity=18.652071, train_loss=2.9259572

Batch 342840, train_perplexity=18.652071, train_loss=2.9259572

Batch 342850, train_perplexity=18.652077, train_loss=2.9259574

Batch 342860, train_perplexity=18.652071, train_loss=2.9259572

Batch 342870, train_perplexity=18.652071, train_loss=2.9259572

Batch 342880, train_perplexity=18.652071, train_loss=2.9259572

Batch 342890, train_perplexity=18.652071, train_loss=2.9259572

Batch 342900, train_perplexity=18.652071, train_loss=2.9259572

Batch 342910, train_perplexity=18.652071, train_loss=2.9259572

Batch 342920, train_perplexity=18.652071, train_loss=2.9259572

Batch 342930, train_perplexity=18.652067, train_loss=2.925957

Batch 342940, train_perplexity=18.652071, train_loss=2.9259572

Batch 342950, train_perplexity=18.652067, train_loss=2.925957

Batch 342960, train_perplexity=18.652071, train_loss=2.9259572

Batch 342970, train_perplexity=18.652071, train_loss=2.9259572

Batch 342980, train_perplexity=18.652067, train_loss=2.925957

Batch 342990, train_perplexity=18.652067, train_loss=2.925957

Batch 343000, train_perplexity=18.652071, train_loss=2.9259572

Batch 343010, train_perplexity=18.652063, train_loss=2.9259567

Batch 343020, train_perplexity=18.652071, train_loss=2.9259572

Batch 343030, train_perplexity=18.652063, train_loss=2.9259567

Batch 343040, train_perplexity=18.652063, train_loss=2.9259567

Batch 343050, train_perplexity=18.652063, train_loss=2.9259567

Batch 343060, train_perplexity=18.652063, train_loss=2.9259567

Batch 343070, train_perplexity=18.652063, train_loss=2.9259567

Batch 343080, train_perplexity=18.652067, train_loss=2.925957

Batch 343090, train_perplexity=18.652063, train_loss=2.9259567

Batch 343100, train_perplexity=18.652067, train_loss=2.925957

Batch 343110, train_perplexity=18.652063, train_loss=2.9259567

Batch 343120, train_perplexity=18.652063, train_loss=2.9259567

Batch 343130, train_perplexity=18.652067, train_loss=2.925957

Batch 343140, train_perplexity=18.652063, train_loss=2.9259567

Batch 343150, train_perplexity=18.652063, train_loss=2.9259567

Batch 343160, train_perplexity=18.652063, train_loss=2.9259567

Batch 343170, train_perplexity=18.652063, train_loss=2.9259567

Batch 343180, train_perplexity=18.652063, train_loss=2.9259567

Batch 343190, train_perplexity=18.652063, train_loss=2.9259567

Batch 343200, train_perplexity=18.652058, train_loss=2.9259565

Batch 343210, train_perplexity=18.652063, train_loss=2.9259567

Batch 343220, train_perplexity=18.652063, train_loss=2.9259567

Batch 343230, train_perplexity=18.652063, train_loss=2.9259567

Batch 343240, train_perplexity=18.652063, train_loss=2.9259567

Batch 343250, train_perplexity=18.652063, train_loss=2.9259567

Batch 343260, train_perplexity=18.652063, train_loss=2.9259567

Batch 343270, train_perplexity=18.652058, train_loss=2.9259565

Batch 343280, train_perplexity=18.652063, train_loss=2.9259567

Batch 343290, train_perplexity=18.652054, train_loss=2.9259562

Batch 343300, train_perplexity=18.652063, train_loss=2.9259567

Batch 343310, train_perplexity=18.652063, train_loss=2.9259567

Batch 343320, train_perplexity=18.652063, train_loss=2.9259567

Batch 343330, train_perplexity=18.652058, train_loss=2.9259565

Batch 343340, train_perplexity=18.652063, train_loss=2.9259567

Batch 343350, train_perplexity=18.652063, train_loss=2.9259567

Batch 343360, train_perplexity=18.652058, train_loss=2.9259565

Batch 343370, train_perplexity=18.652063, train_loss=2.9259567

Batch 343380, train_perplexity=18.652054, train_loss=2.9259562

Batch 343390, train_perplexity=18.652058, train_loss=2.9259565

Batch 343400, train_perplexity=18.652054, train_loss=2.9259562

Batch 343410, train_perplexity=18.652054, train_loss=2.9259562

Batch 343420, train_perplexity=18.652054, train_loss=2.9259562

Batch 343430, train_perplexity=18.652054, train_loss=2.9259562

Batch 343440, train_perplexity=18.652054, train_loss=2.9259562

Batch 343450, train_perplexity=18.652054, train_loss=2.9259562

Batch 343460, train_perplexity=18.652054, train_loss=2.9259562

Batch 343470, train_perplexity=18.652054, train_loss=2.9259562

Batch 343480, train_perplexity=18.652054, train_loss=2.9259562

Batch 343490, train_perplexity=18.652054, train_loss=2.9259562

Batch 343500, train_perplexity=18.652054, train_loss=2.9259562

Batch 343510, train_perplexity=18.652054, train_loss=2.9259562

Batch 343520, train_perplexity=18.65205, train_loss=2.925956

Batch 343530, train_perplexity=18.65205, train_loss=2.925956

Batch 343540, train_perplexity=18.652054, train_loss=2.9259562

Batch 343550, train_perplexity=18.652054, train_loss=2.9259562

Batch 343560, train_perplexity=18.652054, train_loss=2.9259562

Batch 343570, train_perplexity=18.65205, train_loss=2.925956

Batch 343580, train_perplexity=18.652054, train_loss=2.9259562

Batch 343590, train_perplexity=18.652054, train_loss=2.9259562

Batch 343600, train_perplexity=18.652054, train_loss=2.9259562

Batch 343610, train_perplexity=18.65205, train_loss=2.925956

Batch 343620, train_perplexity=18.652054, train_loss=2.9259562

Batch 343630, train_perplexity=18.65205, train_loss=2.925956

Batch 343640, train_perplexity=18.652044, train_loss=2.9259558

Batch 343650, train_perplexity=18.65205, train_loss=2.925956

Batch 343660, train_perplexity=18.65205, train_loss=2.925956

Batch 343670, train_perplexity=18.652054, train_loss=2.9259562

Batch 343680, train_perplexity=18.65205, train_loss=2.925956

Batch 343690, train_perplexity=18.652044, train_loss=2.9259558

Batch 343700, train_perplexity=18.652054, train_loss=2.9259562

Batch 343710, train_perplexity=18.652044, train_loss=2.9259558

Batch 343720, train_perplexity=18.65205, train_loss=2.925956

Batch 343730, train_perplexity=18.652044, train_loss=2.9259558

Batch 343740, train_perplexity=18.652044, train_loss=2.9259558

Batch 343750, train_perplexity=18.652054, train_loss=2.9259562

Batch 343760, train_perplexity=18.65205, train_loss=2.925956

Batch 343770, train_perplexity=18.652044, train_loss=2.9259558

Batch 343780, train_perplexity=18.652044, train_loss=2.9259558

Batch 343790, train_perplexity=18.652044, train_loss=2.9259558

Batch 343800, train_perplexity=18.652044, train_loss=2.9259558

Batch 343810, train_perplexity=18.652044, train_loss=2.9259558

Batch 343820, train_perplexity=18.652044, train_loss=2.9259558

Batch 343830, train_perplexity=18.65204, train_loss=2.9259555

Batch 343840, train_perplexity=18.652044, train_loss=2.9259558

Batch 343850, train_perplexity=18.652044, train_loss=2.9259558

Batch 343860, train_perplexity=18.652044, train_loss=2.9259558

Batch 343870, train_perplexity=18.652044, train_loss=2.9259558

Batch 343880, train_perplexity=18.652044, train_loss=2.9259558

Batch 343890, train_perplexity=18.652044, train_loss=2.9259558

Batch 343900, train_perplexity=18.65204, train_loss=2.9259555

Batch 343910, train_perplexity=18.652044, train_loss=2.9259558

Batch 343920, train_perplexity=18.65204, train_loss=2.9259555

Batch 343930, train_perplexity=18.652044, train_loss=2.9259558

Batch 343940, train_perplexity=18.65204, train_loss=2.9259555

Batch 343950, train_perplexity=18.652044, train_loss=2.9259558

Batch 343960, train_perplexity=18.652044, train_loss=2.9259558

Batch 343970, train_perplexity=18.652037, train_loss=2.9259553
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 343980, train_perplexity=18.65204, train_loss=2.9259555

Batch 343990, train_perplexity=18.652044, train_loss=2.9259558

Batch 344000, train_perplexity=18.652037, train_loss=2.9259553

Batch 344010, train_perplexity=18.65204, train_loss=2.9259555

Batch 344020, train_perplexity=18.652037, train_loss=2.9259553

Batch 344030, train_perplexity=18.652037, train_loss=2.9259553

Batch 344040, train_perplexity=18.652037, train_loss=2.9259553

Batch 344050, train_perplexity=18.652037, train_loss=2.9259553

Batch 344060, train_perplexity=18.652037, train_loss=2.9259553

Batch 344070, train_perplexity=18.652037, train_loss=2.9259553

Batch 344080, train_perplexity=18.652037, train_loss=2.9259553

Batch 344090, train_perplexity=18.652037, train_loss=2.9259553

Batch 344100, train_perplexity=18.652037, train_loss=2.9259553

Batch 344110, train_perplexity=18.652037, train_loss=2.9259553

Batch 344120, train_perplexity=18.652037, train_loss=2.9259553

Batch 344130, train_perplexity=18.652037, train_loss=2.9259553

Batch 344140, train_perplexity=18.652037, train_loss=2.9259553

Batch 344150, train_perplexity=18.652037, train_loss=2.9259553

Batch 344160, train_perplexity=18.652037, train_loss=2.9259553

Batch 344170, train_perplexity=18.652037, train_loss=2.9259553

Batch 344180, train_perplexity=18.652037, train_loss=2.9259553

Batch 344190, train_perplexity=18.652037, train_loss=2.9259553

Batch 344200, train_perplexity=18.652037, train_loss=2.9259553

Batch 344210, train_perplexity=18.652037, train_loss=2.9259553

Batch 344220, train_perplexity=18.65203, train_loss=2.925955

Batch 344230, train_perplexity=18.652037, train_loss=2.9259553

Batch 344240, train_perplexity=18.652037, train_loss=2.9259553

Batch 344250, train_perplexity=18.652037, train_loss=2.9259553

Batch 344260, train_perplexity=18.652037, train_loss=2.9259553

Batch 344270, train_perplexity=18.65203, train_loss=2.925955

Batch 344280, train_perplexity=18.652027, train_loss=2.9259548

Batch 344290, train_perplexity=18.65203, train_loss=2.925955

Batch 344300, train_perplexity=18.652027, train_loss=2.9259548

Batch 344310, train_perplexity=18.65203, train_loss=2.925955

Batch 344320, train_perplexity=18.652027, train_loss=2.9259548

Batch 344330, train_perplexity=18.652027, train_loss=2.9259548

Batch 344340, train_perplexity=18.65203, train_loss=2.925955

Batch 344350, train_perplexity=18.65203, train_loss=2.925955

Batch 344360, train_perplexity=18.652027, train_loss=2.9259548

Batch 344370, train_perplexity=18.652027, train_loss=2.9259548

Batch 344380, train_perplexity=18.65203, train_loss=2.925955

Batch 344390, train_perplexity=18.652027, train_loss=2.9259548

Batch 344400, train_perplexity=18.652027, train_loss=2.9259548

Batch 344410, train_perplexity=18.652027, train_loss=2.9259548

Batch 344420, train_perplexity=18.652027, train_loss=2.9259548

Batch 344430, train_perplexity=18.652027, train_loss=2.9259548

Batch 344440, train_perplexity=18.652027, train_loss=2.9259548

Batch 344450, train_perplexity=18.652027, train_loss=2.9259548

Batch 344460, train_perplexity=18.652027, train_loss=2.9259548

Batch 344470, train_perplexity=18.652027, train_loss=2.9259548

Batch 344480, train_perplexity=18.652027, train_loss=2.9259548

Batch 344490, train_perplexity=18.652027, train_loss=2.9259548

Batch 344500, train_perplexity=18.652027, train_loss=2.9259548

Batch 344510, train_perplexity=18.652027, train_loss=2.9259548

Batch 344520, train_perplexity=18.652027, train_loss=2.9259548

Batch 344530, train_perplexity=18.652027, train_loss=2.9259548

Batch 344540, train_perplexity=18.652027, train_loss=2.9259548

Batch 344550, train_perplexity=18.652027, train_loss=2.9259548

Batch 344560, train_perplexity=18.652027, train_loss=2.9259548

Batch 344570, train_perplexity=18.652018, train_loss=2.9259543

Batch 344580, train_perplexity=18.652027, train_loss=2.9259548

Batch 344590, train_perplexity=18.652018, train_loss=2.9259543

Batch 344600, train_perplexity=18.652018, train_loss=2.9259543

Batch 344610, train_perplexity=18.652018, train_loss=2.9259543

Batch 344620, train_perplexity=18.652027, train_loss=2.9259548

Batch 344630, train_perplexity=18.652018, train_loss=2.9259543

Batch 344640, train_perplexity=18.652018, train_loss=2.9259543

Batch 344650, train_perplexity=18.652018, train_loss=2.9259543

Batch 344660, train_perplexity=18.652018, train_loss=2.9259543

Batch 344670, train_perplexity=18.652023, train_loss=2.9259546

Batch 344680, train_perplexity=18.652018, train_loss=2.9259543

Batch 344690, train_perplexity=18.652018, train_loss=2.9259543

Batch 344700, train_perplexity=18.652018, train_loss=2.9259543

Batch 344710, train_perplexity=18.652018, train_loss=2.9259543

Batch 344720, train_perplexity=18.652018, train_loss=2.9259543

Batch 344730, train_perplexity=18.652018, train_loss=2.9259543

Batch 344740, train_perplexity=18.652018, train_loss=2.9259543

Batch 344750, train_perplexity=18.652018, train_loss=2.9259543

Batch 344760, train_perplexity=18.652018, train_loss=2.9259543

Batch 344770, train_perplexity=18.652018, train_loss=2.9259543

Batch 344780, train_perplexity=18.652018, train_loss=2.9259543

Batch 344790, train_perplexity=18.652018, train_loss=2.9259543

Batch 344800, train_perplexity=18.652014, train_loss=2.925954

Batch 344810, train_perplexity=18.652018, train_loss=2.9259543

Batch 344820, train_perplexity=18.652018, train_loss=2.9259543

Batch 344830, train_perplexity=18.652018, train_loss=2.9259543

Batch 344840, train_perplexity=18.652014, train_loss=2.925954

Batch 344850, train_perplexity=18.652018, train_loss=2.9259543

Batch 344860, train_perplexity=18.652018, train_loss=2.9259543

Batch 344870, train_perplexity=18.65201, train_loss=2.9259539

Batch 344880, train_perplexity=18.652018, train_loss=2.9259543

Batch 344890, train_perplexity=18.652014, train_loss=2.925954

Batch 344900, train_perplexity=18.652014, train_loss=2.925954

Batch 344910, train_perplexity=18.652018, train_loss=2.9259543

Batch 344920, train_perplexity=18.65201, train_loss=2.9259539

Batch 344930, train_perplexity=18.652014, train_loss=2.925954

Batch 344940, train_perplexity=18.65201, train_loss=2.9259539

Batch 344950, train_perplexity=18.652018, train_loss=2.9259543

Batch 344960, train_perplexity=18.652014, train_loss=2.925954

Batch 344970, train_perplexity=18.65201, train_loss=2.9259539

Batch 344980, train_perplexity=18.65201, train_loss=2.9259539

Batch 344990, train_perplexity=18.65201, train_loss=2.9259539

Batch 345000, train_perplexity=18.652014, train_loss=2.925954

Batch 345010, train_perplexity=18.652014, train_loss=2.925954

Batch 345020, train_perplexity=18.65201, train_loss=2.9259539

Batch 345030, train_perplexity=18.65201, train_loss=2.9259539

Batch 345040, train_perplexity=18.65201, train_loss=2.9259539

Batch 345050, train_perplexity=18.65201, train_loss=2.9259539

Batch 345060, train_perplexity=18.65201, train_loss=2.9259539

Batch 345070, train_perplexity=18.65201, train_loss=2.9259539

Batch 345080, train_perplexity=18.65201, train_loss=2.9259539

Batch 345090, train_perplexity=18.65201, train_loss=2.9259539

Batch 345100, train_perplexity=18.652004, train_loss=2.9259536

Batch 345110, train_perplexity=18.65201, train_loss=2.9259539

Batch 345120, train_perplexity=18.65201, train_loss=2.9259539

Batch 345130, train_perplexity=18.65201, train_loss=2.9259539

Batch 345140, train_perplexity=18.65201, train_loss=2.9259539

Batch 345150, train_perplexity=18.65201, train_loss=2.9259539

Batch 345160, train_perplexity=18.652, train_loss=2.9259534

Batch 345170, train_perplexity=18.652004, train_loss=2.9259536

Batch 345180, train_perplexity=18.652, train_loss=2.9259534

Batch 345190, train_perplexity=18.65201, train_loss=2.9259539

Batch 345200, train_perplexity=18.652, train_loss=2.9259534

Batch 345210, train_perplexity=18.652, train_loss=2.9259534

Batch 345220, train_perplexity=18.65201, train_loss=2.9259539

Batch 345230, train_perplexity=18.652004, train_loss=2.9259536

Batch 345240, train_perplexity=18.652, train_loss=2.9259534

Batch 345250, train_perplexity=18.65201, train_loss=2.9259539

Batch 345260, train_perplexity=18.652004, train_loss=2.9259536
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 345270, train_perplexity=18.652, train_loss=2.9259534

Batch 345280, train_perplexity=18.652, train_loss=2.9259534

Batch 345290, train_perplexity=18.652, train_loss=2.9259534

Batch 345300, train_perplexity=18.652004, train_loss=2.9259536

Batch 345310, train_perplexity=18.652, train_loss=2.9259534

Batch 345320, train_perplexity=18.652004, train_loss=2.9259536

Batch 345330, train_perplexity=18.652, train_loss=2.9259534

Batch 345340, train_perplexity=18.652, train_loss=2.9259534

Batch 345350, train_perplexity=18.652, train_loss=2.9259534

Batch 345360, train_perplexity=18.652004, train_loss=2.9259536

Batch 345370, train_perplexity=18.652, train_loss=2.9259534

Batch 345380, train_perplexity=18.652, train_loss=2.9259534

Batch 345390, train_perplexity=18.652, train_loss=2.9259534

Batch 345400, train_perplexity=18.652, train_loss=2.9259534

Batch 345410, train_perplexity=18.652004, train_loss=2.9259536

Batch 345420, train_perplexity=18.652, train_loss=2.9259534

Batch 345430, train_perplexity=18.652, train_loss=2.9259534

Batch 345440, train_perplexity=18.652, train_loss=2.9259534

Batch 345450, train_perplexity=18.652, train_loss=2.9259534

Batch 345460, train_perplexity=18.652, train_loss=2.9259534

Batch 345470, train_perplexity=18.652, train_loss=2.9259534

Batch 345480, train_perplexity=18.652, train_loss=2.9259534

Batch 345490, train_perplexity=18.652, train_loss=2.9259534

Batch 345500, train_perplexity=18.651997, train_loss=2.9259531

Batch 345510, train_perplexity=18.651997, train_loss=2.9259531

Batch 345520, train_perplexity=18.652, train_loss=2.9259534

Batch 345530, train_perplexity=18.652, train_loss=2.9259534

Batch 345540, train_perplexity=18.652, train_loss=2.9259534

Batch 345550, train_perplexity=18.651997, train_loss=2.9259531

Batch 345560, train_perplexity=18.65199, train_loss=2.925953

Batch 345570, train_perplexity=18.65199, train_loss=2.925953

Batch 345580, train_perplexity=18.65199, train_loss=2.925953

Batch 345590, train_perplexity=18.651997, train_loss=2.9259531

Batch 345600, train_perplexity=18.65199, train_loss=2.925953

Batch 345610, train_perplexity=18.651997, train_loss=2.9259531

Batch 345620, train_perplexity=18.651997, train_loss=2.9259531

Batch 345630, train_perplexity=18.65199, train_loss=2.925953

Batch 345640, train_perplexity=18.65199, train_loss=2.925953

Batch 345650, train_perplexity=18.65199, train_loss=2.925953

Batch 345660, train_perplexity=18.651983, train_loss=2.9259524

Batch 345670, train_perplexity=18.65199, train_loss=2.925953

Batch 345680, train_perplexity=18.65199, train_loss=2.925953

Batch 345690, train_perplexity=18.65199, train_loss=2.925953

Batch 345700, train_perplexity=18.65199, train_loss=2.925953

Batch 345710, train_perplexity=18.65199, train_loss=2.925953

Batch 345720, train_perplexity=18.65199, train_loss=2.925953

Batch 345730, train_perplexity=18.65199, train_loss=2.925953

Batch 345740, train_perplexity=18.651987, train_loss=2.9259527

Batch 345750, train_perplexity=18.65199, train_loss=2.925953

Batch 345760, train_perplexity=18.65199, train_loss=2.925953

Batch 345770, train_perplexity=18.65199, train_loss=2.925953

Batch 345780, train_perplexity=18.65199, train_loss=2.925953

Batch 345790, train_perplexity=18.651987, train_loss=2.9259527

Batch 345800, train_perplexity=18.65199, train_loss=2.925953

Batch 345810, train_perplexity=18.65199, train_loss=2.925953

Batch 345820, train_perplexity=18.651987, train_loss=2.9259527

Batch 345830, train_perplexity=18.651987, train_loss=2.9259527

Batch 345840, train_perplexity=18.65199, train_loss=2.925953

Batch 345850, train_perplexity=18.651987, train_loss=2.9259527

Batch 345860, train_perplexity=18.65199, train_loss=2.925953

Batch 345870, train_perplexity=18.651983, train_loss=2.9259524

Batch 345880, train_perplexity=18.651983, train_loss=2.9259524

Batch 345890, train_perplexity=18.651987, train_loss=2.9259527

Batch 345900, train_perplexity=18.651983, train_loss=2.9259524

Batch 345910, train_perplexity=18.651983, train_loss=2.9259524

Batch 345920, train_perplexity=18.651983, train_loss=2.9259524

Batch 345930, train_perplexity=18.651987, train_loss=2.9259527

Batch 345940, train_perplexity=18.651983, train_loss=2.9259524

Batch 345950, train_perplexity=18.651983, train_loss=2.9259524

Batch 345960, train_perplexity=18.651983, train_loss=2.9259524

Batch 345970, train_perplexity=18.651983, train_loss=2.9259524

Batch 345980, train_perplexity=18.651983, train_loss=2.9259524

Batch 345990, train_perplexity=18.651983, train_loss=2.9259524

Batch 346000, train_perplexity=18.651983, train_loss=2.9259524

Batch 346010, train_perplexity=18.651983, train_loss=2.9259524

Batch 346020, train_perplexity=18.651983, train_loss=2.9259524

Batch 346030, train_perplexity=18.651983, train_loss=2.9259524

Batch 346040, train_perplexity=18.651983, train_loss=2.9259524

Batch 346050, train_perplexity=18.651983, train_loss=2.9259524

Batch 346060, train_perplexity=18.651983, train_loss=2.9259524

Batch 346070, train_perplexity=18.651983, train_loss=2.9259524

Batch 346080, train_perplexity=18.651983, train_loss=2.9259524

Batch 346090, train_perplexity=18.651983, train_loss=2.9259524

Batch 346100, train_perplexity=18.651983, train_loss=2.9259524

Batch 346110, train_perplexity=18.651978, train_loss=2.9259522

Batch 346120, train_perplexity=18.651983, train_loss=2.9259524

Batch 346130, train_perplexity=18.651978, train_loss=2.9259522

Batch 346140, train_perplexity=18.651983, train_loss=2.9259524

Batch 346150, train_perplexity=18.651983, train_loss=2.9259524

Batch 346160, train_perplexity=18.651978, train_loss=2.9259522

Batch 346170, train_perplexity=18.651978, train_loss=2.9259522

Batch 346180, train_perplexity=18.651983, train_loss=2.9259524

Batch 346190, train_perplexity=18.651978, train_loss=2.9259522

Batch 346200, train_perplexity=18.651974, train_loss=2.925952

Batch 346210, train_perplexity=18.651974, train_loss=2.925952

Batch 346220, train_perplexity=18.651978, train_loss=2.9259522

Batch 346230, train_perplexity=18.651983, train_loss=2.9259524

Batch 346240, train_perplexity=18.651974, train_loss=2.925952

Batch 346250, train_perplexity=18.651974, train_loss=2.925952

Batch 346260, train_perplexity=18.651978, train_loss=2.9259522

Batch 346270, train_perplexity=18.651974, train_loss=2.925952

Batch 346280, train_perplexity=18.651974, train_loss=2.925952

Batch 346290, train_perplexity=18.651974, train_loss=2.925952

Batch 346300, train_perplexity=18.651974, train_loss=2.925952

Batch 346310, train_perplexity=18.651974, train_loss=2.925952

Batch 346320, train_perplexity=18.651974, train_loss=2.925952

Batch 346330, train_perplexity=18.651974, train_loss=2.925952

Batch 346340, train_perplexity=18.651974, train_loss=2.925952

Batch 346350, train_perplexity=18.651974, train_loss=2.925952

Batch 346360, train_perplexity=18.651974, train_loss=2.925952

Batch 346370, train_perplexity=18.65197, train_loss=2.9259517

Batch 346380, train_perplexity=18.651974, train_loss=2.925952

Batch 346390, train_perplexity=18.651974, train_loss=2.925952

Batch 346400, train_perplexity=18.651974, train_loss=2.925952

Batch 346410, train_perplexity=18.651974, train_loss=2.925952

Batch 346420, train_perplexity=18.651974, train_loss=2.925952

Batch 346430, train_perplexity=18.651974, train_loss=2.925952

Batch 346440, train_perplexity=18.65197, train_loss=2.9259517

Batch 346450, train_perplexity=18.651974, train_loss=2.925952

Batch 346460, train_perplexity=18.651974, train_loss=2.925952

Batch 346470, train_perplexity=18.651974, train_loss=2.925952

Batch 346480, train_perplexity=18.651974, train_loss=2.925952

Batch 346490, train_perplexity=18.65197, train_loss=2.9259517

Batch 346500, train_perplexity=18.651974, train_loss=2.925952

Batch 346510, train_perplexity=18.651964, train_loss=2.9259515

Batch 346520, train_perplexity=18.651964, train_loss=2.9259515

Batch 346530, train_perplexity=18.651964, train_loss=2.9259515

Batch 346540, train_perplexity=18.65197, train_loss=2.9259517

Batch 346550, train_perplexity=18.651964, train_loss=2.9259515

Batch 346560, train_perplexity=18.651964, train_loss=2.9259515

Batch 346570, train_perplexity=18.651964, train_loss=2.9259515
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 346580, train_perplexity=18.651964, train_loss=2.9259515

Batch 346590, train_perplexity=18.651964, train_loss=2.9259515

Batch 346600, train_perplexity=18.651964, train_loss=2.9259515

Batch 346610, train_perplexity=18.651964, train_loss=2.9259515

Batch 346620, train_perplexity=18.651964, train_loss=2.9259515

Batch 346630, train_perplexity=18.651964, train_loss=2.9259515

Batch 346640, train_perplexity=18.651964, train_loss=2.9259515

Batch 346650, train_perplexity=18.651964, train_loss=2.9259515

Batch 346660, train_perplexity=18.651964, train_loss=2.9259515

Batch 346670, train_perplexity=18.651964, train_loss=2.9259515

Batch 346680, train_perplexity=18.651964, train_loss=2.9259515

Batch 346690, train_perplexity=18.651964, train_loss=2.9259515

Batch 346700, train_perplexity=18.651964, train_loss=2.9259515

Batch 346710, train_perplexity=18.651964, train_loss=2.9259515

Batch 346720, train_perplexity=18.65196, train_loss=2.9259512

Batch 346730, train_perplexity=18.651964, train_loss=2.9259515

Batch 346740, train_perplexity=18.65196, train_loss=2.9259512

Batch 346750, train_perplexity=18.651964, train_loss=2.9259515

Batch 346760, train_perplexity=18.65196, train_loss=2.9259512

Batch 346770, train_perplexity=18.651964, train_loss=2.9259515

Batch 346780, train_perplexity=18.651964, train_loss=2.9259515

Batch 346790, train_perplexity=18.65196, train_loss=2.9259512

Batch 346800, train_perplexity=18.651957, train_loss=2.925951

Batch 346810, train_perplexity=18.65196, train_loss=2.9259512

Batch 346820, train_perplexity=18.651957, train_loss=2.925951

Batch 346830, train_perplexity=18.65196, train_loss=2.9259512

Batch 346840, train_perplexity=18.65196, train_loss=2.9259512

Batch 346850, train_perplexity=18.65196, train_loss=2.9259512

Batch 346860, train_perplexity=18.651957, train_loss=2.925951

Batch 346870, train_perplexity=18.651957, train_loss=2.925951

Batch 346880, train_perplexity=18.65196, train_loss=2.9259512

Batch 346890, train_perplexity=18.651957, train_loss=2.925951

Batch 346900, train_perplexity=18.651957, train_loss=2.925951

Batch 346910, train_perplexity=18.651957, train_loss=2.925951

Batch 346920, train_perplexity=18.65196, train_loss=2.9259512

Batch 346930, train_perplexity=18.65196, train_loss=2.9259512

Batch 346940, train_perplexity=18.651957, train_loss=2.925951

Batch 346950, train_perplexity=18.651957, train_loss=2.925951

Batch 346960, train_perplexity=18.651957, train_loss=2.925951

Batch 346970, train_perplexity=18.651957, train_loss=2.925951

Batch 346980, train_perplexity=18.651957, train_loss=2.925951

Batch 346990, train_perplexity=18.651957, train_loss=2.925951

Batch 347000, train_perplexity=18.651957, train_loss=2.925951

Batch 347010, train_perplexity=18.651957, train_loss=2.925951

Batch 347020, train_perplexity=18.651957, train_loss=2.925951

Batch 347030, train_perplexity=18.651957, train_loss=2.925951

Batch 347040, train_perplexity=18.651957, train_loss=2.925951

Batch 347050, train_perplexity=18.651957, train_loss=2.925951

Batch 347060, train_perplexity=18.651957, train_loss=2.925951

Batch 347070, train_perplexity=18.651957, train_loss=2.925951

Batch 347080, train_perplexity=18.651957, train_loss=2.925951

Batch 347090, train_perplexity=18.651957, train_loss=2.925951

Batch 347100, train_perplexity=18.651957, train_loss=2.925951

Batch 347110, train_perplexity=18.651947, train_loss=2.9259505

Batch 347120, train_perplexity=18.651957, train_loss=2.925951

Batch 347130, train_perplexity=18.651957, train_loss=2.925951

Batch 347140, train_perplexity=18.651957, train_loss=2.925951

Batch 347150, train_perplexity=18.651947, train_loss=2.9259505

Batch 347160, train_perplexity=18.651957, train_loss=2.925951

Batch 347170, train_perplexity=18.65195, train_loss=2.9259508

Batch 347180, train_perplexity=18.651947, train_loss=2.9259505

Batch 347190, train_perplexity=18.65195, train_loss=2.9259508

Batch 347200, train_perplexity=18.65195, train_loss=2.9259508

Batch 347210, train_perplexity=18.651947, train_loss=2.9259505

Batch 347220, train_perplexity=18.651947, train_loss=2.9259505

Batch 347230, train_perplexity=18.651947, train_loss=2.9259505

Batch 347240, train_perplexity=18.651947, train_loss=2.9259505

Batch 347250, train_perplexity=18.651947, train_loss=2.9259505

Batch 347260, train_perplexity=18.651947, train_loss=2.9259505

Batch 347270, train_perplexity=18.651947, train_loss=2.9259505

Batch 347280, train_perplexity=18.651943, train_loss=2.9259503

Batch 347290, train_perplexity=18.651947, train_loss=2.9259505

Batch 347300, train_perplexity=18.651947, train_loss=2.9259505

Batch 347310, train_perplexity=18.651947, train_loss=2.9259505

Batch 347320, train_perplexity=18.651947, train_loss=2.9259505

Batch 347330, train_perplexity=18.651947, train_loss=2.9259505

Batch 347340, train_perplexity=18.651947, train_loss=2.9259505

Batch 347350, train_perplexity=18.651943, train_loss=2.9259503

Batch 347360, train_perplexity=18.651947, train_loss=2.9259505

Batch 347370, train_perplexity=18.651947, train_loss=2.9259505

Batch 347380, train_perplexity=18.651947, train_loss=2.9259505

Batch 347390, train_perplexity=18.651947, train_loss=2.9259505

Batch 347400, train_perplexity=18.651947, train_loss=2.9259505

Batch 347410, train_perplexity=18.651943, train_loss=2.9259503

Batch 347420, train_perplexity=18.651943, train_loss=2.9259503

Batch 347430, train_perplexity=18.651943, train_loss=2.9259503

Batch 347440, train_perplexity=18.651943, train_loss=2.9259503

Batch 347450, train_perplexity=18.651943, train_loss=2.9259503

Batch 347460, train_perplexity=18.651947, train_loss=2.9259505

Batch 347470, train_perplexity=18.651943, train_loss=2.9259503

Batch 347480, train_perplexity=18.651937, train_loss=2.92595

Batch 347490, train_perplexity=18.651937, train_loss=2.92595

Batch 347500, train_perplexity=18.651943, train_loss=2.9259503

Batch 347510, train_perplexity=18.651937, train_loss=2.92595

Batch 347520, train_perplexity=18.651937, train_loss=2.92595

Batch 347530, train_perplexity=18.651937, train_loss=2.92595

Batch 347540, train_perplexity=18.651937, train_loss=2.92595

Batch 347550, train_perplexity=18.651943, train_loss=2.9259503

Batch 347560, train_perplexity=18.651937, train_loss=2.92595

Batch 347570, train_perplexity=18.651937, train_loss=2.92595

Batch 347580, train_perplexity=18.651937, train_loss=2.92595

Batch 347590, train_perplexity=18.651937, train_loss=2.92595

Batch 347600, train_perplexity=18.651937, train_loss=2.92595

Batch 347610, train_perplexity=18.651937, train_loss=2.92595

Batch 347620, train_perplexity=18.651937, train_loss=2.92595

Batch 347630, train_perplexity=18.651937, train_loss=2.92595

Batch 347640, train_perplexity=18.651937, train_loss=2.92595

Batch 347650, train_perplexity=18.651937, train_loss=2.92595

Batch 347660, train_perplexity=18.651937, train_loss=2.92595

Batch 347670, train_perplexity=18.651937, train_loss=2.92595

Batch 347680, train_perplexity=18.651937, train_loss=2.92595

Batch 347690, train_perplexity=18.651937, train_loss=2.92595

Batch 347700, train_perplexity=18.651937, train_loss=2.92595

Batch 347710, train_perplexity=18.651937, train_loss=2.92595

Batch 347720, train_perplexity=18.651937, train_loss=2.92595

Batch 347730, train_perplexity=18.651937, train_loss=2.92595

Batch 347740, train_perplexity=18.651934, train_loss=2.9259498

Batch 347750, train_perplexity=18.651937, train_loss=2.92595

Batch 347760, train_perplexity=18.651937, train_loss=2.92595

Batch 347770, train_perplexity=18.651934, train_loss=2.9259498

Batch 347780, train_perplexity=18.651934, train_loss=2.9259498

Batch 347790, train_perplexity=18.651937, train_loss=2.92595

Batch 347800, train_perplexity=18.651934, train_loss=2.9259498

Batch 347810, train_perplexity=18.65193, train_loss=2.9259496

Batch 347820, train_perplexity=18.651937, train_loss=2.92595

Batch 347830, train_perplexity=18.65193, train_loss=2.9259496

Batch 347840, train_perplexity=18.65193, train_loss=2.9259496

Batch 347850, train_perplexity=18.65193, train_loss=2.9259496

Batch 347860, train_perplexity=18.65193, train_loss=2.9259496

Batch 347870, train_perplexity=18.65193, train_loss=2.9259496
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 347880, train_perplexity=18.65193, train_loss=2.9259496

Batch 347890, train_perplexity=18.65193, train_loss=2.9259496

Batch 347900, train_perplexity=18.65193, train_loss=2.9259496

Batch 347910, train_perplexity=18.65193, train_loss=2.9259496

Batch 347920, train_perplexity=18.65192, train_loss=2.925949

Batch 347930, train_perplexity=18.65193, train_loss=2.9259496

Batch 347940, train_perplexity=18.65193, train_loss=2.9259496

Batch 347950, train_perplexity=18.65193, train_loss=2.9259496

Batch 347960, train_perplexity=18.65193, train_loss=2.9259496

Batch 347970, train_perplexity=18.65193, train_loss=2.9259496

Batch 347980, train_perplexity=18.65193, train_loss=2.9259496

Batch 347990, train_perplexity=18.65193, train_loss=2.9259496

Batch 348000, train_perplexity=18.651924, train_loss=2.9259493

Batch 348010, train_perplexity=18.65193, train_loss=2.9259496

Batch 348020, train_perplexity=18.65193, train_loss=2.9259496

Batch 348030, train_perplexity=18.65192, train_loss=2.925949

Batch 348040, train_perplexity=18.65193, train_loss=2.9259496

Batch 348050, train_perplexity=18.65193, train_loss=2.9259496

Batch 348060, train_perplexity=18.651924, train_loss=2.9259493

Batch 348070, train_perplexity=18.65192, train_loss=2.925949

Batch 348080, train_perplexity=18.65192, train_loss=2.925949

Batch 348090, train_perplexity=18.65192, train_loss=2.925949

Batch 348100, train_perplexity=18.65192, train_loss=2.925949

Batch 348110, train_perplexity=18.65192, train_loss=2.925949

Batch 348120, train_perplexity=18.65192, train_loss=2.925949

Batch 348130, train_perplexity=18.65192, train_loss=2.925949

Batch 348140, train_perplexity=18.65192, train_loss=2.925949

Batch 348150, train_perplexity=18.65192, train_loss=2.925949

Batch 348160, train_perplexity=18.651924, train_loss=2.9259493

Batch 348170, train_perplexity=18.65193, train_loss=2.9259496

Batch 348180, train_perplexity=18.65192, train_loss=2.925949

Batch 348190, train_perplexity=18.65192, train_loss=2.925949

Batch 348200, train_perplexity=18.65192, train_loss=2.925949

Batch 348210, train_perplexity=18.65192, train_loss=2.925949

Batch 348220, train_perplexity=18.65192, train_loss=2.925949

Batch 348230, train_perplexity=18.65192, train_loss=2.925949

Batch 348240, train_perplexity=18.65192, train_loss=2.925949

Batch 348250, train_perplexity=18.65192, train_loss=2.925949

Batch 348260, train_perplexity=18.65192, train_loss=2.925949

Batch 348270, train_perplexity=18.65192, train_loss=2.925949

Batch 348280, train_perplexity=18.65192, train_loss=2.925949

Batch 348290, train_perplexity=18.65192, train_loss=2.925949

Batch 348300, train_perplexity=18.65192, train_loss=2.925949

Batch 348310, train_perplexity=18.65192, train_loss=2.925949

Batch 348320, train_perplexity=18.65192, train_loss=2.925949

Batch 348330, train_perplexity=18.651917, train_loss=2.9259489

Batch 348340, train_perplexity=18.65191, train_loss=2.9259486

Batch 348350, train_perplexity=18.651917, train_loss=2.9259489

Batch 348360, train_perplexity=18.65192, train_loss=2.925949

Batch 348370, train_perplexity=18.65192, train_loss=2.925949

Batch 348380, train_perplexity=18.65191, train_loss=2.9259486

Batch 348390, train_perplexity=18.65191, train_loss=2.9259486

Batch 348400, train_perplexity=18.65192, train_loss=2.925949

Batch 348410, train_perplexity=18.65192, train_loss=2.925949

Batch 348420, train_perplexity=18.651917, train_loss=2.9259489

Batch 348430, train_perplexity=18.651917, train_loss=2.9259489

Batch 348440, train_perplexity=18.65191, train_loss=2.9259486

Batch 348450, train_perplexity=18.65191, train_loss=2.9259486

Batch 348460, train_perplexity=18.65191, train_loss=2.9259486

Batch 348470, train_perplexity=18.65191, train_loss=2.9259486

Batch 348480, train_perplexity=18.65191, train_loss=2.9259486

Batch 348490, train_perplexity=18.65191, train_loss=2.9259486

Batch 348500, train_perplexity=18.65191, train_loss=2.9259486

Batch 348510, train_perplexity=18.65191, train_loss=2.9259486

Batch 348520, train_perplexity=18.65191, train_loss=2.9259486

Batch 348530, train_perplexity=18.65191, train_loss=2.9259486

Batch 348540, train_perplexity=18.65191, train_loss=2.9259486

Batch 348550, train_perplexity=18.65191, train_loss=2.9259486

Batch 348560, train_perplexity=18.65191, train_loss=2.9259486

Batch 348570, train_perplexity=18.65191, train_loss=2.9259486

Batch 348580, train_perplexity=18.65191, train_loss=2.9259486

Batch 348590, train_perplexity=18.65191, train_loss=2.9259486

Batch 348600, train_perplexity=18.65191, train_loss=2.9259486

Batch 348610, train_perplexity=18.65191, train_loss=2.9259486

Batch 348620, train_perplexity=18.65191, train_loss=2.9259486

Batch 348630, train_perplexity=18.65191, train_loss=2.9259486

Batch 348640, train_perplexity=18.65191, train_loss=2.9259486

Batch 348650, train_perplexity=18.65191, train_loss=2.9259486

Batch 348660, train_perplexity=18.651907, train_loss=2.9259484

Batch 348670, train_perplexity=18.65191, train_loss=2.9259486

Batch 348680, train_perplexity=18.651907, train_loss=2.9259484

Batch 348690, train_perplexity=18.651907, train_loss=2.9259484

Batch 348700, train_perplexity=18.651907, train_loss=2.9259484

Batch 348710, train_perplexity=18.651907, train_loss=2.9259484

Batch 348720, train_perplexity=18.65191, train_loss=2.9259486

Batch 348730, train_perplexity=18.651903, train_loss=2.9259481

Batch 348740, train_perplexity=18.651907, train_loss=2.9259484

Batch 348750, train_perplexity=18.651907, train_loss=2.9259484

Batch 348760, train_perplexity=18.651903, train_loss=2.9259481

Batch 348770, train_perplexity=18.651907, train_loss=2.9259484

Batch 348780, train_perplexity=18.651903, train_loss=2.9259481

Batch 348790, train_perplexity=18.651907, train_loss=2.9259484

Batch 348800, train_perplexity=18.651907, train_loss=2.9259484

Batch 348810, train_perplexity=18.651903, train_loss=2.9259481

Batch 348820, train_perplexity=18.651903, train_loss=2.9259481

Batch 348830, train_perplexity=18.651903, train_loss=2.9259481

Batch 348840, train_perplexity=18.651903, train_loss=2.9259481

Batch 348850, train_perplexity=18.651903, train_loss=2.9259481

Batch 348860, train_perplexity=18.651903, train_loss=2.9259481

Batch 348870, train_perplexity=18.651903, train_loss=2.9259481

Batch 348880, train_perplexity=18.651903, train_loss=2.9259481

Batch 348890, train_perplexity=18.651903, train_loss=2.9259481

Batch 348900, train_perplexity=18.651903, train_loss=2.9259481

Batch 348910, train_perplexity=18.651894, train_loss=2.9259477

Batch 348920, train_perplexity=18.651897, train_loss=2.925948

Batch 348930, train_perplexity=18.651903, train_loss=2.9259481

Batch 348940, train_perplexity=18.651903, train_loss=2.9259481

Batch 348950, train_perplexity=18.651903, train_loss=2.9259481

Batch 348960, train_perplexity=18.651897, train_loss=2.925948

Batch 348970, train_perplexity=18.651903, train_loss=2.9259481

Batch 348980, train_perplexity=18.651903, train_loss=2.9259481

Batch 348990, train_perplexity=18.651903, train_loss=2.9259481

Batch 349000, train_perplexity=18.651903, train_loss=2.9259481

Batch 349010, train_perplexity=18.651894, train_loss=2.9259477

Batch 349020, train_perplexity=18.651897, train_loss=2.925948

Batch 349030, train_perplexity=18.651897, train_loss=2.925948

Batch 349040, train_perplexity=18.651894, train_loss=2.9259477

Batch 349050, train_perplexity=18.651894, train_loss=2.9259477

Batch 349060, train_perplexity=18.651894, train_loss=2.9259477

Batch 349070, train_perplexity=18.651894, train_loss=2.9259477

Batch 349080, train_perplexity=18.651894, train_loss=2.9259477

Batch 349090, train_perplexity=18.651894, train_loss=2.9259477

Batch 349100, train_perplexity=18.651894, train_loss=2.9259477

Batch 349110, train_perplexity=18.651894, train_loss=2.9259477

Batch 349120, train_perplexity=18.651894, train_loss=2.9259477

Batch 349130, train_perplexity=18.651894, train_loss=2.9259477

Batch 349140, train_perplexity=18.651894, train_loss=2.9259477

Batch 349150, train_perplexity=18.651894, train_loss=2.9259477

Batch 349160, train_perplexity=18.651894, train_loss=2.9259477

Batch 349170, train_perplexity=18.651894, train_loss=2.9259477
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 349180, train_perplexity=18.651894, train_loss=2.9259477

Batch 349190, train_perplexity=18.651894, train_loss=2.9259477

Batch 349200, train_perplexity=18.651894, train_loss=2.9259477

Batch 349210, train_perplexity=18.651894, train_loss=2.9259477

Batch 349220, train_perplexity=18.651894, train_loss=2.9259477

Batch 349230, train_perplexity=18.651894, train_loss=2.9259477

Batch 349240, train_perplexity=18.651894, train_loss=2.9259477

Batch 349250, train_perplexity=18.651894, train_loss=2.9259477

Batch 349260, train_perplexity=18.651894, train_loss=2.9259477

Batch 349270, train_perplexity=18.651894, train_loss=2.9259477

Batch 349280, train_perplexity=18.65189, train_loss=2.9259474

Batch 349290, train_perplexity=18.651894, train_loss=2.9259477

Batch 349300, train_perplexity=18.651894, train_loss=2.9259477

Batch 349310, train_perplexity=18.651894, train_loss=2.9259477

Batch 349320, train_perplexity=18.65189, train_loss=2.9259474

Batch 349330, train_perplexity=18.651894, train_loss=2.9259477

Batch 349340, train_perplexity=18.65189, train_loss=2.9259474

Batch 349350, train_perplexity=18.65189, train_loss=2.9259474

Batch 349360, train_perplexity=18.65189, train_loss=2.9259474

Batch 349370, train_perplexity=18.651884, train_loss=2.9259472

Batch 349380, train_perplexity=18.651884, train_loss=2.9259472

Batch 349390, train_perplexity=18.651884, train_loss=2.9259472

Batch 349400, train_perplexity=18.651884, train_loss=2.9259472

Batch 349410, train_perplexity=18.651884, train_loss=2.9259472

Batch 349420, train_perplexity=18.651884, train_loss=2.9259472

Batch 349430, train_perplexity=18.651884, train_loss=2.9259472

Batch 349440, train_perplexity=18.651884, train_loss=2.9259472

Batch 349450, train_perplexity=18.651884, train_loss=2.9259472

Batch 349460, train_perplexity=18.651884, train_loss=2.9259472

Batch 349470, train_perplexity=18.65189, train_loss=2.9259474

Batch 349480, train_perplexity=18.651884, train_loss=2.9259472

Batch 349490, train_perplexity=18.651884, train_loss=2.9259472

Batch 349500, train_perplexity=18.651884, train_loss=2.9259472

Batch 349510, train_perplexity=18.651884, train_loss=2.9259472

Batch 349520, train_perplexity=18.651884, train_loss=2.9259472

Batch 349530, train_perplexity=18.651884, train_loss=2.9259472

Batch 349540, train_perplexity=18.651884, train_loss=2.9259472

Batch 349550, train_perplexity=18.65188, train_loss=2.925947

Batch 349560, train_perplexity=18.651884, train_loss=2.9259472

Batch 349570, train_perplexity=18.651884, train_loss=2.9259472

Batch 349580, train_perplexity=18.651884, train_loss=2.9259472

Batch 349590, train_perplexity=18.651876, train_loss=2.9259467

Batch 349600, train_perplexity=18.651884, train_loss=2.9259472

Batch 349610, train_perplexity=18.651876, train_loss=2.9259467

Batch 349620, train_perplexity=18.651884, train_loss=2.9259472

Batch 349630, train_perplexity=18.651876, train_loss=2.9259467

Batch 349640, train_perplexity=18.651876, train_loss=2.9259467

Batch 349650, train_perplexity=18.651876, train_loss=2.9259467

Batch 349660, train_perplexity=18.651876, train_loss=2.9259467

Batch 349670, train_perplexity=18.651876, train_loss=2.9259467

Batch 349680, train_perplexity=18.65188, train_loss=2.925947

Batch 349690, train_perplexity=18.651876, train_loss=2.9259467

Batch 349700, train_perplexity=18.651876, train_loss=2.9259467

Batch 349710, train_perplexity=18.651876, train_loss=2.9259467

Batch 349720, train_perplexity=18.651884, train_loss=2.9259472

Batch 349730, train_perplexity=18.65188, train_loss=2.925947

Batch 349740, train_perplexity=18.651876, train_loss=2.9259467

Batch 349750, train_perplexity=18.651876, train_loss=2.9259467

Batch 349760, train_perplexity=18.65188, train_loss=2.925947

Batch 349770, train_perplexity=18.651876, train_loss=2.9259467

Batch 349780, train_perplexity=18.651876, train_loss=2.9259467

Batch 349790, train_perplexity=18.651876, train_loss=2.9259467

Batch 349800, train_perplexity=18.651876, train_loss=2.9259467

Batch 349810, train_perplexity=18.651876, train_loss=2.9259467

Batch 349820, train_perplexity=18.651876, train_loss=2.9259467

Batch 349830, train_perplexity=18.651876, train_loss=2.9259467

Batch 349840, train_perplexity=18.651876, train_loss=2.9259467

Batch 349850, train_perplexity=18.651876, train_loss=2.9259467

Batch 349860, train_perplexity=18.651876, train_loss=2.9259467

Batch 349870, train_perplexity=18.651876, train_loss=2.9259467

Batch 349880, train_perplexity=18.651876, train_loss=2.9259467

Batch 349890, train_perplexity=18.65187, train_loss=2.9259465

Batch 349900, train_perplexity=18.65187, train_loss=2.9259465

Batch 349910, train_perplexity=18.651876, train_loss=2.9259467

Batch 349920, train_perplexity=18.651876, train_loss=2.9259467

Batch 349930, train_perplexity=18.65187, train_loss=2.9259465

Batch 349940, train_perplexity=18.651876, train_loss=2.9259467

Batch 349950, train_perplexity=18.651867, train_loss=2.9259462

Batch 349960, train_perplexity=18.651867, train_loss=2.9259462

Batch 349970, train_perplexity=18.65187, train_loss=2.9259465

Batch 349980, train_perplexity=18.65187, train_loss=2.9259465

Batch 349990, train_perplexity=18.65187, train_loss=2.9259465

Batch 350000, train_perplexity=18.651867, train_loss=2.9259462

Batch 350010, train_perplexity=18.651867, train_loss=2.9259462

Batch 350020, train_perplexity=18.65187, train_loss=2.9259465

Batch 350030, train_perplexity=18.651867, train_loss=2.9259462

Batch 350040, train_perplexity=18.65187, train_loss=2.9259465

Batch 350050, train_perplexity=18.651867, train_loss=2.9259462

Batch 350060, train_perplexity=18.651867, train_loss=2.9259462

Batch 350070, train_perplexity=18.651867, train_loss=2.9259462

Batch 350080, train_perplexity=18.651867, train_loss=2.9259462

Batch 350090, train_perplexity=18.651867, train_loss=2.9259462

Batch 350100, train_perplexity=18.651867, train_loss=2.9259462

Batch 350110, train_perplexity=18.651867, train_loss=2.9259462

Batch 350120, train_perplexity=18.651867, train_loss=2.9259462

Batch 350130, train_perplexity=18.65187, train_loss=2.9259465

Batch 350140, train_perplexity=18.651867, train_loss=2.9259462

Batch 350150, train_perplexity=18.651867, train_loss=2.9259462

Batch 350160, train_perplexity=18.651867, train_loss=2.9259462

Batch 350170, train_perplexity=18.651867, train_loss=2.9259462

Batch 350180, train_perplexity=18.651867, train_loss=2.9259462

Batch 350190, train_perplexity=18.651867, train_loss=2.9259462

Batch 350200, train_perplexity=18.651867, train_loss=2.9259462

Batch 350210, train_perplexity=18.651867, train_loss=2.9259462

Batch 350220, train_perplexity=18.651863, train_loss=2.925946

Batch 350230, train_perplexity=18.651857, train_loss=2.9259458

Batch 350240, train_perplexity=18.651867, train_loss=2.9259462

Batch 350250, train_perplexity=18.651867, train_loss=2.9259462

Batch 350260, train_perplexity=18.651867, train_loss=2.9259462

Batch 350270, train_perplexity=18.651863, train_loss=2.925946

Batch 350280, train_perplexity=18.651857, train_loss=2.9259458

Batch 350290, train_perplexity=18.651857, train_loss=2.9259458

Batch 350300, train_perplexity=18.651857, train_loss=2.9259458

Batch 350310, train_perplexity=18.651863, train_loss=2.925946

Batch 350320, train_perplexity=18.651867, train_loss=2.9259462

Batch 350330, train_perplexity=18.651857, train_loss=2.9259458

Batch 350340, train_perplexity=18.651863, train_loss=2.925946

Batch 350350, train_perplexity=18.651857, train_loss=2.9259458

Batch 350360, train_perplexity=18.651857, train_loss=2.9259458

Batch 350370, train_perplexity=18.651854, train_loss=2.9259455

Batch 350380, train_perplexity=18.651857, train_loss=2.9259458

Batch 350390, train_perplexity=18.651857, train_loss=2.9259458

Batch 350400, train_perplexity=18.651857, train_loss=2.9259458

Batch 350410, train_perplexity=18.651857, train_loss=2.9259458

Batch 350420, train_perplexity=18.651854, train_loss=2.9259455

Batch 350430, train_perplexity=18.651857, train_loss=2.9259458

Batch 350440, train_perplexity=18.651854, train_loss=2.9259455

Batch 350450, train_perplexity=18.651854, train_loss=2.9259455

Batch 350460, train_perplexity=18.651857, train_loss=2.9259458
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 350470, train_perplexity=18.651857, train_loss=2.9259458

Batch 350480, train_perplexity=18.651857, train_loss=2.9259458

Batch 350490, train_perplexity=18.651857, train_loss=2.9259458

Batch 350500, train_perplexity=18.651854, train_loss=2.9259455

Batch 350510, train_perplexity=18.651857, train_loss=2.9259458

Batch 350520, train_perplexity=18.651857, train_loss=2.9259458

Batch 350530, train_perplexity=18.651854, train_loss=2.9259455

Batch 350540, train_perplexity=18.651854, train_loss=2.9259455

Batch 350550, train_perplexity=18.651854, train_loss=2.9259455

Batch 350560, train_perplexity=18.651857, train_loss=2.9259458

Batch 350570, train_perplexity=18.65185, train_loss=2.9259453

Batch 350580, train_perplexity=18.651857, train_loss=2.9259458

Batch 350590, train_perplexity=18.651854, train_loss=2.9259455

Batch 350600, train_perplexity=18.65185, train_loss=2.9259453

Batch 350610, train_perplexity=18.651854, train_loss=2.9259455

Batch 350620, train_perplexity=18.651854, train_loss=2.9259455

Batch 350630, train_perplexity=18.651854, train_loss=2.9259455

Batch 350640, train_perplexity=18.651854, train_loss=2.9259455

Batch 350650, train_perplexity=18.65185, train_loss=2.9259453

Batch 350660, train_perplexity=18.651854, train_loss=2.9259455

Batch 350670, train_perplexity=18.65185, train_loss=2.9259453

Batch 350680, train_perplexity=18.65185, train_loss=2.9259453

Batch 350690, train_perplexity=18.65185, train_loss=2.9259453

Batch 350700, train_perplexity=18.65185, train_loss=2.9259453

Batch 350710, train_perplexity=18.65185, train_loss=2.9259453

Batch 350720, train_perplexity=18.65185, train_loss=2.9259453

Batch 350730, train_perplexity=18.65185, train_loss=2.9259453

Batch 350740, train_perplexity=18.65185, train_loss=2.9259453

Batch 350750, train_perplexity=18.65185, train_loss=2.9259453

Batch 350760, train_perplexity=18.65185, train_loss=2.9259453

Batch 350770, train_perplexity=18.65185, train_loss=2.9259453

Batch 350780, train_perplexity=18.65185, train_loss=2.9259453

Batch 350790, train_perplexity=18.65185, train_loss=2.9259453

Batch 350800, train_perplexity=18.65185, train_loss=2.9259453

Batch 350810, train_perplexity=18.65185, train_loss=2.9259453

Batch 350820, train_perplexity=18.65185, train_loss=2.9259453

Batch 350830, train_perplexity=18.65185, train_loss=2.9259453

Batch 350840, train_perplexity=18.65185, train_loss=2.9259453

Batch 350850, train_perplexity=18.65185, train_loss=2.9259453

Batch 350860, train_perplexity=18.65185, train_loss=2.9259453

Batch 350870, train_perplexity=18.651844, train_loss=2.925945

Batch 350880, train_perplexity=18.651844, train_loss=2.925945

Batch 350890, train_perplexity=18.651844, train_loss=2.925945

Batch 350900, train_perplexity=18.65185, train_loss=2.9259453

Batch 350910, train_perplexity=18.651844, train_loss=2.925945

Batch 350920, train_perplexity=18.651844, train_loss=2.925945

Batch 350930, train_perplexity=18.651844, train_loss=2.925945

Batch 350940, train_perplexity=18.651844, train_loss=2.925945

Batch 350950, train_perplexity=18.65184, train_loss=2.9259448

Batch 350960, train_perplexity=18.65184, train_loss=2.9259448

Batch 350970, train_perplexity=18.65185, train_loss=2.9259453

Batch 350980, train_perplexity=18.651844, train_loss=2.925945

Batch 350990, train_perplexity=18.65184, train_loss=2.9259448

Batch 351000, train_perplexity=18.65184, train_loss=2.9259448

Batch 351010, train_perplexity=18.65184, train_loss=2.9259448

Batch 351020, train_perplexity=18.65184, train_loss=2.9259448

Batch 351030, train_perplexity=18.651844, train_loss=2.925945

Batch 351040, train_perplexity=18.65184, train_loss=2.9259448

Batch 351050, train_perplexity=18.65184, train_loss=2.9259448

Batch 351060, train_perplexity=18.65184, train_loss=2.9259448

Batch 351070, train_perplexity=18.65184, train_loss=2.9259448

Batch 351080, train_perplexity=18.65184, train_loss=2.9259448

Batch 351090, train_perplexity=18.65184, train_loss=2.9259448

Batch 351100, train_perplexity=18.651836, train_loss=2.9259446

Batch 351110, train_perplexity=18.65184, train_loss=2.9259448

Batch 351120, train_perplexity=18.65184, train_loss=2.9259448

Batch 351130, train_perplexity=18.65184, train_loss=2.9259448

Batch 351140, train_perplexity=18.65184, train_loss=2.9259448

Batch 351150, train_perplexity=18.65184, train_loss=2.9259448

Batch 351160, train_perplexity=18.65184, train_loss=2.9259448

Batch 351170, train_perplexity=18.65184, train_loss=2.9259448

Batch 351180, train_perplexity=18.651836, train_loss=2.9259446

Batch 351190, train_perplexity=18.65184, train_loss=2.9259448

Batch 351200, train_perplexity=18.651836, train_loss=2.9259446

Batch 351210, train_perplexity=18.651836, train_loss=2.9259446

Batch 351220, train_perplexity=18.651836, train_loss=2.9259446

Batch 351230, train_perplexity=18.65183, train_loss=2.9259443

Batch 351240, train_perplexity=18.651836, train_loss=2.9259446

Batch 351250, train_perplexity=18.65183, train_loss=2.9259443

Batch 351260, train_perplexity=18.651836, train_loss=2.9259446

Batch 351270, train_perplexity=18.65183, train_loss=2.9259443

Batch 351280, train_perplexity=18.65184, train_loss=2.9259448

Batch 351290, train_perplexity=18.651836, train_loss=2.9259446

Batch 351300, train_perplexity=18.65183, train_loss=2.9259443

Batch 351310, train_perplexity=18.65183, train_loss=2.9259443

Batch 351320, train_perplexity=18.65183, train_loss=2.9259443

Batch 351330, train_perplexity=18.65183, train_loss=2.9259443

Batch 351340, train_perplexity=18.65183, train_loss=2.9259443

Batch 351350, train_perplexity=18.65183, train_loss=2.9259443

Batch 351360, train_perplexity=18.65183, train_loss=2.9259443

Batch 351370, train_perplexity=18.65183, train_loss=2.9259443

Batch 351380, train_perplexity=18.65183, train_loss=2.9259443

Batch 351390, train_perplexity=18.65183, train_loss=2.9259443

Batch 351400, train_perplexity=18.65183, train_loss=2.9259443

Batch 351410, train_perplexity=18.65183, train_loss=2.9259443

Batch 351420, train_perplexity=18.65183, train_loss=2.9259443

Batch 351430, train_perplexity=18.65183, train_loss=2.9259443

Batch 351440, train_perplexity=18.65183, train_loss=2.9259443

Batch 351450, train_perplexity=18.651827, train_loss=2.925944

Batch 351460, train_perplexity=18.651827, train_loss=2.925944

Batch 351470, train_perplexity=18.65183, train_loss=2.9259443

Batch 351480, train_perplexity=18.65183, train_loss=2.9259443

Batch 351490, train_perplexity=18.65183, train_loss=2.9259443

Batch 351500, train_perplexity=18.65183, train_loss=2.9259443

Batch 351510, train_perplexity=18.65183, train_loss=2.9259443

Batch 351520, train_perplexity=18.651823, train_loss=2.9259439

Batch 351530, train_perplexity=18.651827, train_loss=2.925944

Batch 351540, train_perplexity=18.65183, train_loss=2.9259443

Batch 351550, train_perplexity=18.651823, train_loss=2.9259439

Batch 351560, train_perplexity=18.65183, train_loss=2.9259443

Batch 351570, train_perplexity=18.651823, train_loss=2.9259439

Batch 351580, train_perplexity=18.65183, train_loss=2.9259443

Batch 351590, train_perplexity=18.65183, train_loss=2.9259443

Batch 351600, train_perplexity=18.651823, train_loss=2.9259439

Batch 351610, train_perplexity=18.651823, train_loss=2.9259439

Batch 351620, train_perplexity=18.651823, train_loss=2.9259439

Batch 351630, train_perplexity=18.651823, train_loss=2.9259439

Batch 351640, train_perplexity=18.651823, train_loss=2.9259439

Batch 351650, train_perplexity=18.651823, train_loss=2.9259439

Batch 351660, train_perplexity=18.651823, train_loss=2.9259439

Batch 351670, train_perplexity=18.651823, train_loss=2.9259439

Batch 351680, train_perplexity=18.651823, train_loss=2.9259439

Batch 351690, train_perplexity=18.651823, train_loss=2.9259439

Batch 351700, train_perplexity=18.651823, train_loss=2.9259439

Batch 351710, train_perplexity=18.651823, train_loss=2.9259439

Batch 351720, train_perplexity=18.651823, train_loss=2.9259439

Batch 351730, train_perplexity=18.651823, train_loss=2.9259439

Batch 351740, train_perplexity=18.651823, train_loss=2.9259439

Batch 351750, train_perplexity=18.651823, train_loss=2.9259439

Batch 351760, train_perplexity=18.651823, train_loss=2.9259439
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 351770, train_perplexity=18.651817, train_loss=2.9259436

Batch 351780, train_perplexity=18.651823, train_loss=2.9259439

Batch 351790, train_perplexity=18.651823, train_loss=2.9259439

Batch 351800, train_perplexity=18.651823, train_loss=2.9259439

Batch 351810, train_perplexity=18.651817, train_loss=2.9259436

Batch 351820, train_perplexity=18.651823, train_loss=2.9259439

Batch 351830, train_perplexity=18.651817, train_loss=2.9259436

Batch 351840, train_perplexity=18.651817, train_loss=2.9259436

Batch 351850, train_perplexity=18.651823, train_loss=2.9259439

Batch 351860, train_perplexity=18.651823, train_loss=2.9259439

Batch 351870, train_perplexity=18.651817, train_loss=2.9259436

Batch 351880, train_perplexity=18.651823, train_loss=2.9259439

Batch 351890, train_perplexity=18.651814, train_loss=2.9259434

Batch 351900, train_perplexity=18.651817, train_loss=2.9259436

Batch 351910, train_perplexity=18.651814, train_loss=2.9259434

Batch 351920, train_perplexity=18.651814, train_loss=2.9259434

Batch 351930, train_perplexity=18.651814, train_loss=2.9259434

Batch 351940, train_perplexity=18.651814, train_loss=2.9259434

Batch 351950, train_perplexity=18.651814, train_loss=2.9259434

Batch 351960, train_perplexity=18.651814, train_loss=2.9259434

Batch 351970, train_perplexity=18.651817, train_loss=2.9259436

Batch 351980, train_perplexity=18.651814, train_loss=2.9259434

Batch 351990, train_perplexity=18.651814, train_loss=2.9259434

Batch 352000, train_perplexity=18.651814, train_loss=2.9259434

Batch 352010, train_perplexity=18.651814, train_loss=2.9259434

Batch 352020, train_perplexity=18.651814, train_loss=2.9259434

Batch 352030, train_perplexity=18.651814, train_loss=2.9259434

Batch 352040, train_perplexity=18.651814, train_loss=2.9259434

Batch 352050, train_perplexity=18.651814, train_loss=2.9259434

Batch 352060, train_perplexity=18.651814, train_loss=2.9259434

Batch 352070, train_perplexity=18.651814, train_loss=2.9259434

Batch 352080, train_perplexity=18.651814, train_loss=2.9259434

Batch 352090, train_perplexity=18.651814, train_loss=2.9259434

Batch 352100, train_perplexity=18.651804, train_loss=2.925943

Batch 352110, train_perplexity=18.651814, train_loss=2.9259434

Batch 352120, train_perplexity=18.651814, train_loss=2.9259434

Batch 352130, train_perplexity=18.651814, train_loss=2.9259434

Batch 352140, train_perplexity=18.651814, train_loss=2.9259434

Batch 352150, train_perplexity=18.65181, train_loss=2.9259431

Batch 352160, train_perplexity=18.651814, train_loss=2.9259434

Batch 352170, train_perplexity=18.651814, train_loss=2.9259434

Batch 352180, train_perplexity=18.651814, train_loss=2.9259434

Batch 352190, train_perplexity=18.65181, train_loss=2.9259431

Batch 352200, train_perplexity=18.651804, train_loss=2.925943

Batch 352210, train_perplexity=18.651804, train_loss=2.925943

Batch 352220, train_perplexity=18.651804, train_loss=2.925943

Batch 352230, train_perplexity=18.651804, train_loss=2.925943

Batch 352240, train_perplexity=18.651804, train_loss=2.925943

Batch 352250, train_perplexity=18.651804, train_loss=2.925943

Batch 352260, train_perplexity=18.651804, train_loss=2.925943

Batch 352270, train_perplexity=18.651804, train_loss=2.925943

Batch 352280, train_perplexity=18.651804, train_loss=2.925943

Batch 352290, train_perplexity=18.651804, train_loss=2.925943

Batch 352300, train_perplexity=18.651804, train_loss=2.925943

Batch 352310, train_perplexity=18.651804, train_loss=2.925943

Batch 352320, train_perplexity=18.651804, train_loss=2.925943

Batch 352330, train_perplexity=18.651804, train_loss=2.925943

Batch 352340, train_perplexity=18.651804, train_loss=2.925943

Batch 352350, train_perplexity=18.651804, train_loss=2.925943

Batch 352360, train_perplexity=18.651804, train_loss=2.925943

Batch 352370, train_perplexity=18.651804, train_loss=2.925943

Batch 352380, train_perplexity=18.6518, train_loss=2.9259427

Batch 352390, train_perplexity=18.651796, train_loss=2.9259424

Batch 352400, train_perplexity=18.651804, train_loss=2.925943

Batch 352410, train_perplexity=18.6518, train_loss=2.9259427

Batch 352420, train_perplexity=18.651804, train_loss=2.925943

Batch 352430, train_perplexity=18.651804, train_loss=2.925943

Batch 352440, train_perplexity=18.6518, train_loss=2.9259427

Batch 352450, train_perplexity=18.651804, train_loss=2.925943

Batch 352460, train_perplexity=18.6518, train_loss=2.9259427

Batch 352470, train_perplexity=18.651804, train_loss=2.925943

Batch 352480, train_perplexity=18.651796, train_loss=2.9259424

Batch 352490, train_perplexity=18.651796, train_loss=2.9259424

Batch 352500, train_perplexity=18.6518, train_loss=2.9259427

Batch 352510, train_perplexity=18.651796, train_loss=2.9259424

Batch 352520, train_perplexity=18.6518, train_loss=2.9259427

Batch 352530, train_perplexity=18.6518, train_loss=2.9259427

Batch 352540, train_perplexity=18.6518, train_loss=2.9259427

Batch 352550, train_perplexity=18.651796, train_loss=2.9259424

Batch 352560, train_perplexity=18.651796, train_loss=2.9259424

Batch 352570, train_perplexity=18.6518, train_loss=2.9259427

Batch 352580, train_perplexity=18.651796, train_loss=2.9259424

Batch 352590, train_perplexity=18.6518, train_loss=2.9259427

Batch 352600, train_perplexity=18.651796, train_loss=2.9259424

Batch 352610, train_perplexity=18.651796, train_loss=2.9259424

Batch 352620, train_perplexity=18.651796, train_loss=2.9259424

Batch 352630, train_perplexity=18.651796, train_loss=2.9259424

Batch 352640, train_perplexity=18.651796, train_loss=2.9259424

Batch 352650, train_perplexity=18.651796, train_loss=2.9259424

Batch 352660, train_perplexity=18.6518, train_loss=2.9259427

Batch 352670, train_perplexity=18.651796, train_loss=2.9259424

Batch 352680, train_perplexity=18.651796, train_loss=2.9259424

Batch 352690, train_perplexity=18.651796, train_loss=2.9259424

Batch 352700, train_perplexity=18.651796, train_loss=2.9259424

Batch 352710, train_perplexity=18.651796, train_loss=2.9259424

Batch 352720, train_perplexity=18.651796, train_loss=2.9259424

Batch 352730, train_perplexity=18.651796, train_loss=2.9259424

Batch 352740, train_perplexity=18.65179, train_loss=2.9259422

Batch 352750, train_perplexity=18.651796, train_loss=2.9259424

Batch 352760, train_perplexity=18.651796, train_loss=2.9259424

Batch 352770, train_perplexity=18.65179, train_loss=2.9259422

Batch 352780, train_perplexity=18.65179, train_loss=2.9259422

Batch 352790, train_perplexity=18.651796, train_loss=2.9259424

Batch 352800, train_perplexity=18.65179, train_loss=2.9259422

Batch 352810, train_perplexity=18.65179, train_loss=2.9259422

Batch 352820, train_perplexity=18.651787, train_loss=2.925942

Batch 352830, train_perplexity=18.651787, train_loss=2.925942

Batch 352840, train_perplexity=18.651787, train_loss=2.925942

Batch 352850, train_perplexity=18.651787, train_loss=2.925942

Batch 352860, train_perplexity=18.651787, train_loss=2.925942

Batch 352870, train_perplexity=18.651783, train_loss=2.9259417

Batch 352880, train_perplexity=18.65179, train_loss=2.9259422

Batch 352890, train_perplexity=18.651787, train_loss=2.925942

Batch 352900, train_perplexity=18.651787, train_loss=2.925942

Batch 352910, train_perplexity=18.651787, train_loss=2.925942

Batch 352920, train_perplexity=18.651787, train_loss=2.925942

Batch 352930, train_perplexity=18.651787, train_loss=2.925942

Batch 352940, train_perplexity=18.651787, train_loss=2.925942

Batch 352950, train_perplexity=18.651787, train_loss=2.925942

Batch 352960, train_perplexity=18.651787, train_loss=2.925942

Batch 352970, train_perplexity=18.651787, train_loss=2.925942

Batch 352980, train_perplexity=18.651787, train_loss=2.925942

Batch 352990, train_perplexity=18.651787, train_loss=2.925942

Batch 353000, train_perplexity=18.651787, train_loss=2.925942

Batch 353010, train_perplexity=18.651787, train_loss=2.925942

Batch 353020, train_perplexity=18.651787, train_loss=2.925942

Batch 353030, train_perplexity=18.651787, train_loss=2.925942

Batch 353040, train_perplexity=18.651787, train_loss=2.925942

Batch 353050, train_perplexity=18.651787, train_loss=2.925942

Batch 353060, train_perplexity=18.651777, train_loss=2.9259415
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 353070, train_perplexity=18.651787, train_loss=2.925942

Batch 353080, train_perplexity=18.651783, train_loss=2.9259417

Batch 353090, train_perplexity=18.651787, train_loss=2.925942

Batch 353100, train_perplexity=18.651783, train_loss=2.9259417

Batch 353110, train_perplexity=18.651777, train_loss=2.9259415

Batch 353120, train_perplexity=18.651777, train_loss=2.9259415

Batch 353130, train_perplexity=18.651783, train_loss=2.9259417

Batch 353140, train_perplexity=18.651787, train_loss=2.925942

Batch 353150, train_perplexity=18.651777, train_loss=2.9259415

Batch 353160, train_perplexity=18.651783, train_loss=2.9259417

Batch 353170, train_perplexity=18.651777, train_loss=2.9259415

Batch 353180, train_perplexity=18.651777, train_loss=2.9259415

Batch 353190, train_perplexity=18.651787, train_loss=2.925942

Batch 353200, train_perplexity=18.651777, train_loss=2.9259415

Batch 353210, train_perplexity=18.651777, train_loss=2.9259415

Batch 353220, train_perplexity=18.651777, train_loss=2.9259415

Batch 353230, train_perplexity=18.651777, train_loss=2.9259415

Batch 353240, train_perplexity=18.651777, train_loss=2.9259415

Batch 353250, train_perplexity=18.651777, train_loss=2.9259415

Batch 353260, train_perplexity=18.651777, train_loss=2.9259415

Batch 353270, train_perplexity=18.651777, train_loss=2.9259415

Batch 353280, train_perplexity=18.651773, train_loss=2.9259412

Batch 353290, train_perplexity=18.651777, train_loss=2.9259415

Batch 353300, train_perplexity=18.651777, train_loss=2.9259415

Batch 353310, train_perplexity=18.651777, train_loss=2.9259415

Batch 353320, train_perplexity=18.651777, train_loss=2.9259415

Batch 353330, train_perplexity=18.651777, train_loss=2.9259415

Batch 353340, train_perplexity=18.651777, train_loss=2.9259415

Batch 353350, train_perplexity=18.651777, train_loss=2.9259415

Batch 353360, train_perplexity=18.651777, train_loss=2.9259415

Batch 353370, train_perplexity=18.651777, train_loss=2.9259415

Batch 353380, train_perplexity=18.651777, train_loss=2.9259415

Batch 353390, train_perplexity=18.651773, train_loss=2.9259412

Batch 353400, train_perplexity=18.65177, train_loss=2.925941

Batch 353410, train_perplexity=18.65177, train_loss=2.925941

Batch 353420, train_perplexity=18.65177, train_loss=2.925941

Batch 353430, train_perplexity=18.651777, train_loss=2.9259415

Batch 353440, train_perplexity=18.651777, train_loss=2.9259415

Batch 353450, train_perplexity=18.65177, train_loss=2.925941

Batch 353460, train_perplexity=18.65177, train_loss=2.925941

Batch 353470, train_perplexity=18.651773, train_loss=2.9259412

Batch 353480, train_perplexity=18.65177, train_loss=2.925941

Batch 353490, train_perplexity=18.65177, train_loss=2.925941

Batch 353500, train_perplexity=18.65177, train_loss=2.925941

Batch 353510, train_perplexity=18.65177, train_loss=2.925941

Batch 353520, train_perplexity=18.651777, train_loss=2.9259415

Batch 353530, train_perplexity=18.65177, train_loss=2.925941

Batch 353540, train_perplexity=18.65177, train_loss=2.925941

Batch 353550, train_perplexity=18.65177, train_loss=2.925941

Batch 353560, train_perplexity=18.651773, train_loss=2.9259412

Batch 353570, train_perplexity=18.65177, train_loss=2.925941

Batch 353580, train_perplexity=18.65177, train_loss=2.925941

Batch 353590, train_perplexity=18.65177, train_loss=2.925941

Batch 353600, train_perplexity=18.65177, train_loss=2.925941

Batch 353610, train_perplexity=18.65177, train_loss=2.925941

Batch 353620, train_perplexity=18.65177, train_loss=2.925941

Batch 353630, train_perplexity=18.651764, train_loss=2.9259408

Batch 353640, train_perplexity=18.65177, train_loss=2.925941

Batch 353650, train_perplexity=18.65177, train_loss=2.925941

Batch 353660, train_perplexity=18.65177, train_loss=2.925941

Batch 353670, train_perplexity=18.65177, train_loss=2.925941

Batch 353680, train_perplexity=18.65177, train_loss=2.925941

Batch 353690, train_perplexity=18.65176, train_loss=2.9259405

Batch 353700, train_perplexity=18.65177, train_loss=2.925941

Batch 353710, train_perplexity=18.651764, train_loss=2.9259408

Batch 353720, train_perplexity=18.651764, train_loss=2.9259408

Batch 353730, train_perplexity=18.65177, train_loss=2.925941

Batch 353740, train_perplexity=18.651764, train_loss=2.9259408

Batch 353750, train_perplexity=18.65176, train_loss=2.9259405

Batch 353760, train_perplexity=18.651764, train_loss=2.9259408

Batch 353770, train_perplexity=18.65176, train_loss=2.9259405

Batch 353780, train_perplexity=18.65177, train_loss=2.925941

Batch 353790, train_perplexity=18.65176, train_loss=2.9259405

Batch 353800, train_perplexity=18.65176, train_loss=2.9259405

Batch 353810, train_perplexity=18.65176, train_loss=2.9259405

Batch 353820, train_perplexity=18.65176, train_loss=2.9259405

Batch 353830, train_perplexity=18.65176, train_loss=2.9259405

Batch 353840, train_perplexity=18.65176, train_loss=2.9259405

Batch 353850, train_perplexity=18.65176, train_loss=2.9259405

Batch 353860, train_perplexity=18.65176, train_loss=2.9259405

Batch 353870, train_perplexity=18.65176, train_loss=2.9259405

Batch 353880, train_perplexity=18.651764, train_loss=2.9259408

Batch 353890, train_perplexity=18.651756, train_loss=2.9259403

Batch 353900, train_perplexity=18.65176, train_loss=2.9259405

Batch 353910, train_perplexity=18.65176, train_loss=2.9259405

Batch 353920, train_perplexity=18.65176, train_loss=2.9259405

Batch 353930, train_perplexity=18.65176, train_loss=2.9259405

Batch 353940, train_perplexity=18.65176, train_loss=2.9259405

Batch 353950, train_perplexity=18.65176, train_loss=2.9259405

Batch 353960, train_perplexity=18.65176, train_loss=2.9259405

Batch 353970, train_perplexity=18.65176, train_loss=2.9259405

Batch 353980, train_perplexity=18.65176, train_loss=2.9259405

Batch 353990, train_perplexity=18.65176, train_loss=2.9259405

Batch 354000, train_perplexity=18.65176, train_loss=2.9259405

Batch 354010, train_perplexity=18.65176, train_loss=2.9259405

Batch 354020, train_perplexity=18.651756, train_loss=2.9259403

Batch 354030, train_perplexity=18.65176, train_loss=2.9259405

Batch 354040, train_perplexity=18.65176, train_loss=2.9259405

Batch 354050, train_perplexity=18.65175, train_loss=2.92594

Batch 354060, train_perplexity=18.65175, train_loss=2.92594

Batch 354070, train_perplexity=18.65176, train_loss=2.9259405

Batch 354080, train_perplexity=18.651756, train_loss=2.9259403

Batch 354090, train_perplexity=18.651756, train_loss=2.9259403

Batch 354100, train_perplexity=18.65175, train_loss=2.92594

Batch 354110, train_perplexity=18.65175, train_loss=2.92594

Batch 354120, train_perplexity=18.65176, train_loss=2.9259405

Batch 354130, train_perplexity=18.65175, train_loss=2.92594

Batch 354140, train_perplexity=18.65175, train_loss=2.92594

Batch 354150, train_perplexity=18.65175, train_loss=2.92594

Batch 354160, train_perplexity=18.65175, train_loss=2.92594

Batch 354170, train_perplexity=18.65175, train_loss=2.92594

Batch 354180, train_perplexity=18.65175, train_loss=2.92594

Batch 354190, train_perplexity=18.65175, train_loss=2.92594

Batch 354200, train_perplexity=18.65175, train_loss=2.92594

Batch 354210, train_perplexity=18.65175, train_loss=2.92594

Batch 354220, train_perplexity=18.65175, train_loss=2.92594

Batch 354230, train_perplexity=18.65175, train_loss=2.92594

Batch 354240, train_perplexity=18.65175, train_loss=2.92594

Batch 354250, train_perplexity=18.65175, train_loss=2.92594

Batch 354260, train_perplexity=18.65175, train_loss=2.92594

Batch 354270, train_perplexity=18.65175, train_loss=2.92594

Batch 354280, train_perplexity=18.65175, train_loss=2.92594

Batch 354290, train_perplexity=18.65175, train_loss=2.92594

Batch 354300, train_perplexity=18.65175, train_loss=2.92594

Batch 354310, train_perplexity=18.65175, train_loss=2.92594

Batch 354320, train_perplexity=18.65175, train_loss=2.92594

Batch 354330, train_perplexity=18.651747, train_loss=2.9259398

Batch 354340, train_perplexity=18.651747, train_loss=2.9259398

Batch 354350, train_perplexity=18.651747, train_loss=2.9259398

Batch 354360, train_perplexity=18.651747, train_loss=2.9259398

Batch 354370, train_perplexity=18.651743, train_loss=2.9259396
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 354380, train_perplexity=18.651743, train_loss=2.9259396

Batch 354390, train_perplexity=18.651743, train_loss=2.9259396

Batch 354400, train_perplexity=18.65175, train_loss=2.92594

Batch 354410, train_perplexity=18.651743, train_loss=2.9259396

Batch 354420, train_perplexity=18.651747, train_loss=2.9259398

Batch 354430, train_perplexity=18.651743, train_loss=2.9259396

Batch 354440, train_perplexity=18.651743, train_loss=2.9259396

Batch 354450, train_perplexity=18.651743, train_loss=2.9259396

Batch 354460, train_perplexity=18.651743, train_loss=2.9259396

Batch 354470, train_perplexity=18.651743, train_loss=2.9259396

Batch 354480, train_perplexity=18.651743, train_loss=2.9259396

Batch 354490, train_perplexity=18.651743, train_loss=2.9259396

Batch 354500, train_perplexity=18.651743, train_loss=2.9259396

Batch 354510, train_perplexity=18.651743, train_loss=2.9259396

Batch 354520, train_perplexity=18.651743, train_loss=2.9259396

Batch 354530, train_perplexity=18.651743, train_loss=2.9259396

Batch 354540, train_perplexity=18.651743, train_loss=2.9259396

Batch 354550, train_perplexity=18.651743, train_loss=2.9259396

Batch 354560, train_perplexity=18.651743, train_loss=2.9259396

Batch 354570, train_perplexity=18.651743, train_loss=2.9259396

Batch 354580, train_perplexity=18.651743, train_loss=2.9259396

Batch 354590, train_perplexity=18.651743, train_loss=2.9259396

Batch 354600, train_perplexity=18.651743, train_loss=2.9259396

Batch 354610, train_perplexity=18.651743, train_loss=2.9259396

Batch 354620, train_perplexity=18.651743, train_loss=2.9259396

Batch 354630, train_perplexity=18.651743, train_loss=2.9259396

Batch 354640, train_perplexity=18.651743, train_loss=2.9259396

Batch 354650, train_perplexity=18.651743, train_loss=2.9259396

Batch 354660, train_perplexity=18.651743, train_loss=2.9259396

Batch 354670, train_perplexity=18.651737, train_loss=2.9259393

Batch 354680, train_perplexity=18.651737, train_loss=2.9259393

Batch 354690, train_perplexity=18.651743, train_loss=2.9259396

Batch 354700, train_perplexity=18.651737, train_loss=2.9259393

Batch 354710, train_perplexity=18.651737, train_loss=2.9259393

Batch 354720, train_perplexity=18.651733, train_loss=2.925939

Batch 354730, train_perplexity=18.651737, train_loss=2.9259393

Batch 354740, train_perplexity=18.651733, train_loss=2.925939

Batch 354750, train_perplexity=18.651733, train_loss=2.925939

Batch 354760, train_perplexity=18.651733, train_loss=2.925939

Batch 354770, train_perplexity=18.651737, train_loss=2.9259393

Batch 354780, train_perplexity=18.651733, train_loss=2.925939

Batch 354790, train_perplexity=18.651733, train_loss=2.925939

Batch 354800, train_perplexity=18.651733, train_loss=2.925939

Batch 354810, train_perplexity=18.651733, train_loss=2.925939

Batch 354820, train_perplexity=18.651733, train_loss=2.925939

Batch 354830, train_perplexity=18.651733, train_loss=2.925939

Batch 354840, train_perplexity=18.651733, train_loss=2.925939

Batch 354850, train_perplexity=18.651733, train_loss=2.925939

Batch 354860, train_perplexity=18.651737, train_loss=2.9259393

Batch 354870, train_perplexity=18.651733, train_loss=2.925939

Batch 354880, train_perplexity=18.651733, train_loss=2.925939

Batch 354890, train_perplexity=18.651733, train_loss=2.925939

Batch 354900, train_perplexity=18.651733, train_loss=2.925939

Batch 354910, train_perplexity=18.65173, train_loss=2.9259388

Batch 354920, train_perplexity=18.65173, train_loss=2.9259388

Batch 354930, train_perplexity=18.651733, train_loss=2.925939

Batch 354940, train_perplexity=18.65173, train_loss=2.9259388

Batch 354950, train_perplexity=18.65173, train_loss=2.9259388

Batch 354960, train_perplexity=18.651724, train_loss=2.9259386

Batch 354970, train_perplexity=18.65173, train_loss=2.9259388

Batch 354980, train_perplexity=18.651724, train_loss=2.9259386

Batch 354990, train_perplexity=18.65173, train_loss=2.9259388

Batch 355000, train_perplexity=18.651733, train_loss=2.925939

Batch 355010, train_perplexity=18.651724, train_loss=2.9259386

Batch 355020, train_perplexity=18.65173, train_loss=2.9259388

Batch 355030, train_perplexity=18.651733, train_loss=2.925939

Batch 355040, train_perplexity=18.65173, train_loss=2.9259388

Batch 355050, train_perplexity=18.651733, train_loss=2.925939

Batch 355060, train_perplexity=18.651724, train_loss=2.9259386

Batch 355070, train_perplexity=18.65173, train_loss=2.9259388

Batch 355080, train_perplexity=18.65173, train_loss=2.9259388

Batch 355090, train_perplexity=18.651724, train_loss=2.9259386

Batch 355100, train_perplexity=18.651724, train_loss=2.9259386

Batch 355110, train_perplexity=18.651724, train_loss=2.9259386

Batch 355120, train_perplexity=18.651724, train_loss=2.9259386

Batch 355130, train_perplexity=18.651724, train_loss=2.9259386

Batch 355140, train_perplexity=18.651724, train_loss=2.9259386

Batch 355150, train_perplexity=18.651724, train_loss=2.9259386

Batch 355160, train_perplexity=18.65172, train_loss=2.9259384

Batch 355170, train_perplexity=18.651724, train_loss=2.9259386

Batch 355180, train_perplexity=18.651724, train_loss=2.9259386

Batch 355190, train_perplexity=18.651724, train_loss=2.9259386

Batch 355200, train_perplexity=18.651724, train_loss=2.9259386

Batch 355210, train_perplexity=18.651724, train_loss=2.9259386

Batch 355220, train_perplexity=18.651724, train_loss=2.9259386

Batch 355230, train_perplexity=18.651724, train_loss=2.9259386

Batch 355240, train_perplexity=18.651724, train_loss=2.9259386

Batch 355250, train_perplexity=18.651724, train_loss=2.9259386

Batch 355260, train_perplexity=18.651724, train_loss=2.9259386

Batch 355270, train_perplexity=18.651716, train_loss=2.9259381

Batch 355280, train_perplexity=18.651716, train_loss=2.9259381

Batch 355290, train_perplexity=18.651724, train_loss=2.9259386

Batch 355300, train_perplexity=18.651716, train_loss=2.9259381

Batch 355310, train_perplexity=18.651724, train_loss=2.9259386

Batch 355320, train_perplexity=18.651716, train_loss=2.9259381

Batch 355330, train_perplexity=18.651716, train_loss=2.9259381

Batch 355340, train_perplexity=18.651716, train_loss=2.9259381

Batch 355350, train_perplexity=18.65172, train_loss=2.9259384

Batch 355360, train_perplexity=18.651724, train_loss=2.9259386

Batch 355370, train_perplexity=18.651716, train_loss=2.9259381

Batch 355380, train_perplexity=18.65172, train_loss=2.9259384

Batch 355390, train_perplexity=18.65172, train_loss=2.9259384

Batch 355400, train_perplexity=18.651716, train_loss=2.9259381

Batch 355410, train_perplexity=18.651716, train_loss=2.9259381

Batch 355420, train_perplexity=18.651716, train_loss=2.9259381

Batch 355430, train_perplexity=18.651716, train_loss=2.9259381

Batch 355440, train_perplexity=18.651716, train_loss=2.9259381

Batch 355450, train_perplexity=18.651716, train_loss=2.9259381

Batch 355460, train_perplexity=18.651716, train_loss=2.9259381

Batch 355470, train_perplexity=18.651716, train_loss=2.9259381

Batch 355480, train_perplexity=18.651716, train_loss=2.9259381

Batch 355490, train_perplexity=18.651716, train_loss=2.9259381

Batch 355500, train_perplexity=18.651716, train_loss=2.9259381

Batch 355510, train_perplexity=18.651716, train_loss=2.9259381

Batch 355520, train_perplexity=18.65171, train_loss=2.925938

Batch 355530, train_perplexity=18.65171, train_loss=2.925938

Batch 355540, train_perplexity=18.651716, train_loss=2.9259381

Batch 355550, train_perplexity=18.651716, train_loss=2.9259381

Batch 355560, train_perplexity=18.651716, train_loss=2.9259381

Batch 355570, train_perplexity=18.65171, train_loss=2.925938

Batch 355580, train_perplexity=18.651716, train_loss=2.9259381

Batch 355590, train_perplexity=18.65171, train_loss=2.925938

Batch 355600, train_perplexity=18.651716, train_loss=2.9259381

Batch 355610, train_perplexity=18.651716, train_loss=2.9259381

Batch 355620, train_perplexity=18.651707, train_loss=2.9259377

Batch 355630, train_perplexity=18.65171, train_loss=2.925938

Batch 355640, train_perplexity=18.65171, train_loss=2.925938

Batch 355650, train_perplexity=18.65171, train_loss=2.925938

Batch 355660, train_perplexity=18.65171, train_loss=2.925938
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 355670, train_perplexity=18.651707, train_loss=2.9259377

Batch 355680, train_perplexity=18.651707, train_loss=2.9259377

Batch 355690, train_perplexity=18.651716, train_loss=2.9259381

Batch 355700, train_perplexity=18.65171, train_loss=2.925938

Batch 355710, train_perplexity=18.651707, train_loss=2.9259377

Batch 355720, train_perplexity=18.651707, train_loss=2.9259377

Batch 355730, train_perplexity=18.651707, train_loss=2.9259377

Batch 355740, train_perplexity=18.651707, train_loss=2.9259377

Batch 355750, train_perplexity=18.651707, train_loss=2.9259377

Batch 355760, train_perplexity=18.651707, train_loss=2.9259377

Batch 355770, train_perplexity=18.651707, train_loss=2.9259377

Batch 355780, train_perplexity=18.651707, train_loss=2.9259377

Batch 355790, train_perplexity=18.651707, train_loss=2.9259377

Batch 355800, train_perplexity=18.651707, train_loss=2.9259377

Batch 355810, train_perplexity=18.651707, train_loss=2.9259377

Batch 355820, train_perplexity=18.651707, train_loss=2.9259377

Batch 355830, train_perplexity=18.651707, train_loss=2.9259377

Batch 355840, train_perplexity=18.651707, train_loss=2.9259377

Batch 355850, train_perplexity=18.651707, train_loss=2.9259377

Batch 355860, train_perplexity=18.651707, train_loss=2.9259377

Batch 355870, train_perplexity=18.651707, train_loss=2.9259377

Batch 355880, train_perplexity=18.651707, train_loss=2.9259377

Batch 355890, train_perplexity=18.651707, train_loss=2.9259377

Batch 355900, train_perplexity=18.651707, train_loss=2.9259377

Batch 355910, train_perplexity=18.651707, train_loss=2.9259377

Batch 355920, train_perplexity=18.651703, train_loss=2.9259374

Batch 355930, train_perplexity=18.651697, train_loss=2.9259372

Batch 355940, train_perplexity=18.651697, train_loss=2.9259372

Batch 355950, train_perplexity=18.651703, train_loss=2.9259374

Batch 355960, train_perplexity=18.651697, train_loss=2.9259372

Batch 355970, train_perplexity=18.651697, train_loss=2.9259372

Batch 355980, train_perplexity=18.651697, train_loss=2.9259372

Batch 355990, train_perplexity=18.651697, train_loss=2.9259372

Batch 356000, train_perplexity=18.651697, train_loss=2.9259372

Batch 356010, train_perplexity=18.651697, train_loss=2.9259372

Batch 356020, train_perplexity=18.651707, train_loss=2.9259377

Batch 356030, train_perplexity=18.651697, train_loss=2.9259372

Batch 356040, train_perplexity=18.651697, train_loss=2.9259372

Batch 356050, train_perplexity=18.651697, train_loss=2.9259372

Batch 356060, train_perplexity=18.651697, train_loss=2.9259372

Batch 356070, train_perplexity=18.651697, train_loss=2.9259372

Batch 356080, train_perplexity=18.651697, train_loss=2.9259372

Batch 356090, train_perplexity=18.651697, train_loss=2.9259372

Batch 356100, train_perplexity=18.651697, train_loss=2.9259372

Batch 356110, train_perplexity=18.651697, train_loss=2.9259372

Batch 356120, train_perplexity=18.651697, train_loss=2.9259372

Batch 356130, train_perplexity=18.651697, train_loss=2.9259372

Batch 356140, train_perplexity=18.651697, train_loss=2.9259372

Batch 356150, train_perplexity=18.651693, train_loss=2.925937

Batch 356160, train_perplexity=18.651697, train_loss=2.9259372

Batch 356170, train_perplexity=18.651697, train_loss=2.9259372

Batch 356180, train_perplexity=18.651697, train_loss=2.9259372

Batch 356190, train_perplexity=18.651697, train_loss=2.9259372

Batch 356200, train_perplexity=18.651693, train_loss=2.925937

Batch 356210, train_perplexity=18.651693, train_loss=2.925937

Batch 356220, train_perplexity=18.651693, train_loss=2.925937

Batch 356230, train_perplexity=18.65169, train_loss=2.9259367

Batch 356240, train_perplexity=18.65169, train_loss=2.9259367

Batch 356250, train_perplexity=18.651697, train_loss=2.9259372

Batch 356260, train_perplexity=18.651697, train_loss=2.9259372

Batch 356270, train_perplexity=18.651697, train_loss=2.9259372

Batch 356280, train_perplexity=18.65169, train_loss=2.9259367

Batch 356290, train_perplexity=18.65169, train_loss=2.9259367

Batch 356300, train_perplexity=18.65169, train_loss=2.9259367

Batch 356310, train_perplexity=18.651693, train_loss=2.925937

Batch 356320, train_perplexity=18.651693, train_loss=2.925937

Batch 356330, train_perplexity=18.651693, train_loss=2.925937

Batch 356340, train_perplexity=18.651693, train_loss=2.925937

Batch 356350, train_perplexity=18.651693, train_loss=2.925937

Batch 356360, train_perplexity=18.651693, train_loss=2.925937

Batch 356370, train_perplexity=18.65169, train_loss=2.9259367

Batch 356380, train_perplexity=18.65169, train_loss=2.9259367

Batch 356390, train_perplexity=18.65169, train_loss=2.9259367

Batch 356400, train_perplexity=18.65169, train_loss=2.9259367

Batch 356410, train_perplexity=18.65169, train_loss=2.9259367

Batch 356420, train_perplexity=18.65169, train_loss=2.9259367

Batch 356430, train_perplexity=18.65169, train_loss=2.9259367

Batch 356440, train_perplexity=18.65169, train_loss=2.9259367

Batch 356450, train_perplexity=18.65169, train_loss=2.9259367

Batch 356460, train_perplexity=18.65168, train_loss=2.9259362

Batch 356470, train_perplexity=18.65169, train_loss=2.9259367

Batch 356480, train_perplexity=18.65169, train_loss=2.9259367

Batch 356490, train_perplexity=18.65168, train_loss=2.9259362

Batch 356500, train_perplexity=18.65169, train_loss=2.9259367

Batch 356510, train_perplexity=18.65168, train_loss=2.9259362

Batch 356520, train_perplexity=18.65169, train_loss=2.9259367

Batch 356530, train_perplexity=18.65169, train_loss=2.9259367

Batch 356540, train_perplexity=18.65169, train_loss=2.9259367

Batch 356550, train_perplexity=18.65169, train_loss=2.9259367

Batch 356560, train_perplexity=18.651684, train_loss=2.9259365

Batch 356570, train_perplexity=18.65168, train_loss=2.9259362

Batch 356580, train_perplexity=18.651684, train_loss=2.9259365

Batch 356590, train_perplexity=18.651684, train_loss=2.9259365

Batch 356600, train_perplexity=18.651684, train_loss=2.9259365

Batch 356610, train_perplexity=18.651684, train_loss=2.9259365

Batch 356620, train_perplexity=18.65168, train_loss=2.9259362

Batch 356630, train_perplexity=18.65169, train_loss=2.9259367

Batch 356640, train_perplexity=18.65168, train_loss=2.9259362

Batch 356650, train_perplexity=18.65169, train_loss=2.9259367

Batch 356660, train_perplexity=18.65168, train_loss=2.9259362

Batch 356670, train_perplexity=18.651684, train_loss=2.9259365

Batch 356680, train_perplexity=18.65169, train_loss=2.9259367

Batch 356690, train_perplexity=18.65168, train_loss=2.9259362

Batch 356700, train_perplexity=18.65168, train_loss=2.9259362

Batch 356710, train_perplexity=18.65168, train_loss=2.9259362

Batch 356720, train_perplexity=18.65168, train_loss=2.9259362

Batch 356730, train_perplexity=18.65168, train_loss=2.9259362

Batch 356740, train_perplexity=18.65168, train_loss=2.9259362

Batch 356750, train_perplexity=18.65168, train_loss=2.9259362

Batch 356760, train_perplexity=18.651676, train_loss=2.925936

Batch 356770, train_perplexity=18.65168, train_loss=2.9259362

Batch 356780, train_perplexity=18.65168, train_loss=2.9259362

Batch 356790, train_perplexity=18.65168, train_loss=2.9259362

Batch 356800, train_perplexity=18.65168, train_loss=2.9259362

Batch 356810, train_perplexity=18.65168, train_loss=2.9259362

Batch 356820, train_perplexity=18.65168, train_loss=2.9259362

Batch 356830, train_perplexity=18.651676, train_loss=2.925936

Batch 356840, train_perplexity=18.651676, train_loss=2.925936

Batch 356850, train_perplexity=18.65168, train_loss=2.9259362

Batch 356860, train_perplexity=18.65168, train_loss=2.9259362

Batch 356870, train_perplexity=18.651676, train_loss=2.925936

Batch 356880, train_perplexity=18.651676, train_loss=2.925936

Batch 356890, train_perplexity=18.651676, train_loss=2.925936

Batch 356900, train_perplexity=18.651676, train_loss=2.925936

Batch 356910, train_perplexity=18.651676, train_loss=2.925936

Batch 356920, train_perplexity=18.651676, train_loss=2.925936

Batch 356930, train_perplexity=18.65168, train_loss=2.9259362

Batch 356940, train_perplexity=18.65167, train_loss=2.9259357

Batch 356950, train_perplexity=18.651676, train_loss=2.925936

Batch 356960, train_perplexity=18.65167, train_loss=2.9259357
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 356970, train_perplexity=18.65167, train_loss=2.9259357

Batch 356980, train_perplexity=18.65167, train_loss=2.9259357

Batch 356990, train_perplexity=18.65167, train_loss=2.9259357

Batch 357000, train_perplexity=18.65167, train_loss=2.9259357

Batch 357010, train_perplexity=18.65167, train_loss=2.9259357

Batch 357020, train_perplexity=18.65167, train_loss=2.9259357

Batch 357030, train_perplexity=18.65167, train_loss=2.9259357

Batch 357040, train_perplexity=18.65167, train_loss=2.9259357

Batch 357050, train_perplexity=18.65167, train_loss=2.9259357

Batch 357060, train_perplexity=18.65167, train_loss=2.9259357

Batch 357070, train_perplexity=18.65167, train_loss=2.9259357

Batch 357080, train_perplexity=18.65167, train_loss=2.9259357

Batch 357090, train_perplexity=18.65167, train_loss=2.9259357

Batch 357100, train_perplexity=18.65167, train_loss=2.9259357

Batch 357110, train_perplexity=18.65167, train_loss=2.9259357

Batch 357120, train_perplexity=18.65167, train_loss=2.9259357

Batch 357130, train_perplexity=18.65167, train_loss=2.9259357

Batch 357140, train_perplexity=18.651667, train_loss=2.9259355

Batch 357150, train_perplexity=18.651663, train_loss=2.9259353

Batch 357160, train_perplexity=18.65167, train_loss=2.9259357

Batch 357170, train_perplexity=18.651667, train_loss=2.9259355

Batch 357180, train_perplexity=18.65167, train_loss=2.9259357

Batch 357190, train_perplexity=18.65167, train_loss=2.9259357

Batch 357200, train_perplexity=18.651663, train_loss=2.9259353

Batch 357210, train_perplexity=18.651663, train_loss=2.9259353

Batch 357220, train_perplexity=18.651667, train_loss=2.9259355

Batch 357230, train_perplexity=18.651663, train_loss=2.9259353

Batch 357240, train_perplexity=18.651663, train_loss=2.9259353

Batch 357250, train_perplexity=18.651667, train_loss=2.9259355

Batch 357260, train_perplexity=18.651663, train_loss=2.9259353

Batch 357270, train_perplexity=18.651663, train_loss=2.9259353

Batch 357280, train_perplexity=18.651663, train_loss=2.9259353

Batch 357290, train_perplexity=18.651663, train_loss=2.9259353

Batch 357300, train_perplexity=18.651663, train_loss=2.9259353

Batch 357310, train_perplexity=18.651663, train_loss=2.9259353

Batch 357320, train_perplexity=18.651663, train_loss=2.9259353

Batch 357330, train_perplexity=18.651663, train_loss=2.9259353

Batch 357340, train_perplexity=18.651663, train_loss=2.9259353

Batch 357350, train_perplexity=18.651663, train_loss=2.9259353

Batch 357360, train_perplexity=18.651663, train_loss=2.9259353

Batch 357370, train_perplexity=18.651663, train_loss=2.9259353

Batch 357380, train_perplexity=18.651663, train_loss=2.9259353

Batch 357390, train_perplexity=18.651657, train_loss=2.925935

Batch 357400, train_perplexity=18.651663, train_loss=2.9259353

Batch 357410, train_perplexity=18.651663, train_loss=2.9259353

Batch 357420, train_perplexity=18.651663, train_loss=2.9259353

Batch 357430, train_perplexity=18.651663, train_loss=2.9259353

Batch 357440, train_perplexity=18.651663, train_loss=2.9259353

Batch 357450, train_perplexity=18.651663, train_loss=2.9259353

Batch 357460, train_perplexity=18.651663, train_loss=2.9259353

Batch 357470, train_perplexity=18.651663, train_loss=2.9259353

Batch 357480, train_perplexity=18.651663, train_loss=2.9259353

Batch 357490, train_perplexity=18.651657, train_loss=2.925935

Batch 357500, train_perplexity=18.651657, train_loss=2.925935

Batch 357510, train_perplexity=18.651657, train_loss=2.925935

Batch 357520, train_perplexity=18.651663, train_loss=2.9259353

Batch 357530, train_perplexity=18.651653, train_loss=2.9259348

Batch 357540, train_perplexity=18.651653, train_loss=2.9259348

Batch 357550, train_perplexity=18.651663, train_loss=2.9259353

Batch 357560, train_perplexity=18.651653, train_loss=2.9259348

Batch 357570, train_perplexity=18.651657, train_loss=2.925935

Batch 357580, train_perplexity=18.651657, train_loss=2.925935

Batch 357590, train_perplexity=18.651653, train_loss=2.9259348

Batch 357600, train_perplexity=18.651653, train_loss=2.9259348

Batch 357610, train_perplexity=18.651653, train_loss=2.9259348

Batch 357620, train_perplexity=18.651653, train_loss=2.9259348

Batch 357630, train_perplexity=18.651653, train_loss=2.9259348

Batch 357640, train_perplexity=18.651653, train_loss=2.9259348

Batch 357650, train_perplexity=18.651653, train_loss=2.9259348

Batch 357660, train_perplexity=18.651653, train_loss=2.9259348

Batch 357670, train_perplexity=18.651653, train_loss=2.9259348

Batch 357680, train_perplexity=18.651653, train_loss=2.9259348

Batch 357690, train_perplexity=18.651657, train_loss=2.925935

Batch 357700, train_perplexity=18.651653, train_loss=2.9259348

Batch 357710, train_perplexity=18.651653, train_loss=2.9259348

Batch 357720, train_perplexity=18.651653, train_loss=2.9259348

Batch 357730, train_perplexity=18.651653, train_loss=2.9259348

Batch 357740, train_perplexity=18.651653, train_loss=2.9259348

Batch 357750, train_perplexity=18.651653, train_loss=2.9259348

Batch 357760, train_perplexity=18.651653, train_loss=2.9259348

Batch 357770, train_perplexity=18.651653, train_loss=2.9259348

Batch 357780, train_perplexity=18.651653, train_loss=2.9259348

Batch 357790, train_perplexity=18.651653, train_loss=2.9259348

Batch 357800, train_perplexity=18.65165, train_loss=2.9259346

Batch 357810, train_perplexity=18.651644, train_loss=2.9259343

Batch 357820, train_perplexity=18.651653, train_loss=2.9259348

Batch 357830, train_perplexity=18.651653, train_loss=2.9259348

Batch 357840, train_perplexity=18.65165, train_loss=2.9259346

Batch 357850, train_perplexity=18.651644, train_loss=2.9259343

Batch 357860, train_perplexity=18.651644, train_loss=2.9259343

Batch 357870, train_perplexity=18.651653, train_loss=2.9259348

Batch 357880, train_perplexity=18.651644, train_loss=2.9259343

Batch 357890, train_perplexity=18.651644, train_loss=2.9259343

Batch 357900, train_perplexity=18.651644, train_loss=2.9259343

Batch 357910, train_perplexity=18.651644, train_loss=2.9259343

Batch 357920, train_perplexity=18.651644, train_loss=2.9259343

Batch 357930, train_perplexity=18.65165, train_loss=2.9259346

Batch 357940, train_perplexity=18.651644, train_loss=2.9259343

Batch 357950, train_perplexity=18.651644, train_loss=2.9259343

Batch 357960, train_perplexity=18.651644, train_loss=2.9259343

Batch 357970, train_perplexity=18.651644, train_loss=2.9259343

Batch 357980, train_perplexity=18.651644, train_loss=2.9259343

Batch 357990, train_perplexity=18.651644, train_loss=2.9259343

Batch 358000, train_perplexity=18.651644, train_loss=2.9259343

Batch 358010, train_perplexity=18.651644, train_loss=2.9259343

Batch 358020, train_perplexity=18.651644, train_loss=2.9259343

Batch 358030, train_perplexity=18.651644, train_loss=2.9259343

Batch 358040, train_perplexity=18.651644, train_loss=2.9259343

Batch 358050, train_perplexity=18.651644, train_loss=2.9259343

Batch 358060, train_perplexity=18.651644, train_loss=2.9259343

Batch 358070, train_perplexity=18.651644, train_loss=2.9259343

Batch 358080, train_perplexity=18.65164, train_loss=2.925934

Batch 358090, train_perplexity=18.651644, train_loss=2.9259343

Batch 358100, train_perplexity=18.651644, train_loss=2.9259343

Batch 358110, train_perplexity=18.651636, train_loss=2.9259338

Batch 358120, train_perplexity=18.65164, train_loss=2.925934

Batch 358130, train_perplexity=18.65164, train_loss=2.925934

Batch 358140, train_perplexity=18.65164, train_loss=2.925934

Batch 358150, train_perplexity=18.65164, train_loss=2.925934

Batch 358160, train_perplexity=18.651636, train_loss=2.9259338

Batch 358170, train_perplexity=18.65164, train_loss=2.925934

Batch 358180, train_perplexity=18.65164, train_loss=2.925934

Batch 358190, train_perplexity=18.651636, train_loss=2.9259338

Batch 358200, train_perplexity=18.651636, train_loss=2.9259338

Batch 358210, train_perplexity=18.651636, train_loss=2.9259338

Batch 358220, train_perplexity=18.651636, train_loss=2.9259338

Batch 358230, train_perplexity=18.65164, train_loss=2.925934

Batch 358240, train_perplexity=18.651636, train_loss=2.9259338

Batch 358250, train_perplexity=18.651636, train_loss=2.9259338
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 358260, train_perplexity=18.651636, train_loss=2.9259338

Batch 358270, train_perplexity=18.651636, train_loss=2.9259338

Batch 358280, train_perplexity=18.651636, train_loss=2.9259338

Batch 358290, train_perplexity=18.651636, train_loss=2.9259338

Batch 358300, train_perplexity=18.651636, train_loss=2.9259338

Batch 358310, train_perplexity=18.651636, train_loss=2.9259338

Batch 358320, train_perplexity=18.651636, train_loss=2.9259338

Batch 358330, train_perplexity=18.651636, train_loss=2.9259338

Batch 358340, train_perplexity=18.651636, train_loss=2.9259338

Batch 358350, train_perplexity=18.651636, train_loss=2.9259338

Batch 358360, train_perplexity=18.651627, train_loss=2.9259334

Batch 358370, train_perplexity=18.651636, train_loss=2.9259338

Batch 358380, train_perplexity=18.651636, train_loss=2.9259338

Batch 358390, train_perplexity=18.651636, train_loss=2.9259338

Batch 358400, train_perplexity=18.651636, train_loss=2.9259338

Batch 358410, train_perplexity=18.651636, train_loss=2.9259338

Batch 358420, train_perplexity=18.651636, train_loss=2.9259338

Batch 358430, train_perplexity=18.65163, train_loss=2.9259336

Batch 358440, train_perplexity=18.65163, train_loss=2.9259336

Batch 358450, train_perplexity=18.651627, train_loss=2.9259334

Batch 358460, train_perplexity=18.651627, train_loss=2.9259334

Batch 358470, train_perplexity=18.651627, train_loss=2.9259334

Batch 358480, train_perplexity=18.651636, train_loss=2.9259338

Batch 358490, train_perplexity=18.651636, train_loss=2.9259338

Batch 358500, train_perplexity=18.651627, train_loss=2.9259334

Batch 358510, train_perplexity=18.651627, train_loss=2.9259334

Batch 358520, train_perplexity=18.651627, train_loss=2.9259334

Batch 358530, train_perplexity=18.65163, train_loss=2.9259336

Batch 358540, train_perplexity=18.651627, train_loss=2.9259334

Batch 358550, train_perplexity=18.651636, train_loss=2.9259338

Batch 358560, train_perplexity=18.651627, train_loss=2.9259334

Batch 358570, train_perplexity=18.651627, train_loss=2.9259334

Batch 358580, train_perplexity=18.651627, train_loss=2.9259334

Batch 358590, train_perplexity=18.651627, train_loss=2.9259334

Batch 358600, train_perplexity=18.651627, train_loss=2.9259334

Batch 358610, train_perplexity=18.651623, train_loss=2.9259331

Batch 358620, train_perplexity=18.651627, train_loss=2.9259334

Batch 358630, train_perplexity=18.651627, train_loss=2.9259334

Batch 358640, train_perplexity=18.651627, train_loss=2.9259334

Batch 358650, train_perplexity=18.651627, train_loss=2.9259334

Batch 358660, train_perplexity=18.651627, train_loss=2.9259334

Batch 358670, train_perplexity=18.651627, train_loss=2.9259334

Batch 358680, train_perplexity=18.651627, train_loss=2.9259334

Batch 358690, train_perplexity=18.651627, train_loss=2.9259334

Batch 358700, train_perplexity=18.651627, train_loss=2.9259334

Batch 358710, train_perplexity=18.651627, train_loss=2.9259334

Batch 358720, train_perplexity=18.651617, train_loss=2.925933

Batch 358730, train_perplexity=18.651627, train_loss=2.9259334

Batch 358740, train_perplexity=18.651617, train_loss=2.925933

Batch 358750, train_perplexity=18.651627, train_loss=2.9259334

Batch 358760, train_perplexity=18.651627, train_loss=2.9259334

Batch 358770, train_perplexity=18.651623, train_loss=2.9259331

Batch 358780, train_perplexity=18.651623, train_loss=2.9259331

Batch 358790, train_perplexity=18.651623, train_loss=2.9259331

Batch 358800, train_perplexity=18.651623, train_loss=2.9259331

Batch 358810, train_perplexity=18.651623, train_loss=2.9259331

Batch 358820, train_perplexity=18.651623, train_loss=2.9259331

Batch 358830, train_perplexity=18.651623, train_loss=2.9259331

Batch 358840, train_perplexity=18.651623, train_loss=2.9259331

Batch 358850, train_perplexity=18.651623, train_loss=2.9259331

Batch 358860, train_perplexity=18.651617, train_loss=2.925933

Batch 358870, train_perplexity=18.651617, train_loss=2.925933

Batch 358880, train_perplexity=18.651617, train_loss=2.925933

Batch 358890, train_perplexity=18.651617, train_loss=2.925933

Batch 358900, train_perplexity=18.651617, train_loss=2.925933

Batch 358910, train_perplexity=18.651617, train_loss=2.925933

Batch 358920, train_perplexity=18.651617, train_loss=2.925933

Batch 358930, train_perplexity=18.651617, train_loss=2.925933

Batch 358940, train_perplexity=18.651617, train_loss=2.925933

Batch 358950, train_perplexity=18.651617, train_loss=2.925933

Batch 358960, train_perplexity=18.651617, train_loss=2.925933

Batch 358970, train_perplexity=18.651617, train_loss=2.925933

Batch 358980, train_perplexity=18.651617, train_loss=2.925933

Batch 358990, train_perplexity=18.651617, train_loss=2.925933

Batch 359000, train_perplexity=18.651617, train_loss=2.925933

Batch 359010, train_perplexity=18.65161, train_loss=2.9259324

Batch 359020, train_perplexity=18.651617, train_loss=2.925933

Batch 359030, train_perplexity=18.651617, train_loss=2.925933

Batch 359040, train_perplexity=18.651617, train_loss=2.925933

Batch 359050, train_perplexity=18.651613, train_loss=2.9259326

Batch 359060, train_perplexity=18.651613, train_loss=2.9259326

Batch 359070, train_perplexity=18.651617, train_loss=2.925933

Batch 359080, train_perplexity=18.65161, train_loss=2.9259324

Batch 359090, train_perplexity=18.65161, train_loss=2.9259324

Batch 359100, train_perplexity=18.651617, train_loss=2.925933

Batch 359110, train_perplexity=18.65161, train_loss=2.9259324

Batch 359120, train_perplexity=18.65161, train_loss=2.9259324

Batch 359130, train_perplexity=18.65161, train_loss=2.9259324

Batch 359140, train_perplexity=18.65161, train_loss=2.9259324

Batch 359150, train_perplexity=18.65161, train_loss=2.9259324

Batch 359160, train_perplexity=18.65161, train_loss=2.9259324

Batch 359170, train_perplexity=18.65161, train_loss=2.9259324

Batch 359180, train_perplexity=18.65161, train_loss=2.9259324

Batch 359190, train_perplexity=18.65161, train_loss=2.9259324

Batch 359200, train_perplexity=18.65161, train_loss=2.9259324

Batch 359210, train_perplexity=18.65161, train_loss=2.9259324

Batch 359220, train_perplexity=18.65161, train_loss=2.9259324

Batch 359230, train_perplexity=18.65161, train_loss=2.9259324

Batch 359240, train_perplexity=18.65161, train_loss=2.9259324

Batch 359250, train_perplexity=18.65161, train_loss=2.9259324

Batch 359260, train_perplexity=18.65161, train_loss=2.9259324

Batch 359270, train_perplexity=18.65161, train_loss=2.9259324

Batch 359280, train_perplexity=18.65161, train_loss=2.9259324

Batch 359290, train_perplexity=18.65161, train_loss=2.9259324

Batch 359300, train_perplexity=18.65161, train_loss=2.9259324

Batch 359310, train_perplexity=18.651604, train_loss=2.9259322

Batch 359320, train_perplexity=18.651604, train_loss=2.9259322

Batch 359330, train_perplexity=18.65161, train_loss=2.9259324

Batch 359340, train_perplexity=18.65161, train_loss=2.9259324

Batch 359350, train_perplexity=18.65161, train_loss=2.9259324

Batch 359360, train_perplexity=18.651604, train_loss=2.9259322

Batch 359370, train_perplexity=18.65161, train_loss=2.9259324

Batch 359380, train_perplexity=18.6516, train_loss=2.925932

Batch 359390, train_perplexity=18.651604, train_loss=2.9259322

Batch 359400, train_perplexity=18.65161, train_loss=2.9259324

Batch 359410, train_perplexity=18.6516, train_loss=2.925932

Batch 359420, train_perplexity=18.651604, train_loss=2.9259322

Batch 359430, train_perplexity=18.65161, train_loss=2.9259324

Batch 359440, train_perplexity=18.651604, train_loss=2.9259322

Batch 359450, train_perplexity=18.651604, train_loss=2.9259322

Batch 359460, train_perplexity=18.651604, train_loss=2.9259322

Batch 359470, train_perplexity=18.6516, train_loss=2.925932

Batch 359480, train_perplexity=18.6516, train_loss=2.925932

Batch 359490, train_perplexity=18.6516, train_loss=2.925932

Batch 359500, train_perplexity=18.651604, train_loss=2.9259322

Batch 359510, train_perplexity=18.6516, train_loss=2.925932

Batch 359520, train_perplexity=18.6516, train_loss=2.925932

Batch 359530, train_perplexity=18.6516, train_loss=2.925932

Batch 359540, train_perplexity=18.65159, train_loss=2.9259315

Batch 359550, train_perplexity=18.6516, train_loss=2.925932
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 359560, train_perplexity=18.6516, train_loss=2.925932

Batch 359570, train_perplexity=18.651604, train_loss=2.9259322

Batch 359580, train_perplexity=18.6516, train_loss=2.925932

Batch 359590, train_perplexity=18.6516, train_loss=2.925932

Batch 359600, train_perplexity=18.6516, train_loss=2.925932

Batch 359610, train_perplexity=18.6516, train_loss=2.925932

Batch 359620, train_perplexity=18.6516, train_loss=2.925932

Batch 359630, train_perplexity=18.6516, train_loss=2.925932

Batch 359640, train_perplexity=18.6516, train_loss=2.925932

Batch 359650, train_perplexity=18.6516, train_loss=2.925932

Batch 359660, train_perplexity=18.6516, train_loss=2.925932

Batch 359670, train_perplexity=18.6516, train_loss=2.925932

Batch 359680, train_perplexity=18.6516, train_loss=2.925932

Batch 359690, train_perplexity=18.651596, train_loss=2.9259317

Batch 359700, train_perplexity=18.6516, train_loss=2.925932

Batch 359710, train_perplexity=18.6516, train_loss=2.925932

Batch 359720, train_perplexity=18.651596, train_loss=2.9259317

Batch 359730, train_perplexity=18.6516, train_loss=2.925932

Batch 359740, train_perplexity=18.6516, train_loss=2.925932

Batch 359750, train_perplexity=18.65159, train_loss=2.9259315

Batch 359760, train_perplexity=18.65159, train_loss=2.9259315

Batch 359770, train_perplexity=18.65159, train_loss=2.9259315

Batch 359780, train_perplexity=18.65159, train_loss=2.9259315

Batch 359790, train_perplexity=18.65159, train_loss=2.9259315

Batch 359800, train_perplexity=18.65159, train_loss=2.9259315

Batch 359810, train_perplexity=18.65159, train_loss=2.9259315

Batch 359820, train_perplexity=18.6516, train_loss=2.925932

Batch 359830, train_perplexity=18.651596, train_loss=2.9259317

Batch 359840, train_perplexity=18.65159, train_loss=2.9259315

Batch 359850, train_perplexity=18.65159, train_loss=2.9259315

Batch 359860, train_perplexity=18.65159, train_loss=2.9259315

Batch 359870, train_perplexity=18.65159, train_loss=2.9259315

Batch 359880, train_perplexity=18.65159, train_loss=2.9259315

Batch 359890, train_perplexity=18.65159, train_loss=2.9259315

Batch 359900, train_perplexity=18.65159, train_loss=2.9259315

Batch 359910, train_perplexity=18.65159, train_loss=2.9259315

Batch 359920, train_perplexity=18.65159, train_loss=2.9259315

Batch 359930, train_perplexity=18.65159, train_loss=2.9259315

Batch 359940, train_perplexity=18.65159, train_loss=2.9259315

Batch 359950, train_perplexity=18.65159, train_loss=2.9259315

Batch 359960, train_perplexity=18.65159, train_loss=2.9259315

Batch 359970, train_perplexity=18.65159, train_loss=2.9259315

Batch 359980, train_perplexity=18.65159, train_loss=2.9259315

Batch 359990, train_perplexity=18.65159, train_loss=2.9259315

Batch 360000, train_perplexity=18.65159, train_loss=2.9259315

Batch 360010, train_perplexity=18.65159, train_loss=2.9259315

Batch 360020, train_perplexity=18.651583, train_loss=2.925931

Batch 360030, train_perplexity=18.651587, train_loss=2.9259312

Batch 360040, train_perplexity=18.651587, train_loss=2.9259312

Batch 360050, train_perplexity=18.65159, train_loss=2.9259315

Batch 360060, train_perplexity=18.651587, train_loss=2.9259312

Batch 360070, train_perplexity=18.651587, train_loss=2.9259312

Batch 360080, train_perplexity=18.651587, train_loss=2.9259312

Batch 360090, train_perplexity=18.651583, train_loss=2.925931

Batch 360100, train_perplexity=18.651583, train_loss=2.925931

Batch 360110, train_perplexity=18.651583, train_loss=2.925931

Batch 360120, train_perplexity=18.651587, train_loss=2.9259312

Batch 360130, train_perplexity=18.651583, train_loss=2.925931

Batch 360140, train_perplexity=18.651583, train_loss=2.925931

Batch 360150, train_perplexity=18.651583, train_loss=2.925931

Batch 360160, train_perplexity=18.651587, train_loss=2.9259312

Batch 360170, train_perplexity=18.651583, train_loss=2.925931

Batch 360180, train_perplexity=18.651583, train_loss=2.925931

Batch 360190, train_perplexity=18.651583, train_loss=2.925931

Batch 360200, train_perplexity=18.651583, train_loss=2.925931

Batch 360210, train_perplexity=18.651583, train_loss=2.925931

Batch 360220, train_perplexity=18.651583, train_loss=2.925931

Batch 360230, train_perplexity=18.651583, train_loss=2.925931

Batch 360240, train_perplexity=18.651583, train_loss=2.925931

Batch 360250, train_perplexity=18.651583, train_loss=2.925931

Batch 360260, train_perplexity=18.651583, train_loss=2.925931

Batch 360270, train_perplexity=18.651583, train_loss=2.925931

Batch 360280, train_perplexity=18.651583, train_loss=2.925931

Batch 360290, train_perplexity=18.651583, train_loss=2.925931

Batch 360300, train_perplexity=18.651573, train_loss=2.9259305

Batch 360310, train_perplexity=18.651577, train_loss=2.9259307

Batch 360320, train_perplexity=18.651583, train_loss=2.925931

Batch 360330, train_perplexity=18.651577, train_loss=2.9259307

Batch 360340, train_perplexity=18.651577, train_loss=2.9259307

Batch 360350, train_perplexity=18.651577, train_loss=2.9259307

Batch 360360, train_perplexity=18.651573, train_loss=2.9259305

Batch 360370, train_perplexity=18.651583, train_loss=2.925931

Batch 360380, train_perplexity=18.651583, train_loss=2.925931

Batch 360390, train_perplexity=18.651573, train_loss=2.9259305

Batch 360400, train_perplexity=18.651573, train_loss=2.9259305

Batch 360410, train_perplexity=18.651573, train_loss=2.9259305

Batch 360420, train_perplexity=18.651573, train_loss=2.9259305

Batch 360430, train_perplexity=18.651573, train_loss=2.9259305

Batch 360440, train_perplexity=18.651573, train_loss=2.9259305

Batch 360450, train_perplexity=18.651573, train_loss=2.9259305

Batch 360460, train_perplexity=18.651573, train_loss=2.9259305

Batch 360470, train_perplexity=18.651573, train_loss=2.9259305

Batch 360480, train_perplexity=18.651573, train_loss=2.9259305

Batch 360490, train_perplexity=18.65157, train_loss=2.9259303

Batch 360500, train_perplexity=18.651573, train_loss=2.9259305

Batch 360510, train_perplexity=18.651573, train_loss=2.9259305

Batch 360520, train_perplexity=18.651573, train_loss=2.9259305

Batch 360530, train_perplexity=18.65157, train_loss=2.9259303

Batch 360540, train_perplexity=18.651573, train_loss=2.9259305

Batch 360550, train_perplexity=18.651573, train_loss=2.9259305

Batch 360560, train_perplexity=18.651573, train_loss=2.9259305

Batch 360570, train_perplexity=18.651573, train_loss=2.9259305

Batch 360580, train_perplexity=18.651573, train_loss=2.9259305

Batch 360590, train_perplexity=18.651573, train_loss=2.9259305

Batch 360600, train_perplexity=18.65157, train_loss=2.9259303

Batch 360610, train_perplexity=18.65157, train_loss=2.9259303

Batch 360620, train_perplexity=18.651573, train_loss=2.9259305

Batch 360630, train_perplexity=18.65157, train_loss=2.9259303

Batch 360640, train_perplexity=18.65157, train_loss=2.9259303

Batch 360650, train_perplexity=18.65157, train_loss=2.9259303

Batch 360660, train_perplexity=18.651564, train_loss=2.92593

Batch 360670, train_perplexity=18.65157, train_loss=2.9259303

Batch 360680, train_perplexity=18.65157, train_loss=2.9259303

Batch 360690, train_perplexity=18.65157, train_loss=2.9259303

Batch 360700, train_perplexity=18.651564, train_loss=2.92593

Batch 360710, train_perplexity=18.65157, train_loss=2.9259303

Batch 360720, train_perplexity=18.651564, train_loss=2.92593

Batch 360730, train_perplexity=18.65157, train_loss=2.9259303

Batch 360740, train_perplexity=18.651564, train_loss=2.92593

Batch 360750, train_perplexity=18.65157, train_loss=2.9259303

Batch 360760, train_perplexity=18.651564, train_loss=2.92593

Batch 360770, train_perplexity=18.65157, train_loss=2.9259303

Batch 360780, train_perplexity=18.651564, train_loss=2.92593

Batch 360790, train_perplexity=18.651564, train_loss=2.92593

Batch 360800, train_perplexity=18.651564, train_loss=2.92593

Batch 360810, train_perplexity=18.651564, train_loss=2.92593

Batch 360820, train_perplexity=18.651564, train_loss=2.92593

Batch 360830, train_perplexity=18.65156, train_loss=2.9259298

Batch 360840, train_perplexity=18.651564, train_loss=2.92593

Batch 360850, train_perplexity=18.651564, train_loss=2.92593

Batch 360860, train_perplexity=18.651564, train_loss=2.92593
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 360870, train_perplexity=18.651564, train_loss=2.92593

Batch 360880, train_perplexity=18.651556, train_loss=2.9259295

Batch 360890, train_perplexity=18.651564, train_loss=2.92593

Batch 360900, train_perplexity=18.651564, train_loss=2.92593

Batch 360910, train_perplexity=18.651564, train_loss=2.92593

Batch 360920, train_perplexity=18.65156, train_loss=2.9259298

Batch 360930, train_perplexity=18.651564, train_loss=2.92593

Batch 360940, train_perplexity=18.651564, train_loss=2.92593

Batch 360950, train_perplexity=18.651564, train_loss=2.92593

Batch 360960, train_perplexity=18.651564, train_loss=2.92593

Batch 360970, train_perplexity=18.651564, train_loss=2.92593

Batch 360980, train_perplexity=18.651556, train_loss=2.9259295

Batch 360990, train_perplexity=18.651564, train_loss=2.92593

Batch 361000, train_perplexity=18.651564, train_loss=2.92593

Batch 361010, train_perplexity=18.651556, train_loss=2.9259295

Batch 361020, train_perplexity=18.65156, train_loss=2.9259298

Batch 361030, train_perplexity=18.651556, train_loss=2.9259295

Batch 361040, train_perplexity=18.651556, train_loss=2.9259295

Batch 361050, train_perplexity=18.65155, train_loss=2.9259293

Batch 361060, train_perplexity=18.651556, train_loss=2.9259295

Batch 361070, train_perplexity=18.651556, train_loss=2.9259295

Batch 361080, train_perplexity=18.651556, train_loss=2.9259295

Batch 361090, train_perplexity=18.651556, train_loss=2.9259295

Batch 361100, train_perplexity=18.651556, train_loss=2.9259295

Batch 361110, train_perplexity=18.651556, train_loss=2.9259295

Batch 361120, train_perplexity=18.651556, train_loss=2.9259295

Batch 361130, train_perplexity=18.651556, train_loss=2.9259295

Batch 361140, train_perplexity=18.651556, train_loss=2.9259295

Batch 361150, train_perplexity=18.65156, train_loss=2.9259298

Batch 361160, train_perplexity=18.651556, train_loss=2.9259295

Batch 361170, train_perplexity=18.651546, train_loss=2.925929

Batch 361180, train_perplexity=18.65155, train_loss=2.9259293

Batch 361190, train_perplexity=18.65155, train_loss=2.9259293

Batch 361200, train_perplexity=18.651556, train_loss=2.9259295

Batch 361210, train_perplexity=18.65155, train_loss=2.9259293

Batch 361220, train_perplexity=18.651556, train_loss=2.9259295

Batch 361230, train_perplexity=18.651556, train_loss=2.9259295

Batch 361240, train_perplexity=18.65155, train_loss=2.9259293

Batch 361250, train_perplexity=18.651556, train_loss=2.9259295

Batch 361260, train_perplexity=18.65155, train_loss=2.9259293

Batch 361270, train_perplexity=18.651556, train_loss=2.9259295

Batch 361280, train_perplexity=18.651546, train_loss=2.925929

Batch 361290, train_perplexity=18.651546, train_loss=2.925929

Batch 361300, train_perplexity=18.651546, train_loss=2.925929

Batch 361310, train_perplexity=18.65155, train_loss=2.9259293

Batch 361320, train_perplexity=18.651546, train_loss=2.925929

Batch 361330, train_perplexity=18.65155, train_loss=2.9259293

Batch 361340, train_perplexity=18.65155, train_loss=2.9259293

Batch 361350, train_perplexity=18.651546, train_loss=2.925929

Batch 361360, train_perplexity=18.651546, train_loss=2.925929

Batch 361370, train_perplexity=18.651546, train_loss=2.925929

Batch 361380, train_perplexity=18.65155, train_loss=2.9259293

Batch 361390, train_perplexity=18.651546, train_loss=2.925929

Batch 361400, train_perplexity=18.651546, train_loss=2.925929

Batch 361410, train_perplexity=18.651546, train_loss=2.925929

Batch 361420, train_perplexity=18.651546, train_loss=2.925929

Batch 361430, train_perplexity=18.651546, train_loss=2.925929

Batch 361440, train_perplexity=18.651546, train_loss=2.925929

Batch 361450, train_perplexity=18.651546, train_loss=2.925929

Batch 361460, train_perplexity=18.651546, train_loss=2.925929

Batch 361470, train_perplexity=18.651543, train_loss=2.9259288

Batch 361480, train_perplexity=18.651546, train_loss=2.925929

Batch 361490, train_perplexity=18.651546, train_loss=2.925929

Batch 361500, train_perplexity=18.651546, train_loss=2.925929

Batch 361510, train_perplexity=18.651546, train_loss=2.925929

Batch 361520, train_perplexity=18.651546, train_loss=2.925929

Batch 361530, train_perplexity=18.651546, train_loss=2.925929

Batch 361540, train_perplexity=18.651546, train_loss=2.925929

Batch 361550, train_perplexity=18.651543, train_loss=2.9259288

Batch 361560, train_perplexity=18.651546, train_loss=2.925929

Batch 361570, train_perplexity=18.651546, train_loss=2.925929

Batch 361580, train_perplexity=18.651537, train_loss=2.9259286

Batch 361590, train_perplexity=18.651543, train_loss=2.9259288

Batch 361600, train_perplexity=18.651546, train_loss=2.925929

Batch 361610, train_perplexity=18.651537, train_loss=2.9259286

Batch 361620, train_perplexity=18.651546, train_loss=2.925929

Batch 361630, train_perplexity=18.651543, train_loss=2.9259288

Batch 361640, train_perplexity=18.651537, train_loss=2.9259286

Batch 361650, train_perplexity=18.651546, train_loss=2.925929

Batch 361660, train_perplexity=18.651537, train_loss=2.9259286

Batch 361670, train_perplexity=18.651537, train_loss=2.9259286

Batch 361680, train_perplexity=18.651546, train_loss=2.925929

Batch 361690, train_perplexity=18.651537, train_loss=2.9259286

Batch 361700, train_perplexity=18.651537, train_loss=2.9259286

Batch 361710, train_perplexity=18.651537, train_loss=2.9259286

Batch 361720, train_perplexity=18.651537, train_loss=2.9259286

Batch 361730, train_perplexity=18.651537, train_loss=2.9259286

Batch 361740, train_perplexity=18.651537, train_loss=2.9259286

Batch 361750, train_perplexity=18.651537, train_loss=2.9259286

Batch 361760, train_perplexity=18.651537, train_loss=2.9259286

Batch 361770, train_perplexity=18.651537, train_loss=2.9259286

Batch 361780, train_perplexity=18.651537, train_loss=2.9259286

Batch 361790, train_perplexity=18.651537, train_loss=2.9259286

Batch 361800, train_perplexity=18.651537, train_loss=2.9259286

Batch 361810, train_perplexity=18.651537, train_loss=2.9259286

Batch 361820, train_perplexity=18.651537, train_loss=2.9259286

Batch 361830, train_perplexity=18.651537, train_loss=2.9259286

Batch 361840, train_perplexity=18.651537, train_loss=2.9259286

Batch 361850, train_perplexity=18.651537, train_loss=2.9259286

Batch 361860, train_perplexity=18.651537, train_loss=2.9259286

Batch 361870, train_perplexity=18.651537, train_loss=2.9259286

Batch 361880, train_perplexity=18.651537, train_loss=2.9259286

Batch 361890, train_perplexity=18.651537, train_loss=2.9259286

Batch 361900, train_perplexity=18.651537, train_loss=2.9259286

Batch 361910, train_perplexity=18.65153, train_loss=2.925928

Batch 361920, train_perplexity=18.651533, train_loss=2.9259284

Batch 361930, train_perplexity=18.651537, train_loss=2.9259286

Batch 361940, train_perplexity=18.651533, train_loss=2.9259284

Batch 361950, train_perplexity=18.651537, train_loss=2.9259286

Batch 361960, train_perplexity=18.65153, train_loss=2.925928

Batch 361970, train_perplexity=18.65153, train_loss=2.925928

Batch 361980, train_perplexity=18.65153, train_loss=2.925928

Batch 361990, train_perplexity=18.65153, train_loss=2.925928

Batch 362000, train_perplexity=18.65153, train_loss=2.925928

Batch 362010, train_perplexity=18.65153, train_loss=2.925928

Batch 362020, train_perplexity=18.651524, train_loss=2.9259279

Batch 362030, train_perplexity=18.65153, train_loss=2.925928

Batch 362040, train_perplexity=18.651533, train_loss=2.9259284

Batch 362050, train_perplexity=18.65153, train_loss=2.925928

Batch 362060, train_perplexity=18.65153, train_loss=2.925928

Batch 362070, train_perplexity=18.65153, train_loss=2.925928

Batch 362080, train_perplexity=18.65153, train_loss=2.925928

Batch 362090, train_perplexity=18.65153, train_loss=2.925928

Batch 362100, train_perplexity=18.65153, train_loss=2.925928

Batch 362110, train_perplexity=18.65153, train_loss=2.925928

Batch 362120, train_perplexity=18.65153, train_loss=2.925928

Batch 362130, train_perplexity=18.65153, train_loss=2.925928

Batch 362140, train_perplexity=18.65153, train_loss=2.925928

Batch 362150, train_perplexity=18.65153, train_loss=2.925928

Batch 362160, train_perplexity=18.65153, train_loss=2.925928
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 362170, train_perplexity=18.65153, train_loss=2.925928

Batch 362180, train_perplexity=18.65153, train_loss=2.925928

Batch 362190, train_perplexity=18.65153, train_loss=2.925928

Batch 362200, train_perplexity=18.65153, train_loss=2.925928

Batch 362210, train_perplexity=18.65153, train_loss=2.925928

Batch 362220, train_perplexity=18.65152, train_loss=2.9259276

Batch 362230, train_perplexity=18.65152, train_loss=2.9259276

Batch 362240, train_perplexity=18.65153, train_loss=2.925928

Batch 362250, train_perplexity=18.65152, train_loss=2.9259276

Batch 362260, train_perplexity=18.651524, train_loss=2.9259279

Batch 362270, train_perplexity=18.651524, train_loss=2.9259279

Batch 362280, train_perplexity=18.65152, train_loss=2.9259276

Batch 362290, train_perplexity=18.65152, train_loss=2.9259276

Batch 362300, train_perplexity=18.65152, train_loss=2.9259276

Batch 362310, train_perplexity=18.65152, train_loss=2.9259276

Batch 362320, train_perplexity=18.651524, train_loss=2.9259279

Batch 362330, train_perplexity=18.65152, train_loss=2.9259276

Batch 362340, train_perplexity=18.65152, train_loss=2.9259276

Batch 362350, train_perplexity=18.65152, train_loss=2.9259276

Batch 362360, train_perplexity=18.65152, train_loss=2.9259276

Batch 362370, train_perplexity=18.65152, train_loss=2.9259276

Batch 362380, train_perplexity=18.65152, train_loss=2.9259276

Batch 362390, train_perplexity=18.65152, train_loss=2.9259276

Batch 362400, train_perplexity=18.65152, train_loss=2.9259276

Batch 362410, train_perplexity=18.65152, train_loss=2.9259276

Batch 362420, train_perplexity=18.65152, train_loss=2.9259276

Batch 362430, train_perplexity=18.65152, train_loss=2.9259276

Batch 362440, train_perplexity=18.65152, train_loss=2.9259276

Batch 362450, train_perplexity=18.65152, train_loss=2.9259276

Batch 362460, train_perplexity=18.651516, train_loss=2.9259274

Batch 362470, train_perplexity=18.65152, train_loss=2.9259276

Batch 362480, train_perplexity=18.65151, train_loss=2.9259272

Batch 362490, train_perplexity=18.65152, train_loss=2.9259276

Batch 362500, train_perplexity=18.65152, train_loss=2.9259276

Batch 362510, train_perplexity=18.65152, train_loss=2.9259276

Batch 362520, train_perplexity=18.65152, train_loss=2.9259276

Batch 362530, train_perplexity=18.651516, train_loss=2.9259274

Batch 362540, train_perplexity=18.65152, train_loss=2.9259276

Batch 362550, train_perplexity=18.651516, train_loss=2.9259274

Batch 362560, train_perplexity=18.651516, train_loss=2.9259274

Batch 362570, train_perplexity=18.651516, train_loss=2.9259274

Batch 362580, train_perplexity=18.65151, train_loss=2.9259272

Batch 362590, train_perplexity=18.65151, train_loss=2.9259272

Batch 362600, train_perplexity=18.651516, train_loss=2.9259274

Batch 362610, train_perplexity=18.65151, train_loss=2.9259272

Batch 362620, train_perplexity=18.65151, train_loss=2.9259272

Batch 362630, train_perplexity=18.65151, train_loss=2.9259272

Batch 362640, train_perplexity=18.65151, train_loss=2.9259272

Batch 362650, train_perplexity=18.65151, train_loss=2.9259272

Batch 362660, train_perplexity=18.65151, train_loss=2.9259272

Batch 362670, train_perplexity=18.65151, train_loss=2.9259272

Batch 362680, train_perplexity=18.65151, train_loss=2.9259272

Batch 362690, train_perplexity=18.65151, train_loss=2.9259272

Batch 362700, train_perplexity=18.65151, train_loss=2.9259272

Batch 362710, train_perplexity=18.65151, train_loss=2.9259272

Batch 362720, train_perplexity=18.65151, train_loss=2.9259272

Batch 362730, train_perplexity=18.65151, train_loss=2.9259272

Batch 362740, train_perplexity=18.65151, train_loss=2.9259272

Batch 362750, train_perplexity=18.65151, train_loss=2.9259272

Batch 362760, train_perplexity=18.65151, train_loss=2.9259272

Batch 362770, train_perplexity=18.65151, train_loss=2.9259272

Batch 362780, train_perplexity=18.651506, train_loss=2.925927

Batch 362790, train_perplexity=18.65151, train_loss=2.9259272

Batch 362800, train_perplexity=18.651506, train_loss=2.925927

Batch 362810, train_perplexity=18.65151, train_loss=2.9259272

Batch 362820, train_perplexity=18.651503, train_loss=2.9259267

Batch 362830, train_perplexity=18.651503, train_loss=2.9259267

Batch 362840, train_perplexity=18.65151, train_loss=2.9259272

Batch 362850, train_perplexity=18.65151, train_loss=2.9259272

Batch 362860, train_perplexity=18.65151, train_loss=2.9259272

Batch 362870, train_perplexity=18.651503, train_loss=2.9259267

Batch 362880, train_perplexity=18.651506, train_loss=2.925927

Batch 362890, train_perplexity=18.65151, train_loss=2.9259272

Batch 362900, train_perplexity=18.651503, train_loss=2.9259267

Batch 362910, train_perplexity=18.651503, train_loss=2.9259267

Batch 362920, train_perplexity=18.651503, train_loss=2.9259267

Batch 362930, train_perplexity=18.651503, train_loss=2.9259267

Batch 362940, train_perplexity=18.65151, train_loss=2.9259272

Batch 362950, train_perplexity=18.651503, train_loss=2.9259267

Batch 362960, train_perplexity=18.651503, train_loss=2.9259267

Batch 362970, train_perplexity=18.651503, train_loss=2.9259267

Batch 362980, train_perplexity=18.651503, train_loss=2.9259267

Batch 362990, train_perplexity=18.651503, train_loss=2.9259267

Batch 363000, train_perplexity=18.651503, train_loss=2.9259267

Batch 363010, train_perplexity=18.651503, train_loss=2.9259267

Batch 363020, train_perplexity=18.651503, train_loss=2.9259267

Batch 363030, train_perplexity=18.651503, train_loss=2.9259267

Batch 363040, train_perplexity=18.651503, train_loss=2.9259267

Batch 363050, train_perplexity=18.651503, train_loss=2.9259267

Batch 363060, train_perplexity=18.651497, train_loss=2.9259264

Batch 363070, train_perplexity=18.651503, train_loss=2.9259267

Batch 363080, train_perplexity=18.651503, train_loss=2.9259267

Batch 363090, train_perplexity=18.651497, train_loss=2.9259264

Batch 363100, train_perplexity=18.651503, train_loss=2.9259267

Batch 363110, train_perplexity=18.651493, train_loss=2.9259262

Batch 363120, train_perplexity=18.651497, train_loss=2.9259264

Batch 363130, train_perplexity=18.651493, train_loss=2.9259262

Batch 363140, train_perplexity=18.651493, train_loss=2.9259262

Batch 363150, train_perplexity=18.651503, train_loss=2.9259267

Batch 363160, train_perplexity=18.651497, train_loss=2.9259264

Batch 363170, train_perplexity=18.651497, train_loss=2.9259264

Batch 363180, train_perplexity=18.651503, train_loss=2.9259267

Batch 363190, train_perplexity=18.651497, train_loss=2.9259264

Batch 363200, train_perplexity=18.651493, train_loss=2.9259262

Batch 363210, train_perplexity=18.651497, train_loss=2.9259264

Batch 363220, train_perplexity=18.651493, train_loss=2.9259262

Batch 363230, train_perplexity=18.651503, train_loss=2.9259267

Batch 363240, train_perplexity=18.651493, train_loss=2.9259262

Batch 363250, train_perplexity=18.651493, train_loss=2.9259262

Batch 363260, train_perplexity=18.651497, train_loss=2.9259264

Batch 363270, train_perplexity=18.651493, train_loss=2.9259262

Batch 363280, train_perplexity=18.651497, train_loss=2.9259264

Batch 363290, train_perplexity=18.651493, train_loss=2.9259262

Batch 363300, train_perplexity=18.651493, train_loss=2.9259262

Batch 363310, train_perplexity=18.651493, train_loss=2.9259262

Batch 363320, train_perplexity=18.651493, train_loss=2.9259262

Batch 363330, train_perplexity=18.651493, train_loss=2.9259262

Batch 363340, train_perplexity=18.651493, train_loss=2.9259262

Batch 363350, train_perplexity=18.651493, train_loss=2.9259262

Batch 363360, train_perplexity=18.651493, train_loss=2.9259262

Batch 363370, train_perplexity=18.651493, train_loss=2.9259262

Batch 363380, train_perplexity=18.651493, train_loss=2.9259262

Batch 363390, train_perplexity=18.651493, train_loss=2.9259262

Batch 363400, train_perplexity=18.651493, train_loss=2.9259262

Batch 363410, train_perplexity=18.651493, train_loss=2.9259262

Batch 363420, train_perplexity=18.651493, train_loss=2.9259262

Batch 363430, train_perplexity=18.65149, train_loss=2.925926

Batch 363440, train_perplexity=18.651493, train_loss=2.9259262

Batch 363450, train_perplexity=18.651493, train_loss=2.9259262

Batch 363460, train_perplexity=18.65149, train_loss=2.925926
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 363470, train_perplexity=18.65149, train_loss=2.925926

Batch 363480, train_perplexity=18.65149, train_loss=2.925926

Batch 363490, train_perplexity=18.651493, train_loss=2.9259262

Batch 363500, train_perplexity=18.65149, train_loss=2.925926

Batch 363510, train_perplexity=18.651484, train_loss=2.9259257

Batch 363520, train_perplexity=18.651484, train_loss=2.9259257

Batch 363530, train_perplexity=18.651493, train_loss=2.9259262

Batch 363540, train_perplexity=18.65149, train_loss=2.925926

Batch 363550, train_perplexity=18.651484, train_loss=2.9259257

Batch 363560, train_perplexity=18.65149, train_loss=2.925926

Batch 363570, train_perplexity=18.651484, train_loss=2.9259257

Batch 363580, train_perplexity=18.651493, train_loss=2.9259262

Batch 363590, train_perplexity=18.65148, train_loss=2.9259255

Batch 363600, train_perplexity=18.651484, train_loss=2.9259257

Batch 363610, train_perplexity=18.651484, train_loss=2.9259257

Batch 363620, train_perplexity=18.651484, train_loss=2.9259257

Batch 363630, train_perplexity=18.651484, train_loss=2.9259257

Batch 363640, train_perplexity=18.651484, train_loss=2.9259257

Batch 363650, train_perplexity=18.651484, train_loss=2.9259257

Batch 363660, train_perplexity=18.651484, train_loss=2.9259257

Batch 363670, train_perplexity=18.651484, train_loss=2.9259257

Batch 363680, train_perplexity=18.651484, train_loss=2.9259257

Batch 363690, train_perplexity=18.651484, train_loss=2.9259257

Batch 363700, train_perplexity=18.651484, train_loss=2.9259257

Batch 363710, train_perplexity=18.651484, train_loss=2.9259257

Batch 363720, train_perplexity=18.651484, train_loss=2.9259257

Batch 363730, train_perplexity=18.65148, train_loss=2.9259255

Batch 363740, train_perplexity=18.651484, train_loss=2.9259257

Batch 363750, train_perplexity=18.651484, train_loss=2.9259257

Batch 363760, train_perplexity=18.651484, train_loss=2.9259257

Batch 363770, train_perplexity=18.65148, train_loss=2.9259255

Batch 363780, train_perplexity=18.65148, train_loss=2.9259255

Batch 363790, train_perplexity=18.65148, train_loss=2.9259255

Batch 363800, train_perplexity=18.65148, train_loss=2.9259255

Batch 363810, train_perplexity=18.651476, train_loss=2.9259253

Batch 363820, train_perplexity=18.65148, train_loss=2.9259255

Batch 363830, train_perplexity=18.65148, train_loss=2.9259255

Batch 363840, train_perplexity=18.65148, train_loss=2.9259255

Batch 363850, train_perplexity=18.651476, train_loss=2.9259253

Batch 363860, train_perplexity=18.65148, train_loss=2.9259255

Batch 363870, train_perplexity=18.651476, train_loss=2.9259253

Batch 363880, train_perplexity=18.651476, train_loss=2.9259253

Batch 363890, train_perplexity=18.651484, train_loss=2.9259257

Batch 363900, train_perplexity=18.651476, train_loss=2.9259253

Batch 363910, train_perplexity=18.651476, train_loss=2.9259253

Batch 363920, train_perplexity=18.651476, train_loss=2.9259253

Batch 363930, train_perplexity=18.65148, train_loss=2.9259255

Batch 363940, train_perplexity=18.651476, train_loss=2.9259253

Batch 363950, train_perplexity=18.651476, train_loss=2.9259253

Batch 363960, train_perplexity=18.651476, train_loss=2.9259253

Batch 363970, train_perplexity=18.651476, train_loss=2.9259253

Batch 363980, train_perplexity=18.65147, train_loss=2.925925

Batch 363990, train_perplexity=18.651476, train_loss=2.9259253

Batch 364000, train_perplexity=18.651466, train_loss=2.9259248

Batch 364010, train_perplexity=18.651476, train_loss=2.9259253

Batch 364020, train_perplexity=18.651476, train_loss=2.9259253

Batch 364030, train_perplexity=18.651476, train_loss=2.9259253

Batch 364040, train_perplexity=18.651476, train_loss=2.9259253

Batch 364050, train_perplexity=18.651476, train_loss=2.9259253

Batch 364060, train_perplexity=18.65147, train_loss=2.925925

Batch 364070, train_perplexity=18.651476, train_loss=2.9259253

Batch 364080, train_perplexity=18.651476, train_loss=2.9259253

Batch 364090, train_perplexity=18.65147, train_loss=2.925925

Batch 364100, train_perplexity=18.651476, train_loss=2.9259253

Batch 364110, train_perplexity=18.651466, train_loss=2.9259248

Batch 364120, train_perplexity=18.651466, train_loss=2.9259248

Batch 364130, train_perplexity=18.651466, train_loss=2.9259248

Batch 364140, train_perplexity=18.651476, train_loss=2.9259253

Batch 364150, train_perplexity=18.651466, train_loss=2.9259248

Batch 364160, train_perplexity=18.651476, train_loss=2.9259253

Batch 364170, train_perplexity=18.651466, train_loss=2.9259248

Batch 364180, train_perplexity=18.65147, train_loss=2.925925

Batch 364190, train_perplexity=18.65147, train_loss=2.925925

Batch 364200, train_perplexity=18.651466, train_loss=2.9259248

Batch 364210, train_perplexity=18.651466, train_loss=2.9259248

Batch 364220, train_perplexity=18.651466, train_loss=2.9259248

Batch 364230, train_perplexity=18.651466, train_loss=2.9259248

Batch 364240, train_perplexity=18.65147, train_loss=2.925925

Batch 364250, train_perplexity=18.651466, train_loss=2.9259248

Batch 364260, train_perplexity=18.651466, train_loss=2.9259248

Batch 364270, train_perplexity=18.651466, train_loss=2.9259248

Batch 364280, train_perplexity=18.651466, train_loss=2.9259248

Batch 364290, train_perplexity=18.651466, train_loss=2.9259248

Batch 364300, train_perplexity=18.651466, train_loss=2.9259248

Batch 364310, train_perplexity=18.651466, train_loss=2.9259248

Batch 364320, train_perplexity=18.651463, train_loss=2.9259245

Batch 364330, train_perplexity=18.651466, train_loss=2.9259248

Batch 364340, train_perplexity=18.651466, train_loss=2.9259248

Batch 364350, train_perplexity=18.651466, train_loss=2.9259248

Batch 364360, train_perplexity=18.651466, train_loss=2.9259248

Batch 364370, train_perplexity=18.651463, train_loss=2.9259245

Batch 364380, train_perplexity=18.651466, train_loss=2.9259248

Batch 364390, train_perplexity=18.651463, train_loss=2.9259245

Batch 364400, train_perplexity=18.651463, train_loss=2.9259245

Batch 364410, train_perplexity=18.651466, train_loss=2.9259248

Batch 364420, train_perplexity=18.651466, train_loss=2.9259248

Batch 364430, train_perplexity=18.651457, train_loss=2.9259243

Batch 364440, train_perplexity=18.651457, train_loss=2.9259243

Batch 364450, train_perplexity=18.651457, train_loss=2.9259243

Batch 364460, train_perplexity=18.651457, train_loss=2.9259243

Batch 364470, train_perplexity=18.651466, train_loss=2.9259248

Batch 364480, train_perplexity=18.651463, train_loss=2.9259245

Batch 364490, train_perplexity=18.651466, train_loss=2.9259248

Batch 364500, train_perplexity=18.651463, train_loss=2.9259245

Batch 364510, train_perplexity=18.651457, train_loss=2.9259243

Batch 364520, train_perplexity=18.651457, train_loss=2.9259243

Batch 364530, train_perplexity=18.651457, train_loss=2.9259243

Batch 364540, train_perplexity=18.651463, train_loss=2.9259245

Batch 364550, train_perplexity=18.651457, train_loss=2.9259243

Batch 364560, train_perplexity=18.651457, train_loss=2.9259243

Batch 364570, train_perplexity=18.651457, train_loss=2.9259243

Batch 364580, train_perplexity=18.651463, train_loss=2.9259245

Batch 364590, train_perplexity=18.651453, train_loss=2.925924

Batch 364600, train_perplexity=18.651457, train_loss=2.9259243

Batch 364610, train_perplexity=18.651457, train_loss=2.9259243

Batch 364620, train_perplexity=18.651457, train_loss=2.9259243

Batch 364630, train_perplexity=18.651457, train_loss=2.9259243

Batch 364640, train_perplexity=18.651457, train_loss=2.9259243

Batch 364650, train_perplexity=18.651457, train_loss=2.9259243

Batch 364660, train_perplexity=18.651457, train_loss=2.9259243

Batch 364670, train_perplexity=18.651457, train_loss=2.9259243

Batch 364680, train_perplexity=18.651457, train_loss=2.9259243

Batch 364690, train_perplexity=18.651457, train_loss=2.9259243

Batch 364700, train_perplexity=18.65145, train_loss=2.9259238

Batch 364710, train_perplexity=18.651457, train_loss=2.9259243

Batch 364720, train_perplexity=18.65145, train_loss=2.9259238

Batch 364730, train_perplexity=18.651457, train_loss=2.9259243

Batch 364740, train_perplexity=18.651457, train_loss=2.9259243

Batch 364750, train_perplexity=18.651457, train_loss=2.9259243
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 364760, train_perplexity=18.651457, train_loss=2.9259243

Batch 364770, train_perplexity=18.651457, train_loss=2.9259243

Batch 364780, train_perplexity=18.651453, train_loss=2.925924

Batch 364790, train_perplexity=18.651453, train_loss=2.925924

Batch 364800, train_perplexity=18.65145, train_loss=2.9259238

Batch 364810, train_perplexity=18.65145, train_loss=2.9259238

Batch 364820, train_perplexity=18.65145, train_loss=2.9259238

Batch 364830, train_perplexity=18.65145, train_loss=2.9259238

Batch 364840, train_perplexity=18.65145, train_loss=2.9259238

Batch 364850, train_perplexity=18.65145, train_loss=2.9259238

Batch 364860, train_perplexity=18.65145, train_loss=2.9259238

Batch 364870, train_perplexity=18.65145, train_loss=2.9259238

Batch 364880, train_perplexity=18.65145, train_loss=2.9259238

Batch 364890, train_perplexity=18.65145, train_loss=2.9259238

Batch 364900, train_perplexity=18.65145, train_loss=2.9259238

Batch 364910, train_perplexity=18.65145, train_loss=2.9259238

Batch 364920, train_perplexity=18.65145, train_loss=2.9259238

Batch 364930, train_perplexity=18.65145, train_loss=2.9259238

Batch 364940, train_perplexity=18.65145, train_loss=2.9259238

Batch 364950, train_perplexity=18.65145, train_loss=2.9259238

Batch 364960, train_perplexity=18.65145, train_loss=2.9259238

Batch 364970, train_perplexity=18.65145, train_loss=2.9259238

Batch 364980, train_perplexity=18.651443, train_loss=2.9259236

Batch 364990, train_perplexity=18.65145, train_loss=2.9259238

Batch 365000, train_perplexity=18.65145, train_loss=2.9259238

Batch 365010, train_perplexity=18.65145, train_loss=2.9259238

Batch 365020, train_perplexity=18.651443, train_loss=2.9259236

Batch 365030, train_perplexity=18.65144, train_loss=2.9259233

Batch 365040, train_perplexity=18.651443, train_loss=2.9259236

Batch 365050, train_perplexity=18.651443, train_loss=2.9259236

Batch 365060, train_perplexity=18.651443, train_loss=2.9259236

Batch 365070, train_perplexity=18.65145, train_loss=2.9259238

Batch 365080, train_perplexity=18.651443, train_loss=2.9259236

Batch 365090, train_perplexity=18.65144, train_loss=2.9259233

Batch 365100, train_perplexity=18.65145, train_loss=2.9259238

Batch 365110, train_perplexity=18.651443, train_loss=2.9259236

Batch 365120, train_perplexity=18.65145, train_loss=2.9259238

Batch 365130, train_perplexity=18.651443, train_loss=2.9259236

Batch 365140, train_perplexity=18.65144, train_loss=2.9259233

Batch 365150, train_perplexity=18.65144, train_loss=2.9259233

Batch 365160, train_perplexity=18.651443, train_loss=2.9259236

Batch 365170, train_perplexity=18.65144, train_loss=2.9259233

Batch 365180, train_perplexity=18.65144, train_loss=2.9259233

Batch 365190, train_perplexity=18.65144, train_loss=2.9259233

Batch 365200, train_perplexity=18.65144, train_loss=2.9259233

Batch 365210, train_perplexity=18.65144, train_loss=2.9259233

Batch 365220, train_perplexity=18.65144, train_loss=2.9259233

Batch 365230, train_perplexity=18.65144, train_loss=2.9259233

Batch 365240, train_perplexity=18.651436, train_loss=2.925923

Batch 365250, train_perplexity=18.65144, train_loss=2.9259233

Batch 365260, train_perplexity=18.651436, train_loss=2.925923

Batch 365270, train_perplexity=18.65144, train_loss=2.9259233

Batch 365280, train_perplexity=18.651436, train_loss=2.925923

Batch 365290, train_perplexity=18.651436, train_loss=2.925923

Batch 365300, train_perplexity=18.651436, train_loss=2.925923

Batch 365310, train_perplexity=18.65144, train_loss=2.9259233

Batch 365320, train_perplexity=18.65144, train_loss=2.9259233

Batch 365330, train_perplexity=18.65144, train_loss=2.9259233

Batch 365340, train_perplexity=18.65144, train_loss=2.9259233

Batch 365350, train_perplexity=18.65143, train_loss=2.9259229

Batch 365360, train_perplexity=18.65143, train_loss=2.9259229

Batch 365370, train_perplexity=18.65144, train_loss=2.9259233

Batch 365380, train_perplexity=18.65144, train_loss=2.9259233

Batch 365390, train_perplexity=18.65144, train_loss=2.9259233

Batch 365400, train_perplexity=18.65143, train_loss=2.9259229

Batch 365410, train_perplexity=18.65144, train_loss=2.9259233

Batch 365420, train_perplexity=18.651436, train_loss=2.925923

Batch 365430, train_perplexity=18.65144, train_loss=2.9259233

Batch 365440, train_perplexity=18.65143, train_loss=2.9259229

Batch 365450, train_perplexity=18.65143, train_loss=2.9259229

Batch 365460, train_perplexity=18.65143, train_loss=2.9259229

Batch 365470, train_perplexity=18.65143, train_loss=2.9259229

Batch 365480, train_perplexity=18.65143, train_loss=2.9259229

Batch 365490, train_perplexity=18.65143, train_loss=2.9259229

Batch 365500, train_perplexity=18.65143, train_loss=2.9259229

Batch 365510, train_perplexity=18.65143, train_loss=2.9259229

Batch 365520, train_perplexity=18.65143, train_loss=2.9259229

Batch 365530, train_perplexity=18.65143, train_loss=2.9259229

Batch 365540, train_perplexity=18.65143, train_loss=2.9259229

Batch 365550, train_perplexity=18.65143, train_loss=2.9259229

Batch 365560, train_perplexity=18.65143, train_loss=2.9259229

Batch 365570, train_perplexity=18.65143, train_loss=2.9259229

Batch 365580, train_perplexity=18.65143, train_loss=2.9259229

Batch 365590, train_perplexity=18.65143, train_loss=2.9259229

Batch 365600, train_perplexity=18.65143, train_loss=2.9259229

Batch 365610, train_perplexity=18.65143, train_loss=2.9259229

Batch 365620, train_perplexity=18.65143, train_loss=2.9259229

Batch 365630, train_perplexity=18.651426, train_loss=2.9259226

Batch 365640, train_perplexity=18.65143, train_loss=2.9259229

Batch 365650, train_perplexity=18.65143, train_loss=2.9259229

Batch 365660, train_perplexity=18.651426, train_loss=2.9259226

Batch 365670, train_perplexity=18.651426, train_loss=2.9259226

Batch 365680, train_perplexity=18.651426, train_loss=2.9259226

Batch 365690, train_perplexity=18.651426, train_loss=2.9259226

Batch 365700, train_perplexity=18.651423, train_loss=2.9259224

Batch 365710, train_perplexity=18.651426, train_loss=2.9259226

Batch 365720, train_perplexity=18.651426, train_loss=2.9259226

Batch 365730, train_perplexity=18.651426, train_loss=2.9259226

Batch 365740, train_perplexity=18.651423, train_loss=2.9259224

Batch 365750, train_perplexity=18.651423, train_loss=2.9259224

Batch 365760, train_perplexity=18.651423, train_loss=2.9259224

Batch 365770, train_perplexity=18.651426, train_loss=2.9259226

Batch 365780, train_perplexity=18.651426, train_loss=2.9259226

Batch 365790, train_perplexity=18.651426, train_loss=2.9259226

Batch 365800, train_perplexity=18.651423, train_loss=2.9259224

Batch 365810, train_perplexity=18.651423, train_loss=2.9259224

Batch 365820, train_perplexity=18.651423, train_loss=2.9259224

Batch 365830, train_perplexity=18.651426, train_loss=2.9259226

Batch 365840, train_perplexity=18.651426, train_loss=2.9259226

Batch 365850, train_perplexity=18.651423, train_loss=2.9259224

Batch 365860, train_perplexity=18.651423, train_loss=2.9259224

Batch 365870, train_perplexity=18.651423, train_loss=2.9259224

Batch 365880, train_perplexity=18.651423, train_loss=2.9259224

Batch 365890, train_perplexity=18.651423, train_loss=2.9259224

Batch 365900, train_perplexity=18.651423, train_loss=2.9259224

Batch 365910, train_perplexity=18.651423, train_loss=2.9259224

Batch 365920, train_perplexity=18.651423, train_loss=2.9259224

Batch 365930, train_perplexity=18.651423, train_loss=2.9259224

Batch 365940, train_perplexity=18.651423, train_loss=2.9259224

Batch 365950, train_perplexity=18.651423, train_loss=2.9259224

Batch 365960, train_perplexity=18.651423, train_loss=2.9259224

Batch 365970, train_perplexity=18.651417, train_loss=2.9259222

Batch 365980, train_perplexity=18.651423, train_loss=2.9259224

Batch 365990, train_perplexity=18.651413, train_loss=2.925922

Batch 366000, train_perplexity=18.651417, train_loss=2.9259222

Batch 366010, train_perplexity=18.651423, train_loss=2.9259224

Batch 366020, train_perplexity=18.651423, train_loss=2.9259224

Batch 366030, train_perplexity=18.651413, train_loss=2.925922

Batch 366040, train_perplexity=18.651423, train_loss=2.9259224

Batch 366050, train_perplexity=18.651423, train_loss=2.9259224
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 366060, train_perplexity=18.651417, train_loss=2.9259222

Batch 366070, train_perplexity=18.651413, train_loss=2.925922

Batch 366080, train_perplexity=18.651413, train_loss=2.925922

Batch 366090, train_perplexity=18.651417, train_loss=2.9259222

Batch 366100, train_perplexity=18.651417, train_loss=2.9259222

Batch 366110, train_perplexity=18.651417, train_loss=2.9259222

Batch 366120, train_perplexity=18.651413, train_loss=2.925922

Batch 366130, train_perplexity=18.651413, train_loss=2.925922

Batch 366140, train_perplexity=18.651413, train_loss=2.925922

Batch 366150, train_perplexity=18.651413, train_loss=2.925922

Batch 366160, train_perplexity=18.651413, train_loss=2.925922

Batch 366170, train_perplexity=18.651413, train_loss=2.925922

Batch 366180, train_perplexity=18.651413, train_loss=2.925922

Batch 366190, train_perplexity=18.651413, train_loss=2.925922

Batch 366200, train_perplexity=18.651413, train_loss=2.925922

Batch 366210, train_perplexity=18.651413, train_loss=2.925922

Batch 366220, train_perplexity=18.651413, train_loss=2.925922

Batch 366230, train_perplexity=18.651413, train_loss=2.925922

Batch 366240, train_perplexity=18.651413, train_loss=2.925922

Batch 366250, train_perplexity=18.651413, train_loss=2.925922

Batch 366260, train_perplexity=18.651413, train_loss=2.925922

Batch 366270, train_perplexity=18.651413, train_loss=2.925922

Batch 366280, train_perplexity=18.65141, train_loss=2.9259217

Batch 366290, train_perplexity=18.651413, train_loss=2.925922

Batch 366300, train_perplexity=18.651413, train_loss=2.925922

Batch 366310, train_perplexity=18.651413, train_loss=2.925922

Batch 366320, train_perplexity=18.65141, train_loss=2.9259217

Batch 366330, train_perplexity=18.651413, train_loss=2.925922

Batch 366340, train_perplexity=18.651403, train_loss=2.9259214

Batch 366350, train_perplexity=18.651403, train_loss=2.9259214

Batch 366360, train_perplexity=18.65141, train_loss=2.9259217

Batch 366370, train_perplexity=18.651403, train_loss=2.9259214

Batch 366380, train_perplexity=18.651403, train_loss=2.9259214

Batch 366390, train_perplexity=18.65141, train_loss=2.9259217

Batch 366400, train_perplexity=18.651403, train_loss=2.9259214

Batch 366410, train_perplexity=18.65141, train_loss=2.9259217

Batch 366420, train_perplexity=18.65141, train_loss=2.9259217

Batch 366430, train_perplexity=18.65141, train_loss=2.9259217

Batch 366440, train_perplexity=18.651403, train_loss=2.9259214

Batch 366450, train_perplexity=18.651403, train_loss=2.9259214

Batch 366460, train_perplexity=18.651403, train_loss=2.9259214

Batch 366470, train_perplexity=18.651403, train_loss=2.9259214

Batch 366480, train_perplexity=18.651403, train_loss=2.9259214

Batch 366490, train_perplexity=18.651403, train_loss=2.9259214

Batch 366500, train_perplexity=18.651403, train_loss=2.9259214

Batch 366510, train_perplexity=18.65141, train_loss=2.9259217

Batch 366520, train_perplexity=18.651403, train_loss=2.9259214

Batch 366530, train_perplexity=18.651403, train_loss=2.9259214

Batch 366540, train_perplexity=18.651403, train_loss=2.9259214

Batch 366550, train_perplexity=18.651396, train_loss=2.925921

Batch 366560, train_perplexity=18.651403, train_loss=2.9259214

Batch 366570, train_perplexity=18.651403, train_loss=2.9259214

Batch 366580, train_perplexity=18.651403, train_loss=2.9259214

Batch 366590, train_perplexity=18.651403, train_loss=2.9259214

Batch 366600, train_perplexity=18.651396, train_loss=2.925921

Batch 366610, train_perplexity=18.6514, train_loss=2.9259212

Batch 366620, train_perplexity=18.651403, train_loss=2.9259214

Batch 366630, train_perplexity=18.6514, train_loss=2.9259212

Batch 366640, train_perplexity=18.651403, train_loss=2.9259214

Batch 366650, train_perplexity=18.651396, train_loss=2.925921

Batch 366660, train_perplexity=18.651403, train_loss=2.9259214

Batch 366670, train_perplexity=18.651403, train_loss=2.9259214

Batch 366680, train_perplexity=18.6514, train_loss=2.9259212

Batch 366690, train_perplexity=18.651396, train_loss=2.925921

Batch 366700, train_perplexity=18.651396, train_loss=2.925921

Batch 366710, train_perplexity=18.651403, train_loss=2.9259214

Batch 366720, train_perplexity=18.651403, train_loss=2.9259214

Batch 366730, train_perplexity=18.651396, train_loss=2.925921

Batch 366740, train_perplexity=18.651396, train_loss=2.925921

Batch 366750, train_perplexity=18.651396, train_loss=2.925921

Batch 366760, train_perplexity=18.651396, train_loss=2.925921

Batch 366770, train_perplexity=18.651396, train_loss=2.925921

Batch 366780, train_perplexity=18.651396, train_loss=2.925921

Batch 366790, train_perplexity=18.651396, train_loss=2.925921

Batch 366800, train_perplexity=18.651396, train_loss=2.925921

Batch 366810, train_perplexity=18.651396, train_loss=2.925921

Batch 366820, train_perplexity=18.651396, train_loss=2.925921

Batch 366830, train_perplexity=18.651396, train_loss=2.925921

Batch 366840, train_perplexity=18.651396, train_loss=2.925921

Batch 366850, train_perplexity=18.65139, train_loss=2.9259207

Batch 366860, train_perplexity=18.651396, train_loss=2.925921

Batch 366870, train_perplexity=18.651396, train_loss=2.925921

Batch 366880, train_perplexity=18.651396, train_loss=2.925921

Batch 366890, train_perplexity=18.651396, train_loss=2.925921

Batch 366900, train_perplexity=18.651396, train_loss=2.925921

Batch 366910, train_perplexity=18.651396, train_loss=2.925921

Batch 366920, train_perplexity=18.65139, train_loss=2.9259207

Batch 366930, train_perplexity=18.651396, train_loss=2.925921

Batch 366940, train_perplexity=18.651386, train_loss=2.9259205

Batch 366950, train_perplexity=18.65139, train_loss=2.9259207

Batch 366960, train_perplexity=18.651396, train_loss=2.925921

Batch 366970, train_perplexity=18.65139, train_loss=2.9259207

Batch 366980, train_perplexity=18.65139, train_loss=2.9259207

Batch 366990, train_perplexity=18.65139, train_loss=2.9259207

Batch 367000, train_perplexity=18.651386, train_loss=2.9259205

Batch 367010, train_perplexity=18.65139, train_loss=2.9259207

Batch 367020, train_perplexity=18.65139, train_loss=2.9259207

Batch 367030, train_perplexity=18.651386, train_loss=2.9259205

Batch 367040, train_perplexity=18.651386, train_loss=2.9259205

Batch 367050, train_perplexity=18.651386, train_loss=2.9259205

Batch 367060, train_perplexity=18.65139, train_loss=2.9259207

Batch 367070, train_perplexity=18.651386, train_loss=2.9259205

Batch 367080, train_perplexity=18.65139, train_loss=2.9259207

Batch 367090, train_perplexity=18.651386, train_loss=2.9259205

Batch 367100, train_perplexity=18.65139, train_loss=2.9259207

Batch 367110, train_perplexity=18.65139, train_loss=2.9259207

Batch 367120, train_perplexity=18.651386, train_loss=2.9259205

Batch 367130, train_perplexity=18.651386, train_loss=2.9259205

Batch 367140, train_perplexity=18.651386, train_loss=2.9259205

Batch 367150, train_perplexity=18.651386, train_loss=2.9259205

Batch 367160, train_perplexity=18.651386, train_loss=2.9259205

Batch 367170, train_perplexity=18.651386, train_loss=2.9259205

Batch 367180, train_perplexity=18.651386, train_loss=2.9259205

Batch 367190, train_perplexity=18.651386, train_loss=2.9259205

Batch 367200, train_perplexity=18.651386, train_loss=2.9259205

Batch 367210, train_perplexity=18.651386, train_loss=2.9259205

Batch 367220, train_perplexity=18.651386, train_loss=2.9259205

Batch 367230, train_perplexity=18.651386, train_loss=2.9259205

Batch 367240, train_perplexity=18.651386, train_loss=2.9259205

Batch 367250, train_perplexity=18.651386, train_loss=2.9259205

Batch 367260, train_perplexity=18.651382, train_loss=2.9259202

Batch 367270, train_perplexity=18.651386, train_loss=2.9259205

Batch 367280, train_perplexity=18.651382, train_loss=2.9259202

Batch 367290, train_perplexity=18.651382, train_loss=2.9259202

Batch 367300, train_perplexity=18.651382, train_loss=2.9259202

Batch 367310, train_perplexity=18.651386, train_loss=2.9259205

Batch 367320, train_perplexity=18.651386, train_loss=2.9259205

Batch 367330, train_perplexity=18.651386, train_loss=2.9259205

Batch 367340, train_perplexity=18.651377, train_loss=2.92592

Batch 367350, train_perplexity=18.651377, train_loss=2.92592
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 367360, train_perplexity=18.651377, train_loss=2.92592

Batch 367370, train_perplexity=18.651377, train_loss=2.92592

Batch 367380, train_perplexity=18.651377, train_loss=2.92592

Batch 367390, train_perplexity=18.651382, train_loss=2.9259202

Batch 367400, train_perplexity=18.651377, train_loss=2.92592

Batch 367410, train_perplexity=18.651373, train_loss=2.9259198

Batch 367420, train_perplexity=18.651377, train_loss=2.92592

Batch 367430, train_perplexity=18.651377, train_loss=2.92592

Batch 367440, train_perplexity=18.651377, train_loss=2.92592

Batch 367450, train_perplexity=18.651377, train_loss=2.92592

Batch 367460, train_perplexity=18.651377, train_loss=2.92592

Batch 367470, train_perplexity=18.651377, train_loss=2.92592

Batch 367480, train_perplexity=18.651377, train_loss=2.92592

Batch 367490, train_perplexity=18.651373, train_loss=2.9259198

Batch 367500, train_perplexity=18.651377, train_loss=2.92592

Batch 367510, train_perplexity=18.651373, train_loss=2.9259198

Batch 367520, train_perplexity=18.651377, train_loss=2.92592

Batch 367530, train_perplexity=18.65137, train_loss=2.9259195

Batch 367540, train_perplexity=18.651373, train_loss=2.9259198

Batch 367550, train_perplexity=18.651377, train_loss=2.92592

Batch 367560, train_perplexity=18.651377, train_loss=2.92592

Batch 367570, train_perplexity=18.65137, train_loss=2.9259195

Batch 367580, train_perplexity=18.651377, train_loss=2.92592

Batch 367590, train_perplexity=18.651377, train_loss=2.92592

Batch 367600, train_perplexity=18.651373, train_loss=2.9259198

Batch 367610, train_perplexity=18.65137, train_loss=2.9259195

Batch 367620, train_perplexity=18.65137, train_loss=2.9259195

Batch 367630, train_perplexity=18.65137, train_loss=2.9259195

Batch 367640, train_perplexity=18.651373, train_loss=2.9259198

Batch 367650, train_perplexity=18.65137, train_loss=2.9259195

Batch 367660, train_perplexity=18.65137, train_loss=2.9259195

Batch 367670, train_perplexity=18.65137, train_loss=2.9259195

Batch 367680, train_perplexity=18.651373, train_loss=2.9259198

Batch 367690, train_perplexity=18.651373, train_loss=2.9259198

Batch 367700, train_perplexity=18.65137, train_loss=2.9259195

Batch 367710, train_perplexity=18.65137, train_loss=2.9259195

Batch 367720, train_perplexity=18.651373, train_loss=2.9259198

Batch 367730, train_perplexity=18.65137, train_loss=2.9259195

Batch 367740, train_perplexity=18.65137, train_loss=2.9259195

Batch 367750, train_perplexity=18.65137, train_loss=2.9259195

Batch 367760, train_perplexity=18.65137, train_loss=2.9259195

Batch 367770, train_perplexity=18.65137, train_loss=2.9259195

Batch 367780, train_perplexity=18.65136, train_loss=2.925919

Batch 367790, train_perplexity=18.65137, train_loss=2.9259195

Batch 367800, train_perplexity=18.65137, train_loss=2.9259195

Batch 367810, train_perplexity=18.65137, train_loss=2.9259195

Batch 367820, train_perplexity=18.65137, train_loss=2.9259195

Batch 367830, train_perplexity=18.65137, train_loss=2.9259195

Batch 367840, train_perplexity=18.65137, train_loss=2.9259195

Batch 367850, train_perplexity=18.65137, train_loss=2.9259195

Batch 367860, train_perplexity=18.65137, train_loss=2.9259195

Batch 367870, train_perplexity=18.65137, train_loss=2.9259195

Batch 367880, train_perplexity=18.65137, train_loss=2.9259195

Batch 367890, train_perplexity=18.651363, train_loss=2.9259193

Batch 367900, train_perplexity=18.65137, train_loss=2.9259195

Batch 367910, train_perplexity=18.65137, train_loss=2.9259195

Batch 367920, train_perplexity=18.65136, train_loss=2.925919

Batch 367930, train_perplexity=18.65137, train_loss=2.9259195

Batch 367940, train_perplexity=18.65136, train_loss=2.925919

Batch 367950, train_perplexity=18.65136, train_loss=2.925919

Batch 367960, train_perplexity=18.65136, train_loss=2.925919

Batch 367970, train_perplexity=18.65136, train_loss=2.925919

Batch 367980, train_perplexity=18.65136, train_loss=2.925919

Batch 367990, train_perplexity=18.651363, train_loss=2.9259193

Batch 368000, train_perplexity=18.65136, train_loss=2.925919

Batch 368010, train_perplexity=18.651363, train_loss=2.9259193

Batch 368020, train_perplexity=18.65136, train_loss=2.925919

Batch 368030, train_perplexity=18.65136, train_loss=2.925919

Batch 368040, train_perplexity=18.65136, train_loss=2.925919

Batch 368050, train_perplexity=18.65136, train_loss=2.925919

Batch 368060, train_perplexity=18.65136, train_loss=2.925919

Batch 368070, train_perplexity=18.65136, train_loss=2.925919

Batch 368080, train_perplexity=18.65136, train_loss=2.925919

Batch 368090, train_perplexity=18.65136, train_loss=2.925919

Batch 368100, train_perplexity=18.65136, train_loss=2.925919

Batch 368110, train_perplexity=18.65136, train_loss=2.925919

Batch 368120, train_perplexity=18.65136, train_loss=2.925919

Batch 368130, train_perplexity=18.65136, train_loss=2.925919

Batch 368140, train_perplexity=18.65136, train_loss=2.925919

Batch 368150, train_perplexity=18.65136, train_loss=2.925919

Batch 368160, train_perplexity=18.65136, train_loss=2.925919

Batch 368170, train_perplexity=18.65136, train_loss=2.925919

Batch 368180, train_perplexity=18.65136, train_loss=2.925919

Batch 368190, train_perplexity=18.65135, train_loss=2.9259186

Batch 368200, train_perplexity=18.65136, train_loss=2.925919

Batch 368210, train_perplexity=18.65135, train_loss=2.9259186

Batch 368220, train_perplexity=18.651356, train_loss=2.9259188

Batch 368230, train_perplexity=18.651356, train_loss=2.9259188

Batch 368240, train_perplexity=18.651356, train_loss=2.9259188

Batch 368250, train_perplexity=18.65135, train_loss=2.9259186

Batch 368260, train_perplexity=18.651356, train_loss=2.9259188

Batch 368270, train_perplexity=18.65136, train_loss=2.925919

Batch 368280, train_perplexity=18.651356, train_loss=2.9259188

Batch 368290, train_perplexity=18.651356, train_loss=2.9259188

Batch 368300, train_perplexity=18.651356, train_loss=2.9259188

Batch 368310, train_perplexity=18.65135, train_loss=2.9259186

Batch 368320, train_perplexity=18.65135, train_loss=2.9259186

Batch 368330, train_perplexity=18.65135, train_loss=2.9259186

Batch 368340, train_perplexity=18.65135, train_loss=2.9259186

Batch 368350, train_perplexity=18.65135, train_loss=2.9259186

Batch 368360, train_perplexity=18.65135, train_loss=2.9259186

Batch 368370, train_perplexity=18.65135, train_loss=2.9259186

Batch 368380, train_perplexity=18.65135, train_loss=2.9259186

Batch 368390, train_perplexity=18.65135, train_loss=2.9259186

Batch 368400, train_perplexity=18.65135, train_loss=2.9259186

Batch 368410, train_perplexity=18.65135, train_loss=2.9259186

Batch 368420, train_perplexity=18.65135, train_loss=2.9259186

Batch 368430, train_perplexity=18.65135, train_loss=2.9259186

Batch 368440, train_perplexity=18.65135, train_loss=2.9259186

Batch 368450, train_perplexity=18.65135, train_loss=2.9259186

Batch 368460, train_perplexity=18.651356, train_loss=2.9259188

Batch 368470, train_perplexity=18.65135, train_loss=2.9259186

Batch 368480, train_perplexity=18.65135, train_loss=2.9259186

Batch 368490, train_perplexity=18.65135, train_loss=2.9259186

Batch 368500, train_perplexity=18.65135, train_loss=2.9259186

Batch 368510, train_perplexity=18.651346, train_loss=2.9259183

Batch 368520, train_perplexity=18.651346, train_loss=2.9259183

Batch 368530, train_perplexity=18.65135, train_loss=2.9259186

Batch 368540, train_perplexity=18.651346, train_loss=2.9259183

Batch 368550, train_perplexity=18.651342, train_loss=2.925918

Batch 368560, train_perplexity=18.651342, train_loss=2.925918

Batch 368570, train_perplexity=18.651346, train_loss=2.9259183

Batch 368580, train_perplexity=18.65135, train_loss=2.9259186

Batch 368590, train_perplexity=18.651346, train_loss=2.9259183

Batch 368600, train_perplexity=18.651346, train_loss=2.9259183

Batch 368610, train_perplexity=18.651342, train_loss=2.925918

Batch 368620, train_perplexity=18.651346, train_loss=2.9259183

Batch 368630, train_perplexity=18.651342, train_loss=2.925918

Batch 368640, train_perplexity=18.651346, train_loss=2.9259183

Batch 368650, train_perplexity=18.651342, train_loss=2.925918

Batch 368660, train_perplexity=18.651342, train_loss=2.925918
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 368670, train_perplexity=18.651342, train_loss=2.925918

Batch 368680, train_perplexity=18.651342, train_loss=2.925918

Batch 368690, train_perplexity=18.651342, train_loss=2.925918

Batch 368700, train_perplexity=18.651342, train_loss=2.925918

Batch 368710, train_perplexity=18.651342, train_loss=2.925918

Batch 368720, train_perplexity=18.651342, train_loss=2.925918

Batch 368730, train_perplexity=18.651342, train_loss=2.925918

Batch 368740, train_perplexity=18.651342, train_loss=2.925918

Batch 368750, train_perplexity=18.651342, train_loss=2.925918

Batch 368760, train_perplexity=18.651337, train_loss=2.9259179

Batch 368770, train_perplexity=18.651342, train_loss=2.925918

Batch 368780, train_perplexity=18.651342, train_loss=2.925918

Batch 368790, train_perplexity=18.651337, train_loss=2.9259179

Batch 368800, train_perplexity=18.651342, train_loss=2.925918

Batch 368810, train_perplexity=18.651337, train_loss=2.9259179

Batch 368820, train_perplexity=18.651337, train_loss=2.9259179

Batch 368830, train_perplexity=18.651342, train_loss=2.925918

Batch 368840, train_perplexity=18.651342, train_loss=2.925918

Batch 368850, train_perplexity=18.651337, train_loss=2.9259179

Batch 368860, train_perplexity=18.651337, train_loss=2.9259179

Batch 368870, train_perplexity=18.651342, train_loss=2.925918

Batch 368880, train_perplexity=18.651337, train_loss=2.9259179

Batch 368890, train_perplexity=18.651337, train_loss=2.9259179

Batch 368900, train_perplexity=18.651333, train_loss=2.9259176

Batch 368910, train_perplexity=18.651337, train_loss=2.9259179

Batch 368920, train_perplexity=18.651337, train_loss=2.9259179

Batch 368930, train_perplexity=18.651333, train_loss=2.9259176

Batch 368940, train_perplexity=18.651333, train_loss=2.9259176

Batch 368950, train_perplexity=18.651337, train_loss=2.9259179

Batch 368960, train_perplexity=18.651333, train_loss=2.9259176

Batch 368970, train_perplexity=18.651333, train_loss=2.9259176

Batch 368980, train_perplexity=18.651333, train_loss=2.9259176

Batch 368990, train_perplexity=18.651333, train_loss=2.9259176

Batch 369000, train_perplexity=18.651337, train_loss=2.9259179

Batch 369010, train_perplexity=18.651333, train_loss=2.9259176

Batch 369020, train_perplexity=18.651333, train_loss=2.9259176

Batch 369030, train_perplexity=18.651333, train_loss=2.9259176

Batch 369040, train_perplexity=18.651333, train_loss=2.9259176

Batch 369050, train_perplexity=18.651329, train_loss=2.9259174

Batch 369060, train_perplexity=18.651333, train_loss=2.9259176

Batch 369070, train_perplexity=18.651333, train_loss=2.9259176

Batch 369080, train_perplexity=18.651333, train_loss=2.9259176

Batch 369090, train_perplexity=18.651323, train_loss=2.9259171

Batch 369100, train_perplexity=18.651333, train_loss=2.9259176

Batch 369110, train_perplexity=18.651329, train_loss=2.9259174

Batch 369120, train_perplexity=18.651333, train_loss=2.9259176

Batch 369130, train_perplexity=18.651333, train_loss=2.9259176

Batch 369140, train_perplexity=18.651329, train_loss=2.9259174

Batch 369150, train_perplexity=18.651333, train_loss=2.9259176

Batch 369160, train_perplexity=18.651333, train_loss=2.9259176

Batch 369170, train_perplexity=18.651329, train_loss=2.9259174

Batch 369180, train_perplexity=18.651333, train_loss=2.9259176

Batch 369190, train_perplexity=18.651333, train_loss=2.9259176

Batch 369200, train_perplexity=18.651323, train_loss=2.9259171

Batch 369210, train_perplexity=18.651323, train_loss=2.9259171

Batch 369220, train_perplexity=18.651329, train_loss=2.9259174

Batch 369230, train_perplexity=18.651329, train_loss=2.9259174

Batch 369240, train_perplexity=18.651323, train_loss=2.9259171

Batch 369250, train_perplexity=18.651323, train_loss=2.9259171

Batch 369260, train_perplexity=18.651329, train_loss=2.9259174

Batch 369270, train_perplexity=18.651323, train_loss=2.9259171

Batch 369280, train_perplexity=18.651323, train_loss=2.9259171

Batch 369290, train_perplexity=18.651323, train_loss=2.9259171

Batch 369300, train_perplexity=18.651323, train_loss=2.9259171

Batch 369310, train_perplexity=18.651323, train_loss=2.9259171

Batch 369320, train_perplexity=18.651323, train_loss=2.9259171

Batch 369330, train_perplexity=18.651329, train_loss=2.9259174

Batch 369340, train_perplexity=18.651323, train_loss=2.9259171

Batch 369350, train_perplexity=18.651323, train_loss=2.9259171

Batch 369360, train_perplexity=18.651323, train_loss=2.9259171

Batch 369370, train_perplexity=18.651323, train_loss=2.9259171

Batch 369380, train_perplexity=18.651323, train_loss=2.9259171

Batch 369390, train_perplexity=18.651323, train_loss=2.9259171

Batch 369400, train_perplexity=18.651323, train_loss=2.9259171

Batch 369410, train_perplexity=18.651323, train_loss=2.9259171

Batch 369420, train_perplexity=18.65132, train_loss=2.925917

Batch 369430, train_perplexity=18.651323, train_loss=2.9259171

Batch 369440, train_perplexity=18.651323, train_loss=2.9259171

Batch 369450, train_perplexity=18.65132, train_loss=2.925917

Batch 369460, train_perplexity=18.651323, train_loss=2.9259171

Batch 369470, train_perplexity=18.651323, train_loss=2.9259171

Batch 369480, train_perplexity=18.65132, train_loss=2.925917

Batch 369490, train_perplexity=18.651323, train_loss=2.9259171

Batch 369500, train_perplexity=18.65132, train_loss=2.925917

Batch 369510, train_perplexity=18.651323, train_loss=2.9259171

Batch 369520, train_perplexity=18.651316, train_loss=2.9259167

Batch 369530, train_perplexity=18.651316, train_loss=2.9259167

Batch 369540, train_perplexity=18.65132, train_loss=2.925917

Batch 369550, train_perplexity=18.651316, train_loss=2.9259167

Batch 369560, train_perplexity=18.651316, train_loss=2.9259167

Batch 369570, train_perplexity=18.651316, train_loss=2.9259167

Batch 369580, train_perplexity=18.651316, train_loss=2.9259167

Batch 369590, train_perplexity=18.651316, train_loss=2.9259167

Batch 369600, train_perplexity=18.651316, train_loss=2.9259167

Batch 369610, train_perplexity=18.651316, train_loss=2.9259167

Batch 369620, train_perplexity=18.651316, train_loss=2.9259167

Batch 369630, train_perplexity=18.651316, train_loss=2.9259167

Batch 369640, train_perplexity=18.651316, train_loss=2.9259167

Batch 369650, train_perplexity=18.651316, train_loss=2.9259167

Batch 369660, train_perplexity=18.651316, train_loss=2.9259167

Batch 369670, train_perplexity=18.651316, train_loss=2.9259167

Batch 369680, train_perplexity=18.651316, train_loss=2.9259167

Batch 369690, train_perplexity=18.651316, train_loss=2.9259167

Batch 369700, train_perplexity=18.651316, train_loss=2.9259167

Batch 369710, train_perplexity=18.651316, train_loss=2.9259167

Batch 369720, train_perplexity=18.651316, train_loss=2.9259167

Batch 369730, train_perplexity=18.651316, train_loss=2.9259167

Batch 369740, train_perplexity=18.651316, train_loss=2.9259167

Batch 369750, train_perplexity=18.651316, train_loss=2.9259167

Batch 369760, train_perplexity=18.651316, train_loss=2.9259167

Batch 369770, train_perplexity=18.651316, train_loss=2.9259167

Batch 369780, train_perplexity=18.651306, train_loss=2.9259162

Batch 369790, train_perplexity=18.65131, train_loss=2.9259164

Batch 369800, train_perplexity=18.651316, train_loss=2.9259167

Batch 369810, train_perplexity=18.65131, train_loss=2.9259164

Batch 369820, train_perplexity=18.651316, train_loss=2.9259167

Batch 369830, train_perplexity=18.651306, train_loss=2.9259162

Batch 369840, train_perplexity=18.651306, train_loss=2.9259162

Batch 369850, train_perplexity=18.651306, train_loss=2.9259162

Batch 369860, train_perplexity=18.65131, train_loss=2.9259164

Batch 369870, train_perplexity=18.651306, train_loss=2.9259162

Batch 369880, train_perplexity=18.651306, train_loss=2.9259162

Batch 369890, train_perplexity=18.651306, train_loss=2.9259162

Batch 369900, train_perplexity=18.651306, train_loss=2.9259162

Batch 369910, train_perplexity=18.651306, train_loss=2.9259162

Batch 369920, train_perplexity=18.651306, train_loss=2.9259162

Batch 369930, train_perplexity=18.651306, train_loss=2.9259162

Batch 369940, train_perplexity=18.651306, train_loss=2.9259162

Batch 369950, train_perplexity=18.651306, train_loss=2.9259162
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 369960, train_perplexity=18.651306, train_loss=2.9259162

Batch 369970, train_perplexity=18.651302, train_loss=2.925916

Batch 369980, train_perplexity=18.651306, train_loss=2.9259162

Batch 369990, train_perplexity=18.651306, train_loss=2.9259162

Batch 370000, train_perplexity=18.651306, train_loss=2.9259162

Batch 370010, train_perplexity=18.651306, train_loss=2.9259162

Batch 370020, train_perplexity=18.651306, train_loss=2.9259162

Batch 370030, train_perplexity=18.651306, train_loss=2.9259162

Batch 370040, train_perplexity=18.651306, train_loss=2.9259162

Batch 370050, train_perplexity=18.651306, train_loss=2.9259162

Batch 370060, train_perplexity=18.651297, train_loss=2.9259157

Batch 370070, train_perplexity=18.651306, train_loss=2.9259162

Batch 370080, train_perplexity=18.651306, train_loss=2.9259162

Batch 370090, train_perplexity=18.651306, train_loss=2.9259162

Batch 370100, train_perplexity=18.651302, train_loss=2.925916

Batch 370110, train_perplexity=18.651297, train_loss=2.9259157

Batch 370120, train_perplexity=18.651302, train_loss=2.925916

Batch 370130, train_perplexity=18.651306, train_loss=2.9259162

Batch 370140, train_perplexity=18.651297, train_loss=2.9259157

Batch 370150, train_perplexity=18.651302, train_loss=2.925916

Batch 370160, train_perplexity=18.651302, train_loss=2.925916

Batch 370170, train_perplexity=18.651297, train_loss=2.9259157

Batch 370180, train_perplexity=18.651297, train_loss=2.9259157

Batch 370190, train_perplexity=18.651302, train_loss=2.925916

Batch 370200, train_perplexity=18.651297, train_loss=2.9259157

Batch 370210, train_perplexity=18.651297, train_loss=2.9259157

Batch 370220, train_perplexity=18.651297, train_loss=2.9259157

Batch 370230, train_perplexity=18.651297, train_loss=2.9259157

Batch 370240, train_perplexity=18.651297, train_loss=2.9259157

Batch 370250, train_perplexity=18.651297, train_loss=2.9259157

Batch 370260, train_perplexity=18.651297, train_loss=2.9259157

Batch 370270, train_perplexity=18.651297, train_loss=2.9259157

Batch 370280, train_perplexity=18.651297, train_loss=2.9259157

Batch 370290, train_perplexity=18.651302, train_loss=2.925916

Batch 370300, train_perplexity=18.651297, train_loss=2.9259157

Batch 370310, train_perplexity=18.651297, train_loss=2.9259157

Batch 370320, train_perplexity=18.651297, train_loss=2.9259157

Batch 370330, train_perplexity=18.651297, train_loss=2.9259157

Batch 370340, train_perplexity=18.651297, train_loss=2.9259157

Batch 370350, train_perplexity=18.651293, train_loss=2.9259155

Batch 370360, train_perplexity=18.651297, train_loss=2.9259157

Batch 370370, train_perplexity=18.651297, train_loss=2.9259157

Batch 370380, train_perplexity=18.651297, train_loss=2.9259157

Batch 370390, train_perplexity=18.651297, train_loss=2.9259157

Batch 370400, train_perplexity=18.651297, train_loss=2.9259157

Batch 370410, train_perplexity=18.651289, train_loss=2.9259152

Batch 370420, train_perplexity=18.651297, train_loss=2.9259157

Batch 370430, train_perplexity=18.651293, train_loss=2.9259155

Batch 370440, train_perplexity=18.651289, train_loss=2.9259152

Batch 370450, train_perplexity=18.651293, train_loss=2.9259155

Batch 370460, train_perplexity=18.651297, train_loss=2.9259157

Batch 370470, train_perplexity=18.651293, train_loss=2.9259155

Batch 370480, train_perplexity=18.651289, train_loss=2.9259152

Batch 370490, train_perplexity=18.651289, train_loss=2.9259152

Batch 370500, train_perplexity=18.651293, train_loss=2.9259155

Batch 370510, train_perplexity=18.651297, train_loss=2.9259157

Batch 370520, train_perplexity=18.651289, train_loss=2.9259152

Batch 370530, train_perplexity=18.651293, train_loss=2.9259155

Batch 370540, train_perplexity=18.651289, train_loss=2.9259152

Batch 370550, train_perplexity=18.651289, train_loss=2.9259152

Batch 370560, train_perplexity=18.651289, train_loss=2.9259152

Batch 370570, train_perplexity=18.651289, train_loss=2.9259152

Batch 370580, train_perplexity=18.651289, train_loss=2.9259152

Batch 370590, train_perplexity=18.651289, train_loss=2.9259152

Batch 370600, train_perplexity=18.651289, train_loss=2.9259152

Batch 370610, train_perplexity=18.651289, train_loss=2.9259152

Batch 370620, train_perplexity=18.651289, train_loss=2.9259152

Batch 370630, train_perplexity=18.651289, train_loss=2.9259152

Batch 370640, train_perplexity=18.651289, train_loss=2.9259152

Batch 370650, train_perplexity=18.651289, train_loss=2.9259152

Batch 370660, train_perplexity=18.65128, train_loss=2.9259148

Batch 370670, train_perplexity=18.651289, train_loss=2.9259152

Batch 370680, train_perplexity=18.651285, train_loss=2.925915

Batch 370690, train_perplexity=18.65128, train_loss=2.9259148

Batch 370700, train_perplexity=18.651285, train_loss=2.925915

Batch 370710, train_perplexity=18.651285, train_loss=2.925915

Batch 370720, train_perplexity=18.651289, train_loss=2.9259152

Batch 370730, train_perplexity=18.651289, train_loss=2.9259152

Batch 370740, train_perplexity=18.651285, train_loss=2.925915

Batch 370750, train_perplexity=18.651285, train_loss=2.925915

Batch 370760, train_perplexity=18.65128, train_loss=2.9259148

Batch 370770, train_perplexity=18.651285, train_loss=2.925915

Batch 370780, train_perplexity=18.65128, train_loss=2.9259148

Batch 370790, train_perplexity=18.651285, train_loss=2.925915

Batch 370800, train_perplexity=18.651285, train_loss=2.925915

Batch 370810, train_perplexity=18.651285, train_loss=2.925915

Batch 370820, train_perplexity=18.651285, train_loss=2.925915

Batch 370830, train_perplexity=18.65128, train_loss=2.9259148

Batch 370840, train_perplexity=18.651285, train_loss=2.925915

Batch 370850, train_perplexity=18.651285, train_loss=2.925915

Batch 370860, train_perplexity=18.651289, train_loss=2.9259152

Batch 370870, train_perplexity=18.65128, train_loss=2.9259148

Batch 370880, train_perplexity=18.65128, train_loss=2.9259148

Batch 370890, train_perplexity=18.65128, train_loss=2.9259148

Batch 370900, train_perplexity=18.65128, train_loss=2.9259148

Batch 370910, train_perplexity=18.65128, train_loss=2.9259148

Batch 370920, train_perplexity=18.65128, train_loss=2.9259148

Batch 370930, train_perplexity=18.65128, train_loss=2.9259148

Batch 370940, train_perplexity=18.651276, train_loss=2.9259145

Batch 370950, train_perplexity=18.65128, train_loss=2.9259148

Batch 370960, train_perplexity=18.65128, train_loss=2.9259148

Batch 370970, train_perplexity=18.65128, train_loss=2.9259148

Batch 370980, train_perplexity=18.65128, train_loss=2.9259148

Batch 370990, train_perplexity=18.65128, train_loss=2.9259148

Batch 371000, train_perplexity=18.65128, train_loss=2.9259148

Batch 371010, train_perplexity=18.65128, train_loss=2.9259148

Batch 371020, train_perplexity=18.651276, train_loss=2.9259145

Batch 371030, train_perplexity=18.65128, train_loss=2.9259148

Batch 371040, train_perplexity=18.651272, train_loss=2.9259143

Batch 371050, train_perplexity=18.651276, train_loss=2.9259145

Batch 371060, train_perplexity=18.65128, train_loss=2.9259148

Batch 371070, train_perplexity=18.651272, train_loss=2.9259143

Batch 371080, train_perplexity=18.651276, train_loss=2.9259145

Batch 371090, train_perplexity=18.651272, train_loss=2.9259143

Batch 371100, train_perplexity=18.651276, train_loss=2.9259145

Batch 371110, train_perplexity=18.651272, train_loss=2.9259143

Batch 371120, train_perplexity=18.651272, train_loss=2.9259143

Batch 371130, train_perplexity=18.651272, train_loss=2.9259143

Batch 371140, train_perplexity=18.651272, train_loss=2.9259143

Batch 371150, train_perplexity=18.651272, train_loss=2.9259143

Batch 371160, train_perplexity=18.651272, train_loss=2.9259143

Batch 371170, train_perplexity=18.651272, train_loss=2.9259143

Batch 371180, train_perplexity=18.651272, train_loss=2.9259143

Batch 371190, train_perplexity=18.651272, train_loss=2.9259143

Batch 371200, train_perplexity=18.651272, train_loss=2.9259143

Batch 371210, train_perplexity=18.651272, train_loss=2.9259143

Batch 371220, train_perplexity=18.651272, train_loss=2.9259143

Batch 371230, train_perplexity=18.651272, train_loss=2.9259143

Batch 371240, train_perplexity=18.651272, train_loss=2.9259143
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 371250, train_perplexity=18.651272, train_loss=2.9259143

Batch 371260, train_perplexity=18.651272, train_loss=2.9259143

Batch 371270, train_perplexity=18.651272, train_loss=2.9259143

Batch 371280, train_perplexity=18.651272, train_loss=2.9259143

Batch 371290, train_perplexity=18.651272, train_loss=2.9259143

Batch 371300, train_perplexity=18.651272, train_loss=2.9259143

Batch 371310, train_perplexity=18.651272, train_loss=2.9259143

Batch 371320, train_perplexity=18.651272, train_loss=2.9259143

Batch 371330, train_perplexity=18.651266, train_loss=2.925914

Batch 371340, train_perplexity=18.651272, train_loss=2.9259143

Batch 371350, train_perplexity=18.651266, train_loss=2.925914

Batch 371360, train_perplexity=18.651266, train_loss=2.925914

Batch 371370, train_perplexity=18.651272, train_loss=2.9259143

Batch 371380, train_perplexity=18.651272, train_loss=2.9259143

Batch 371390, train_perplexity=18.651272, train_loss=2.9259143

Batch 371400, train_perplexity=18.651272, train_loss=2.9259143

Batch 371410, train_perplexity=18.651272, train_loss=2.9259143

Batch 371420, train_perplexity=18.651266, train_loss=2.925914

Batch 371430, train_perplexity=18.651262, train_loss=2.9259138

Batch 371440, train_perplexity=18.651262, train_loss=2.9259138

Batch 371450, train_perplexity=18.651262, train_loss=2.9259138

Batch 371460, train_perplexity=18.651262, train_loss=2.9259138

Batch 371470, train_perplexity=18.651262, train_loss=2.9259138

Batch 371480, train_perplexity=18.651262, train_loss=2.9259138

Batch 371490, train_perplexity=18.651262, train_loss=2.9259138

Batch 371500, train_perplexity=18.651262, train_loss=2.9259138

Batch 371510, train_perplexity=18.651262, train_loss=2.9259138

Batch 371520, train_perplexity=18.651262, train_loss=2.9259138

Batch 371530, train_perplexity=18.651262, train_loss=2.9259138

Batch 371540, train_perplexity=18.651262, train_loss=2.9259138

Batch 371550, train_perplexity=18.651262, train_loss=2.9259138

Batch 371560, train_perplexity=18.651262, train_loss=2.9259138

Batch 371570, train_perplexity=18.651262, train_loss=2.9259138

Batch 371580, train_perplexity=18.651262, train_loss=2.9259138

Batch 371590, train_perplexity=18.651262, train_loss=2.9259138

Batch 371600, train_perplexity=18.651262, train_loss=2.9259138

Batch 371610, train_perplexity=18.651262, train_loss=2.9259138

Batch 371620, train_perplexity=18.651262, train_loss=2.9259138

Batch 371630, train_perplexity=18.651262, train_loss=2.9259138

Batch 371640, train_perplexity=18.651262, train_loss=2.9259138

Batch 371650, train_perplexity=18.651262, train_loss=2.9259138

Batch 371660, train_perplexity=18.651262, train_loss=2.9259138

Batch 371670, train_perplexity=18.651258, train_loss=2.9259136

Batch 371680, train_perplexity=18.651258, train_loss=2.9259136

Batch 371690, train_perplexity=18.651258, train_loss=2.9259136

Batch 371700, train_perplexity=18.651253, train_loss=2.9259133

Batch 371710, train_perplexity=18.651258, train_loss=2.9259136

Batch 371720, train_perplexity=18.651253, train_loss=2.9259133

Batch 371730, train_perplexity=18.651253, train_loss=2.9259133

Batch 371740, train_perplexity=18.651253, train_loss=2.9259133

Batch 371750, train_perplexity=18.651253, train_loss=2.9259133

Batch 371760, train_perplexity=18.651253, train_loss=2.9259133

Batch 371770, train_perplexity=18.651253, train_loss=2.9259133

Batch 371780, train_perplexity=18.651253, train_loss=2.9259133

Batch 371790, train_perplexity=18.651253, train_loss=2.9259133

Batch 371800, train_perplexity=18.651253, train_loss=2.9259133

Batch 371810, train_perplexity=18.651253, train_loss=2.9259133

Batch 371820, train_perplexity=18.651253, train_loss=2.9259133

Batch 371830, train_perplexity=18.651253, train_loss=2.9259133

Batch 371840, train_perplexity=18.651253, train_loss=2.9259133

Batch 371850, train_perplexity=18.651253, train_loss=2.9259133

Batch 371860, train_perplexity=18.651253, train_loss=2.9259133

Batch 371870, train_perplexity=18.651253, train_loss=2.9259133

Batch 371880, train_perplexity=18.651253, train_loss=2.9259133

Batch 371890, train_perplexity=18.651253, train_loss=2.9259133

Batch 371900, train_perplexity=18.651253, train_loss=2.9259133

Batch 371910, train_perplexity=18.651249, train_loss=2.925913

Batch 371920, train_perplexity=18.651253, train_loss=2.9259133

Batch 371930, train_perplexity=18.651253, train_loss=2.9259133

Batch 371940, train_perplexity=18.651253, train_loss=2.9259133

Batch 371950, train_perplexity=18.651249, train_loss=2.925913

Batch 371960, train_perplexity=18.651253, train_loss=2.9259133

Batch 371970, train_perplexity=18.651249, train_loss=2.925913

Batch 371980, train_perplexity=18.651249, train_loss=2.925913

Batch 371990, train_perplexity=18.651249, train_loss=2.925913

Batch 372000, train_perplexity=18.651253, train_loss=2.9259133

Batch 372010, train_perplexity=18.651245, train_loss=2.9259129

Batch 372020, train_perplexity=18.651249, train_loss=2.925913

Batch 372030, train_perplexity=18.651249, train_loss=2.925913

Batch 372040, train_perplexity=18.651253, train_loss=2.9259133

Batch 372050, train_perplexity=18.651249, train_loss=2.925913

Batch 372060, train_perplexity=18.651253, train_loss=2.9259133

Batch 372070, train_perplexity=18.651249, train_loss=2.925913

Batch 372080, train_perplexity=18.651249, train_loss=2.925913

Batch 372090, train_perplexity=18.651245, train_loss=2.9259129

Batch 372100, train_perplexity=18.651245, train_loss=2.9259129

Batch 372110, train_perplexity=18.651245, train_loss=2.9259129

Batch 372120, train_perplexity=18.65124, train_loss=2.9259126

Batch 372130, train_perplexity=18.651245, train_loss=2.9259129

Batch 372140, train_perplexity=18.651245, train_loss=2.9259129

Batch 372150, train_perplexity=18.651245, train_loss=2.9259129

Batch 372160, train_perplexity=18.651245, train_loss=2.9259129

Batch 372170, train_perplexity=18.651245, train_loss=2.9259129

Batch 372180, train_perplexity=18.65124, train_loss=2.9259126

Batch 372190, train_perplexity=18.651245, train_loss=2.9259129

Batch 372200, train_perplexity=18.651245, train_loss=2.9259129

Batch 372210, train_perplexity=18.651245, train_loss=2.9259129

Batch 372220, train_perplexity=18.651245, train_loss=2.9259129

Batch 372230, train_perplexity=18.651245, train_loss=2.9259129

Batch 372240, train_perplexity=18.65124, train_loss=2.9259126

Batch 372250, train_perplexity=18.651245, train_loss=2.9259129

Batch 372260, train_perplexity=18.65124, train_loss=2.9259126

Batch 372270, train_perplexity=18.651236, train_loss=2.9259124

Batch 372280, train_perplexity=18.65124, train_loss=2.9259126

Batch 372290, train_perplexity=18.651245, train_loss=2.9259129

Batch 372300, train_perplexity=18.651245, train_loss=2.9259129

Batch 372310, train_perplexity=18.65124, train_loss=2.9259126

Batch 372320, train_perplexity=18.651245, train_loss=2.9259129

Batch 372330, train_perplexity=18.651236, train_loss=2.9259124

Batch 372340, train_perplexity=18.651236, train_loss=2.9259124

Batch 372350, train_perplexity=18.651245, train_loss=2.9259129

Batch 372360, train_perplexity=18.651236, train_loss=2.9259124

Batch 372370, train_perplexity=18.651236, train_loss=2.9259124

Batch 372380, train_perplexity=18.651236, train_loss=2.9259124

Batch 372390, train_perplexity=18.65124, train_loss=2.9259126

Batch 372400, train_perplexity=18.651236, train_loss=2.9259124

Batch 372410, train_perplexity=18.651236, train_loss=2.9259124

Batch 372420, train_perplexity=18.651236, train_loss=2.9259124

Batch 372430, train_perplexity=18.651236, train_loss=2.9259124

Batch 372440, train_perplexity=18.651236, train_loss=2.9259124

Batch 372450, train_perplexity=18.651236, train_loss=2.9259124

Batch 372460, train_perplexity=18.651236, train_loss=2.9259124

Batch 372470, train_perplexity=18.651236, train_loss=2.9259124

Batch 372480, train_perplexity=18.651236, train_loss=2.9259124

Batch 372490, train_perplexity=18.65124, train_loss=2.9259126

Batch 372500, train_perplexity=18.651236, train_loss=2.9259124

Batch 372510, train_perplexity=18.651236, train_loss=2.9259124

Batch 372520, train_perplexity=18.651236, train_loss=2.9259124

Batch 372530, train_perplexity=18.651236, train_loss=2.9259124
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 372540, train_perplexity=18.651236, train_loss=2.9259124

Batch 372550, train_perplexity=18.651236, train_loss=2.9259124

Batch 372560, train_perplexity=18.651226, train_loss=2.925912

Batch 372570, train_perplexity=18.651236, train_loss=2.9259124

Batch 372580, train_perplexity=18.651236, train_loss=2.9259124

Batch 372590, train_perplexity=18.651236, train_loss=2.9259124

Batch 372600, train_perplexity=18.651236, train_loss=2.9259124

Batch 372610, train_perplexity=18.651226, train_loss=2.925912

Batch 372620, train_perplexity=18.651236, train_loss=2.9259124

Batch 372630, train_perplexity=18.651236, train_loss=2.9259124

Batch 372640, train_perplexity=18.651232, train_loss=2.9259121

Batch 372650, train_perplexity=18.651232, train_loss=2.9259121

Batch 372660, train_perplexity=18.651232, train_loss=2.9259121

Batch 372670, train_perplexity=18.651236, train_loss=2.9259124

Batch 372680, train_perplexity=18.651226, train_loss=2.925912

Batch 372690, train_perplexity=18.651226, train_loss=2.925912

Batch 372700, train_perplexity=18.651226, train_loss=2.925912

Batch 372710, train_perplexity=18.651232, train_loss=2.9259121

Batch 372720, train_perplexity=18.651232, train_loss=2.9259121

Batch 372730, train_perplexity=18.651226, train_loss=2.925912

Batch 372740, train_perplexity=18.651226, train_loss=2.925912

Batch 372750, train_perplexity=18.651226, train_loss=2.925912

Batch 372760, train_perplexity=18.651226, train_loss=2.925912

Batch 372770, train_perplexity=18.651226, train_loss=2.925912

Batch 372780, train_perplexity=18.651226, train_loss=2.925912

Batch 372790, train_perplexity=18.651226, train_loss=2.925912

Batch 372800, train_perplexity=18.651226, train_loss=2.925912

Batch 372810, train_perplexity=18.651226, train_loss=2.925912

Batch 372820, train_perplexity=18.651226, train_loss=2.925912

Batch 372830, train_perplexity=18.651226, train_loss=2.925912

Batch 372840, train_perplexity=18.651226, train_loss=2.925912

Batch 372850, train_perplexity=18.651226, train_loss=2.925912

Batch 372860, train_perplexity=18.651218, train_loss=2.9259114

Batch 372870, train_perplexity=18.651226, train_loss=2.925912

Batch 372880, train_perplexity=18.651226, train_loss=2.925912

Batch 372890, train_perplexity=18.651226, train_loss=2.925912

Batch 372900, train_perplexity=18.651218, train_loss=2.9259114

Batch 372910, train_perplexity=18.651226, train_loss=2.925912

Batch 372920, train_perplexity=18.651226, train_loss=2.925912

Batch 372930, train_perplexity=18.651218, train_loss=2.9259114

Batch 372940, train_perplexity=18.651226, train_loss=2.925912

Batch 372950, train_perplexity=18.651218, train_loss=2.9259114

Batch 372960, train_perplexity=18.651222, train_loss=2.9259117

Batch 372970, train_perplexity=18.651226, train_loss=2.925912

Batch 372980, train_perplexity=18.651218, train_loss=2.9259114

Batch 372990, train_perplexity=18.651218, train_loss=2.9259114

Batch 373000, train_perplexity=18.651218, train_loss=2.9259114

Batch 373010, train_perplexity=18.651218, train_loss=2.9259114

Batch 373020, train_perplexity=18.651218, train_loss=2.9259114

Batch 373030, train_perplexity=18.651218, train_loss=2.9259114

Batch 373040, train_perplexity=18.651218, train_loss=2.9259114

Batch 373050, train_perplexity=18.651218, train_loss=2.9259114

Batch 373060, train_perplexity=18.651218, train_loss=2.9259114

Batch 373070, train_perplexity=18.651218, train_loss=2.9259114

Batch 373080, train_perplexity=18.651218, train_loss=2.9259114

Batch 373090, train_perplexity=18.651222, train_loss=2.9259117

Batch 373100, train_perplexity=18.651218, train_loss=2.9259114

Batch 373110, train_perplexity=18.651222, train_loss=2.9259117

Batch 373120, train_perplexity=18.651218, train_loss=2.9259114

Batch 373130, train_perplexity=18.651218, train_loss=2.9259114

Batch 373140, train_perplexity=18.651218, train_loss=2.9259114

Batch 373150, train_perplexity=18.651218, train_loss=2.9259114

Batch 373160, train_perplexity=18.651218, train_loss=2.9259114

Batch 373170, train_perplexity=18.651218, train_loss=2.9259114

Batch 373180, train_perplexity=18.651218, train_loss=2.9259114

Batch 373190, train_perplexity=18.651218, train_loss=2.9259114

Batch 373200, train_perplexity=18.651218, train_loss=2.9259114

Batch 373210, train_perplexity=18.651213, train_loss=2.9259112

Batch 373220, train_perplexity=18.651218, train_loss=2.9259114

Batch 373230, train_perplexity=18.651213, train_loss=2.9259112

Batch 373240, train_perplexity=18.651213, train_loss=2.9259112

Batch 373250, train_perplexity=18.651218, train_loss=2.9259114

Batch 373260, train_perplexity=18.651209, train_loss=2.925911

Batch 373270, train_perplexity=18.651218, train_loss=2.9259114

Batch 373280, train_perplexity=18.651209, train_loss=2.925911

Batch 373290, train_perplexity=18.651209, train_loss=2.925911

Batch 373300, train_perplexity=18.651213, train_loss=2.9259112

Batch 373310, train_perplexity=18.651209, train_loss=2.925911

Batch 373320, train_perplexity=18.651209, train_loss=2.925911

Batch 373330, train_perplexity=18.651209, train_loss=2.925911

Batch 373340, train_perplexity=18.651209, train_loss=2.925911

Batch 373350, train_perplexity=18.651213, train_loss=2.9259112

Batch 373360, train_perplexity=18.651209, train_loss=2.925911

Batch 373370, train_perplexity=18.651213, train_loss=2.9259112

Batch 373380, train_perplexity=18.651209, train_loss=2.925911

Batch 373390, train_perplexity=18.651209, train_loss=2.925911

Batch 373400, train_perplexity=18.651209, train_loss=2.925911

Batch 373410, train_perplexity=18.651209, train_loss=2.925911

Batch 373420, train_perplexity=18.651209, train_loss=2.925911

Batch 373430, train_perplexity=18.651213, train_loss=2.9259112

Batch 373440, train_perplexity=18.651209, train_loss=2.925911

Batch 373450, train_perplexity=18.651209, train_loss=2.925911

Batch 373460, train_perplexity=18.6512, train_loss=2.9259105

Batch 373470, train_perplexity=18.651209, train_loss=2.925911

Batch 373480, train_perplexity=18.651209, train_loss=2.925911

Batch 373490, train_perplexity=18.651209, train_loss=2.925911

Batch 373500, train_perplexity=18.651205, train_loss=2.9259107

Batch 373510, train_perplexity=18.651205, train_loss=2.9259107

Batch 373520, train_perplexity=18.651209, train_loss=2.925911

Batch 373530, train_perplexity=18.651205, train_loss=2.9259107

Batch 373540, train_perplexity=18.6512, train_loss=2.9259105

Batch 373550, train_perplexity=18.651205, train_loss=2.9259107

Batch 373560, train_perplexity=18.651209, train_loss=2.925911

Batch 373570, train_perplexity=18.651209, train_loss=2.925911

Batch 373580, train_perplexity=18.651205, train_loss=2.9259107

Batch 373590, train_perplexity=18.651209, train_loss=2.925911

Batch 373600, train_perplexity=18.6512, train_loss=2.9259105

Batch 373610, train_perplexity=18.651209, train_loss=2.925911

Batch 373620, train_perplexity=18.6512, train_loss=2.9259105

Batch 373630, train_perplexity=18.6512, train_loss=2.9259105

Batch 373640, train_perplexity=18.6512, train_loss=2.9259105

Batch 373650, train_perplexity=18.6512, train_loss=2.9259105

Batch 373660, train_perplexity=18.6512, train_loss=2.9259105

Batch 373670, train_perplexity=18.6512, train_loss=2.9259105

Batch 373680, train_perplexity=18.6512, train_loss=2.9259105

Batch 373690, train_perplexity=18.6512, train_loss=2.9259105

Batch 373700, train_perplexity=18.6512, train_loss=2.9259105

Batch 373710, train_perplexity=18.651205, train_loss=2.9259107

Batch 373720, train_perplexity=18.6512, train_loss=2.9259105

Batch 373730, train_perplexity=18.6512, train_loss=2.9259105

Batch 373740, train_perplexity=18.6512, train_loss=2.9259105

Batch 373750, train_perplexity=18.6512, train_loss=2.9259105

Batch 373760, train_perplexity=18.6512, train_loss=2.9259105

Batch 373770, train_perplexity=18.6512, train_loss=2.9259105

Batch 373780, train_perplexity=18.651196, train_loss=2.9259102

Batch 373790, train_perplexity=18.6512, train_loss=2.9259105

Batch 373800, train_perplexity=18.6512, train_loss=2.9259105

Batch 373810, train_perplexity=18.6512, train_loss=2.9259105

Batch 373820, train_perplexity=18.6512, train_loss=2.9259105

Batch 373830, train_perplexity=18.651196, train_loss=2.9259102
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 373840, train_perplexity=18.6512, train_loss=2.9259105

Batch 373850, train_perplexity=18.6512, train_loss=2.9259105

Batch 373860, train_perplexity=18.651196, train_loss=2.9259102

Batch 373870, train_perplexity=18.651192, train_loss=2.92591

Batch 373880, train_perplexity=18.6512, train_loss=2.9259105

Batch 373890, train_perplexity=18.6512, train_loss=2.9259105

Batch 373900, train_perplexity=18.651196, train_loss=2.9259102

Batch 373910, train_perplexity=18.651196, train_loss=2.9259102

Batch 373920, train_perplexity=18.651196, train_loss=2.9259102

Batch 373930, train_perplexity=18.651196, train_loss=2.9259102

Batch 373940, train_perplexity=18.6512, train_loss=2.9259105

Batch 373950, train_perplexity=18.651196, train_loss=2.9259102

Batch 373960, train_perplexity=18.651192, train_loss=2.92591

Batch 373970, train_perplexity=18.651192, train_loss=2.92591

Batch 373980, train_perplexity=18.651192, train_loss=2.92591

Batch 373990, train_perplexity=18.651192, train_loss=2.92591

Batch 374000, train_perplexity=18.651192, train_loss=2.92591

Batch 374010, train_perplexity=18.651192, train_loss=2.92591

Batch 374020, train_perplexity=18.651192, train_loss=2.92591

Batch 374030, train_perplexity=18.651192, train_loss=2.92591

Batch 374040, train_perplexity=18.651192, train_loss=2.92591

Batch 374050, train_perplexity=18.651192, train_loss=2.92591

Batch 374060, train_perplexity=18.651192, train_loss=2.92591

Batch 374070, train_perplexity=18.651192, train_loss=2.92591

Batch 374080, train_perplexity=18.651192, train_loss=2.92591

Batch 374090, train_perplexity=18.651192, train_loss=2.92591

Batch 374100, train_perplexity=18.651186, train_loss=2.9259098

Batch 374110, train_perplexity=18.651192, train_loss=2.92591

Batch 374120, train_perplexity=18.651192, train_loss=2.92591

Batch 374130, train_perplexity=18.651192, train_loss=2.92591

Batch 374140, train_perplexity=18.651182, train_loss=2.9259095

Batch 374150, train_perplexity=18.651186, train_loss=2.9259098

Batch 374160, train_perplexity=18.651186, train_loss=2.9259098

Batch 374170, train_perplexity=18.651192, train_loss=2.92591

Batch 374180, train_perplexity=18.651192, train_loss=2.92591

Batch 374190, train_perplexity=18.651182, train_loss=2.9259095

Batch 374200, train_perplexity=18.651186, train_loss=2.9259098

Batch 374210, train_perplexity=18.651192, train_loss=2.92591

Batch 374220, train_perplexity=18.651192, train_loss=2.92591

Batch 374230, train_perplexity=18.651182, train_loss=2.9259095

Batch 374240, train_perplexity=18.651186, train_loss=2.9259098

Batch 374250, train_perplexity=18.651192, train_loss=2.92591

Batch 374260, train_perplexity=18.651182, train_loss=2.9259095

Batch 374270, train_perplexity=18.651182, train_loss=2.9259095

Batch 374280, train_perplexity=18.651182, train_loss=2.9259095

Batch 374290, train_perplexity=18.651182, train_loss=2.9259095

Batch 374300, train_perplexity=18.651182, train_loss=2.9259095

Batch 374310, train_perplexity=18.651182, train_loss=2.9259095

Batch 374320, train_perplexity=18.651182, train_loss=2.9259095

Batch 374330, train_perplexity=18.651182, train_loss=2.9259095

Batch 374340, train_perplexity=18.651182, train_loss=2.9259095

Batch 374350, train_perplexity=18.651182, train_loss=2.9259095

Batch 374360, train_perplexity=18.651182, train_loss=2.9259095

Batch 374370, train_perplexity=18.651182, train_loss=2.9259095

Batch 374380, train_perplexity=18.651182, train_loss=2.9259095

Batch 374390, train_perplexity=18.651182, train_loss=2.9259095

Batch 374400, train_perplexity=18.651182, train_loss=2.9259095

Batch 374410, train_perplexity=18.651182, train_loss=2.9259095

Batch 374420, train_perplexity=18.651182, train_loss=2.9259095

Batch 374430, train_perplexity=18.651173, train_loss=2.925909

Batch 374440, train_perplexity=18.651182, train_loss=2.9259095

Batch 374450, train_perplexity=18.651182, train_loss=2.9259095

Batch 374460, train_perplexity=18.651182, train_loss=2.9259095

Batch 374470, train_perplexity=18.651178, train_loss=2.9259093

Batch 374480, train_perplexity=18.651182, train_loss=2.9259095

Batch 374490, train_perplexity=18.651178, train_loss=2.9259093

Batch 374500, train_perplexity=18.651182, train_loss=2.9259095

Batch 374510, train_perplexity=18.651178, train_loss=2.9259093

Batch 374520, train_perplexity=18.651178, train_loss=2.9259093

Batch 374530, train_perplexity=18.651182, train_loss=2.9259095

Batch 374540, train_perplexity=18.651178, train_loss=2.9259093

Batch 374550, train_perplexity=18.651173, train_loss=2.925909

Batch 374560, train_perplexity=18.651182, train_loss=2.9259095

Batch 374570, train_perplexity=18.651173, train_loss=2.925909

Batch 374580, train_perplexity=18.651178, train_loss=2.9259093

Batch 374590, train_perplexity=18.651178, train_loss=2.9259093

Batch 374600, train_perplexity=18.651178, train_loss=2.9259093

Batch 374610, train_perplexity=18.651173, train_loss=2.925909

Batch 374620, train_perplexity=18.651173, train_loss=2.925909

Batch 374630, train_perplexity=18.651178, train_loss=2.9259093

Batch 374640, train_perplexity=18.651173, train_loss=2.925909

Batch 374650, train_perplexity=18.651173, train_loss=2.925909

Batch 374660, train_perplexity=18.651173, train_loss=2.925909

Batch 374670, train_perplexity=18.651173, train_loss=2.925909

Batch 374680, train_perplexity=18.651173, train_loss=2.925909

Batch 374690, train_perplexity=18.651173, train_loss=2.925909

Batch 374700, train_perplexity=18.651173, train_loss=2.925909

Batch 374710, train_perplexity=18.651173, train_loss=2.925909

Batch 374720, train_perplexity=18.651173, train_loss=2.925909

Batch 374730, train_perplexity=18.651173, train_loss=2.925909

Batch 374740, train_perplexity=18.651173, train_loss=2.925909

Batch 374750, train_perplexity=18.651173, train_loss=2.925909

Batch 374760, train_perplexity=18.651173, train_loss=2.925909

Batch 374770, train_perplexity=18.651173, train_loss=2.925909

Batch 374780, train_perplexity=18.651173, train_loss=2.925909

Batch 374790, train_perplexity=18.651173, train_loss=2.925909

Batch 374800, train_perplexity=18.651165, train_loss=2.9259086

Batch 374810, train_perplexity=18.651165, train_loss=2.9259086

Batch 374820, train_perplexity=18.651169, train_loss=2.9259088

Batch 374830, train_perplexity=18.651173, train_loss=2.925909

Batch 374840, train_perplexity=18.651173, train_loss=2.925909

Batch 374850, train_perplexity=18.651169, train_loss=2.9259088

Batch 374860, train_perplexity=18.651169, train_loss=2.9259088

Batch 374870, train_perplexity=18.651165, train_loss=2.9259086

Batch 374880, train_perplexity=18.651165, train_loss=2.9259086

Batch 374890, train_perplexity=18.651165, train_loss=2.9259086

Batch 374900, train_perplexity=18.651165, train_loss=2.9259086

Batch 374910, train_perplexity=18.651165, train_loss=2.9259086

Batch 374920, train_perplexity=18.651165, train_loss=2.9259086

Batch 374930, train_perplexity=18.651165, train_loss=2.9259086

Batch 374940, train_perplexity=18.651165, train_loss=2.9259086

Batch 374950, train_perplexity=18.651165, train_loss=2.9259086

Batch 374960, train_perplexity=18.651165, train_loss=2.9259086

Batch 374970, train_perplexity=18.651165, train_loss=2.9259086

Batch 374980, train_perplexity=18.651169, train_loss=2.9259088

Batch 374990, train_perplexity=18.651165, train_loss=2.9259086

Batch 375000, train_perplexity=18.65116, train_loss=2.9259083

Batch 375010, train_perplexity=18.651165, train_loss=2.9259086

Batch 375020, train_perplexity=18.651165, train_loss=2.9259086

Batch 375030, train_perplexity=18.651165, train_loss=2.9259086

Batch 375040, train_perplexity=18.651165, train_loss=2.9259086

Batch 375050, train_perplexity=18.651165, train_loss=2.9259086

Batch 375060, train_perplexity=18.651165, train_loss=2.9259086

Batch 375070, train_perplexity=18.651165, train_loss=2.9259086

Batch 375080, train_perplexity=18.651165, train_loss=2.9259086

Batch 375090, train_perplexity=18.651165, train_loss=2.9259086

Batch 375100, train_perplexity=18.65116, train_loss=2.9259083

Batch 375110, train_perplexity=18.651165, train_loss=2.9259086

Batch 375120, train_perplexity=18.65116, train_loss=2.9259083

Batch 375130, train_perplexity=18.651165, train_loss=2.9259086
